{
    "source": [
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow.keras as keras\n",
                "import seaborn as sns\n",
                "from keras.utils import to_categorical\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense,BatchNormalization\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv('/kaggle/input/league-of-legends-diamond-ranked-games-10-min/high_diamond_ranked_10min.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.head(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df.iloc[:,1:40]",
                "ASSIGN.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "dataset = df.iloc[:,1:40]\n",
                "dataset.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = dataset.ASSIGN('pearson')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "corr = dataset.corr('pearson')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(data = dataset,x='blueWins')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(data = dataset,x='blueWins')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "dataset.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "dataset.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = dataset.iloc[:,0:19]",
                "ASSIGN.drop(columns = ['blueDeaths'],inplace = True)",
                "ASSIGN.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "blue_df = dataset.iloc[:,0:19]\n",
                "blue_df.drop(columns = ['blueDeaths'],inplace = True)\n",
                "blue_df.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = blue_df.ASSIGN('pearson')",
                "plt.figure(figsize = (10,10))",
                "sns.heatmap(ASSIGN,annot = True)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "corr = blue_df.corr('pearson')\n",
                "plt.figure(figsize = (10,10))\n",
                "sns.heatmap(corr,annot = True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "corr['blueWins'].sort_values(ascending=False)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "corr['blueWins'].sort_values(ascending=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = blue_df.iloc[:,1:]",
                "ASSIGN = blue_df.iloc[:,1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = blue_df.iloc[:,1:]\n",
                "y = blue_df.iloc[:,1]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = StandardScaler()",
                "ASSIGN.fit(X)",
                "ASSIGN = pd.DataFrame(scaler.transform(ASSIGN),columns=ASSIGN.columns)",
                "ASSIGN.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "scaler = StandardScaler()\n",
                "scaler.fit(X)\n",
                "X = pd.DataFrame(scaler.transform(X),columns=X.columns)\n",
                "X.head(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = blue_df['blueWins']",
                "ASSIGN = to_categorical(ASSIGN, 2)",
                "y"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y = blue_df['blueWins']\n",
                "y = to_categorical(y, 2)\n",
                "y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.20,random_state=0)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units=18,activation='relu',input_dim=len(X.columns)))",
                "ASSIGN.add(Dense(36,activation = 'relu'))",
                "ASSIGN.add(Dense(72,activation = 'relu'))",
                "ASSIGN.add(Dense(units=2,activation='softmax'))",
                "ASSIGN.summary()"
            ],
            "output_type": "stream",
            "content_old": [
                "model = Sequential()\n",
                "model.add(Dense(units=18,activation='relu',input_dim=len(X.columns)))\n",
                "model.add(Dense(36,activation = 'relu'))\n",
                "model.add(Dense(72,activation = 'relu'))\n",
                "model.add(Dense(units=2,activation='softmax'))\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = model.fit(X_train,y_train,",
                "ASSIGN=50,",
                "ASSIGN=(X_test,y_test))"
            ],
            "output_type": "stream",
            "content_old": [
                "history = model.fit(X_train,y_train,\n",
                "                   epochs=50,\n",
                "                   validation_data=(X_test,y_test))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(8,8))",
                "plt.plot(history.history['val_accuracy'])",
                "plt.title('Accuracy curves')",
                "plt.xlabel('epochs')",
                "plt.ylabel('Accuracy')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(8,8))\n",
                "plt.plot(history.history['val_accuracy'])\n",
                "#plt.legend(['Training Accuracy','Validation Accuracy'])\n",
                "plt.title('Accuracy curves')\n",
                "plt.xlabel('epochs')\n",
                "plt.ylabel('Accuracy')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import xgboost as xgb\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", parse_dates=['timestamp'])",
                "ASSIGN = pd.read_csv(\"..path\", parse_dates=['timestamp'])",
                "ASSIGN = pd.read_csv(\"..path\", parse_dates=['timestamp'])",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train = pd.read_csv(\"../input/train.csv\", parse_dates=['timestamp'])\n",
                "df_test = pd.read_csv(\"../input/test.csv\", parse_dates=['timestamp'])\n",
                "df_macro = pd.read_csv(\"../input/macro.csv\", parse_dates=['timestamp'])\n",
                "\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df_train['price_doc'].hist(bins=50)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train['price_doc'].hist(bins=50)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df_train['price_doc'].values",
                "ASSIGN = df_test['id']",
                "df_train.drop(['id', 'price_doc'], axis=1, inplace=True)",
                "df_test.drop(['id'], axis=1, inplace=True)",
                "ASSIGN = len(df_train)",
                "ASSIGN = pd.concat([df_train, df_test])",
                "ASSIGN = pd.merge_ordered(ASSIGN, df_macro, on='timestamp', how='left')",
                "print(ASSIGN.shape)",
                "ASSIGN = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)",
                "ASSIGN = month_year.value_counts().to_dict()",
                "ASSIGN['month_year_cnt'] = ASSIGN.map(ASSIGN)",
                "ASSIGN = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)",
                "ASSIGN = week_year.value_counts().to_dict()",
                "ASSIGN['week_year_cnt'] = ASSIGN.map(ASSIGN)",
                "ASSIGN['month'] = ASSIGN.timestamp.dt.month",
                "ASSIGN['dow'] = ASSIGN.timestamp.dt.dayofweek",
                "ASSIGN['rel_floor'] = ASSIGN['floor'] path['max_floor'].astype(float)",
                "ASSIGN['rel_kitch_sq'] = ASSIGN['kitch_sq'] path['full_sq'].astype(float)",
                "ASSIGN.drop(['timestamp'], axis=1, inplace=True)"
            ],
            "output_type": "stream",
            "content_old": [
                "y_train = df_train['price_doc'].values\n",
                "id_test = df_test['id']\n",
                "\n",
                "df_train.drop(['id', 'price_doc'], axis=1, inplace=True)\n",
                "df_test.drop(['id'], axis=1, inplace=True)\n",
                "\n",
                "# Build df_all = (df_train+df_test).join(df_macro)\n",
                "num_train = len(df_train)\n",
                "df_all = pd.concat([df_train, df_test])\n",
                "df_all = pd.merge_ordered(df_all, df_macro, on='timestamp', how='left')\n",
                "print(df_all.shape)\n",
                "\n",
                "# Add month-year\n",
                "month_year = (df_all.timestamp.dt.month + df_all.timestamp.dt.year * 100)\n",
                "month_year_cnt_map = month_year.value_counts().to_dict()\n",
                "df_all['month_year_cnt'] = month_year.map(month_year_cnt_map)\n",
                "\n",
                "# Add week-year count\n",
                "week_year = (df_all.timestamp.dt.weekofyear + df_all.timestamp.dt.year * 100)\n",
                "week_year_cnt_map = week_year.value_counts().to_dict()\n",
                "df_all['week_year_cnt'] = week_year.map(week_year_cnt_map)\n",
                "\n",
                "# Add month and day-of-week\n",
                "df_all['month'] = df_all.timestamp.dt.month\n",
                "df_all['dow'] = df_all.timestamp.dt.dayofweek\n",
                "\n",
                "# Other feature engineering\n",
                "df_all['rel_floor'] = df_all['floor'] / df_all['max_floor'].astype(float)\n",
                "df_all['rel_kitch_sq'] = df_all['kitch_sq'] / df_all['full_sq'].astype(float)\n",
                "\n",
                "# Remove timestamp column (may overfit the model in train)\n",
                "df_all.drop(['timestamp'], axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_all.select_dtypes(exclude=['object'])",
                "ASSIGN = df_all.select_dtypes(include=['object']).copy()",
                "for c in ASSIGN:",
                "ASSIGN[c] = pd.factorize(ASSIGN[c])[0]",
                "ASSIGN = pd.concat([df_numeric, df_obj], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Deal with categorical values\n",
                "df_numeric = df_all.select_dtypes(exclude=['object'])\n",
                "df_obj = df_all.select_dtypes(include=['object']).copy()\n",
                "\n",
                "for c in df_obj:\n",
                "    df_obj[c] = pd.factorize(df_obj[c])[0]\n",
                "\n",
                "df_values = pd.concat([df_numeric, df_obj], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df_values.values",
                "print(ASSIGN.shape)",
                "ASSIGN = X_all[:num_train]",
                "ASSIGN = X_all[num_train:]",
                "ASSIGN = df_values.columns"
            ],
            "output_type": "stream",
            "content_old": [
                "# Convert to numpy values\n",
                "X_all = df_values.values\n",
                "print(X_all.shape)\n",
                "\n",
                "X_train = X_all[:num_train]\n",
                "X_test = X_all[num_train:]\n",
                "\n",
                "df_columns = df_values.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {",
                "'eta': 0.05,",
                "'max_depth': 5,",
                "'subsample': 0.7,",
                "'colsample_bytree': 0.7,",
                "'objective': 'reg:linear',",
                "'eval_metric': 'rmse',",
                "'silent': 1",
                "}",
                "ASSIGN = xgb.DMatrix(X_train, y_train, feature_names=df_columns)",
                "ASSIGN = xgb.DMatrix(X_test, feature_names=df_columns)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "xgb_params = {\n",
                "    'eta': 0.05,\n",
                "    'max_depth': 5,\n",
                "    'subsample': 0.7,\n",
                "    'colsample_bytree': 0.7,\n",
                "    'objective': 'reg:linear',\n",
                "    'eval_metric': 'rmse',\n",
                "    'silent': 1\n",
                "}\n",
                "\n",
                "dtrain = xgb.DMatrix(X_train, y_train, feature_names=df_columns)\n",
                "dtest = xgb.DMatrix(X_test, feature_names=df_columns)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 383"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Uncomment to tune XGB `num_boost_rounds`\n",
                "\n",
                "#cv_result = xgb.cv(xgb_params, dtrain, num_boost_round=1000, early_stopping_rounds=20,\n",
                "#    verbose_eval=True, show_stdv=False)\n",
                "#cv_result[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
                "#num_boost_rounds = len(cv_result)\n",
                "\n",
                "num_boost_round = 383"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1, 1, figsize=(8, 16))",
                "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1, 1, figsize=(8, 16))\n",
                "xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = model.predict(dtest)",
                "ASSIGN = pd.DataFrame({'id': id_test, 'price_doc': y_pred})",
                "ASSIGN.to_csv('sub.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_pred = model.predict(dtest)\n",
                "\n",
                "df_sub = pd.DataFrame({'id': id_test, 'price_doc': y_pred})\n",
                "\n",
                "df_sub.to_csv('sub.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# import libraries",
                "import torch",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 0",
                "ASSIGN = 20",
                "ASSIGN = 0.2",
                "ASSIGN = transforms.ToTensor()",
                "ASSIGN = datasets.MNIST(root='data', train=True,",
                "ASSIGN=True, transform=transform)",
                "ASSIGN = datasets.MNIST(root='data', train=False,",
                "ASSIGN=True, transform=transform)",
                "ASSIGN = len(train_data)",
                "ASSIGN = list(range(num_train))",
                "np.random.shuffle(ASSIGN)",
                "ASSIGN = int(np.floor(valid_size * num_train))",
                "ASSIGN = indices[split:], indices[:split]",
                "ASSIGN = SubsetRandomSampler(train_idx)",
                "ASSIGN = SubsetRandomSampler(valid_idx)",
                "ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size,",
                "ASSIGN=train_sampler, num_workers=num_workers)",
                "ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size,",
                "ASSIGN=valid_sampler, num_workers=num_workers)",
                "ASSIGN = torch.utils.data.DataLoader(test_data, batch_size=batch_size,",
                "ASSIGN=ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from torchvision import datasets",
                "import torchvision.transforms as transforms",
                "from torch.utils.data.sampler import SubsetRandomSampler",
                "",
                "# number of subprocesses to use for data loading",
                "num_workers = 0",
                "# how many samples per batch to load",
                "batch_size = 20",
                "# percentage of training set to use as validation",
                "valid_size = 0.2",
                "",
                "# convert data to torch.FloatTensor",
                "transform = transforms.ToTensor()",
                "",
                "# choose the training and test datasets",
                "train_data = datasets.MNIST(root='data', train=True,",
                "                                   download=True, transform=transform)",
                "test_data = datasets.MNIST(root='data', train=False,",
                "                                  download=True, transform=transform)",
                "",
                "# obtain training indices that will be used for validation",
                "num_train = len(train_data)",
                "indices = list(range(num_train))",
                "np.random.shuffle(indices)",
                "split = int(np.floor(valid_size * num_train))",
                "train_idx, valid_idx = indices[split:], indices[:split]",
                "",
                "# define samplers for obtaining training and validation batches",
                "train_sampler = SubsetRandomSampler(train_idx)",
                "valid_sampler = SubsetRandomSampler(valid_idx)",
                "",
                "# prepare data loaders",
                "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,",
                "    sampler=train_sampler, num_workers=num_workers)",
                "valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ",
                "    sampler=valid_sampler, num_workers=num_workers)",
                "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, ",
                "    num_workers=num_workers)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = iter(train_loader)",
                "ASSIGN = dataiter.next()",
                "ASSIGN = ASSIGN.numpy()",
                "ASSIGN = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "ASSIGN = fig.add_subplot(2, 20path, idx+1, xticks=[], yticks=[])",
                "ASSIGN.imshow(np.squeeze(ASSIGN[idx]), cmap='gray')",
                "ASSIGN.set_title(str(labels[idx].item()))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt",
                "%matplotlib inline",
                "    ",
                "# obtain one batch of training images",
                "dataiter = iter(train_loader)",
                "images, labels = dataiter.next()",
                "images = images.numpy()",
                "",
                "# plot the images in the batch, along with the corresponding labels",
                "fig = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])",
                "    ax.imshow(np.squeeze(images[idx]), cmap='gray')",
                "    # print out the correct label for each image",
                "    # .item() gets the value contained in a Tensor",
                "    ax.set_title(str(labels[idx].item()))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.squeeze(images[1])",
                "ASSIGN = plt.figure(figsize = (12,12))",
                "ASSIGN = fig.add_subplot(111)",
                "ASSIGN.imshow(ASSIGN, cmap='gray')",
                "ASSIGN = img.shape",
                "ASSIGN = img.max()path",
                "for x in range(width):",
                "for y in range(height):",
                "ASSIGN = round(img[x][y],2) if img[x][y] !=0 else 0",
                "ASSIGN.annotate(str(ASSIGN), xy=(y,x),",
                "ASSIGN='center',",
                "ASSIGN='center',",
                "ASSIGN='white' if img[x][y]<thresh else 'black')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "img = np.squeeze(images[1])",
                "",
                "fig = plt.figure(figsize = (12,12)) ",
                "ax = fig.add_subplot(111)",
                "ax.imshow(img, cmap='gray')",
                "width, height = img.shape",
                "thresh = img.max()/2.5",
                "for x in range(width):",
                "    for y in range(height):",
                "        val = round(img[x][y],2) if img[x][y] !=0 else 0",
                "        ax.annotate(str(val), xy=(y,x),",
                "                    horizontalalignment='center',",
                "                    verticalalignment='center',",
                "                    color='white' if img[x][y]<thresh else 'black')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "class Net(nn.Module):",
                "def __init__(self):",
                "super(Net, self).__init__()",
                "ASSIGN = 512",
                "ASSIGN = 512",
                "self.fc1 = nn.Linear(28 * 28, ASSIGN)",
                "self.fc2 = nn.Linear(ASSIGN, ASSIGN)",
                "self.fc3 = nn.Linear(ASSIGN, 10)",
                "self.dropout = nn.Dropout(0.2)",
                "def forward(self, x):",
                "ASSIGN = ASSIGN.view(-1, 28 * 28)",
                "ASSIGN = F.relu(self.fc1(ASSIGN))",
                "ASSIGN = self.dropout(ASSIGN)",
                "ASSIGN = F.relu(self.fc2(ASSIGN))",
                "ASSIGN = self.dropout(ASSIGN)",
                "ASSIGN = self.fc3(ASSIGN)",
                "return x",
                "ASSIGN = Net()",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import torch.nn as nn",
                "import torch.nn.functional as F",
                "",
                "# define the NN architecture",
                "class Net(nn.Module):",
                "    def __init__(self):",
                "        super(Net, self).__init__()",
                "        # number of hidden nodes in each layer (512)",
                "        hidden_1 = 512",
                "        hidden_2 = 512",
                "        # linear layer (784 -> hidden_1)",
                "        self.fc1 = nn.Linear(28 * 28, hidden_1)",
                "        # linear layer (n_hidden -> hidden_2)",
                "        self.fc2 = nn.Linear(hidden_1, hidden_2)",
                "        # linear layer (n_hidden -> 10)",
                "        self.fc3 = nn.Linear(hidden_2, 10)",
                "        # dropout layer (p=0.2)",
                "        # dropout prevents overfitting of data",
                "        self.dropout = nn.Dropout(0.2)",
                "",
                "    def forward(self, x):",
                "        # flatten image input",
                "        x = x.view(-1, 28 * 28)",
                "        # add hidden layer, with relu activation function",
                "        x = F.relu(self.fc1(x))",
                "        # add dropout layer",
                "        x = self.dropout(x)",
                "        # add hidden layer, with relu activation function",
                "        x = F.relu(self.fc2(x))",
                "        # add dropout layer",
                "        x = self.dropout(x)",
                "        # add output layer",
                "        x = self.fc3(x)",
                "        return x",
                "",
                "# initialize the NN",
                "model = Net()",
                "print(model)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = nn.CrossEntropyLoss()",
                "ASSIGN = torch.optim.SGD(model.parameters(), lr=0.01)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# specify loss function (categorical cross-entropy)",
                "criterion = nn.CrossEntropyLoss()",
                "",
                "# specify optimizer (stochastic gradient descent) and learning rate = 0.01",
                "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "validate_data",
                "train_model"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 50",
                "ASSIGN = np.Inf",
                "for epoch in range(ASSIGN):",
                "ASSIGN = 0.0",
                "ASSIGN = 0.0",
                "model.train()",
                "for data, target in train_loader:",
                "optimizer.zero_grad()",
                "ASSIGN = model(data)",
                "ASSIGN = criterion(output, target)",
                "ASSIGN.backward()",
                "optimizer.step()",
                "ASSIGN += ASSIGN.item()*data.size(0)",
                "model.eval()",
                "for data, target in valid_loader:",
                "ASSIGN = model(data)",
                "ASSIGN = criterion(output, target)",
                "ASSIGN += ASSIGN.item()*data.size(0)",
                "ASSIGN = train_losspath(train_loader.sampler)",
                "ASSIGN = valid_losspath(valid_loader.sampler)",
                "print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(",
                "epoch+1,",
                "ASSIGN,",
                "valid_loss",
                "))",
                "if ASSIGN <= ASSIGN:",
                "print('Validation ASSIGN decreased ({:.6f} --> {:.6f}). Saving model ...'.format(",
                "ASSIGN,",
                "ASSIGN))",
                "torch.save(model.state_dict(), 'model.pt')",
                "ASSIGN = valid_loss"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# number of epochs to train the model",
                "n_epochs = 50",
                "",
                "# initialize tracker for minimum validation loss",
                "valid_loss_min = np.Inf # set initial \"min\" to infinity",
                "",
                "for epoch in range(n_epochs):",
                "    # monitor training loss",
                "    train_loss = 0.0",
                "    valid_loss = 0.0",
                "    ",
                "    ###################",
                "    # train the model #",
                "    ###################",
                "    model.train() # prep model for training",
                "    for data, target in train_loader:",
                "        # clear the gradients of all optimized variables",
                "        optimizer.zero_grad()",
                "        # forward pass: compute predicted outputs by passing inputs to the model",
                "        output = model(data)",
                "        # calculate the loss",
                "        loss = criterion(output, target)",
                "        # backward pass: compute gradient of the loss with respect to model parameters",
                "        loss.backward()",
                "        # perform a single optimization step (parameter update)",
                "        optimizer.step()",
                "        # update running training loss",
                "        train_loss += loss.item()*data.size(0)",
                "        ",
                "    ######################    ",
                "    # validate the model #",
                "    ######################",
                "    model.eval() # prep model for evaluation",
                "    for data, target in valid_loader:",
                "        # forward pass: compute predicted outputs by passing inputs to the model",
                "        output = model(data)",
                "        # calculate the loss",
                "        loss = criterion(output, target)",
                "        # update running validation loss ",
                "        valid_loss += loss.item()*data.size(0)",
                "        ",
                "    # print training/validation statistics ",
                "    # calculate average loss over an epoch",
                "    train_loss = train_loss/len(train_loader.sampler)",
                "    valid_loss = valid_loss/len(valid_loader.sampler)",
                "    ",
                "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(",
                "        epoch+1, ",
                "        train_loss,",
                "        valid_loss",
                "        ))",
                "    ",
                "    # save model if validation loss has decreased",
                "    if valid_loss <= valid_loss_min:",
                "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(",
                "        valid_loss_min,",
                "        valid_loss))",
                "        torch.save(model.state_dict(), 'model.pt')",
                "        valid_loss_min = valid_loss"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "model.load_state_dict(torch.load('model.pt'))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.load_state_dict(torch.load('model.pt'))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 0.0",
                "ASSIGN = list(0. for i in range(10))",
                "ASSIGN = list(0. for i in range(10))",
                "model.eval()",
                "for data, target in test_loader:",
                "ASSIGN = model(data)",
                "ASSIGN = criterion(output, target)",
                "ASSIGN += ASSIGN.item()*data.size(0)",
                "ASSIGN = torch.max(output, 1)",
                "ASSIGN = np.squeeze(pred.eq(target.data.view_as(pred)))",
                "for i in range(len(target)):",
                "ASSIGN = target.data[i]",
                "ASSIGN[ASSIGN] += ASSIGN[i].item()",
                "ASSIGN[ASSIGN] += 1",
                "ASSIGN = test_losspath(test_loader.sampler)",
                "print('Test Loss: {:.6f}\\n'.format(ASSIGN))",
                "for i in range(10):",
                "if ASSIGN[i] > 0:",
                "print('Test Accuracy of %5s: %2d%% (%2dpath%2d)' % (",
                "str(i), 100 * ASSIGN[i] path[i],",
                "np.sum(ASSIGN[i]), np.sum(ASSIGN[i])))",
                "else:",
                "print('Test Accuracy of %5s: Npath(no training examples)' % (classes[i]))",
                "print('\\nTest Accuracy (Overall): %2d%% (%2dpath%2d)' % (",
                "100. * np.sum(ASSIGN) path(ASSIGN),",
                "np.sum(ASSIGN), np.sum(ASSIGN)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# initialize lists to monitor test loss and accuracy",
                "test_loss = 0.0",
                "class_correct = list(0. for i in range(10))",
                "class_total = list(0. for i in range(10))",
                "",
                "model.eval() # prep model for evaluation",
                "",
                "for data, target in test_loader:",
                "    # forward pass: compute predicted outputs by passing inputs to the model",
                "    output = model(data)",
                "    # calculate the loss",
                "    loss = criterion(output, target)",
                "    # update test loss ",
                "    test_loss += loss.item()*data.size(0)",
                "    # convert output probabilities to predicted class",
                "    _, pred = torch.max(output, 1)",
                "    # compare predictions to true label",
                "    correct = np.squeeze(pred.eq(target.data.view_as(pred)))",
                "    # calculate test accuracy for each object class",
                "    for i in range(len(target)):",
                "        label = target.data[i]",
                "        class_correct[label] += correct[i].item()",
                "        class_total[label] += 1",
                "",
                "# calculate and print avg test loss",
                "test_loss = test_loss/len(test_loader.sampler)",
                "print('Test Loss: {:.6f}\\n'.format(test_loss))",
                "",
                "for i in range(10):",
                "    if class_total[i] > 0:",
                "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (",
                "            str(i), 100 * class_correct[i] / class_total[i],",
                "            np.sum(class_correct[i]), np.sum(class_total[i])))",
                "    else:",
                "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))",
                "",
                "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (",
                "    100. * np.sum(class_correct) / np.sum(class_total),",
                "    np.sum(class_correct), np.sum(class_total)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = iter(test_loader)",
                "ASSIGN = dataiter.next()",
                "ASSIGN = model(images)",
                "ASSIGN = torch.max(output, 1)",
                "ASSIGN = ASSIGN.numpy()",
                "ASSIGN = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "ASSIGN = fig.add_subplot(2, 20path, idx+1, xticks=[], yticks=[])",
                "ASSIGN.imshow(np.squeeze(ASSIGN[idx]), cmap='gray')",
                "ASSIGN.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),",
                "ASSIGN=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# obtain one batch of test images",
                "dataiter = iter(test_loader)",
                "images, labels = dataiter.next()",
                "",
                "# get sample outputs",
                "output = model(images)",
                "# convert output probabilities to predicted class",
                "_, preds = torch.max(output, 1)",
                "# prep images for display",
                "images = images.numpy()",
                "",
                "# plot the images in the batch, along with predicted and true labels",
                "fig = plt.figure(figsize=(25, 4))",
                "for idx in np.arange(20):",
                "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])",
                "    ax.imshow(np.squeeze(images[idx]), cmap='gray')",
                "    ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())),",
                "                 color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv(\"../input/titanic/titanic.csv\")",
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.drop(['PassengerId','Name','SibSp','Parch','Ticket','Cabin','Embarked'],axis='columns',inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df.drop('Survived',axis='columns')",
                "ASSIGN = df.Survived"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs = df.drop('Survived',axis='columns')",
                "target = df.Survived"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "inputs.Age[:10]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs.Age[:10]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs.Age = inputs.Age.fillna(inputs.Age.mean())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs.Age = inputs.Age.fillna(inputs.Age.mean())",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = LabelEncoder()",
                "ASSIGN = LabelEncoder()",
                "ASSIGN = LabelEncoder()",
                "ASSIGN=LabelEncoder()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import LabelEncoder",
                "le_Pclass = LabelEncoder()",
                "le_Sex = LabelEncoder()",
                "le_Age = LabelEncoder()",
                "le_Fare=LabelEncoder()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "inputs['Pclass_n'] = le_Pclass.fit_transform(inputs['Pclass'])",
                "inputs['Sex_n'] = le_Sex.fit_transform(inputs['Sex'])",
                "inputs['Age_n'] = le_Age.fit_transform(inputs['Age'])",
                "inputs['Fare_n'] = le_Fare.fit_transform(inputs['Fare'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs['Pclass_n'] = le_Pclass.fit_transform(inputs['Pclass'])",
                "inputs['Sex_n'] = le_Sex.fit_transform(inputs['Sex'])",
                "inputs['Age_n'] = le_Age.fit_transform(inputs['Age'])",
                "inputs['Fare_n'] = le_Fare.fit_transform(inputs['Fare'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "inputs"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = inputs.drop(['Age','Sex','Fare','Pclass'],axis='columns')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs_n = inputs.drop(['Age','Sex','Fare','Pclass'],axis='columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "inputs_n"
            ],
            "output_type": "not_existent",
            "content_old": [
                "inputs_n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = tree.DecisionTreeClassifier()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn import tree",
                "model = tree.DecisionTreeClassifier()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model.fit(inputs_n,target)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.fit(inputs_n,target)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.score(inputs_n,target)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.score(inputs_n,target)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.predict([[0,30,2,80]])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.predict([[0,30,2,80]])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "init_notebook_mode(connected=True)",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN.head()",
                "ASSIGN = ASSIGN.rename(columns={'Countrypath':'Country'})",
                "ASSIGN = ASSIGN.rename(columns={'ObservationDate':'Date'})",
                "ASSIGN = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False)",
                "ASSIGN = ASSIGN.drop_duplicates(subset = ['Country'])",
                "ASSIGN = ASSIGN[ASSIGN['Confirmed']>0]",
                "ASSIGN = df[df['Confirmed']>0]",
                "ASSIGN = ASSIGN.groupby(['Date','Country']).sum().reset_index()",
                "df_countrydate",
                "ASSIGN = px.choropleth(df_countrydate,",
                "ASSIGN=\"Country\",",
                "ASSIGN = \"country names\",",
                "ASSIGN=\"Confirmed\",",
                "ASSIGN=\"Country\",",
                "ASSIGN=\"Date\"",
                ")",
                "ASSIGN.update_layout(",
                "ASSIGN = 'Global Spread of Coronavirus',",
                "ASSIGN = 0.5,",
                "ASSIGN=dict(",
                "ASSIGN = False,",
                "ASSIGN = False,",
                "))",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import libraries",
                "import numpy as np ",
                "import pandas as pd ",
                "import plotly as py",
                "import plotly.express as px",
                "import plotly.graph_objs as go",
                "from plotly.subplots import make_subplots",
                "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot",
                "init_notebook_mode(connected=True)",
                "# Read Data",
                "df = pd.read_csv(\"/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv\")",
                "df.head()",
                "",
                "# Rename columns",
                "df = df.rename(columns={'Country/Region':'Country'})",
                "df = df.rename(columns={'ObservationDate':'Date'})",
                "# Manipulate Dataframe",
                "df_countries = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False)",
                "df_countries = df_countries.drop_duplicates(subset = ['Country'])",
                "df_countries = df_countries[df_countries['Confirmed']>0]",
                "df_countrydate = df[df['Confirmed']>0]",
                "df_countrydate = df_countrydate.groupby(['Date','Country']).sum().reset_index()",
                "df_countrydate",
                "# Creating the visualization",
                "fig = px.choropleth(df_countrydate, ",
                "                    locations=\"Country\", ",
                "                    locationmode = \"country names\",",
                "                    color=\"Confirmed\", ",
                "                    hover_name=\"Country\", ",
                "                    animation_frame=\"Date\"",
                "                   )",
                "fig.update_layout(",
                "    title_text = 'Global Spread of Coronavirus',",
                "    title_x = 0.5,",
                "    geo=dict(",
                "        showframe = False,",
                "        showcoastlines = False,",
                "    ))",
                "    ",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(check_output([, ]).decode())"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as pl\n",
                "from matplotlib import cm as cm\n",
                "import plotly.plotly as py\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train=pd.read_csv(\"../input/train.csv\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "pd.options.display.max_rows = 999",
                "pd.options.display.max_columns=999",
                "train.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "pd.options.display.max_rows = 999\n",
                "pd.options.display.max_columns=999\n",
                "train.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.count()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train.columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.columns"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.corr()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pl.figure()",
                "ASSIGN = fig.add_subplot(111)",
                "ASSIGN = cm.get_cmap('jet', 80)",
                "ASSIGN = ax1.imshow(train.corr(), interpolation=\"nearest\", cmap=cmap)",
                "ASSIGN.grid(True)",
                "pl.title('Abalone Feature Correlation')",
                "ASSIGN.colorbar(ASSIGN, ticks=[.75,.8,.85,.90,.95,1])",
                "pl.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig = pl.figure()\n",
                "ax1 = fig.add_subplot(111)\n",
                "cmap = cm.get_cmap('jet', 80)\n",
                "cax = ax1.imshow(train.corr(), interpolation=\"nearest\", cmap=cmap)\n",
                "ax1.grid(True)\n",
                "pl.title('Abalone Feature Correlation')\n",
                "#labels=['Id',\t'MSSubClass',\t'LotFrontage',\t'LotArea',\t'OverallQual',\t'OverallCond',\t'YearBuilt',\t'YearRemodAdd',\t'MasVnrArea',\t'BsmtFinSF1',\t'BsmtFinSF2',\t'BsmtUnfSF',\t'TotalBsmtSF',\t'1stFlrSF',\t'2ndFlrSF',\t'LowQualFinSF',\t'GrLivArea',\t'BsmtFullBath',\t'BsmtHalfBath',\t'FullBath',\t'HalfBath',\t'BedroomAbvGr',\t'KitchenAbvGr',\t'TotRmsAbvGrd',\t'Fireplaces',\t'GarageYrBlt',\t'GarageCars',\t'GarageArea',\t'WoodDeckSF',\t'OpenPorchSF',\t'EnclosedPorch',\t'3SsnPorch',\t'ScreenPorch',\t'PoolArea',\t'MiscVal',\t'MoSold',\t'YrSold',\t'SalePrice',]\n",
                "#ax1.set_xticklabels(labels,fontsize=5)\n",
                "#ax1.set_yticklabels(labels,fontsize=5)\n",
                "# Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
                "fig.colorbar(cax, ticks=[.75,.8,.85,.90,.95,1])\n",
                "pl.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pl.plot(train[\"YearBuilt\"], train[\"GarageYrBlt\"], \"o\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "\n",
                "pl.plot(train[\"YearBuilt\"], train[\"GarageYrBlt\"], \"o\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pl.plot(train[\"TotalBsmtSF\"], train[\"1stFlrSF\"], \"o\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "pl.plot(train[\"TotalBsmtSF\"], train[\"1stFlrSF\"], \"o\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "pl.plot(train[\"GarageCars\"], train[\"GarageArea\"], \"o\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "\n",
                "pl.plot(train[\"GarageCars\"], train[\"GarageArea\"], \"o\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "pl.plot(train[\"SalePrice\"], train[\"MiscVal\"], \"o\")",
                "ASSIGN=train[[\"SalePrice\",\"MiscVal\"]]",
                "ASSIGN.corr()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "pl.plot(train[\"SalePrice\"], train[\"MiscVal\"], \"o\")\n",
                "traincor=train[[\"SalePrice\",\"MiscVal\"]]\n",
                "traincor.corr()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Alley2=train['Alley'].fillna('NoAlley')",
                "train[\"Alley2\"]=Alley2"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Alley2=train['Alley'].fillna('NoAlley')\n",
                "train[\"Alley2\"]=Alley2"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.count()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv('/kaggle/input/integer-sequence-learning/train.csv.zip')\n",
                "test = pd.read_csv('/kaggle/input/integer-sequence-learning/test.csv.zip')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train.Sequence.values[0].split(',')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "val = train.Sequence.values[0].split(',')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "val"
            ],
            "output_type": "execute_result",
            "content_old": [
                "val"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.reshape(np.array(val),(7, 2))",
                "val_shaped",
                "ASSIGN = [int(i[0]) for i in val_shaped]",
                "ASSIGN = [int(i[1]) for i in val_shaped]",
                "ASSIGN = np.reshape(np.array(X),(7,1))",
                "ASSIGN = np.reshape(np.array(y),(7,1))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "val_shaped = np.reshape(np.array(val),(7, 2))\n",
                "val_shaped\n",
                "X = [int(i[0]) for i in val_shaped]\n",
                "y = [int(i[1]) for i in val_shaped]\n",
                "X_shaped = np.reshape(np.array(X),(7,1))\n",
                "y_shaped = np.reshape(np.array(y),(7,1))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_shaped"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_shaped"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_shaped"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_shaped"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = PolynomialFeatures(len(X))",
                "ASSIGN = poly.fit_transform(X_shaped)",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(ASSIGN, y_shaped)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.preprocessing import PolynomialFeatures \n",
                "from sklearn.linear_model import LinearRegression\n",
                "  \n",
                "poly = PolynomialFeatures(len(X))\n",
                "X_poly = poly.fit_transform(X_shaped)\n",
                "  \n",
                "# fit the transformed features to Linear Regression\n",
                "poly_model = LinearRegression()\n",
                "poly_model.fit(X_poly, y_shaped)\n"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = poly_model.predict(X_poly)",
                "predicted"
            ],
            "output_type": "execute_result",
            "content_old": [
                "predicted = poly_model.predict(X_poly)\n",
                "predicted"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_shaped"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_shaped"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "predicted"
            ],
            "output_type": "execute_result",
            "content_old": [
                "predicted"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_shaped[6][0] -predicted[6][0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_shaped[6][0] -predicted[6][0]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "print(os.listdir(\"../input\"))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path', encoding='latin-1')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Rest = pd.read_csv('../input/zomato.csv', encoding='latin-1')\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Rest.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Rest.head()\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Rest.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Rest.shape"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "Rest.to_csv(\"Rest.csv\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Rest.to_csv(\"Rest.csv\")\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "sys.path.append('..path')",
                "binder.bind(globals())",
                "print()",
                "ASSIGN = pd.read_csv('..path', nrows=50000)",
                "ASSIGN = ASSIGN.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +",
                "'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +",
                "'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +",
                "'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +",
                "'fare_amount > 0'",
                ")",
                "ASSIGN = data.fare_amount",
                "ASSIGN = ['pickup_longitude',",
                "'pickup_latitude',",
                "'dropoff_longitude',",
                "'dropoff_latitude']",
                "ASSIGN = data[base_features]",
                "train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1)",
                "ASSIGN = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)",
                "print()",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.linear_model import LinearRegression",
                "from sklearn.model_selection import train_test_split",
                "",
                "# Environment Set-Up for feedback system.",
                "import sys",
                "sys.path.append('../input/ml-insights-tools')",
                "from learntools.core import binder",
                "binder.bind(globals())",
                "from ex3 import *",
                "print(\"Setup Complete\")",
                "",
                "# Data manipulation code below here",
                "data = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)",
                "",
                "# Remove data with extreme outlier coordinates or negative fares",
                "data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +",
                "                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +",
                "                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +",
                "                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +",
                "                  'fare_amount > 0'",
                "                  )",
                "",
                "y = data.fare_amount",
                "",
                "base_features = ['pickup_longitude',",
                "                 'pickup_latitude',",
                "                 'dropoff_longitude',",
                "                 'dropoff_latitude']",
                "",
                "X = data[base_features]",
                "",
                "",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)",
                "first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)",
                "print(\"Data sample:\")",
                "data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'pickup_longitude'",
                "ASSIGN = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "pdp.pdp_plot(ASSIGN, ASSIGN)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from matplotlib import pyplot as plt",
                "from pdpbox import pdp, get_dataset, info_plots",
                "",
                "feat_name = 'pickup_longitude'",
                "pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "",
                "pdp.pdp_plot(pdp_dist, feat_name)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for feat_name in base_features:",
                "ASSIGN = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "pdp.pdp_plot(ASSIGN, feat_name)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for feat_name in base_features:",
                "    pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)",
                "    pdp.pdp_plot(pdp_dist, feat_name)",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ['pickup_longitude', 'dropoff_longitude']",
                "ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Add your code here",
                "feats = ['pickup_longitude', 'dropoff_longitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ['pickup_latitude', 'dropoff_latitude']",
                "ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "feats = ['pickup_latitude', 'dropoff_latitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ['pickup_latitude', 'dropoff_longitude']",
                "ASSIGN = ['pickup_longitude', 'dropoff_longitude']",
                "ASSIGN = pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "pdp.pdp_interact_plot(pdp_interact_out=ASSIGN, feature_names=ASSIGN, plot_type='contour')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "feats = ['pickup_latitude', 'dropoff_longitude']",
                "feats = ['pickup_longitude', 'dropoff_longitude']",
                "inter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)",
                "",
                "pdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "(np.random.rand(10) < .5).astype(float)",
                "np.random.choice([1, -1], 10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "(np.random.rand(10) < .5).astype(float)",
                "np.random.choice([1, -1], 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "binder.bind(globals())",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Set up feedback system",
                "from learntools.core import binder",
                "binder.bind(globals())",
                "from learntools.sql_advanced.ex1 import *",
                "print(\"Setup Complete\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = bigquery.Client()",
                "ASSIGN = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")",
                "ASSIGN = client.get_dataset(dataset_ref)",
                "ASSIGN = dataset_ref.table(\"posts_questions\")",
                "ASSIGN = client.get_table(table_ref)",
                "ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()"
            ],
            "output_type": "stream",
            "content_old": [
                "from google.cloud import bigquery",
                "",
                "# Create a \"Client\" object",
                "client = bigquery.Client()",
                "",
                "# Construct a reference to the \"stackoverflow\" dataset",
                "dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")",
                "",
                "# API request - fetch the dataset",
                "dataset = client.get_dataset(dataset_ref)",
                "",
                "# Construct a reference to the \"posts_questions\" table",
                "table_ref = dataset_ref.table(\"posts_questions\")",
                "",
                "# API request - fetch the table",
                "table = client.get_table(table_ref)",
                "",
                "# Preview the first five lines of the table",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = dataset_ref.table(\"posts_answers\")",
                "ASSIGN = client.get_table(table_ref)",
                "client.list_rows(ASSIGN, max_results=5).to_dataframe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Construct a reference to the \"posts_answers\" table",
                "table_ref = dataset_ref.table(\"posts_answers\")",
                "",
                "# API request - fetch the table",
                "table = client.get_table(table_ref)",
                "",
                "# Preview the first five lines of the table",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = \"\"\"",
                "SELECT q.id AS q_id,",
                "MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer",
                "FROM `bigquery-public-data.stackoverflow.posts_questions` AS q",
                "INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a",
                "ON q.id = a.parent_id",
                "WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'",
                "GROUP BY q_id",
                "ORDER BY time_to_answer",
                "\"\"\"",
                "ASSIGN = client.query(first_query).result().to_dataframe()",
                "print(\"Percentage of answered questions: %s%%\" % \\",
                "(sum(ASSIGN[\"time_to_answer\"].notnull()) path(ASSIGN) * 100))",
                "print(, len(ASSIGN))",
                "ASSIGN.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "first_query = \"\"\"",
                "              SELECT q.id AS q_id,",
                "                  MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND)) as time_to_answer",
                "              FROM `bigquery-public-data.stackoverflow.posts_questions` AS q",
                "                  INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a",
                "              ON q.id = a.parent_id",
                "              WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'",
                "              GROUP BY q_id",
                "              ORDER BY time_to_answer",
                "              \"\"\"",
                "",
                "first_result = client.query(first_query).result().to_dataframe()",
                "print(\"Percentage of answered questions: %s%%\" % \\",
                "      (sum(first_result[\"time_to_answer\"].notnull()) / len(first_result) * 100))",
                "print(\"Number of questions:\", len(first_result))",
                "first_result.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "!pip install pycaret --quiet"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np",
                "import pandas as pd",
                "from sklearn.model_selection import KFold",
                "",
                "#import regression module",
                "from pycaret.regression import *"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(f\"{BASE_PATH}path\")",
                "ASSIGN = pd.read_csv(f\"{BASE_PATH}path\")",
                "ASSIGN = pd.read_csv(f\"{BASE_PATH}path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "BASE_PATH = '../input/trends-assessment-prediction'",
                "",
                "fnc_df = pd.read_csv(f\"{BASE_PATH}/fnc.csv\")",
                "loading_df = pd.read_csv(f\"{BASE_PATH}/loading.csv\")",
                "labels_df = pd.read_csv(f\"{BASE_PATH}/train_scores.csv\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = list(fnc_df.columns[1:]), list(loading_df.columns[1:])",
                "ASSIGN = fnc_df.merge(loading_df, on=\"Id\")",
                "labels_df[\"is_train\"] = True",
                "ASSIGN = ASSIGN.merge(labels_df, on=\"Id\", how=\"left\")",
                "ASSIGN = df[df[\"is_train\"] != True].copy()",
                "ASSIGN = ASSIGN[ASSIGN[\"is_train\"] == True].copy()",
                "print(f'Shape of train data: {ASSIGN.shape}, Shape of test data: {ASSIGN.shape}')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])",
                "df = fnc_df.merge(loading_df, on=\"Id\")",
                "labels_df[\"is_train\"] = True",
                "df = df.merge(labels_df, on=\"Id\", how=\"left\")",
                "",
                "test_df = df[df[\"is_train\"] != True].copy()",
                "df = df[df[\"is_train\"] == True].copy()",
                "print(f'Shape of train data: {df.shape}, Shape of test data: {test_df.shape}')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']",
                "df.drop(['is_train'], axis=1, inplace=True)",
                "ASSIGN = ASSIGN.drop(target_cols + ['is_train'], axis=1)",
                "df[fnc_features] *= FNC_SCALE",
                "ASSIGN[fnc_features] *= FNC_SCALE"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target_cols = ['age', 'domain1_var1', 'domain1_var2', 'domain2_var1', 'domain2_var2']",
                "df.drop(['is_train'], axis=1, inplace=True)",
                "test_df = test_df.drop(target_cols + ['is_train'], axis=1)",
                "",
                "",
                "# Giving less importance to FNC features since they are easier to overfit due to high dimensionality.",
                "FNC_SCALE = 1/500",
                "df[fnc_features] *= FNC_SCALE",
                "test_df[fnc_features] *= FNC_SCALE"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def get_train_data(target):",
                "ASSIGN = [tar for tar in target_cols if tar != target]",
                "ASSIGN = df.drop( other_targets, axis=1)",
                "return train_df"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def get_train_data(target):",
                "    other_targets = [tar for tar in target_cols if tar != target]",
                "    train_df = df.drop( other_targets, axis=1)",
                "    return train_df"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 'age'",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = 'age'",
                "",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['tr']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "blacklist_models = ['tr']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE', ## competition metric",
                "    turbo = True",
                ")",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = create_model(",
                "ASSIGN='br',",
                "ASSIGN=10",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "br_age = create_model(",
                "    estimator='br',",
                "    fold=10",
                ")"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = tune_model(",
                "ASSIGN='br',",
                "ASSIGN=10,",
                "ASSIGN = 'mae',",
                "ASSIGN=50",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# here we are tuning the above created model",
                "tuned_br_age = tune_model(",
                "    estimator='br',",
                "    fold=10,",
                "    optimize = 'mae',",
                "    n_iter=50",
                ")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_model(estimator = tuned_br_age, plot = 'learning')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# plot_model(estimator = None, plot = residuals)",
                "plot_model(estimator = tuned_br_age, plot = 'learning')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_model(estimator = tuned_br_age, plot = 'residuals')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plot_model(estimator = tuned_br_age, plot = 'residuals')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_model(estimator = tuned_br_age, plot = 'feature')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plot_model(estimator = tuned_br_age, plot = 'feature')"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "evaluate_model(estimator=tuned_br_age)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "evaluate_model(estimator=tuned_br_age)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = predict_model(tuned_br_age, data=test_df)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "predictions =  predict_model(tuned_br_age, data=test_df)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "predictions.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "predictions.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = target_cols[0]",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[0]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = target_cols[1]",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[1]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = target_cols[2]",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[2]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = target_cols[3]",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[3]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = target_cols[4]",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "compare_models(",
                "ASSIGN = blacklist_models,",
                "ASSIGN = 10,",
                "ASSIGN = 'MAE',",
                "ASSIGN = True",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[4]",
                "train_df = get_train_data(target)",
                "",
                "setup_reg = setup(",
                "    data = train_df,",
                "    target = target,",
                "    train_size=0.8,",
                "    numeric_imputation = 'mean',",
                "    silent = True",
                ")",
                "",
                "compare_models(",
                "    blacklist = blacklist_models,",
                "    fold = 10,",
                "    sort = 'MAE',",
                "    turbo = True",
                ")"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = {",
                "'age': 'br',",
                "'domain1_var1':'catboost',",
                "'domain1_var2':'svm',",
                "'domain2_var1':'catboost',",
                "'domain2_var2':'catboost',",
                "}",
                "def tune_and_ensemble(target):",
                "ASSIGN = get_train_data(target)",
                "ASSIGN = setup(",
                "ASSIGN = train_df,",
                "ASSIGN = ASSIGN,",
                "ASSIGN=0.8,",
                "ASSIGN = 'mean',",
                "ASSIGN = True",
                ")",
                "ASSIGN = target_models_dict[target]",
                "ASSIGN = tune_model(model_name, fold=10)",
                "ASSIGN = ensemble_model(tuned_model, fold=10)",
                "return model"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# mapping targets to their corresponding models",
                "",
                "models = []",
                "",
                "target_models_dict = {",
                "    'age': 'br',",
                "    'domain1_var1':'catboost',",
                "    'domain1_var2':'svm',",
                "    'domain2_var1':'catboost',",
                "    'domain2_var2':'catboost',",
                "}",
                "",
                "def tune_and_ensemble(target):",
                "    train_df = get_train_data(target)    ",
                "    exp_reg = setup(",
                "        data = train_df,",
                "        target = target,",
                "        train_size=0.8,",
                "        numeric_imputation = 'mean',",
                "        silent = True",
                "    )",
                "    ",
                "    model_name = target_models_dict[target]",
                "    tuned_model = tune_model(model_name, fold=10)",
                "    model = ensemble_model(tuned_model, fold=10)",
                "    return model",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = target_cols[0]",
                "ASSIGN = tune_and_ensemble(target)",
                "models.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[0]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = target_cols[1]",
                "ASSIGN = tune_and_ensemble(target)",
                "models.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[1]",
                "model = tune_and_ensemble(target)",
                "models.append(model)",
                "%tb"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = target_cols[2]",
                "ASSIGN = tune_and_ensemble(target)",
                "models.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[2]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = target_cols[3]",
                "ASSIGN = tune_and_ensemble(target)",
                "models.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[3]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = target_cols[4]",
                "ASSIGN = tune_and_ensemble(target)",
                "models.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target = target_cols[4]",
                "model = tune_and_ensemble(target)",
                "models.append(model)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "def finalize_model_pipeline(model, target):",
                "finalize_model(model)",
                "save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)",
                "ASSIGN = predict_model(model, data=test_df)",
                "test_df[target] = ASSIGN['Label'].values"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### create a pipeline or function to run for all targets",
                "",
                "def finalize_model_pipeline(model, target):",
                "    # this will train the model on holdout data",
                "    finalize_model(model)",
                "    save_model(model, f'{target}_{target_models_dict[target]}', verbose=True)",
                "    # making predictions on test data",
                "    predictions = predict_model(model, data=test_df)",
                "    test_df[target] = predictions['Label'].values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for index, target in enumerate(target_cols):",
                "ASSIGN = models[index]",
                "finalize_model_pipeline(ASSIGN,target)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for index, target in enumerate(target_cols):",
                "    model = models[index]",
                "    finalize_model_pipeline(model,target)"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")",
                "ASSIGN[\"Id\"] = ASSIGN[\"Id\"].astype(\"str\") + \"_\" + ASSIGN[\"variable\"].astype(\"str\")",
                "ASSIGN = ASSIGN.drop(\"variable\", axis=1).sort_values(\"Id\")",
                "assert ASSIGN.shape[0] == test_df.shape[0]*5",
                "ASSIGN.to_csv(\"submission1.csv\", index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sub_df = pd.melt(test_df[[\"Id\", \"age\", \"domain1_var1\", \"domain1_var2\", \"domain2_var1\", \"domain2_var2\"]], id_vars=[\"Id\"], value_name=\"Predicted\")",
                "sub_df[\"Id\"] = sub_df[\"Id\"].astype(\"str\") + \"_\" +  sub_df[\"variable\"].astype(\"str\")",
                "",
                "sub_df = sub_df.drop(\"variable\", axis=1).sort_values(\"Id\")",
                "assert sub_df.shape[0] == test_df.shape[0]*5",
                "",
                "sub_df.to_csv(\"submission1.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sub_df.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"..path\", index_col=0)",
                "pd.set_option(\"display.max_rows\", 5)",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd\n",
                "\n",
                "reviews = pd.read_csv(\"../input/wine-reviews/winemag-data-130k-v2.csv\", index_col=0)\n",
                "pd.set_option(\"display.max_rows\", 5)\n",
                "\n",
                "from learntools.core import binder; binder.bind(globals())\n",
                "from learntools.pandas.indexing_selecting_and_assigning import *\n",
                "print(\"Setup complete.\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "reviews.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "reviews.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "warnings.filterwarnings(\"ignore\")",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "import matplotlib.pyplot as plt\n",
                "from collections import Counter\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "data = pd.read_csv(\"/kaggle/input/fish-market/Fish.csv\")\n",
                "data.info()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = data.isnull().sum().sum()",
                "data.columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "x = data.isnull().sum().sum()\n",
                "data.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = data.Species",
                "Counter(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "species_list = data.Species\n",
                "Counter(species_list)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = \"Perch\" , \"Bream\" , \"Roach\" , \"Pike\" , \"Smelt\" , \"Parkki\" , \"Whitefish\"",
                "ASSIGN = [56,35,20,17,14,11,6]",
                "ASSIGN = (0,0,0,0,0,0,0)",
                "fig1,ax1 = plt.subplots()",
                "ax1.pie(ASSIGN, ASSIGN=ASSIGN, ASSIGN=ASSIGN, autopct='%1.1f%%',shadow=True, startangle=90)",
                "ax1.axis('equal')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "labels = \"Perch\" , \"Bream\" , \"Roach\" , \"Pike\" , \"Smelt\" , \"Parkki\" , \"Whitefish\"\n",
                "sizes = [56,35,20,17,14,11,6]\n",
                "explode = (0,0,0,0,0,0,0)\n",
                "fig1,ax1 = plt.subplots()\n",
                "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',shadow=True, startangle=90)\n",
                "ax1.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Weight[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Weight[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Weight[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Weight[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Weight[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Weight[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Weight[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"weight of fish in Gram g\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Weight[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Weight[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Weight[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Weight[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Weight[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Weight[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Weight[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"weight of fish in Gram g\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length1[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length1[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length1[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length1[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length1[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length1[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length1[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"vertical length in cm\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length1[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length1[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length1[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length1[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length1[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length1[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length1[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"vertical length in cm\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length2[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length2[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length2[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length2[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length2[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length2[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length2[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"diagonal length in cm\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length2[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length2[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length2[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length2[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length2[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length2[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length2[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"diagonal length in cm\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length3[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length3[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length3[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length3[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length3[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length3[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length3[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"cross length in cm\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Length3[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Length3[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Length3[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Length3[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Length3[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Length3[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Length3[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"cross length in cm\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Height[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Height[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Height[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Height[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Height[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Height[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Height[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"height in cm\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Height[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Height[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Height[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Height[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Height[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Height[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Height[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"height in cm\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Width[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Width[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Width[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Width[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Width[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Width[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Width[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)",
                "plt.xlabel(\"index of Spices\")",
                "plt.ylabel(\"diagonal width in cm\")",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(data.index[data.Species == \"Bream\"] , data.Width[data.Species == \"Bream\"],c=\"red\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Perch\"] , data.Width[data.Species == \"Perch\"],c=\"aqua\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Roach\"] , data.Width[data.Species == \"Roach\"],c=\"orange\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Pike\"] , data.Width[data.Species == \"Pike\"],c=\"purple\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Smelt\"] , data.Width[data.Species == \"Smelt\"],c=\"black\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Parkki\"] , data.Width[data.Species == \"Parkki\"],c=\"green\" , alpha = 0.5)\n",
                "plt.scatter(data.index[data.Species == \"Whitefish\"] , data.Width[data.Species == \"Whitefish\"],c=\"brown\" , alpha = 0.5)\n",
                "plt.xlabel(\"index of Spices\")\n",
                "plt.ylabel(\"diagonal width in cm\")\n",
                "plt.grid()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = data.Species",
                "ASSIGN = data.drop([\"Species\"],axis = 1)",
                "ASSIGN = train_test_split(x,y,test_size = 0.2,random_state = 42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y = data.Species\n",
                "x = data.drop([\"Species\"],axis = 1)\n",
                "from sklearn.model_selection import train_test_split\n",
                "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 0.0",
                "ASSIGN = 1",
                "ASSIGN = []",
                "for each in range(1,100):",
                "ASSIGN = KNeighborsClassifier(n_neighbors = each)",
                "ASSIGN.fit(x_train,y_train)",
                "ASSIGN.append(ASSIGN.score(x_test,y_test))",
                "if (ASSIGN < ASSIGN.score(x_test,y_test) ):",
                "ASSIGN = knn.score(x_test,y_test)",
                "ASSIGN = ASSIGN+1",
                "plt.plot(ASSIGN,color = \"purple\" , alpha = 1 )",
                "plt.grid()"
            ],
            "output_type": "display_data",
            "content_old": [
                "knn_score = 0.0\n",
                "highest_indx = 1\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "score_list = []\n",
                "for each in range(1,100):\n",
                "    knn = KNeighborsClassifier(n_neighbors = each)\n",
                "    knn.fit(x_train,y_train)\n",
                "    score_list.append(knn.score(x_test,y_test))\n",
                "    if (knn_score < knn.score(x_test,y_test) ):\n",
                "        knn_score = knn.score(x_test,y_test)\n",
                "        highest_indx = highest_indx+1\n",
                "        \n",
                "plt.plot(score_list,color = \"purple\" , alpha = 1 )    \n",
                "plt.grid()         "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,knn_score)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"KNN Max Accuracy : \",knn_score)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = LogisticRegression()",
                "ASSIGN.fit(x_train,y_train)",
                "ASSIGN = lr.score(x_test,y_test)",
                "print(,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.linear_model import LogisticRegression\n",
                "lr = LogisticRegression()\n",
                "lr.fit(x_train,y_train)\n",
                "lr_score = lr.score(x_test,y_test)\n",
                "print(\"Logistic Regression Accuracy : \",lr_score)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = GaussianNB()",
                "ASSIGN.fit(x_train,y_train)",
                "ASSIGN = naive_bayes.score(x_test,y_test)",
                "print(,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "naive_bayes = GaussianNB()\n",
                "naive_bayes.fit(x_train,y_train)\n",
                "nb_score = naive_bayes.score(x_test,y_test)\n",
                "print(\"Naive Bayes Accuracy : \",nb_score)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = RandomForestClassifier(n_estimators = 100)",
                "ASSIGN.fit(x_train,y_train)",
                "ASSIGN = rfc.score(x_test,y_test)",
                "print(,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "rfc = RandomForestClassifier(n_estimators = 100)\n",
                "rfc.fit(x_train,y_train)\n",
                "rf_score = rfc.score(x_test,y_test)\n",
                "print(\"Random Forest Accuracy : \",rf_score)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = {\"Logistic Regression\" : lr_score,\"Random Forest\" : rf_score,\"K-Nearest Neighbour\" : knn_score ,\"Naive Bayes\": nb_score ,\"K-Nearest Neighbour\" : knn_score }",
                "dict1"
            ],
            "output_type": "execute_result",
            "content_old": [
                "dict1 = {\"Logistic Regression\" : lr_score,\"Random Forest\" : rf_score,\"K-Nearest Neighbour\" : knn_score ,\"Naive Bayes\": nb_score ,\"K-Nearest Neighbour\" : knn_score }\n",
                "dict1"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = '",
                "ASSIGN = '",
                "ASSIGN = '",
                "ASSIGN = '",
                "register_matplotlib_converters()",
                "warnings.filterwarnings('ignore')",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "#essential libraries\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import random\n",
                "from urllib.request import urlopen\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sb\n",
                "import plotly.express as ex\n",
                "import plotly.graph_objs as gp\n",
                "from plotly.subplots import make_subplots\n",
                "import plotly.figure_factory as ff\n",
                "import folium\n",
                "\n",
                "#Colour codes\n",
                "conf = '#393e46' \n",
                "deth = '#ff2e63'  \n",
                "cure = '#21bf73'\n",
                "acti = '#fe9801'\n",
                "\n",
                "#Extra Libraries\n",
                "from pandas.plotting import register_matplotlib_converters\n",
                "register_matplotlib_converters()   \n",
                "\n",
                "#To remove the warnings\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path',parse_dates=['Date'], dayfirst=True)",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_excel('path',sheet_name='India', parse_dates=['Date'])",
                "ASSIGN = pd.read_excel('path',sheet_name='Italy', parse_dates=['Date'])",
                "ASSIGN = pd.read_excel('path',sheet_name='Korea', parse_dates=['Date'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_india = pd.read_csv('/kaggle/input/covid19-in-india/covid_19_india.csv',parse_dates=['Date'], dayfirst=True)\n",
                "df_coordinates = pd.read_csv('/kaggle/input/coronavirus-cases-in-india/Indian Coordinates.csv')\n",
                "df_India_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='India', parse_dates=['Date'])\n",
                "df_Italy_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Italy', parse_dates=['Date'])\n",
                "df_Korea_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Korea', parse_dates=['Date'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_coordinates.dropna(axis = 1, inplace = True)",
                "df_coordinates.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_coordinates.dropna(axis = 1, inplace = True)\n",
                "df_coordinates.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_coordinates.rename(columns = {'Name of State path':'Statepath'}, inplace = True)",
                "df_coordinates.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_coordinates.rename(columns = {'Name of State / UT':'State/UnionTerritory'}, inplace = True)\n",
                "df_coordinates.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_india.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india.dropna(axis = 0, inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_india.dropna(axis = 0, inplace = True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india[\"Statepath\"].replace({'Chattisgarh': 'Chhattisgarh ',",
                "'Chhattisgarh' :'Chhattisgarh ',",
                "'Puducherry' : 'Pondicherry',",
                "'Himachal Pradesh' : 'Himachal Pradesh ',",
                "'Madhya Pradesh' : 'Madhya Pradesh ',",
                "'Bihar':'Bihar ',",
                "'Himachal Pradesh':'Himachal Pradesh ',",
                "'Manipur':'Manipur ',",
                "'West Bengal':'West Bengal ',",
                "'Goa' : 'Goa '}, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_india[\"State/UnionTerritory\"].replace({'Chattisgarh': 'Chhattisgarh ',\n",
                "                                          'Chhattisgarh' :'Chhattisgarh ',\n",
                "                                          'Puducherry' : 'Pondicherry',\n",
                "                                          'Himachal Pradesh' : 'Himachal Pradesh ',\n",
                "                                          'Madhya Pradesh' : 'Madhya Pradesh ',\n",
                "                                          'Bihar':'Bihar ',\n",
                "                                          'Himachal Pradesh':'Himachal Pradesh ',\n",
                "                                          'Manipur':'Manipur ',\n",
                "                                          'West Bengal':'West Bengal ',\n",
                "                                          'Goa' : 'Goa '}, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.merge(ASSIGN, df_coordinates, how='left', on='Statepath')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_india = pd.merge(df_india, df_coordinates, how='left', on='State/UnionTerritory')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.isnull().sum()\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "df_india[['Latitude','Longitude']] = df_india[['Latitude','Longitude']].fillna(0)",
                "df_india.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india[['Latitude','Longitude']] = df_india[['Latitude','Longitude']].fillna(0)\n",
                "df_india.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.drop('Sno', axis = 1)",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india = df_india.drop('Sno', axis = 1) \n",
                "df_india.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_india.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.shape"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "df_india.to_csv('Processed_data.csv')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_india.to_csv('Processed_data.csv')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN[['Date', 'Statepath', 'Latitude', 'Longitude','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths']]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Rearranging the columns\n",
                "df_india = df_india[['Date', 'State/UnionTerritory', 'Latitude', 'Longitude','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths']]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']",
                "df_india['Active'] = (df_india['ConfirmedIndianNational'] + df_india['ConfirmedForeignNational']) - df_india['Deaths'] - df_india['Cured']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Total_cases = ['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']\n",
                "\n",
                "#Active = Confirmed - Deaths - Cured\n",
                "df_india['Active'] = (df_india['ConfirmedIndianNational'] + df_india['ConfirmedForeignNational']) - df_india['Deaths'] - df_india['Cured']"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_india.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_india[Total_cases] = df_india[Total_cases].fillna(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Fill Null Values with Zeros\n",
                "df_india[Total_cases] = df_india[Total_cases].fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df_india[df_india['Statepath'].str.contains('Maharashtra')]",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# cases in Maharashtra\n",
                "maha = df_india[df_india['State/UnionTerritory'].str.contains('Maharashtra')]\n",
                "maha.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df_india[df_india['Statepath'].str.contains('Kerala')]",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# cases in Kerala\n",
                "kerala = df_india[df_india['State/UnionTerritory'].str.contains('Kerala')]\n",
                "kerala.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_india.groupby('Date')['ConfirmedIndianNational','ConfirmedForeignNational','Deaths', 'Cured', 'Active'].sum().reset_index()",
                "ASSIGN.style.background_gradient(cmap='Reds')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india_latest = df_india.groupby('Date')['ConfirmedIndianNational','ConfirmedForeignNational','Deaths', 'Cured', 'Active'].sum().reset_index()\n",
                "df_india_latest.style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Indian_National = df_india['ConfirmedIndianNational'].sum()",
                "ASSIGN = df_india['ConfirmedForeignNational'].sum()",
                "ASSIGN ={\"Indian\": Indian_National,\"Foriengners\":Foreigners}",
                "ASSIGN=['orange','blue']",
                "plt.figure(figsize = (10,10))",
                "plt.pie(ASSIGN.values(),labels=ASSIGN.keys(),ASSIGN=ASSIGN,shadow=True,explode=(0.1, 0.1), autopct='%1.2f%%')",
                "plt.axis('equal')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Indian_National = df_india['ConfirmedIndianNational'].sum()\n",
                "Foreigners = df_india['ConfirmedForeignNational'].sum()\n",
                "dct ={\"Indian\": Indian_National,\"Foriengners\":Foreigners}\n",
                "colors=['orange','blue']\n",
                "plt.figure(figsize = (10,10))\n",
                "plt.pie(dct.values(),labels=dct.keys(),colors=colors,shadow=True,explode=(0.1, 0.1), autopct='%1.2f%%')\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_india_latest[df_india_latest['Date']==max(df_india_latest['Date'])].reset_index(drop=True)",
                "ASSIGN.style.background_gradient(cmap='copper')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "slate = df_india_latest[df_india_latest['Date']==max(df_india_latest['Date'])].reset_index(drop=True)\n",
                "slate.style.background_gradient(cmap='copper')"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = slate.melt(id_vars=\"Date\", value_vars=['Active','Cured','Deaths'])",
                "plot"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot = slate.melt(id_vars=\"Date\", value_vars=['Active','Cured','Deaths'])\n",
                "plot"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ex.treemap(plot, path=['variable'], values=\"value\", height=500, width=800,",
                "ASSIGN=[acti,cure,deth])",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "matt = ex.treemap(plot, path=['variable'], values=\"value\", height=500, width=800,\n",
                "                color_discrete_sequence=[acti,cure,deth])\n",
                "matt.show() "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_india[df_india['Date']==max(df_india['Date'])].reset_index()",
                "ASSIGN = india_latest.groupby('Statepath')['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'].sum().reset_index()",
                "ASSIGN.style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "india_latest = df_india[df_india['Date']==max(df_india['Date'])].reset_index()\n",
                "india_latest_groupby = india_latest.groupby('State/UnionTerritory')['ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active'].sum().reset_index()\n",
                "india_latest_groupby.style.background_gradient(cmap='OrRd')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = india_latest_groupby.sort_values(by='ConfirmedIndianNational', ascending=False)",
                "ASSIGN = ASSIGN.reset_index(drop=True)",
                "ASSIGN.style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#confirmed cases\n",
                "state_confirmed = india_latest_groupby.sort_values(by='ConfirmedIndianNational', ascending=False)\n",
                "state_confirmed = state_confirmed.reset_index(drop=True)\n",
                "state_confirmed.style.background_gradient(cmap='OrRd')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = state_confirmed[state_confirmed['Deaths']>0][['Statepath','Deaths']]",
                "ASSIGN.sort_values('Deaths',ascending=False).reset_index(drop=True).style.background_gradient(cmap='OrRd')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "states_with_death = state_confirmed[state_confirmed['Deaths']>0][['State/UnionTerritory','Deaths']]\n",
                "states_with_death.sort_values('Deaths',ascending=False).reset_index(drop=True).style.background_gradient(cmap='OrRd')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] ==",
                "state_confirmed['Deaths']+ state_confirmed['Cured']]",
                "ASSIGN = ASSIGN[['Statepath','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]",
                "ASSIGN = ASSIGN.sort_values('ConfirmedIndianNational', ascending=False)",
                "ASSIGN['Cured'].count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "no_recovery = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == \n",
                "                              state_confirmed['Deaths']+ state_confirmed['Cured']]\n",
                "no_recovery = no_recovery[['State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]\n",
                "no_recovery = no_recovery.sort_values('ConfirmedIndianNational', ascending=False)\n",
                "no_recovery['Cured'].count()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)",
                "for i in range(0, len(df_india)):",
                "folium.Circle(",
                "ASSIGN=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],",
                "ASSIGN='crimson',",
                "ASSIGN = '<li><bold>Statepath: '+str(df_india.iloc[i]['Statepath'])+",
                "'<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+",
                "'<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+",
                "'<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+",
                "'<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),",
                "ASSIGN=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)",
                "India"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#India\n",
                "India = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)\n",
                "for i in range(0, len(df_india)):\n",
                "    folium.Circle(\n",
                "        location=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],\n",
                "                  color='crimson',\n",
                "                  tooltip = '<li><bold>State/UnionTerritory : '+str(df_india.iloc[i]['State/UnionTerritory'])+\n",
                "                            '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+\n",
                "                            '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+\n",
                "                            '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+\n",
                "                            '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),\n",
                "                  radius=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)\n",
                "India\n",
                "\n",
                "#The output is not 100% correct as there was some issue with the cordinates."
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()",
                "ASSIGN = ASSIGN.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],",
                "ASSIGN='Case', value_name='Count')",
                "ASSIGN.head()",
                "ASSIGN=ex.area(graph, x='Date', y='Count', color='Case',",
                "ASSIGN = 'Cases over time', color_discrete_sequence=[cure, deth, acti])",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()\n",
                "graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],\n",
                "         var_name='Case', value_name='Count')\n",
                "graph.head()\n",
                "\n",
                "fig=ex.area(graph, x='Date', y='Count', color='Case',\n",
                "           title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Cure_over_Death = df_india.groupby('Date').sum().reset_index()",
                "Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100",
                "Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100",
                "Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',",
                "ASSIGN=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],",
                "ASSIGN='Ratio',",
                "ASSIGN='Value')",
                "ASSIGN = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,",
                "ASSIGN='Cure_over_Death', color_discrete_sequence=[deth,cure])",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Cure_over_Death = df_india.groupby('Date').sum().reset_index()\n",
                "\n",
                "Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100\n",
                "Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100\n",
                "\n",
                "Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',\n",
                "                          value_vars=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],\n",
                "                          var_name='Ratio',\n",
                "                          value_name='Value')\n",
                "\n",
                "fig = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,\n",
                "             title='Cure_over_Death', color_discrete_sequence=[deth,cure])\n",
                "\n",
                "fig.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df_india.drop(['Latitude', 'Longitude'], axis=1)",
                "ASSIGN['TotalConfirmed'] = ASSIGN['ConfirmedIndianNational'] + ASSIGN['ConfirmedForeignNational']",
                "ASSIGN = ASSIGN[['Date', 'Statepath','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_india_data = df_india.drop(['Latitude', 'Longitude'], axis=1)\n",
                "df_india_data['TotalConfirmed'] = df_india_data['ConfirmedIndianNational'] + df_india_data['ConfirmedForeignNational']\n",
                "df_india_data = df_india_data[['Date', 'State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]\n",
                "df_india_data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['Statepath'].unique().apply(len)",
                "ASSIGN = pd.DataFrame(ASSIGN).reset_index()",
                "ASSIGN = ex.line(spread, x='Date', y='Statepath', text='Statepath',",
                "ASSIGN='Number of Statepath',",
                "ASSIGN=[conf,deth, cure])",
                "ASSIGN.update_traces(textposition='top center')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "\n",
                "spread = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['State/UnionTerritory'].unique().apply(len)\n",
                "spread = pd.DataFrame(spread).reset_index()\n",
                "\n",
                "spread_graph = ex.line(spread, x='Date', y='State/UnionTerritory', text='State/UnionTerritory',\n",
                "              title='Number of State/UnionTerritory to which COVID-19 spread over the time',\n",
                "              color_discrete_sequence=[conf,deth, cure])\n",
                "spread_graph.update_traces(textposition='top center')\n",
                "spread_graph.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data.groupby(['Date', 'Statepath'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)",
                "ex.line(ASSIGN, x=\"Date\", y=\"TotalConfirmed\", color='Statepath', title='ASSIGN over time', height=600)"
            ],
            "output_type": "display_data",
            "content_old": [
                "Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)\n",
                "\n",
                "ex.line(Spread, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory', title='Spread over time', height=600)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = india_latest_groupby",
                "ASSIGN['TotalConfirmed'] = ASSIGN['ConfirmedIndianNational'] + ASSIGN['ConfirmedForeignNational']",
                "ASSIGN = ASSIGN[['Statepath','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "latest_date = india_latest_groupby\n",
                "latest_date['TotalConfirmed'] = latest_date['ConfirmedIndianNational'] + latest_date['ConfirmedForeignNational']\n",
                "latest_date = latest_date[['State/UnionTerritory','TotalConfirmed','ConfirmedIndianNational','ConfirmedForeignNational','Cured','Deaths','Active']]\n",
                "latest_date.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True),",
                "ASSIGN=\"TotalConfirmed\", y=\"Statepath\", title='Confirmed Cases', text='TotalConfirmed', orientation='h',",
                "ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])",
                "ASSIGN.update_traces(marker_color='",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), \n",
                "             x=\"TotalConfirmed\", y=\"State/UnionTerritory\", title='Confirmed Cases', text='TotalConfirmed', orientation='h', \n",
                "             width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])\n",
                "Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Confirmed_bar.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True),",
                "ASSIGN=\"Deaths\", y=\"Statepath\", title='Death in each state', text='Deaths', orientation='h',",
                "ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])",
                "ASSIGN.update_traces(marker_color='",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), \n",
                "             x=\"Deaths\", y=\"State/UnionTerritory\", title='Death in each state', text='Deaths', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])\n",
                "Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Death_rate_bar.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True),",
                "ASSIGN=\"Cured\", y=\"Statepath\", title='Cured cases', text='Cured', orientation='h',",
                "ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4])",
                "ASSIGN.update_traces(marker_color='",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), \n",
                "             x=\"Cured\", y=\"State/UnionTerritory\", title='Cured cases', text='Cured', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])\n",
                "cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "cure_bar.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True),",
                "ASSIGN=\"Active\", y=\"Statepath\", title='Active cases', text='Active', orientation='h',",
                "ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10])",
                "ASSIGN.update_traces(marker_color='",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), \n",
                "             x=\"Active\", y=\"State/UnionTerritory\", title='Active cases', text='Active', orientation='h', \n",
                "             width=800, height=700, range_x = [0, max(latest_date['Active'])+10])\n",
                "Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')\n",
                "Active_cases.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2)",
                "Top_50 = latest_date[latest_date['TotalConfirmed']>20]",
                "Top_50 = Top_50.sort_values('Death Rate', ascending=False)",
                "ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),",
                "ASSIGN=\"Death Rate\", y=\"Statepath\", text='Death Rate', orientation='h',",
                "ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')",
                "ASSIGN.update_traces(marker_color='",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)\n",
                "Top_50 = latest_date[latest_date['TotalConfirmed']>20]\n",
                "Top_50 = Top_50.sort_values('Death Rate', ascending=False)\n",
                "\n",
                "Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), \n",
                "             x=\"Death Rate\", y=\"State/UnionTerritory\", text='Death Rate', orientation='h', \n",
                "             width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')\n",
                "Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')\n",
                "Plot.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()",
                "ASSIGN = ASSIGN.reset_index()",
                "ASSIGN = ex.bar(Date_vs_confirmed, x=\"Date\", y=\"TotalConfirmed\", color='Statepath', orientation='v', height=600,",
                "ASSIGN='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Date vs Confirmed\n",
                "Date_vs_confirmed = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_confirmed = Date_vs_confirmed.reset_index()\n",
                "\n",
                "Date_vs_confirmed_fig = ex.bar(Date_vs_confirmed, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_confirmed_fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()",
                "ASSIGN = ASSIGN.reset_index()",
                "ASSIGN = ex.bar(Date_vs_cured, x=\"Date\", y=\"Cured\", color='Statepath', orientation='v', height=600,",
                "ASSIGN='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Date vs Cured\n",
                "Date_vs_cured = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_cured = Date_vs_cured.reset_index()\n",
                "\n",
                "Date_vs_cured_fig = ex.bar(Date_vs_cured, x=\"Date\", y=\"Cured\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_cured_fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Date_vs_Deaths = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()",
                "Date_vs_Deaths = Date_vs_Deaths.reset_index()",
                "Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=\"Date\", y=\"Deaths\", color='Statepath', orientation='v', height=600,",
                "ASSIGN='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)",
                "Date_vs_Deaths_fig.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Date vs Active\n",
                "Date_vs_Deaths = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()\n",
                "Date_vs_Deaths = Date_vs_Deaths.reset_index()\n",
                "\n",
                "Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=\"Date\", y=\"Deaths\", color='State/UnionTerritory', orientation='v', height=600,\n",
                "                        title='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)\n",
                "Date_vs_Deaths_fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data.groupby(['Statepath', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']",
                "ASSIGN = ASSIGN.sum().diff().reset_index()",
                "ASSIGN = new_cases['Statepath'] != new_cases['Statepath'].shift(1)",
                "ASSIGN.loc[ASSIGN, 'TotalConfirmed'] = np.nan",
                "ASSIGN.loc[ASSIGN, 'Deaths'] = np.nan",
                "ASSIGN.loc[ASSIGN, 'Cured'] = np.nan",
                "ASSIGN = ex.bar(new_cases, x=\"Date\", y=\"TotalConfirmed\", color='Statepath',title='New cases')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "new_cases = df_india_data.groupby(['State/UnionTerritory', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']\n",
                "new_cases = new_cases.sum().diff().reset_index()\n",
                "\n",
                "mat = new_cases['State/UnionTerritory'] != new_cases['State/UnionTerritory'].shift(1)\n",
                "\n",
                "new_cases.loc[mat, 'TotalConfirmed'] = np.nan\n",
                "new_cases.loc[mat, 'Deaths'] = np.nan\n",
                "new_cases.loc[mat, 'Cured'] = np.nan\n",
                "\n",
                "New_cases_plot = ex.bar(new_cases, x=\"Date\", y=\"TotalConfirmed\", color='State/UnionTerritory',title='New cases')\n",
                "New_cases_plot.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Death_vs_Conf = latest_date.sort_values('Deaths', ascending=False).iloc[:15, :]",
                "Death_vs_Conf_plot = ex.scatter(Death_vs_Conf,",
                "ASSIGN='TotalConfirmed', y='Deaths', color='Statepath',",
                "ASSIGN='Statepath', log_x=True, log_y=True, title='Deaths vs Confirmed')",
                "Death_vs_Conf_plot.update_traces(textposition='top center')",
                "Death_vs_Conf_plot.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Deaths vs Confirmed\n",
                "Death_vs_Conf = latest_date.sort_values('Deaths', ascending=False).iloc[:15, :]\n",
                "\n",
                "Death_vs_Conf_plot = ex.scatter(Death_vs_Conf, \n",
                "                 x='TotalConfirmed', y='Deaths', color='State/UnionTerritory',\n",
                "                 text='State/UnionTerritory', log_x=True, log_y=True, title='Deaths vs Confirmed')\n",
                "Death_vs_Conf_plot.update_traces(textposition='top center')\n",
                "Death_vs_Conf_plot.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Cured_vs_Conf = latest_date.sort_values('Cured', ascending=False).iloc[:15, :]",
                "Cured_vs_Conf_plot = ex.scatter(Death_vs_Conf,",
                "ASSIGN='TotalConfirmed', y='Cured', color='Statepath',",
                "ASSIGN='Statepath', log_x=True, log_y=True, title='Cured vs Confirmed')",
                "Cured_vs_Conf_plot.update_traces(textposition='top center')",
                "Cured_vs_Conf_plot.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Cured vs Confirmed\n",
                "Cured_vs_Conf = latest_date.sort_values('Cured', ascending=False).iloc[:15, :]\n",
                "\n",
                "Cured_vs_Conf_plot = ex.scatter(Death_vs_Conf, \n",
                "                 x='TotalConfirmed', y='Cured', color='State/UnionTerritory',\n",
                "                 text='State/UnionTerritory', log_x=True, log_y=True, title='Cured vs Confirmed')\n",
                "Cured_vs_Conf_plot.update_traces(textposition='top center')\n",
                "Cured_vs_Conf_plot.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "\"\"\"",
                "df_India_perday",
                "df_Italy_perday",
                "df_Korea_perday",
                "\"\"\"",
                "ASSIGN = make_subplots(",
                "ASSIGN=2, cols=2,",
                "ASSIGN=[[{}, {}],",
                "[{\"colspan\": 2}, None]],",
                "ASSIGN=(\"S.Korea\",\"Italy\", \"India\"))",
                "ASSIGN.add_trace(gp.Bar(x=df_Korea_perday['Date'], y=df_Korea_perday['Total Cases'],",
                "ASSIGN= dict(color=df_Korea_perday['Total Cases'], coloraxis=\"coloraxis\")),",
                "1, 1)",
                "ASSIGN.add_trace(gp.Bar(x=df_Italy_perday['Date'], y=df_Italy_perday['Total Cases'],",
                "ASSIGN= dict(color=df_Italy_perday['Total Cases'], coloraxis=\"coloraxis\")),",
                "1, 2)",
                "ASSIGN.add_trace(gp.Bar(x=df_india_data['Date'], y=df_india_data['TotalConfirmed'],",
                "ASSIGN= dict(color=df_india_data['TotalConfirmed'], coloraxis=\"coloraxis\")),",
                "2, 1)",
                "ASSIGN.update_layout(coloraxis=dict(colorscale='RdBu'), showlegend=False,title_text=\"Total Confirmed cases(Cumulative)\")",
                "ASSIGN.update_layout(plot_bgcolor='rgb(250, 242, 242)')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "\"\"\"\n",
                "df_India_perday\n",
                "df_Italy_perday\n",
                "df_Korea_perday\n",
                "\"\"\"\n",
                "\n",
                "Comparison = make_subplots(\n",
                "    rows=2, cols=2,\n",
                "    specs=[[{}, {}],\n",
                "           [{\"colspan\": 2}, None]],\n",
                "    subplot_titles=(\"S.Korea\",\"Italy\", \"India\"))\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_Korea_perday['Date'], y=df_Korea_perday['Total Cases'],\n",
                "                    marker= dict(color=df_Korea_perday['Total Cases'], coloraxis=\"coloraxis\")),\n",
                "              1, 1)\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_Italy_perday['Date'], y=df_Italy_perday['Total Cases'],\n",
                "                    marker= dict(color=df_Italy_perday['Total Cases'], coloraxis=\"coloraxis\")),\n",
                "              1, 2)\n",
                "\n",
                "Comparison.add_trace(gp.Bar(x=df_india_data['Date'], y=df_india_data['TotalConfirmed'],\n",
                "                    marker= dict(color=df_india_data['TotalConfirmed'], coloraxis=\"coloraxis\")),\n",
                "              2, 1)\n",
                "\n",
                "Comparison.update_layout(coloraxis=dict(colorscale='RdBu'), showlegend=False,title_text=\"Total Confirmed cases(Cumulative)\")\n",
                "\n",
                "Comparison.update_layout(plot_bgcolor='rgb(250, 242, 242)')\n",
                "Comparison.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_india_data['TotalConfirmed'].sum()",
                "ASSIGN = df_Italy_perday['Total Cases'].sum()",
                "South_Korea = df_Korea_perday['Total Cases'].sum()",
                "ASSIGN ={\"India\": India,\"Italy\":Italy, 'South Korea':South_Korea}",
                "ASSIGN=['red','blue', 'yellow']",
                "plt.figure(figsize = (10,10))",
                "plt.pie(ASSIGN.values(),labels=ASSIGN.keys(),ASSIGN=ASSIGN,shadow=True,explode=(0.1, 0.1, 0.1), autopct='%1.2f%%')",
                "plt.axis('equal')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "India = df_india_data['TotalConfirmed'].sum()\n",
                "Italy = df_Italy_perday['Total Cases'].sum()\n",
                "South_Korea = df_Korea_perday['Total Cases'].sum()\n",
                "dict ={\"India\": India,\"Italy\":Italy, 'South Korea':South_Korea}\n",
                "colors=['red','blue', 'yellow']\n",
                "plt.figure(figsize = (10,10))\n",
                "plt.pie(dict.values(),labels=dict.keys(),colors=colors,shadow=True,explode=(0.1, 0.1, 0.1), autopct='%1.2f%%')\n",
                "plt.axis('equal')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(os.listdir('..path'))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'Mall_Customers.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "df1 = pd.read_csv('../input/Mall_Customers.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'Mall_Customers.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 12, 10)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotScatterMatrix(df1, 12, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "ASSIGN=\"github_repos\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# import package with helper functions ",
                "import bq_helper",
                "",
                "# create a helper object for this dataset",
                "github = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "                                              dataset_name=\"github_repos\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = (\"\"\"",
                "-- Select all the columns we want in our joined table",
                "SELECT L.license, COUNT(sf.path) AS number_of_files",
                "FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "-- Table to merge into sample_files",
                "INNER JOIN `bigquery-public-data.github_repos.licenses` as L",
                "ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "GROUP BY L.license",
                "ORDER BY number_of_files DESC",
                "\"\"\")",
                "ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# You can use two dashes (--) to add comments in SQL",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT L.license, COUNT(sf.path) AS number_of_files",
                "        FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.licenses` as L ",
                "            ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "        GROUP BY L.license",
                "        ORDER BY number_of_files DESC",
                "        \"\"\")",
                "",
                "file_count_by_license = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(file_count_by_license)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# print out all the returned results",
                "print(file_count_by_license)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "github.head(\"sample_commits\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# print the first couple rows of the \"sample_commits\" table",
                "github.head(\"sample_commits\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = (\"\"\"",
                "WITH repolist AS",
                "(",
                "SELECT repo_name",
                "FROM `bigquery-public-data.github_repos.sample_files`",
                "WHERE path LIKE '%.py'",
                "GROUP BY repo_name",
                ")",
                "SELECT sf.repo_name, COUNT(sc.commit) AS number_of_commits",
                "FROM",
                "repolist as sf",
                "INNER JOIN `bigquery-public-data.github_repos.sample_commits` as sc",
                "ON sc.repo_name = sf.repo_name",
                "GROUP BY sf.repo_name",
                "ORDER BY number_of_commits DESC",
                "\"\"\")",
                "github.estimate_query_size(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# You can use two dashes (--) to add comments in SQL",
                "query_commit = (\"\"\"",
                "        WITH repolist AS ",
                "            (",
                "                SELECT repo_name",
                "                FROM  `bigquery-public-data.github_repos.sample_files`",
                "                WHERE path LIKE '%.py'",
                "                GROUP BY repo_name",
                "            )",
                "        SELECT sf.repo_name, COUNT(sc.commit) AS number_of_commits",
                "        FROM ",
                "        repolist as sf ",
                "        INNER JOIN `bigquery-public-data.github_repos.sample_commits` as sc",
                "            ON sc.repo_name = sf.repo_name ",
                "        GROUP BY sf.repo_name",
                "        ORDER BY number_of_commits DESC",
                "        \"\"\")",
                "",
                "# check how big this query will be",
                "github.estimate_query_size(query_commit)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = github.query_to_pandas_safe(query_commit, max_gb_scanned=6)",
                "python_commits"
            ],
            "output_type": "not_existent",
            "content_old": [
                "python_commits = github.query_to_pandas_safe(query_commit, max_gb_scanned=6)",
                "python_commits"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "init_notebook_mode(connected=True)",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "display_data",
            "content_old": [
                "\n",
                "\n",
                "#Importing relevant libraries\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "import plotly as py\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "import plotly.graph_objs as go\n",
                "from plotly.subplots import make_subplots\n",
                "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
                "init_notebook_mode(connected=True)\n",
                "\n",
                "##Importing data into notebook\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Reading the data by pandas..Trying this you may have to change location according to local location\n",
                "\n",
                "corona_data=pd.read_csv('/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv')\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "corona_data.head(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Viewing first three rows of data for quick insight\n",
                "corona_data.head(3)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "corona_data.tail(2)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Viewing last two rows of data\n",
                "corona_data.tail(2)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=px.choropleth(corona_data,",
                "ASSIGN=\"Countrypath\",",
                "ASSIGN = \"country names\",",
                "ASSIGN=\"Confirmed\",",
                "ASSIGN=\"Countrypath\",",
                "ASSIGN=\"ObservationDate\"",
                ")",
                "ASSIGN.update_layout(",
                "ASSIGN = 'Global Spread of Coronavirus',",
                "ASSIGN = 0.5,",
                "ASSIGN=dict(",
                "ASSIGN = False,",
                "ASSIGN = False,",
                "))",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "choro_map=px.choropleth(corona_data, \n",
                "                    locations=\"Country/Region\", \n",
                "                    locationmode = \"country names\",\n",
                "                    color=\"Confirmed\", \n",
                "                    hover_name=\"Country/Region\", \n",
                "                    animation_frame=\"ObservationDate\"\n",
                "                   )\n",
                "\n",
                "choro_map.update_layout(\n",
                "    title_text = 'Global Spread of Coronavirus',\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "    \n",
                "choro_map.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.pie(corona_data, values = 'Confirmed',names='Countrypath', height=600)",
                "ASSIGN.update_traces(textposition='inside', textinfo='percent+label')",
                "ASSIGN.update_layout(",
                "ASSIGN = 0.5,",
                "ASSIGN=dict(",
                "ASSIGN = False,",
                "ASSIGN = False,",
                "))",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "pie_chart = px.pie(corona_data, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)",
                "ASSIGN = ASSIGN.drop_duplicates(subset = ['Countrypath'])",
                "ASSIGN = ASSIGN.iloc[0:10]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Manipulating the dataframe\n",
                "top10 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)\n",
                "top10  = top10.drop_duplicates(subset = ['Country/Region'])\n",
                "top10 = top10.iloc[0:10]\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.pie(top10, values = 'Confirmed',names='Countrypath', height=600)",
                "ASSIGN.update_traces(textposition='inside', textinfo='percent+label')",
                "ASSIGN.update_layout(",
                "ASSIGN = 0.5,",
                "ASSIGN=dict(",
                "ASSIGN = False,",
                "ASSIGN = False,",
                "))",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "pie_chart_top10 = px.pie(top10, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart_top10.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart_top10.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart_top10.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)",
                "ASSIGN = ASSIGN.drop_duplicates(subset = ['Countrypath'])",
                "ASSIGN = ASSIGN.iloc[-20:-1]",
                "last20"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Manipulating the dataframe\n",
                "last20 = corona_data.groupby(['Country/Region', 'ObservationDate']).sum().reset_index().sort_values('Confirmed', ascending=False)\n",
                "last20  = last20.drop_duplicates(subset = ['Country/Region'])\n",
                "last20 = last20.iloc[-20:-1]\n",
                "last20"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.pie(last20, values = 'Confirmed',names='Countrypath', height=600)",
                "ASSIGN.update_traces(textposition='inside', textinfo='percent+label')",
                "ASSIGN.update_layout(",
                "ASSIGN = 0.5,",
                "ASSIGN=dict(",
                "ASSIGN = False,",
                "ASSIGN = False,",
                "))",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "pie_chart_last20 = px.pie(last20, values = 'Confirmed',names='Country/Region', height=600)\n",
                "pie_chart_last20.update_traces(textposition='inside', textinfo='percent+label')\n",
                "\n",
                "pie_chart_last20.update_layout(\n",
                "    title_x = 0.5,\n",
                "    geo=dict(\n",
                "        showframe = False,\n",
                "        showcoastlines = False,\n",
                "    ))\n",
                "\n",
                "pie_chart_last20.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = corona_data.groupby(['Countrypath', 'ObservationDate'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index().sort_values('ObservationDate', ascending=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "bar_data = corona_data.groupby(['Country/Region', 'ObservationDate'])['Confirmed', 'Deaths', 'Recovered'].sum().reset_index().sort_values('ObservationDate', ascending=True)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.bar(bar_data, x=\"ObservationDate\", y=\"Confirmed\", color='Countrypath', text = 'Confirmed', orientation='v', height=1300,width=1000,",
                "ASSIGN='Increase in COVID-19 Cases')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "bar_fig = px.bar(bar_data, x=\"ObservationDate\", y=\"Confirmed\", color='Country/Region', text = 'Confirmed', orientation='v', height=1300,width=1000,\n",
                "             title='Increase in COVID-19 Cases')\n",
                "bar_fig.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.bar(bar_data, x=\"ObservationDate\", y=\"Deaths\", color='Countrypath', text = 'Deaths', orientation='v', height=1000,width=900,",
                "ASSIGN='COVID-19 Deaths since February to April 11th')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "bar_fig2 = px.bar(bar_data, x=\"ObservationDate\", y=\"Deaths\", color='Country/Region', text = 'Deaths', orientation='v', height=1000,width=900,\n",
                "             title='COVID-19 Deaths since February to April 11th')\n",
                "bar_fig2.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = px.bar(bar_data, x=\"ObservationDate\", y=\"Recovered\", color='Countrypath', text = 'Recovered', orientation='v', height=1000,width=900,",
                "ASSIGN='COVID-19 Recovered Cases since February to April 11th')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "bar_fig3 = px.bar(bar_data, x=\"ObservationDate\", y=\"Recovered\", color='Country/Region', text = 'Recovered', orientation='v', height=1000,width=900,\n",
                "             title='COVID-19 Recovered Cases since February to April 11th')\n",
                "bar_fig3.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = corona_data.groupby('ObservationDate').sum().reset_index()",
                "ASSIGN = ASSIGN.melt(id_vars='ObservationDate',",
                "ASSIGN=['Confirmed',",
                "'Recovered',",
                "'Deaths'],",
                "ASSIGN='Ratio',",
                "ASSIGN='Value')",
                "ASSIGN = px.line(line_data, x=\"ObservationDate\", y=\"Value\", line_shape=\"spline\",color='Ratio',",
                "ASSIGN='Confirmed cases, Recovered cases, and Death Over Time')",
                "ASSIGN.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "line_data = corona_data.groupby('ObservationDate').sum().reset_index()\n",
                "\n",
                "line_data = line_data.melt(id_vars='ObservationDate', \n",
                "                 value_vars=['Confirmed', \n",
                "                             'Recovered', \n",
                "                             'Deaths'], \n",
                "                 var_name='Ratio', \n",
                "                 value_name='Value')\n",
                "\n",
                "line_fig = px.line(line_data, x=\"ObservationDate\", y=\"Value\", line_shape=\"spline\",color='Ratio', \n",
                "              title='Confirmed cases, Recovered cases, and Death Over Time')\n",
                "line_fig.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('..path', dtype=np.object)",
                "ASSIGN = multiple.iloc[0,:]",
                "ASSIGN = multiple.iloc[1:,:]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "import numpy as np",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "import matplotlib.patches as patches",
                "",
                "%matplotlib inline",
                "",
                "multiple = pd.read_csv('../input/multipleChoiceResponses.csv', dtype=np.object)",
                "mulcQ = multiple.iloc[0,:]",
                "mulcA = multiple.iloc[1:,:]",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = mulcA[round(mulcA.iloc[:,0].astype(int) path) <= 3].index",
                "ASSIGN = ASSIGN.drop(fast, axis=0)",
                "ASSIGN = mulcA.Q5",
                "ASSIGN.value_counts(normalize=True).plot(kind='bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fast = mulcA[round(mulcA.iloc[:,0].astype(int) / 60) <= 3].index",
                "mulcA = mulcA.drop(fast, axis=0)",
                "rol = mulcA.Q5",
                "rol.value_counts(normalize=True).plot(kind='bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = mulcA.Q9",
                "ASSIGN.value_counts().plot(kind='bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rem = mulcA.Q9",
                "rem.value_counts().plot(kind='bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,",
                "'60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,",
                "'150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,",
                "'500,000+':18}},inplace = True)",
                "mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',",
                "'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',",
                "'Czech Republic':'Czech R.'}},inplace = True)",
                "ASSIGN = multiple.filter(regex=\"(Q{t}$|Q{t}_)\".format(t = 16))[1:]",
                "ASSIGN = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',",
                "'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'Cpath++','Q16_Part_9':'MATLAB',",
                "'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C",
                "'Q16_Part_15':'Ruby','Q16_Part_16':'SASpath'}",
                "ASSIGN= q16.rename(columns=q16_col).fillna(0).replace('[^\\\\d]',1, regex=True)",
                "ASSIGN.pop('Q16_Part_17')",
                "ASSIGN.pop('Q16_Part_18')",
                "ASSIGN.pop('Q16_OTHER_TEXT')",
                "ASSIGN = list(q16_lim.iloc[:0])",
                "for i in ASSIGN:",
                "SLICE= ASSIGN['{}'.format(i)]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "ASSIGN = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),",
                "ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean()]",
                "plt.figure(figsize=(20,10))",
                "plt.bar(np.arange(11),ASSIGN,color=['dodgerblue','c','tomato','silver','midnightblue','tan'])",
                "plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',",
                "'Env. Sciense','Arts'))",
                "plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#",
                "mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,",
                "                       '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,",
                "                       '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,",
                "                                 '500,000+':18}},inplace = True)",
                "mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',",
                "                       'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',",
                "                      'Czech Republic':'Czech R.'}},inplace = True)",
                "",
                "q16 = multiple.filter(regex=\"(Q{t}$|Q{t}_)\".format(t = 16))[1:]",
                "q16_col = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',",
                "           'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'C/C++','Q16_Part_9':'MATLAB',",
                "           'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C#/.NET','Q16_Part_14':'PHP',",
                "           'Q16_Part_15':'Ruby','Q16_Part_16':'SAS/STATA'}",
                "",
                "q16_lim= q16.rename(columns=q16_col).fillna(0).replace('[^\\\\d]',1, regex=True)",
                "q16_lim.pop('Q16_Part_17')",
                "q16_lim.pop('Q16_Part_18')",
                "q16_lim.pop('Q16_OTHER_TEXT')",
                "lab = list(q16_lim.iloc[:0])",
                "for i in lab:",
                "    mulcA[i]= q16_lim['{}'.format(i)]",
                "#",
                "com_sci = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "eng_nco = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "mat_sta = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "biu_dis = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "phy_ast = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "inf_tec = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "med_sci = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "soc_sci = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "hum_tie = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "env_sci = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "fin_art = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]",
                "",
                "",
                "rem_prom = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),",
                "            soc_sci.Q9.mean(),hum_tie.Q9.mean(),env_sci.Q9.mean(),fin_art.Q9.mean()]",
                "plt.figure(figsize=(20,10))",
                "plt.bar(np.arange(11),rem_prom,color=['dodgerblue','c','tomato','silver','midnightblue','tan'])",
                "plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',",
                "                            'Env. Sciense','Arts'))",
                "plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "ASSIGN = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True),",
                "ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True)",
                ",ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True)],",
                "ASSIGN=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59'",
                ",'age60-69','age70-79','age80+']).T",
                "ASSIGN = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "age_18_21 = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_22_24 = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_25_29 = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_30_34 = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_35_39 = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_40_44 = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_45_49 = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_50_54 = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_55_59 = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_60_69 = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_70_79 = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "age_80m = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]",
                "",
                "df_arol = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True),",
                "                        age_35_39.value_counts(normalize=True),age_40_44.value_counts(normalize=True),age_45_49.value_counts(normalize=True),age_50_54.value_counts(normalize=True)",
                "                        ,age_55_59.value_counts(normalize=True),age_60_69.value_counts(normalize=True),age_70_79.value_counts(normalize=True),age_80m.value_counts(normalize=True)],",
                "                       index=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59'",
                "                              ,'age60-69','age70-79','age80+']).T",
                "",
                "axes = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35))",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,3))",
                "ASSIGN = ['Comp. Science','Engeneering','Mathematics','Inf. Technology','Business','Physics','Medical','Soc. Science']",
                "ASSIGN = pd.DataFrame([com_sci.Q3.value_counts(),eng_nco.Q3.value_counts(),mat_sta.Q3.value_counts(),inf_tec.Q3.value_counts()",
                ",biu_dis.Q3.value_counts(),phy_ast.Q3.value_counts(),med_sci.Q3.value_counts(),soc_sci.Q3.value_counts()]",
                ",index=[names])",
                "ASSIGN= ASSIGN.drop(['Other'],axis=1).T",
                "ASSIGN = plt.subplots(4, 2, sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Comp. Science'].values[:10])),color='cadetblue')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Engeneering'].values[:10])),color='slategray')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Mathematics'].values[:10])),color='seagreen')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Inf. Technology'].values[:10])),color='palevioletred')",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Business'].values[:10])),color='darkblue')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Physics'].values[:10])),color='khaki')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Medical'].values[:10])),color='k')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(ASSIGN.index[:10],list(map(int,ASSIGN['Soc. Science'].values[:10])),color='firebrick')",
                "axes[3, 1].set_title('Soc. Science')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize=(15,3))",
                "names = ['Comp. Science','Engeneering','Mathematics','Inf. Technology','Business','Physics','Medical','Soc. Science']",
                "df_coun = pd.DataFrame([com_sci.Q3.value_counts(),eng_nco.Q3.value_counts(),mat_sta.Q3.value_counts(),inf_tec.Q3.value_counts()",
                "                        ,biu_dis.Q3.value_counts(),phy_ast.Q3.value_counts(),med_sci.Q3.value_counts(),soc_sci.Q3.value_counts()]",
                "                       ,index=[names])",
                "df_coun= df_coun.drop(['Other'],axis=1).T",
                "fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(df_coun.index[:10],list(map(int,df_coun['Comp. Science'].values[:10])),color='cadetblue')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(df_coun.index[:10],list(map(int,df_coun['Engeneering'].values[:10])),color='slategray')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(df_coun.index[:10],list(map(int,df_coun['Mathematics'].values[:10])),color='seagreen')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(df_coun.index[:10],list(map(int,df_coun['Inf. Technology'].values[:10])),color='palevioletred')",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(df_coun.index[:10],list(map(int,df_coun['Business'].values[:10])),color='darkblue')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(df_coun.index[:10],list(map(int,df_coun['Physics'].values[:10])),color='khaki')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(df_coun.index[:10],list(map(int,df_coun['Medical'].values[:10])),color='k')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(df_coun.index[:10],list(map(int,df_coun['Soc. Science'].values[:10])),color='firebrick')",
                "axes[3, 1].set_title('Soc. Science')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),",
                "mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['Cpath++'].sum(),mulcA['MATLAB'].sum(),",
                "mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C",
                "mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SASpath'].sum()],index=lab)",
                "ASSIGN.plot(kind='bar',color='brown')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lengs = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),",
                "                           mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['C/C++'].sum(),mulcA['MATLAB'].sum(),",
                "                           mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C#/.NET'].sum(),",
                "                           mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SAS/STATA'].sum()],index=lab)",
                "lengs.plot(kind='bar',color='brown')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=[mulcA[i][mulcA.Q5=='Computer science (software engineering, etc.)'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Engineering (non-computer focused)'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Mathematics or statistics'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='A business discipline (accounting, economics, finance, etc.)'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Physics or astronomy'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Information technology, networking, or system administration'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Medical or life sciences (biology, chemistry, medicine, etc.)'].sum() for i in lab ]",
                "ASSIGN=[mulcA[i][mulcA.Q5=='Social sciences (anthropology, psychology, sociology, etc.)'].sum() for i in lab ]",
                "ASSIGN = plt.subplots(4, 2, sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(lab,ASSIGN,color='c')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(lab,ASSIGN,color='r')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(lab,ASSIGN,color='k')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(lab,ASSIGN)",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(lab,ASSIGN,color = 'tomato')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(lab,ASSIGN,color='b')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(lab,ASSIGN,color='y')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(lab,ASSIGN,color='g')",
                "axes[3, 1].set_title('Soc. Science')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "com_sci_lengs=[mulcA[i][mulcA.Q5=='Computer science (software engineering, etc.)'].sum() for i in lab ]",
                "eng_nco_lengs=[mulcA[i][mulcA.Q5=='Engineering (non-computer focused)'].sum() for i in lab ]",
                "mat_sta_lengs=[mulcA[i][mulcA.Q5=='Mathematics or statistics'].sum() for i in lab ]",
                "biu_dis_lengs=[mulcA[i][mulcA.Q5=='A business discipline (accounting, economics, finance, etc.)'].sum() for i in lab ]",
                "phy_ast_lengs=[mulcA[i][mulcA.Q5=='Physics or astronomy'].sum() for i in lab ]",
                "inf_tec_lengs=[mulcA[i][mulcA.Q5=='Information technology, networking, or system administration'].sum() for i in lab ]",
                "med_sci_lengs=[mulcA[i][mulcA.Q5=='Medical or life sciences (biology, chemistry, medicine, etc.)'].sum() for i in lab ]",
                "soc_sci_lengs=[mulcA[i][mulcA.Q5=='Social sciences (anthropology, psychology, sociology, etc.)'].sum() for i in lab ]",
                "",
                "fig, axes = plt.subplots(4, 2,  sharey=True,figsize=(15,20))",
                "axes[0, 0].barh(lab,com_sci_lengs,color='c')",
                "axes[0, 0].set_title('Comp. Science')",
                "axes[0, 1].barh(lab,eng_nco_lengs,color='r')",
                "axes[0, 1].set_title('Engeneering')",
                "axes[1, 0].barh(lab,mat_sta_lengs,color='k')",
                "axes[1, 0].set_title('Mathematics')",
                "axes[1, 1].barh(lab,inf_tec_lengs)",
                "axes[1, 1].set_title('Inf. Technology')",
                "axes[2, 0].barh(lab,biu_dis_lengs,color = 'tomato')",
                "axes[2, 0].set_title('Business')",
                "axes[2, 1].barh(lab,phy_ast_lengs,color='b')",
                "axes[2, 1].set_title('Physics')",
                "axes[3, 0].barh(lab,med_sci_lengs,color='y')",
                "axes[3, 0].set_title('Medical')",
                "axes[3, 1].barh(lab,soc_sci_lengs,color='g')",
                "axes[3, 1].set_title('Soc. Science')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = mulcA.Q5[mulcA.Q1 == \"Female\"]",
                "ASSIGN = mulcA.Q5[mulcA.Q1 == \"Male\"]",
                "ASSIGN = pd.DataFrame([fem_oc.value_counts(normalize=True),mal_oc.value_counts(normalize=True)],index=['Female','Male']).T",
                "ASSIGN = df_gen.plot.barh(rot=0, subplots=True,figsize=(15,15))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fem_oc = mulcA.Q5[mulcA.Q1 == \"Female\"]",
                "mal_oc = mulcA.Q5[mulcA.Q1 == \"Male\"]",
                "df_gen = pd.DataFrame([fem_oc.value_counts(normalize=True),mal_oc.value_counts(normalize=True)],index=['Female','Male']).T",
                "axes = df_gen.plot.barh(rot=0, subplots=True,figsize=(15,15))    "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input/'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Import pandas\n",
                "import pandas as pd\n",
                "\n",
                "# Read in the file content in a DataFrame called discoveries\n",
                "discoveries = pd.read_csv('/kaggle/input/week6dataset/discoveries.csv')\n",
                "\n",
                "# Display the first five lines of the DataFrame\n",
                "discoveries.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(discoveries.dtypes)",
                "ASSIGN = pd.to_datetime(ASSIGN)",
                "print(discoveries.dtypes)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the data type of each column in discoveries\n",
                "print(discoveries.dtypes)\n",
                "\n",
                "# Convert the date column to a datestamp type\n",
                "discoveries['date'] = pd.to_datetime(discoveries['date'])\n",
                "\n",
                "# Print the data type of each column in discoveries, again\n",
                "print(discoveries.dtypes)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "discoveries.set_index('date', inplace = True)",
                "ASSIGN = discoveries.plot(color='blue')",
                "ASSIGN.set_xlabel('Date')",
                "ASSIGN.set_ylabel('Number of great discoveries')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "\n",
                "# Set the date column as the index of your DataFrame discoveries\n",
                "discoveries.set_index('date', inplace = True)\n",
                "\n",
                "# Plot the time series in your DataFrame\n",
                "ax = discoveries.plot(color='blue')\n",
                "\n",
                "# Specify the x-axis label in your plot\n",
                "ax.set_xlabel('Date')\n",
                "\n",
                "# Specify the y-axis label in your plot\n",
                "ax.set_ylabel('Number of great discoveries')\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.style.use('ggplot')",
                "ASSIGN = discoveries.plot()",
                "ASSIGN.set_title('ggplot Style')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import the matplotlib.pyplot sub-module\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Use the ggplot style\n",
                "plt.style.use('ggplot')\n",
                "ax2 = discoveries.plot()\n",
                "\n",
                "# Set the title\n",
                "ax2.set_title('ggplot Style')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.style.use('fivethirtyeight')",
                "ASSIGN = discoveries.plot()",
                "ASSIGN.set_title('FiveThirtyEight Style')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import the matplotlib.pyplot sub-module\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Use the fivethirtyeight style\n",
                "plt.style.use('fivethirtyeight')\n",
                "\n",
                "# Plot the time series\n",
                "ax1 = discoveries.plot()\n",
                "ax1.set_title('FiveThirtyEight Style')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = discoveries.plot(color='blue',figsize =(8, 3), linewidth=2, fontsize=6)",
                "ASSIGN.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot a line chart of the discoveries DataFrame using the specified arguments\n",
                "ax = discoveries.plot(color='blue',figsize =(8, 3), linewidth=2, fontsize=6)\n",
                "\n",
                "# Specify the title in your plot\n",
                "ax.set_title('Number of great inventions and scientific discoveries from 1860 to 1959', fontsize=8)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = discoveries ['1939-01-01': '1958-01-01']",
                "ASSIGN = discoveries_subset_2.plot(color='blue', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Select the subset of data between 1939 and 1958\n",
                "discoveries_subset_2 = discoveries ['1939-01-01': '1958-01-01']\n",
                "# Plot the time series in your DataFrame as a blue area chart\n",
                "ax = discoveries_subset_2.plot(color='blue', fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = discoveries.plot(color='blue', fontsize=6)",
                "ASSIGN.axvline('1939-01-01', color='red', linestyle='--')",
                "ASSIGN.axhline(4, color='green', linestyle='--')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot your the discoveries time series\n",
                "ax = discoveries.plot(color='blue', fontsize=6)\n",
                "\n",
                "# Add a red vertical line\n",
                "ax.axvline('1939-01-01', color='red', linestyle='--')\n",
                "\n",
                "# Add a green horizontal line\n",
                "ax.axhline(4, color='green', linestyle='--')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = discoveries.plot(color='blue', fontsize=6)",
                "ASSIGN.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)",
                "ASSIGN.axhspan(6, 8, color='green', alpha=0.3)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot your the discoveries time series\n",
                "ax = discoveries.plot(color='blue', fontsize=6)\n",
                "\n",
                "# Add a vertical red shaded region\n",
                "ax.axvspan('1900-01-01', '1915-01-01', color='red', alpha=0.3)\n",
                "\n",
                "# Add a horizontal green shaded region\n",
                "ax.axhspan(6, 8, color='green', alpha=0.3)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('path', parse_dates = ['datestamp'])",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "co2_levels = pd.read_csv('/kaggle/input/week6dataset/co2_levels.csv', parse_dates = ['datestamp'])\n",
                "print(co2_levels)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.set_index('datestamp')",
                "print(ASSIGN.isnull().sum())"
            ],
            "output_type": "stream",
            "content_old": [
                "# Set datestamp column as index\n",
                "co2_levels = co2_levels.set_index('datestamp')\n",
                "\n",
                "# Print out the number of missing values\n",
                "print(co2_levels.isnull().sum())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.fillna(method='bfill')",
                "print(ASSIGN.isnull().sum())"
            ],
            "output_type": "stream",
            "content_old": [
                "# Impute missing values with the next valid observation\n",
                "co2_levels = co2_levels.fillna(method='bfill')\n",
                "\n",
                "# Print out the number of missing values\n",
                "print(co2_levels.isnull().sum())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = co2_levels.rolling(window=52).mean()",
                "ASSIGN = co2_levels.rolling(window=52).std()",
                "ASSIGN = ASSIGN + (ASSIGN * 2)",
                "ASSIGN = ASSIGN - (ASSIGN * 2)",
                "ASSIGN = ma.plot(linewidth=0.8, fontsize=6)",
                "ASSIGN.set_xlabel('Date', fontsize=10)",
                "ASSIGN.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)",
                "ASSIGN.set_title('Rolling mean and variance of CO2 levels\\nin Mauai Hawaii from 1958 to 2001', fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Compute the 52 weeks rolling mean of the co2_levels DataFrame\n",
                "ma = co2_levels.rolling(window=52).mean()\n",
                "\n",
                "# Compute the 52 weeks rolling standard deviation of the co2_levels DataFrame\n",
                "mstd = co2_levels.rolling(window=52).std()\n",
                "\n",
                "# Add the upper bound column to the ma DataFrame\n",
                "ma['upper'] = ma['co2'] + (mstd['co2'] * 2)\n",
                "\n",
                "# Add the lower bound column to the ma DataFrame\n",
                "ma['lower'] = ma['co2'] - (mstd['co2'] * 2)\n",
                "\n",
                "# Plot the content of the ma DataFrame\n",
                "ax = ma.plot(linewidth=0.8, fontsize=6)\n",
                "\n",
                "# Specify labels, legend, and show the plot\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_ylabel('CO2 levels in Mauai Hawaii', fontsize=10)\n",
                "ax.set_title('Rolling mean and variance of CO2 levels\\nin Mauai Hawaii from 1958 to 2001', fontsize=10)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = co2_levels.index.month",
                "ASSIGN = co2_levels.groupby(index_month).mean()",
                "ASSIGN.plot(fontsize = 6)",
                "plt.legend(fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Get month for each dates in the index of co2_levels\n",
                "index_month = co2_levels.index.month\n",
                "\n",
                "# Compute the mean CO2 levels for each month of the year\n",
                "mean_co2_levels_by_month = co2_levels.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean CO2 levels for each month of the year\n",
                "mean_co2_levels_by_month.plot(fontsize = 6)\n",
                "\n",
                "# Specify the fontsize on the legend\n",
                "plt.legend(fontsize=10)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(co2_levels.describe())",
                "print(co2_levels.co2.min())",
                "print(co2_levels.co2.max())"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print out summary statistics of the co2_levels DataFrame\n",
                "print(co2_levels.describe())\n",
                "\n",
                "# Print out the minima of the co2 column in the co2_levels DataFrame\n",
                "print(co2_levels.co2.min())\n",
                "\n",
                "# Print out the maxima of the co2 column in the co2_levels DataFrame\n",
                "print(co2_levels.co2.max())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = co2_levels.plot(kind = 'hist', bins = 50, fontsize=6)",
                "ASSIGN.set_xlabel('CO2', fontsize=10)",
                "ASSIGN.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)",
                "plt.legend(fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Generate a histogram\n",
                "ax = co2_levels.plot(kind = 'hist', bins = 50, fontsize=6)\n",
                "\n",
                "# Set the labels and display the plot\n",
                "ax.set_xlabel('CO2', fontsize=10)\n",
                "ax.set_ylabel('Histogram of CO2 levels in Maui Hawaii', fontsize=10)\n",
                "plt.legend(fontsize=10)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = co2_levels.plot(kind = 'density', linewidth = 4, fontsize=6)",
                "ASSIGN.set_xlabel('CO2', fontsize=10)",
                "ASSIGN.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Display density plot of CO2 levels values\n",
                "ax = co2_levels.plot(kind = 'density', linewidth = 4, fontsize=6)\n",
                "\n",
                "# Annotate x-axis labels\n",
                "ax.set_xlabel('CO2', fontsize=10)\n",
                "\n",
                "# Annotate y-axis labels\n",
                "ax.set_ylabel('Density plot of CO2 levels in Maui Hawaii', fontsize=10)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.style.use('fivethirtyeight')",
                "ASSIGN = tsaplots.plot_acf(co2_levels['co2'], lags=24)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import required libraries\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('fivethirtyeight')\n",
                "from statsmodels.graphics import tsaplots\n",
                "\n",
                "# Display the autocorrelation plot of your time series\n",
                "fig = tsaplots.plot_acf(co2_levels['co2'], lags=24)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.style.use('fivethirtyeight')",
                "ASSIGN = tsaplots.plot_pacf(co2_levels['co2'], lags=24)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import required libraries\n",
                "import matplotlib.pyplot as plt\n",
                "plt.style.use('fivethirtyeight')\n",
                "from statsmodels.graphics import tsaplots\n",
                "\n",
                "# Display the partial autocorrelation plot of your time series\n",
                "fig = tsaplots.plot_pacf(co2_levels['co2'], lags=24)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = sm.tsa.seasonal_decompose(co2_levels)",
                "print(ASSIGN.seasonal)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Import statsmodels.api as sm\n",
                "import statsmodels.api as sm\n",
                "\n",
                "# Perform time series decompositon\n",
                "decomposition = sm.tsa.seasonal_decompose(co2_levels)\n",
                "\n",
                "# Print the seasonality component\n",
                "print(decomposition.seasonal)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = decomposition.ASSIGN",
                "ASSIGN = trend.plot(figsize=(12, 6), fontsize=6)",
                "ASSIGN.set_xlabel('Date', fontsize=10)",
                "ASSIGN.set_title('Seasonal component the CO2 time-series', fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Extract the trend component\n",
                "trend = decomposition.trend\n",
                "\n",
                "# Plot the values of the trend\n",
                "ax = trend.plot(figsize=(12, 6), fontsize=6)\n",
                "\n",
                "# Specify axis labels\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_title('Seasonal component the CO2 time-series', fontsize=10)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path', parse_dates = ['Month'], index_col = 'Month')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "airline = pd.read_csv('/kaggle/input/week6dataset/airline_passengers.csv', parse_dates = ['Month'], index_col = 'Month')\n",
                "airline.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = airline.plot(color = 'blue', fontsize=12)",
                "ASSIGN.axvline('1955-12-01', color='red', linestyle='--')",
                "ASSIGN.set_xlabel('Date', fontsize=12)",
                "ASSIGN.set_title('Number of Monthly Airline Passengers', fontsize=12)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot the time series in your dataframe\n",
                "ax = airline.plot(color = 'blue', fontsize=12)\n",
                "\n",
                "# Add a red vertical line at the date 1955-12-01\n",
                "ax.axvline('1955-12-01', color='red', linestyle='--')\n",
                "\n",
                "# Specify the labels in your plot\n",
                "ax.set_xlabel('Date', fontsize=12)\n",
                "ax.set_title('Number of Monthly Airline Passengers', fontsize=12)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(airline.isnull().sum())",
                "print(airline.describe())"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print out the number of missing values\n",
                "print(airline.isnull().sum())\n",
                "\n",
                "# Print out summary statistics of the airline DataFrame\n",
                "print(airline.describe())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = airline.boxplot()",
                "ASSIGN.set_title('Boxplot of Monthly Airline\\nPassengers Count', fontsize=20)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Display boxplot of airline values\n",
                "ax = airline.boxplot()\n",
                "\n",
                "# Specify the title of your plot\n",
                "ax.set_title('Boxplot of Monthly Airline\\nPassengers Count', fontsize=20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = airline.index.month",
                "ASSIGN = airline.groupby(index_month).mean()",
                "ASSIGN.plot()",
                "plt.legend(fontsize=20)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Get month for each dates from the index of airline\n",
                "index_month = airline.index.month\n",
                "\n",
                "# Compute the mean number of passengers for each month of the year\n",
                "mean_airline_by_month = airline.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean number of passengers for each month of the year\n",
                "mean_airline_by_month.plot()\n",
                "plt.legend(fontsize=20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = sm.tsa.seasonal_decompose(airline)",
                "ASSIGN = decomposition.ASSIGN",
                "ASSIGN = decomposition.ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Import statsmodels.api as sm\n",
                "import statsmodels.api as sm\n",
                "\n",
                "# Perform time series decompositon\n",
                "decomposition = sm.tsa.seasonal_decompose(airline)\n",
                "\n",
                "# Extract the trend and seasonal components\n",
                "trend = decomposition.trend\n",
                "seasonal = decomposition.seasonal"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.concat([trend, seasonal], axis = 1)",
                "ASSIGN.columns = ['trend', 'seasonal']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "airline_decomposed = pd.concat([trend, seasonal], axis = 1)\n",
                "airline_decomposed.columns = ['trend', 'seasonal']"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print(airline_decomposed.head())",
                "ASSIGN = airline_decomposed.plot(figsize=(12, 6), fontsize=15)",
                "ASSIGN.set_xlabel('Date', fontsize=15)",
                "plt.legend(fontsize=15)",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the first 5 rows of airline_decomposed\n",
                "print(airline_decomposed.head())\n",
                "\n",
                "# Plot the values of the df_decomposed DataFrame\n",
                "ax = airline_decomposed.plot(figsize=(12, 6), fontsize=15)\n",
                "\n",
                "# Specify axis labels\n",
                "ax.set_xlabel('Date', fontsize=15)\n",
                "plt.legend(fontsize=15)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "display(ASSIGN.head(5))"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Read in meat DataFrame\n",
                "meat = pd.read_csv('/kaggle/input/week6dataset/meat.csv')\n",
                "\n",
                "# Review the first five lines of the meat DataFrame\n",
                "display(meat.head(5))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.to_datetime(ASSIGN )",
                "ASSIGN = ASSIGN.set_index('date')",
                "display(ASSIGN.describe())"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Convert the date column to a datestamp type\n",
                "meat['date'] = pd.to_datetime(meat['date'] )\n",
                "\n",
                "# Set the date column as the index of your DataFrame meat\n",
                "meat = meat.set_index('date')\n",
                "\n",
                "# Print the summary statistics of the DataFrame\n",
                "display(meat.describe())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = meat.plot(linewidth = 2, fontsize = 12)",
                "ASSIGN.set_xlabel('Date')",
                "ASSIGN.legend(fontsize=15)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot time series dataset\n",
                "ax = meat.plot(linewidth = 2, fontsize = 12)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = meat.plot.area(fontsize=12)",
                "ASSIGN.set_xlabel('Date')",
                "ASSIGN.legend(fontsize=15)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot an area chart\n",
                "ax = meat.plot.area(fontsize=12)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=15)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = meat.plot(colormap='cubehelix', fontsize=15)",
                "ASSIGN.set_xlabel('Date')",
                "ASSIGN.legend(fontsize=18)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot time series dataset using the cubehelix color palette\n",
                "ax = meat.plot(colormap='cubehelix', fontsize=15)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=18)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = meat.plot(colormap = 'PuOr', fontsize=15)",
                "ASSIGN.set_xlabel('Date')",
                "ASSIGN.legend(fontsize=18)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot time series dataset using the cubehelix color palette\n",
                "ax = meat.plot(colormap = 'PuOr', fontsize=15)\n",
                "\n",
                "# Additional customizations\n",
                "ax.set_xlabel('Date')\n",
                "ax.legend(fontsize=18)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = meat.mean(axis = 0)",
                "ASSIGN = pd.DataFrame(ASSIGN).transpose()",
                "ASSIGN.index = ['mean']",
                "meat_mean"
            ],
            "output_type": "execute_result",
            "content_old": [
                "meat_mean = meat.mean(axis = 0)\n",
                "meat_mean = pd.DataFrame(meat_mean).transpose()\n",
                "meat_mean.index = ['mean']\n",
                "meat_mean"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = meat.plot(fontsize=6, linewidth=1)",
                "ASSIGN.set_xlabel('Date', fontsize=6)",
                "ASSIGN.table(cellText=meat_mean.values,",
                "ASSIGN = [0.15]*len(meat_mean.columns),",
                "ASSIGN=meat_mean.index,",
                "ASSIGN=meat_mean.columns,",
                "ASSIGN='top')",
                "ASSIGN.legend(ASSIGN='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot the meat data\n",
                "ax = meat.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Add x-axis labels\n",
                "ax.set_xlabel('Date', fontsize=6)\n",
                "\n",
                "# Add summary table information to the plot\n",
                "ax.table(cellText=meat_mean.values,\n",
                "         colWidths = [0.15]*len(meat_mean.columns),\n",
                "         rowLabels=meat_mean.index,\n",
                "         colLabels=meat_mean.columns,\n",
                "         loc='top')\n",
                "\n",
                "# Specify the fontsize and location of your legend\n",
                "ax.legend(loc='upper center', bbox_to_anchor=(0.5, 0.95), ncol=3, fontsize=6)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "meat.plot(subplots=True,",
                "ASSIGN=(2, 4),",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN='viridis',",
                "ASSIGN=2,",
                "ASSIGN=False,",
                "ASSIGN=0.2)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Create a facetted graph with 2 rows and 4 columns\n",
                "meat.plot(subplots=True, \n",
                "          layout=(2, 4), \n",
                "          sharex=False, \n",
                "          sharey=False, \n",
                "          colormap='viridis', \n",
                "          fontsize=2, \n",
                "          legend=False, \n",
                "          linewidth=0.2)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(meat[['beef', 'pork']].corr(method='spearman'))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the correlation matrix between the beef and pork columns using the spearman method\n",
                "print(meat[['beef', 'pork']].corr(method='spearman'))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the correlation matrix between the pork, veal and turkey columns using the pearson method\n",
                "print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = meat.corr(method='spearman')",
                "sns.heatmap(ASSIGN,",
                "ASSIGN=True,",
                "ASSIGN=0.4,",
                "ASSIGN={\"size\": 10})",
                "plt.xticks(rotation=90)",
                "plt.yticks(rotation=0)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import seaborn library\n",
                "import seaborn as sns\n",
                "\n",
                "# Get correlation matrix of the meat DataFrame\n",
                "corr_meat = meat.corr(method='spearman')\n",
                "\n",
                "\n",
                "# Customize the heatmap of the corr_meat correlation matrix\n",
                "sns.heatmap(corr_meat,\n",
                "            annot=True,\n",
                "            linewidths=0.4,\n",
                "            annot_kws={\"size\": 10})\n",
                "\n",
                "plt.xticks(rotation=90)\n",
                "plt.yticks(rotation=0) \n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = meat.corr(method = 'pearson')",
                "ASSIGN = sns.clustermap(corr_meat,",
                "ASSIGN=True,",
                "ASSIGN=True,",
                "ASSIGN=(10, 10))",
                "plt.setp(ASSIGN.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)",
                "plt.setp(ASSIGN.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Import seaborn library\n",
                "import seaborn as sns\n",
                "\n",
                "# Get correlation matrix of the meat DataFrame\n",
                "corr_meat = meat.corr(method = 'pearson')\n",
                "\n",
                "# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\n",
                "fig = sns.clustermap(corr_meat,\n",
                "                     row_cluster=True,\n",
                "                     col_cluster=True,\n",
                "                     figsize=(10, 10))\n",
                "\n",
                "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
                "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('path', parse_dates = ['datestamp'])",
                "display(ASSIGN.head(5))",
                "print(ASSIGN.dtypes)",
                "ASSIGN = pd.to_datetime(ASSIGN)",
                "ASSIGN = ASSIGN.set_index('datestamp')",
                "display(ASSIGN.isnull().sum())"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Read in jobs file\n",
                "jobs = pd.read_csv('/kaggle/input/week6dataset/employment.csv', parse_dates = ['datestamp'])\n",
                "\n",
                "# Review the first five lines of your DataFrame\n",
                "display(jobs.head(5))\n",
                "\n",
                "# Review the type of each column in your DataFrame\n",
                "print(jobs.dtypes)\n",
                "\n",
                "# Convert datestamp column to a datetime object\n",
                "jobs['datestamp'] = pd.to_datetime(jobs['datestamp'])\n",
                "\n",
                "# Set the datestamp columns as the index of your DataFrame\n",
                "jobs = jobs.set_index('datestamp')\n",
                "\n",
                "# Check the number of missing values in each column\n",
                "display(jobs.isnull().sum())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "jobs.boxplot(fontsize=6, vert=False)",
                "plt.show()",
                "display(jobs.describe())"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Generate a boxplot\n",
                "jobs.boxplot(fontsize=6, vert=False)\n",
                "plt.show()\n",
                "\n",
                "# Generate numerical summaries\n",
                "display(jobs.describe())"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]",
                "print(ASSIGN.head())",
                "ASSIGN = jobs_subset.plot(subplots=True,",
                "ASSIGN=(2,2),",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=0.7,",
                "ASSIGN=3,",
                "ASSIGN=False)",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# A subset of the jobs DataFrame\n",
                "jobs_subset = jobs[['Finance', 'Information', 'Manufacturing', 'Construction']]\n",
                "\n",
                "# Print the first 5 rows of jobs_subset\n",
                "print(jobs_subset.head())\n",
                "\n",
                "# Create a facetted graph with 2 rows and 2 columns\n",
                "ax = jobs_subset.plot(subplots=True,\n",
                "                      layout=(2,2),\n",
                "                      sharex=False,\n",
                "                      sharey=False,\n",
                "                      linewidth=0.7,\n",
                "                      fontsize=3,\n",
                "                      legend=False)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = jobs.plot(colormap = 'Spectral', fontsize=6, linewidth=0.8)",
                "ASSIGN.set_xlabel('Date', fontsize=10)",
                "ASSIGN.set_ylabel('Unemployment Rate', fontsize=10)",
                "ASSIGN.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)",
                "ASSIGN.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))",
                "ASSIGN.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)",
                "ASSIGN.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot all time series in the jobs DataFrame\n",
                "ax = jobs.plot(colormap = 'Spectral', fontsize=6, linewidth=0.8)\n",
                "\n",
                "# Set labels and legend\n",
                "ax.set_xlabel('Date', fontsize=10)\n",
                "ax.set_ylabel('Unemployment Rate', fontsize=10)\n",
                "ax.set_title('Unemployment rate of U.S. workers by industry', fontsize=10)\n",
                "ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
                "\n",
                "# Annotate your plots with vertical lines\n",
                "ax.axvline('2001-07-01', color='blue', linestyle='--', linewidth=0.8)\n",
                "ax.axvline('2008-09-01', color='blue', linestyle='--', linewidth=0.8)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = jobs.index.month",
                "ASSIGN = jobs.groupby(index_month).mean()",
                "ASSIGN = jobs_by_month.plot(fontsize=6, linewidth=1)",
                "ASSIGN.set_xlabel('Month', fontsize=10)",
                "ASSIGN.set_ylabel('Mean unemployment rate', fontsize=10)",
                "ASSIGN.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Extract the month from the index of jobs\n",
                "index_month = jobs.index.month\n",
                "\n",
                "# Compute the mean unemployment rate for each month\n",
                "jobs_by_month = jobs.groupby(index_month).mean()\n",
                "\n",
                "# Plot the mean unemployment rate for each month\n",
                "ax = jobs_by_month.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Set axis labels and legend\n",
                "ax.set_xlabel('Month', fontsize=10)\n",
                "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
                "ax.legend(bbox_to_anchor=(0.8, 0.6), fontsize=10)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = jobs.index.year",
                "ASSIGN = jobs.groupby(index_year).mean()",
                "ASSIGN = jobs_by_year.plot(fontsize=6, linewidth=1)",
                "ASSIGN.set_xlabel('Year', fontsize=10)",
                "ASSIGN.set_ylabel('Mean unemployment rate', fontsize=10)",
                "ASSIGN.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Extract of the year in each date indices of the jobs DataFrame\n",
                "index_year = jobs.index.year\n",
                "\n",
                "# Compute the mean unemployment rate for each year\n",
                "jobs_by_year = jobs.groupby(index_year).mean()\n",
                "\n",
                "# Plot the mean unemployment rate for each year\n",
                "ax = jobs_by_year.plot(fontsize=6, linewidth=1)\n",
                "\n",
                "# Set axis labels and legend\n",
                "ax.set_xlabel('Year', fontsize=10)\n",
                "ax.set_ylabel('Mean unemployment rate', fontsize=10)\n",
                "ax.legend(bbox_to_anchor=(0.1, 0.5), fontsize=10)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = jobs.columns",
                "for ts in ASSIGN:",
                "ASSIGN = sm.tsa.seasonal_decompose(jobs[ts])",
                "ASSIGN[ts] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Initialize dictionary\n",
                "jobs_decomp = {}\n",
                "\n",
                "# Get the names of each time series in the DataFrame\n",
                "jobs_names = jobs.columns\n",
                "\n",
                "# Run time series decomposition on each time series of the DataFrame\n",
                "for ts in jobs_names:\n",
                "    ts_decomposition = sm.tsa.seasonal_decompose(jobs[ts])\n",
                "    jobs_decomp[ts] = ts_decomposition"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = {}",
                "for ts in jobs_names:",
                "ASSIGN[ts] = jobs_decomp[ts].seasonal",
                "ASSIGN = pd.DataFrame(jobs_seasonal)",
                "ASSIGN.index.name = None",
                "ASSIGN.plot(subplots=True,",
                "ASSIGN=(4, 4),",
                "ASSIGN=False,",
                "ASSIGN=2,",
                "ASSIGN=0.3,",
                "ASSIGN=False)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "jobs_seasonal = {}\n",
                "\n",
                "# Extract the seasonal values for the decomposition of each time series\n",
                "for ts in jobs_names:\n",
                "    jobs_seasonal[ts] = jobs_decomp[ts].seasonal\n",
                "    \n",
                "# Create a DataFrame from the jobs_seasonal dictionnary\n",
                "seasonality_df = pd.DataFrame(jobs_seasonal)\n",
                "\n",
                "# Remove the label for the index\n",
                "seasonality_df.index.name = None\n",
                "\n",
                "# Create a faceted plot of the seasonality_df DataFrame\n",
                "seasonality_df.plot(subplots=True,\n",
                "                   layout=(4, 4),\n",
                "                   sharey=False,\n",
                "                   fontsize=2,\n",
                "                   linewidth=0.3,\n",
                "                   legend=False)\n",
                "\n",
                "# Show plot\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = seasonality_df.corr(method='spearman')",
                "ASSIGN = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))",
                "plt.setp(ASSIGN.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)",
                "plt.setp(ASSIGN.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Get correlation matrix of the seasonality_df DataFrame\n",
                "seasonality_corr = seasonality_df.corr(method='spearman')\n",
                "\n",
                "# Customize the clustermap of the seasonality_corr correlation matrix\n",
                "fig = sns.clustermap(seasonality_corr, annot=True, annot_kws={\"size\": 4}, linewidths=.4, figsize=(15, 10))\n",
                "plt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\n",
                "plt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\r",
                "from sklearn.preprocessing import StandardScaler\r",
                "import matplotlib.pyplot as plt # plotting\r",
                "import numpy as np # linear algebra\r",
                "import os # accessing directory structure\r",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(os.listdir('..path'))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\r",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "    nunique = df.nunique()\r",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\r",
                "    nRow, nCol = df.shape\r",
                "    columnNames = list(df)\r",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\r",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\r",
                "    for i in range(min(nCol, nGraphShown)):\r",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\r",
                "        columnDf = df.iloc[:, i]\r",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\r",
                "            valueCounts = columnDf.value_counts()\r",
                "            valueCounts.plot.bar()\r",
                "        else:\r",
                "            columnDf.hist()\r",
                "        plt.ylabel('counts')\r",
                "        plt.xticks(rotation = 90)\r",
                "        plt.title(f'{columnNames[i]} (column {i})')\r",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\r",
                "def plotCorrelationMatrix(df, graphWidth):",
                "    filename = df.dataframeName\r",
                "    df = df.dropna('columns') # drop columns with NaN\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    if df.shape[1] < 2:\r",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\r",
                "        return\r",
                "    corr = df.corr()\r",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\r",
                "    corrMat = plt.matshow(corr, fignum = 1)\r",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\r",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\r",
                "    plt.gca().xaxis.tick_bottom()\r",
                "    plt.colorbar(corrMat)\r",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\r",
                "def plotScatterMatrix(df, plotSize, textSize):",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\r",
                "    # Remove rows and columns that would lead to df being singular\r",
                "    df = df.dropna('columns')\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    columnNames = list(df)\r",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots",
                "        columnNames = columnNames[:10]",
                "    df = df[columnNames]\r",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\r",
                "    corrs = df.corr().values\r",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\r",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\r",
                "    plt.suptitle('Scatter and Density Plot')\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'bank.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# bank.csv has 11162 rows in reality, but we are only loading/previewing the first 1000 rows",
                "df1 = pd.read_csv('../input/bank.csv', delimiter=',', nrows = nRowsRead)",
                "df1.dataframeName = 'bank.csv'",
                "nRow, nCol = df1.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 20, 10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotScatterMatrix(df1, 20, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('path(6).csv', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'owid-covid-data (6).csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# owid-covid-data (6).csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('/kaggle/input/owid-covid-data (6).csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'owid-covid-data (6).csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 20, 10)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotScatterMatrix(df1, 20, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\r",
                "from sklearn.preprocessing import StandardScaler\r",
                "import matplotlib.pyplot as plt # plotting\r",
                "import numpy as np # linear algebra\r",
                "import os # accessing directory structure\r",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(os.listdir('..path'))",
                "print(os.listdir('..path'))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(os.listdir('../input'))",
                "print(os.listdir('../input/ida-csv-zip-350-kb-'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\r",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "    nunique = df.nunique()\r",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\r",
                "    nRow, nCol = df.shape\r",
                "    columnNames = list(df)\r",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\r",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\r",
                "    for i in range(min(nCol, nGraphShown)):\r",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\r",
                "        columnDf = df.iloc[:, i]\r",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\r",
                "            valueCounts = columnDf.value_counts()\r",
                "            valueCounts.plot.bar()\r",
                "        else:\r",
                "            columnDf.hist()\r",
                "        plt.ylabel('counts')\r",
                "        plt.xticks(rotation = 90)\r",
                "        plt.title(f'{columnNames[i]} (column {i})')\r",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\r",
                "def plotCorrelationMatrix(df, graphWidth):",
                "    filename = df.dataframeName\r",
                "    df = df.dropna('columns') # drop columns with NaN\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    if df.shape[1] < 2:\r",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\r",
                "        return\r",
                "    corr = df.corr()\r",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\r",
                "    corrMat = plt.matshow(corr, fignum = 1)\r",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\r",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\r",
                "    plt.gca().xaxis.tick_bottom()\r",
                "    plt.colorbar(corrMat)\r",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\r",
                "def plotScatterMatrix(df, plotSize, textSize):",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\r",
                "    # Remove rows and columns that would lead to df being singular\r",
                "    df = df.dropna('columns')\r",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\r",
                "    columnNames = list(df)\r",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots",
                "        columnNames = columnNames[:10]",
                "    df = df[columnNames]\r",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\r",
                "    corrs = df.corr().values\r",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\r",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\r",
                "    plt.suptitle('Scatter and Density Plot')\r",
                "    plt.show()\r",
                ""
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'IDAData.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDAData.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df1 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDAData.csv', delimiter=',', nrows = nRowsRead)",
                "df1.dataframeName = 'IDAData.csv'",
                "nRow, nCol = df1.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'IDACountry.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDACountry.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df2 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDACountry.csv', delimiter=',', nrows = nRowsRead)",
                "df2.dataframeName = 'IDACountry.csv'",
                "nRow, nCol = df2.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df2.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df2.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotPerColumnDistribution(df2, 10, 5)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('..path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'IDASeries.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file",
                "# IDASeries.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows",
                "df3 = pd.read_csv('../input/ida-csv-zip-350-kb-/IDASeries.csv', delimiter=',', nrows = nRowsRead)",
                "df3.dataframeName = 'IDASeries.csv'",
                "nRow, nCol = df3.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df3.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df3.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotPerColumnDistribution(df3, 10, 5)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 1000",
                "ASSIGN = pd.read_csv('path', delimiter=',', nrows = nRowsRead)",
                "ASSIGN.dataframeName = 'IRIS_TYPE_CLF.csv'",
                "nRow, nCol = ASSIGN.shape",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ],
            "output_type": "stream",
            "content_old": [
                "nRowsRead = 1000 # specify 'None' if want to read whole file\n",
                "# IRIS_TYPE_CLF.csv may have more rows in reality, but we are only loading/previewing the first 1000 rows\n",
                "df1 = pd.read_csv('/kaggle/input/IRIS_TYPE_CLF.csv', delimiter=',', nrows = nRowsRead)\n",
                "df1.dataframeName = 'IRIS_TYPE_CLF.csv'\n",
                "nRow, nCol = df1.shape\n",
                "print(f'There are {nRow} rows and {nCol} columns')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df1.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df1.head(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotPerColumnDistribution(df1, 10, 5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotCorrelationMatrix(df1, 8)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotCorrelationMatrix(df1, 8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotScatterMatrix(df1, 12, 10)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plotScatterMatrix(df1, 12, 10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(os.listdir('..path'))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "print(\"Setup Complete\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "binder.bind(globals())",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Set up code checking\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.data_viz_to_coder.ex2 import *\n",
                "print(\"Setup Complete\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "museum_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Print the last five rows of the data \n",
                "museum_data.head() # Your code here"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "museum_data.tail()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Last 5 rows of the data\n",
                "museum_data.tail()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "list(museum_data.columns)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "list(museum_data.columns)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def count_unique_values(df) :",
                "ASSIGN = list(df.columns)",
                "print('Count unique values :')",
                "for i in ASSIGN :",
                "ASSIGN = len(df[i].unique())",
                "print(i,':',ASSIGN)",
                "def check_missing_values(df) :",
                "ASSIGN = len(df)",
                "ASSIGN = list(df.columns)",
                "ASSIGN = []",
                "ASSIGN = []",
                "print('Variable with missing values :')",
                "for i in ASSIGN :",
                "ASSIGN = np.sum(df[i].isna())",
                "ASSIGN = round(count*100path, 2)",
                "if ASSIGN > 0 :",
                "print(i,':',ASSIGN,'path',ASSIGN,'%')",
                "ASSIGN.append(i)",
                "ASSIGN.append(ASSIGN)",
                "return missing_var, missing_count",
                "def stepwise_selection(X, y,",
                "ASSIGN=[],",
                "ASSIGN=0.01,",
                "ASSIGN = 0.05,",
                "ASSIGN=True):",
                "ASSIGN = list(initial_list)",
                "while True:",
                "ASSIGN=False",
                "ASSIGN = list(set(X.columns)-set(included))",
                "ASSIGN = pd.Series(index=excluded)",
                "for new_column in ASSIGN:",
                "ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))",
                ",family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "ASSIGN[new_column] = ASSIGN.pvalues[new_column]",
                "ASSIGN = new_pval.min()",
                "if ASSIGN < ASSIGN:",
                "ASSIGN = new_pval.argmin()",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN=True",
                "if ASSIGN:",
                "print('Add {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN))",
                "ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))",
                ",family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "ASSIGN = model.ASSIGN.iloc[1:]",
                "ASSIGN = pvalues.max()",
                "if ASSIGN > ASSIGN:",
                "ASSIGN=True",
                "ASSIGN = pvalues.argmax()",
                "ASSIGN.remove(ASSIGN)",
                "if ASSIGN:",
                "print('Drop {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN))",
                "if not ASSIGN:",
                "break",
                "return included",
                "def dataset_ready(x_train, y_train) :",
                "ASSIGN = pd.get_dummies(x_train)",
                "ASSIGN = [1]*len(X)",
                "ASSIGN['gdp_pop'] = np.log(ASSIGN['gdp_per_capita']*ASSIGN['population'])",
                "ASSIGN = ['gdp_per_capita','population']",
                "for i in ASSIGN :",
                "ASSIGN = np.log(ASSIGN)",
                "ASSIGN = pd.Series(X.columns)",
                "ASSIGN = list(X.filter(like='continent').columns)",
                "for i in ASSIGN :",
                "ASSIGN = i+'_gdp'",
                "ASSIGN = ASSIGN*ASSIGN",
                "for i in ASSIGN :",
                "ASSIGN = i+'_population'",
                "ASSIGN = ASSIGN*ASSIGN",
                "ASSIGN = y_train",
                "return X,Y"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Function used in this notebook",
                "def count_unique_values(df) :",
                "    var = list(df.columns)",
                "    print('Count unique values :')",
                "    ",
                "    for i in var :",
                "        count = len(df[i].unique())",
                "        print(i,':',count)",
                "",
                "def check_missing_values(df) :",
                "    n = len(df)",
                "    var = list(df.columns)",
                "    missing_var = []",
                "    missing_count = []",
                "    print('Variable with missing values :')",
                "    ",
                "    for i in var :",
                "        count = np.sum(df[i].isna())",
                "        count_percentage = round(count*100/n, 2)",
                "        if count > 0 :",
                "            print(i,':',count,'//',count_percentage,'%')",
                "            missing_var.append(i)",
                "            missing_count.append(count_percentage)",
                "    ",
                "    return missing_var, missing_count",
                "",
                "def stepwise_selection(X, y, ",
                "                       initial_list=[], ",
                "                       threshold_in=0.01, ",
                "                       threshold_out = 0.05, ",
                "                       verbose=True):",
                " ",
                "    included = list(initial_list)",
                "    while True:",
                "        changed=False",
                "        # forward step",
                "        excluded = list(set(X.columns)-set(included))",
                "        new_pval = pd.Series(index=excluded)",
                "        for new_column in excluded:",
                "            model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))",
                "                                ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "            new_pval[new_column] = model.pvalues[new_column]",
                "        best_pval = new_pval.min()",
                "        if best_pval < threshold_in:",
                "            best_feature = new_pval.argmin()",
                "            included.append(best_feature)",
                "            changed=True",
                "            if verbose:",
                "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))",
                "",
                "        # backward step",
                "        model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))",
                "                            ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()",
                "        # use all coefs except intercept",
                "        pvalues = model.pvalues.iloc[1:]",
                "        worst_pval = pvalues.max() # null if pvalues is empty",
                "        if worst_pval > threshold_out:",
                "            changed=True",
                "            worst_feature = pvalues.argmax()",
                "            included.remove(worst_feature)",
                "            if verbose:",
                "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))",
                "        if not changed:",
                "            break",
                "    return included",
                "",
                "def dataset_ready(x_train, y_train) :",
                "    # Make dummy variable for categorical variable",
                "    X = pd.get_dummies(x_train)",
                "",
                "    # Make Intercept",
                "    X['Intercept'] = [1]*len(X)",
                "",
                "    # Make interaction between 'gdp_per_capita' and 'population'",
                "    X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])",
                "",
                "    # Scale continuous variable with log function",
                "    cont_var = ['gdp_per_capita','population']",
                "    for i in cont_var :",
                "        X[i] = np.log(X[i])",
                "",
                "    # Make interaction between 'continent' and 'gdp'",
                "    col = pd.Series(X.columns)",
                "    var1 = list(X.filter(like='continent').columns)",
                "    for i in var1 :",
                "        string = i+'_gdp'",
                "        X[string] = X[i]*X['gdp_per_capita']   ",
                "",
                "    # Make interaction between 'continent' and 'population'",
                "    for i in var1 :",
                "        string = i+'_population'",
                "        X[string] = X[i]*X['population']  ",
                "",
                "    # Target variable",
                "    Y = y_train",
                "    ",
                "    return X,Y",
                "",
                "# I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "rcParams['figure.figsize'] = [10,5]",
                "warnings.filterwarnings('ignore')",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "sns.set()",
                "sns.set_style('whitegrid')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Load and configure notebook settings",
                "import pandas as pd",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.gridspec as gridspec",
                "import seaborn as sns",
                "import statsmodels.api as sm",
                "%matplotlib inline",
                "",
                "from matplotlib.pylab import rcParams",
                "# For every plotting cell use this",
                "rcParams['figure.figsize'] = [10,5]",
                "",
                "import warnings",
                "warnings.filterwarnings('ignore')",
                "",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "",
                "sns.set()",
                "sns.set_style('whitegrid')"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Load dataset",
                "df_train = pd.read_csv('../input/master.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Overview of the dataset",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train.rename(columns={' gdp_for_year ($) ':'gdp_for_year', 'gdp_per_capita ($)':'gdp_per_capita'}, inplace=True)",
                "ASSIGN = df_train.copy()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Change some variable name",
                "df_train.rename(columns={' gdp_for_year ($) ':'gdp_for_year', 'gdp_per_capita ($)':'gdp_per_capita'}, inplace=True)",
                "df_train_v2 = df_train.copy()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Summary of the dataset",
                "df_train.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Data types of the dataset :')",
                "print(df_train.dtypes)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Overview data types of the dataset",
                "print('Data types of the dataset :')",
                "print(df_train.dtypes)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "for i in df_train['gdp_for_year'] :",
                "ASSIGN = i.ASSIGN(',')",
                "ASSIGN = ''",
                "for j in ASSIGN :",
                "ASSIGN = ASSIGN + j",
                "ASSIGN.append(int(ASSIGN))",
                "df_train['gdp_for_year'] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Change 'gdp_for_year' data type",
                "change_var = []",
                "for i in df_train['gdp_for_year'] :",
                "    split = i.split(',')",
                "    val = ''",
                "    for j in split :",
                "        val = val + j",
                "    change_var.append(int(val))",
                "    ",
                "df_train['gdp_for_year'] = change_var"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [10,5]",
                "sns.heatmap(df_train.corr(), annot=True, linewidths=0.2, cmap='coolwarm' )",
                "plt.title('Correlation heatmap of the dataset', size=15, fontweight='bold') ;",
                "plt.xticks(rotation=45)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# See the correlation between the variables",
                "rcParams['figure.figsize'] = [10,5]",
                "sns.heatmap(df_train.corr(), annot=True, linewidths=0.2, cmap='coolwarm' )",
                "plt.title('Correlation heatmap of the dataset', size=15, fontweight='bold') ;",
                "plt.xticks(rotation=45)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = check_missing_values(df_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Check for missing values in the dataset",
                "missing_var, missing_count = check_missing_values(df_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train_v2.drop(columns=['country-year','HDI for year','gdp_for_year'], inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Remove unwanted variable",
                "df_train_v2.drop(columns=['country-year','HDI for year','gdp_for_year'], inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['country','year','sex','age','generation']",
                "count_unique_values(df_train[ASSIGN])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Check how many unique values in categorical variable",
                "category_var = ['country','year','sex','age','generation']",
                "count_unique_values(df_train[category_var])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [15,5]",
                "ASSIGN = gridspec.GridSpec(1,2)",
                "ASSIGN = plt.subplot(gs[0,0])",
                "ASSIGN = plt.subplot(gs[0,1])",
                "sns.distplot(df_train['suicides_no'], color='",
                "ASSIGN.set_title('Distribution of the suicides_no', size=15, fontweight='bold') ;",
                "sns.distplot(np.log(df_train[df_train['suicides_no']>0]['suicides_no']), color='",
                "ASSIGN.set_title('Distribution of the log of population', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Check the distribution of target variable 'suicides_no'",
                "rcParams['figure.figsize'] = [15,5]",
                "gs = gridspec.GridSpec(1,2)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "",
                "# Plot 1 - Distribution of the target variable",
                "sns.distplot(df_train['suicides_no'], color='#7f181b', kde=True, hist=False, ax=ax1) ;",
                "ax1.set_title('Distribution of the suicides_no', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Distribution of the log(target variable)",
                "sns.distplot(np.log(df_train[df_train['suicides_no']>0]['suicides_no']), color='#7f181b', kde=True, hist=True, ax=ax2) ;",
                "ax2.set_title('Distribution of the log of population', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_train_v2['ASSIGN'].unique()",
                "ASSIGN = ['Europe','Central America','South America','Asia','Central America'",
                ",'Australia','Europe','Asia','Central America','Asia'",
                ",'Central America','Europe','Europe','Central America'",
                ",'Europe','South America','Europe','Africa'",
                ",'North America','South America','South America','Central America','Europe','Central America'",
                ",'Asia','Europe','Europe','Central America','South America'",
                ",'Central America','Europe','Oceania','Europe','Europe','Asia'",
                ",'Europe','Europe','Central America','Central America','South America','Europe'",
                ",'Europe','Europe','Asia','Europe','Central America','Asia'",
                ",'Asia','Oceania','Asia','Asia','Europe'",
                ",'Europe','Europe','Asia','Asia','Europe'",
                ",'Africa','North America','Asia','Europe','Europe'",
                ",'Oceania','Central America','Europe','Asia','Central America','South America'",
                ",'Asia','Europe','Europe','Central America','Asia'",
                ",'Asia','Europe','Europe'",
                ",'Central America','Central America'",
                ",'Central America','Europe','Europe'",
                ",'Africa','Asia','Europe','Europe','Africa'",
                ",'Europe','Asia','South America','Europe','Europe'",
                ",'Asia','Central America','Asia','Asia'",
                ",'Europe','Asia','Europe'",
                ",'North America','South America','Asia']",
                "ASSIGN = []",
                "for i in range(len(ASSIGN)) :",
                "ASSIGN = len(df_train[df_train['country']==country[i]])",
                "for j in range(ASSIGN) :",
                "ASSIGN.append(ASSIGN[i])",
                "df_train_v2['continent'] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Make new variable 'continent' that represent continent of each country",
                "# Based on wikipedia.com",
                "country = df_train_v2['country'].unique()",
                "new_val = ['Europe','Central America','South America','Asia','Central America'",
                "          ,'Australia','Europe','Asia','Central America','Asia'",
                "          ,'Central America','Europe','Europe','Central America'",
                "          ,'Europe','South America','Europe','Africa'",
                "          ,'North America','South America','South America','Central America','Europe','Central America'",
                "          ,'Asia','Europe','Europe','Central America','South America'",
                "          ,'Central America','Europe','Oceania','Europe','Europe','Asia'",
                "          ,'Europe','Europe','Central America','Central America','South America','Europe'",
                "          ,'Europe','Europe','Asia','Europe','Central America','Asia'",
                "          ,'Asia','Oceania','Asia','Asia','Europe'",
                "          ,'Europe','Europe','Asia','Asia','Europe'",
                "          ,'Africa','North America','Asia','Europe','Europe'",
                "          ,'Oceania','Central America','Europe','Asia','Central America','South America'",
                "          ,'Asia','Europe','Europe','Central America','Asia'",
                "          ,'Asia','Europe','Europe'",
                "          ,'Central America','Central America'",
                "          ,'Central America','Europe','Europe'",
                "          ,'Africa','Asia','Europe','Europe','Africa'",
                "          ,'Europe','Asia','South America','Europe','Europe'",
                "          ,'Asia','Central America','Asia','Asia'",
                "          ,'Europe','Asia','Europe'",
                "          ,'North America','South America','Asia']",
                "new_var = []",
                "",
                "for i in range(len(country)) :",
                "    n = len(df_train[df_train['country']==country[i]])",
                "    for j in range(n) :",
                "        new_var.append(new_val[i])",
                "        ",
                "df_train_v2['continent'] = new_var"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df_train_v2.groupby(by=['country','continent']).median()[['suicides_no','population','gdp_per_capita']].sort_values('suicides_no',ascending=False).reset_index()",
                "ASSIGN = list(df_check.head(10)['country'])",
                "print('Top 10 country with highest suicide median ')",
                "print(ASSIGN.head(10))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Top 10 country with highest suicide median",
                "# We use median because the distribution is skewed to the right",
                "df_check = df_train_v2.groupby(by=['country','continent']).median()[['suicides_no','population','gdp_per_capita']].sort_values('suicides_no',ascending=False).reset_index()",
                "cat = list(df_check.head(10)['country'])",
                "print('Top 10 country with highest suicide median ')",
                "print(df_check.head(10))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train.groupby(by=['country','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN = plt.subplot(gs[0,0])",
                "ASSIGN = plt.subplot(gs[1,0])",
                "for i in cat[:3] :",
                "sns.lineplot(data=ASSIGN[ASSIGN['country']==i], x='year', y='suicides_no', ax=ASSIGN) ;",
                "ASSIGN.legend(cat[:3], loc=7, bbox_to_anchor=(1.3, 0.5)) ;",
                "ASSIGN.set_title('Number of suicide growth of top 3 country', size=15, fontweight='bold') ;",
                "for i in cat[3:] :",
                "sns.lineplot(data=ASSIGN[ASSIGN['country']==i], x='year', y='suicides_no', ax=ASSIGN) ;",
                "ASSIGN.legend(cat[3:], loc=7, bbox_to_anchor=(1.3, 0.5)) ;",
                "ASSIGN.set_xlabel('Number of suicide growth of reminding country', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Top 10 country number of suicides growth year by year",
                "# We use median because the distribution is skewed to the right",
                "df_check = df_train.groupby(by=['country','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "gs = gridspec.GridSpec(2,1)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[1,0])",
                "",
                "# Plot 1 - Line plot for top 3 country",
                "for i in cat[:3] :",
                "    sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax1) ;",
                "    ",
                "ax1.legend(cat[:3], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax1.set_title('Number of suicide growth of top 3 country', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Line plot for reminding country",
                "for i in cat[3:] :",
                "    sns.lineplot(data=df_check[df_check['country']==i], x='year', y='suicides_no', ax=ax2) ;",
                "    ",
                "ax2.legend(cat[3:], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax2.set_xlabel('Number of suicide growth of reminding country', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = list(df_train_v2['ASSIGN'].unique())",
                "ASSIGN = pd.Series(ASSIGN)",
                "ASSIGN = []",
                "print('Count contry in each ASSIGN recorded in the dataset :')",
                "for i in ASSIGN :",
                "ASSIGN = len(new_val[new_val==i])",
                "ASSIGN.append(ASSIGN)",
                "print(i,':',ASSIGN)",
                "ASSIGN = pd.DataFrame({'continent':continent, 'count':count})",
                "ASSIGN.sort_values(by='ASSIGN', ascending=False, inplace=True)",
                "sns.catplot(data=ASSIGN, x='ASSIGN', y='ASSIGN') ;",
                "plt.xticks(rotation=45)",
                "plt.title('How many country in each ASSIGN', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Count how many country in each continent recorded in the dataset",
                "continent = list(df_train_v2['continent'].unique())",
                "new_val = pd.Series(new_val)",
                "count = []",
                "",
                "print('Count contry in each continent recorded in the dataset :')",
                "for i in continent :",
                "    n = len(new_val[new_val==i])",
                "    count.append(n)",
                "    print(i,':',n)",
                " ",
                "#  Plot",
                "df = pd.DataFrame({'continent':continent, 'count':count})",
                "df.sort_values(by='count', ascending=False, inplace=True)",
                "sns.catplot(data=df, x='continent', y='count') ;",
                "plt.xticks(rotation=45)",
                "plt.title('How many country in each continent', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train_v2.groupby(by=['continent','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN = plt.subplot(gs[0,0])",
                "ASSIGN = plt.subplot(gs[1,0])",
                "sns.lineplot(data=ASSIGN[ASSIGN['continent']=='North America'], x='year', y='suicides_no', ax=ASSIGN ) ;",
                "ASSIGN.legend(['North America'], loc=7, bbox_to_anchor=(1.3, 0.5)) ;",
                "ASSIGN.set_title('Number of suicide growth of North America', size=15, fontweight='bold') ;",
                "ASSIGN = pd.Series(ASSIGN)",
                "for i in ASSIGN[ASSIGN!='North America'] :",
                "sns.lineplot(data=ASSIGN[ASSIGN['ASSIGN']==i], x='year', y='suicides_no', ax=ASSIGN) ;",
                "ASSIGN.legend(ASSIGN[ASSIGN!='North America'], loc=7, bbox_to_anchor=(1.3, 0.5)) ;",
                "ASSIGN.set_xlabel('Number of suicide growth of reminding ASSIGN', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Number of suicides growth in each continent",
                "df_check = df_train_v2.groupby(by=['continent','year']).median()['suicides_no'].reset_index()",
                "rcParams['figure.figsize'] = [10,6]",
                "gs = gridspec.GridSpec(2,1)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[1,0])",
                "",
                "# Plot 1 - Line plot for North America",
                "sns.lineplot(data=df_check[df_check['continent']=='North America'], x='year', y='suicides_no', ax=ax1 ) ;",
                "ax1.legend(['North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax1.set_title('Number of suicide growth of North America', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Line plot for reminding continent",
                "continent = pd.Series(continent)",
                "for i in continent[continent!='North America'] :",
                "    sns.lineplot(data=df_check[df_check['continent']==i], x='year', y='suicides_no', ax=ax2) ;",
                "    ",
                "ax2.legend(continent[continent!='North America'], loc=7,  bbox_to_anchor=(1.3, 0.5)) ;",
                "ax2.set_xlabel('Number of suicide growth of reminding continent', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [15,5]",
                "ASSIGN = gridspec.GridSpec(1,2)",
                "ASSIGN = plt.subplot(gs[0,0])",
                "ASSIGN = plt.subplot(gs[0,1])",
                "sns.barplot(data=df_train_v2, x='sex', y='suicides_no', hue='age', ax=ASSIGN",
                ",hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ASSIGN.set_title('Disrtibution of suicide count by sex', size=15, fontweight='bold') ;",
                "sns.barplot(data=df_train_v2, x='sex', y='suicidespath', hue='age', ax=ASSIGN",
                ",hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ASSIGN.set_title('Disrtibution of suicide count (rescale) by sex ', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Check the effect of gender and sex",
                "rcParams['figure.figsize'] = [15,5]",
                "gs = gridspec.GridSpec(1,2)",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "",
                "# Plot 1 - based on",
                "sns.barplot(data=df_train_v2, x='sex', y='suicides_no', hue='age', ax=ax1",
                "           ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ax1.set_title('Disrtibution of suicide count by sex', size=15, fontweight='bold') ;",
                "",
                "# Plot 2",
                "sns.barplot(data=df_train_v2, x='sex', y='suicides/100k pop', hue='age', ax=ax2",
                "           ,hue_order=['5-14 years','15-24 years','25-34 years','35-54 years','55-74 years','75+ years']) ;",
                "ax2.set_title('Disrtibution of suicide count (rescale) by sex ', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [16,5]",
                "ASSIGN = gridspec.GridSpec(1,3, width_ratios=[2,8,6])",
                "ASSIGN = plt.subplot(gs[0,0])",
                "ASSIGN = plt.subplot(gs[0,1])",
                "ASSIGN = plt.subplot(gs[0,2])",
                "sns.barplot(data=df_train_v2[df_train_v2['continent']=='North America'], x='continent', y='suicides_no', ax=ASSIGN",
                ",hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z'], hue='generation',) ;",
                "ASSIGN.get_legend().remove()",
                "ASSIGN.set_title('(NA)', size=15, fontweight='bold') ;",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Europe','South America','Asia','Australia'])]",
                ", x='continent', y='suicides_no'",
                ",hue='generation', ax=ax2",
                ",hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ASSIGN.set_title('Suicide count by generation (E,SA,AS,AUS)', size=15, fontweight='bold') ;",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Central America','Oceania','Africa'])]",
                ", x='continent', y='suicides_no'",
                ",hue='generation', ax=ax3",
                ",hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ASSIGN.get_legend().remove()",
                "ASSIGN.set_title('(CA,O,AF)', size=15, fontweight='bold') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Check the effect of variable 'generation'",
                "rcParams['figure.figsize'] = [16,5]",
                "gs = gridspec.GridSpec(1,3, width_ratios=[2,8,6])",
                "ax1 = plt.subplot(gs[0,0])",
                "ax2 = plt.subplot(gs[0,1])",
                "ax3 = plt.subplot(gs[0,2])",
                "",
                "# Plot 1 - North America perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent']=='North America'], x='continent', y='suicides_no', ax=ax1",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z'], hue='generation',) ;",
                "ax1.get_legend().remove()",
                "ax1.set_title('(NA)', size=15, fontweight='bold') ;",
                "",
                "# Plot 2 - Europe, South America, Asia, Australia perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Europe','South America','Asia','Australia'])]",
                "            , x='continent', y='suicides_no'",
                "            ,hue='generation', ax=ax2",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ax2.set_title('Suicide count by generation (E,SA,AS,AUS)', size=15, fontweight='bold') ;",
                "",
                "# Plot 3 - Central America, Oceania, Africa perspective",
                "sns.barplot(data=df_train_v2[df_train_v2['continent'].isin(['Central America','Oceania','Africa'])]",
                "            , x='continent', y='suicides_no'",
                "            ,hue='generation', ax=ax3",
                "           ,hue_order=['G.I. Generation','Silent','Boomers','Generation X','Milennials','Generation Z']) ;",
                "ax3.get_legend().remove()",
                "ax3.set_title('(CA,O,AF)', size=15, fontweight='bold') ;"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.Series(df_train_v2.columns)",
                "ASSIGN = df_train_v2[df_train_v2>0].dropna()",
                "ASSIGN = dummy[~dummy.isin(['country','suicides_no','suicidespath'])]",
                "ASSIGN = 'suicidespath'",
                "ASSIGN = train_test_split(df_train_v3[x], df_train_v3[y], test_size=0.2, random_state=11)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Split train and validation set",
                "from sklearn.model_selection import train_test_split",
                "dummy = pd.Series(df_train_v2.columns)",
                "df_train_v3 = df_train_v2[df_train_v2>0].dropna()",
                "",
                "x = dummy[~dummy.isin(['country','suicides_no','suicides/100k pop'])]",
                "y = 'suicides/100k pop'",
                "                      ",
                "x_train, x_valid, y_train, y_valid = train_test_split(df_train_v3[x], df_train_v3[y], test_size=0.2, random_state=11)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X, Y = dataset_ready(x_train, y_train)",
                "X2, Y2 = dataset_ready(x_valid, y_valid)",
                "ASSIGN = stepwise_selection(X,Y, list(X.columns))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Preparing dataset",
                "X, Y = dataset_ready(x_train, y_train)",
                "X2, Y2 = dataset_ready(x_valid, y_valid)",
                "",
                "best_var = stepwise_selection(X,Y, list(X.columns))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = sm.genmod.GLM(endog=Y, exog=X[best_var]",
                ",family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log))",
                "ASSIGN = GLM_gamma.fit()",
                "print(ASSIGN.summary())",
                "print('Model AIC :',ASSIGN.aic)",
                "print('Model BIC :',ASSIGN.bic)",
                "print('Model deviance :',ASSIGN.deviance)",
                "print('Model RMSE :',rmse(ASSIGN.predict(X2[best_var]),Y2))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Generalize Linear Model - Gamma distribution",
                "from statsmodels.tools.eval_measures import rmse",
                "import statsmodels.api as sm",
                "GLM_gamma = sm.genmod.GLM(endog=Y, exog=X[best_var]",
                "                            ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log))",
                "GLM_result = GLM_gamma.fit()",
                "print(GLM_result.summary())",
                "print('Model AIC :',GLM_result.aic)",
                "print('Model BIC :',GLM_result.bic)",
                "print('Model deviance :',GLM_result.deviance)",
                "print('Model RMSE :',rmse(GLM_result.predict(X2[best_var]),Y2))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.Series(df_train_v3.columns)",
                "ASSIGN = var[~var.isin(['country','suicides_no','suicidespath'])]",
                "ASSIGN = 'suicidespath'",
                "ASSIGN = df_train_v3[x]",
                "ASSIGN = df_train_v3[y]",
                "ASSIGN = len(X3)",
                "ASSIGN = len(Y3)",
                "ASSIGN.loc[ASSIGN+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America']",
                "ASSIGN.loc[ASSIGN+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America']",
                "ASSIGN.loc[ASSIGN+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America']",
                "ASSIGN.loc[ASSIGN+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America']",
                "ASSIGN.loc[ASSIGN+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America']",
                "ASSIGN.loc[ASSIGN+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America']",
                "ASSIGN.loc[ASSIGN+1] = 26.95",
                "ASSIGN.loc[ASSIGN+2] = 6.75",
                "ASSIGN.loc[ASSIGN+3] = 28.35",
                "ASSIGN.loc[ASSIGN+4] = 9.46",
                "ASSIGN.loc[ASSIGN+5] = 21.06",
                "ASSIGN.loc[ASSIGN+6] = 5.42",
                "ASSIGN,ASSIGN = dataset_ready(ASSIGN, ASSIGN)",
                "ASSIGN = ASSIGN.loc[nx+1:nx+6]",
                "ASSIGN = ASSIGN.loc[ny+1:nx+6]",
                "ASSIGN = GLM_result.ASSIGN(X3[best_var])",
                "for i in range(len(ASSIGN)) :",
                "print('Option',i+1)",
                "print('Predicted suicide rates :',round(ASSIGN.iloc[i],2))",
                "print('Actual suicide rates :',ASSIGN.iloc[i])",
                "print('')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Preparing prediction",
                "var = pd.Series(df_train_v3.columns)",
                "x = var[~var.isin(['country','suicides_no','suicides/100k pop'])]",
                "y = 'suicides/100k pop'",
                "",
                "X3 = df_train_v3[x]",
                "Y3 = df_train_v3[y]",
                "",
                "# Input data for prediction",
                "nx = len(X3)",
                "ny = len(Y3)",
                "X3.loc[nx+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America']",
                "X3.loc[nx+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America']",
                "X3.loc[nx+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America']",
                "X3.loc[nx+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America']",
                "Y3.loc[ny+1] = 26.95",
                "Y3.loc[ny+2] = 6.75",
                "Y3.loc[ny+3] = 28.35",
                "Y3.loc[ny+4] = 9.46",
                "Y3.loc[ny+5] = 21.06",
                "Y3.loc[ny+6] = 5.42",
                "",
                "# Tranform the data to be ready for precition",
                "X3,Y3 = dataset_ready(X3, Y3)",
                "X3 = X3.loc[nx+1:nx+6]",
                "Y3 = Y3.loc[ny+1:nx+6]",
                "",
                "# Predict",
                "predict = GLM_result.predict(X3[best_var])",
                "for i in range(len(predict)) :",
                "    print('Option',i+1)",
                "    print('Predicted suicide rates :',round(predict.iloc[i],2))",
                "    print('Actual suicide rates :',Y3.iloc[i])",
                "    print('')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "warnings.filterwarnings('ignore')",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "pd.options.display.float_format = '{:.5f}'.format",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Setting package umum ",
                "import pandas as pd",
                "import pandas_profiling as pp",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.gridspec as gridspec",
                "import seaborn as sns",
                "from tqdm import tqdm_notebook as tqdm",
                "import time",
                "import tensorflow as tf",
                "%matplotlib inline",
                "",
                "from matplotlib.pylab import rcParams",
                "# For every plotting cell use this",
                "# grid = gridspec.GridSpec(n_row,n_col)",
                "# ax = plt.subplot(grid[i])",
                "# fig, axes = plt.subplots()",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "",
                "import warnings",
                "warnings.filterwarnings('ignore')",
                "from tqdm import tqdm",
                "",
                "pd.set_option('display.max_rows', 50)",
                "pd.set_option('display.max_columns', 50)",
                "pd.options.display.float_format = '{:.5f}'.format",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential([Dense(units=1, input_shape=[1])])",
                "ASSIGN.compile(optimizer='sgd', loss='mean_squared_error')",
                "ASSIGN = np.array([-1, 0, 1, 2, 3, 4], dtype=float)",
                "ASSIGN = np.array([-3, -1, 1, 3, 5, 7], dtype=float)",
                "ASSIGN.fit(ASSIGN, ASSIGN, epochs=500)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Try Keras",
                "from keras import Sequential",
                "from keras.layers import Dense",
                "",
                "# Define model",
                "model = Sequential([Dense(units=1, input_shape=[1])])",
                "",
                "# Compile model",
                "model.compile(optimizer='sgd', loss='mean_squared_error')",
                "",
                "# Define data",
                "xs = np.array([-1, 0, 1, 2, 3, 4], dtype=float)",
                "ys = np.array([-3, -1, 1, 3, 5, 7], dtype=float)",
                "",
                "# Train model",
                "model.fit(xs, ys, epochs=500)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.predict([6])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Predict",
                "model.predict([6])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units=1, input_shape=[1]))",
                "ASSIGN.compile(optimizer='sgd', loss='mean_squared_error')",
                "ASSIGN = np.array([0,1,2,3,4,5,6], dtype=float)",
                "ASSIGN = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)",
                "ASSIGN.fit(ASSIGN, ASSIGN, epochs=500)",
                "ASSIGN.predict([7])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### House price",
                "from keras import Sequential",
                "from keras.layers import Dense",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Dense(units=1, input_shape=[1]))",
                "",
                "# Compile model",
                "model.compile(optimizer='sgd', loss='mean_squared_error')",
                "",
                "# Define data",
                "xs = np.array([0,1,2,3,4,5,6], dtype=float)",
                "ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)",
                "",
                "# Train model",
                "model.fit(xs, ys, epochs=500)",
                "",
                "# Predict",
                "model.predict([7])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "(train_img, train_lab), (test_img, test_lab) = fashion_mnist.load_data()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load data",
                "from keras.datasets import fashion_mnist",
                "(train_img, train_lab), (test_img, test_lab) = fashion_mnist.load_data()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.imshow(train_img[11]) ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot one image",
                "plt.imshow(train_img[11]) ;"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN path",
                "ASSIGN = ASSIGN path"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Normalize data",
                "train_img = train_img / 225",
                "test_img = test_img / 225"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = time.time()",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "def __init__(self, threshold) :",
                "self.thres = threshold",
                "def on_epoch_end(self, epoch, logs={}) :",
                "if (logs.get('accuracy')>self.thres) :",
                "print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "self.model.stop_training=True",
                "ASSIGN = EarlyStop(0.9)",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Flatten(input_shape=(28,28)))",
                "ASSIGN.add(Dense(512, activation=tf.nn.relu))",
                "ASSIGN.add(Dense(512, activation=tf.nn.relu))",
                "ASSIGN.add(Dense(10, activation=tf.nn.softmax))",
                "ASSIGN.compile(optimizer=tf.optimizers.Adam(),",
                "ASSIGN='sparse_categorical_crossentropy',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(train_img, train_lab, epochs=10, ASSIGN=[ASSIGN])",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make keras model",
                "import tensorflow as tf",
                "from keras import Sequential",
                "from keras.layers import Flatten,Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.9)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Flatten(input_shape=(28,28)))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(10, activation=tf.nn.softmax))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.optimizers.Adam(),",
                "              loss='sparse_categorical_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.evaluate(test_img, test_lab)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Evaluate model",
                "model.evaluate(test_img, test_lab)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = model.predict(test_img)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Predict ",
                "pred = model.predict(test_img)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "np.argmax(pred[0])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### See the predicted label",
                "np.argmax(pred[0])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "(train_img, train_lab), (test_img, test_lab) = mnist.load_data()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load data",
                "from keras.datasets import mnist",
                "(train_img, train_lab), (test_img, test_lab) = mnist.load_data()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN path",
                "ASSIGN = ASSIGN path"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Normalize data",
                "train_img = train_img / 225",
                "test_img = test_img / 225"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(train_img.shape)",
                "print(len(set(train_lab)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### See the shape of the data",
                "print(train_img.shape)",
                "print(len(set(train_lab)))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = time.time()",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "def __init__(self, threshold) :",
                "self.thres = threshold",
                "def on_epoch_end(self, epoch, logs={}) :",
                "if (logs.get('accuracy')>self.thres) :",
                "print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "self.model.stop_training=True",
                "ASSIGN = EarlyStop(0.99)",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Flatten(input_shape=(28,28)))",
                "ASSIGN.add(Dense(512, activation=tf.nn.relu))",
                "ASSIGN.add(Dense(512, activation=tf.nn.relu))",
                "ASSIGN.add(Dense(10, activation=tf.nn.softmax))",
                "ASSIGN.compile(optimizer=tf.optimizers.Adam(),",
                "ASSIGN='sparse_categorical_crossentropy',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(train_img, train_lab, epochs=10, ASSIGN=[ASSIGN])",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make keras model",
                "import tensorflow as tf",
                "from keras import Sequential",
                "from keras.layers import Flatten,Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.99)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Flatten(input_shape=(28,28)))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(512, activation=tf.nn.relu))",
                "model.add(Dense(10, activation=tf.nn.softmax))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.optimizers.Adam(),",
                "              loss='sparse_categorical_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "model.fit(train_img, train_lab, epochs=10, callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Download the data",
                "!wget --no-check-certificate \\",
                "  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\",
                "  -O /tmp/cats_and_dogs_filtered.zip"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = time.time()",
                "ASSIGN = 'path'",
                "ASSIGN = zipfile.ZipFile(data_zip, 'r')",
                "ASSIGN.extractall()",
                "ASSIGN.close()",
                "print('Unzip Data Completed')",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Unzip the data",
                "import zipfile",
                "import time",
                "",
                "start = time.time()",
                "",
                "data_zip = '/tmp/cats_and_dogs_filtered.zip'",
                "data_ref = zipfile.ZipFile(data_zip, 'r')",
                "data_ref.extractall()",
                "data_ref.close()",
                "print('Unzip Data Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'path'",
                "ASSIGN = os.path.join(base_dir, 'train')",
                "ASSIGN = os.path.join(base_dir, 'validation')",
                "ASSIGN = os.path.join(train_dir, 'cats')",
                "ASSIGN = os.path.join(train_dir, 'dogs')",
                "ASSIGN = os.path.join(val_dir, 'cats')",
                "ASSIGN = os.path.join(val_dir, 'dogs')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Set directory path for data train and validation",
                "import os",
                "base_dir = '/kaggle/working/cats_and_dogs_filtered'",
                "train_dir = os.path.join(base_dir, 'train')",
                "val_dir = os.path.join(base_dir, 'validation')",
                "",
                "# Directory with our training cat/dog pictures",
                "train_cats_dir = os.path.join(train_dir, 'cats')",
                "train_dogs_dir = os.path.join(train_dir, 'dogs')",
                "",
                "# Directory with our validation cat/dog pictures",
                "val_cats_dir = os.path.join(val_dir, 'cats')",
                "val_dogs_dir = os.path.join(val_dir, 'dogs')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = os.listdir(train_cats_dir)",
                "ASSIGN = os.listdir(train_dogs_dir)",
                "ASSIGN = os.listdir(val_cats_dir)",
                "ASSIGN = os.listdir(val_dogs_dir)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get the name of the file for train and validation",
                "train_cat_fn = os.listdir(train_cats_dir)",
                "train_dog_fn = os.listdir(train_dogs_dir)",
                "",
                "val_cat_fn = os.listdir(val_cats_dir)",
                "val_dog_fn = os.listdir(val_dogs_dir)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(1,2)",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN = mpimg.imread(os.path.join(train_cats_dir, train_cat_fn[0]))",
                "ASSIGN.imshow(ASSIGN)",
                "ASSIGN.axis('off') ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN = mpimg.imread(os.path.join(train_dogs_dir, train_dog_fn[0]))",
                "ASSIGN.imshow(ASSIGN)",
                "ASSIGN.axis('off') ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Image example",
                "import matplotlib.image as mpimg",
                "rcParams['figure.figsize'] = [10,5]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(1,2)",
                "",
                "# Cat image",
                "ax = plt.subplot(grid[0])",
                "cat_img = mpimg.imread(os.path.join(train_cats_dir, train_cat_fn[0]))",
                "ax.imshow(cat_img)",
                "ax.axis('off') ;",
                "",
                "# Dog image",
                "ax = plt.subplot(grid[1])",
                "dog_img = mpimg.imread(os.path.join(train_dogs_dir, train_dog_fn[0]))",
                "ax.imshow(dog_img)",
                "ax.axis('off') ;"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ImageDataGenerator(rescale=1path)",
                "ASSIGN = ImageDataGenerator(rescale=1path)",
                "ASSIGN = train_datagen.flow_from_directory(train_dir,",
                "ASSIGN=20,",
                "ASSIGN='binary',",
                "ASSIGN=(150,150))",
                "ASSIGN = val_datagen.flow_from_directory(val_dir,",
                "ASSIGN=20,",
                "ASSIGN='binary',",
                "ASSIGN=(150,150))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Preprocess the image",
                "from keras.preprocessing.image import ImageDataGenerator",
                "",
                "# Define generator",
                "train_datagen = ImageDataGenerator(rescale=1/255)",
                "val_datagen = ImageDataGenerator(rescale=1/255)",
                "",
                "# Define flow for train gen",
                "train_gen = train_datagen.flow_from_directory(train_dir,",
                "                                              batch_size=20,",
                "                                              class_mode='binary',",
                "                                              target_size=(150,150))",
                "",
                "# Define flow for validation gen",
                "val_gen = val_datagen.flow_from_directory(val_dir,",
                "                                          batch_size=20,",
                "                                          class_mode='binary',",
                "                                          target_size=(150,150))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = time.time()",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "def __init__(self, threshold) :",
                "self.thres = threshold",
                "def on_epoch_end(self, epoch, logs={}) :",
                "if (logs.get('val_accuracy')>self.thres) :",
                "print('\\nReached',self.thres*100,' Validation Accuracy so stop train!')",
                "self.model.stop_training=True",
                "ASSIGN = EarlyStop(0.72)",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)))",
                "ASSIGN.add(MaxPooling2D(2,2))",
                "ASSIGN.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))",
                "ASSIGN.add(MaxPooling2D(2,2))",
                "ASSIGN.add(Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)))",
                "ASSIGN.add(MaxPooling2D(2,2))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(512, activation='relu'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),",
                "ASSIGN='binary_crossentropy',",
                "ASSIGN=['accuracy'])",
                "ASSIGN = model.fit_generator(train_gen,",
                "ASSIGN=val_gen,",
                "ASSIGN=100,",
                "ASSIGN=20,",
                "ASSIGN=50,",
                "ASSIGN=1,",
                "ASSIGN=[ASSIGN])",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make keras model",
                "import tensorflow as tf",
                "import keras",
                "from keras import Sequential",
                "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense",
                "import time",
                "",
                "start = time.time()",
                "",
                "# Set early stopping",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('val_accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,' Validation Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.72)",
                "",
                "# Define model",
                "model = Sequential()",
                "model.add(Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(150,150,3)))",
                "model.add(MaxPooling2D(2,2))",
                "",
                "model.add(Flatten())",
                "model.add(Dense(512, activation='relu'))",
                "model.add(Dense(1, activation='sigmoid'))",
                "",
                "# Compile model",
                "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),",
                "              loss='binary_crossentropy',",
                "              metrics=['accuracy'])",
                "",
                "# Train model",
                "history = model.fit_generator(train_gen, ",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=100,",
                "                              epochs=20,",
                "                              validation_steps=50,",
                "                              verbose=1,",
                "                              callbacks=[callbacks])",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "ASSIGN = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "ASSIGN = list_cat_fn + list_dog_fn",
                "for fn in ASSIGN :",
                "ASSIGN = load_img(fn, target_size=(150,150))",
                "ASSIGN = img_to_array(img)",
                "ASSIGN = np.expand_dims(ASSIGN, axis=0)",
                "ASSIGN = np.vstack([vect_img])",
                "ASSIGN = model.predict(ready_img, batch_size=10)",
                "ASSIGN == 0 :",
                "print(fn,'is a cat')",
                "else :",
                "print(fn,'is a dog')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Try predict some image",
                "from keras.preprocessing.image import load_img, img_to_array",
                "from random import sample",
                "list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "list_fn = list_cat_fn + list_dog_fn",
                "",
                "# Iteration to predict",
                "for fn in list_fn :",
                "    ",
                "    # Load and preprocess image",
                "    img = load_img(fn, target_size=(150,150))",
                "    vect_img = img_to_array(img)",
                "    vect_img = np.expand_dims(vect_img, axis=0)",
                "    ready_img = np.vstack([vect_img])",
                "    ",
                "    # Predict",
                "    classes = model.predict(ready_img, batch_size=10)",
                "    if classes == 0 :",
                "        print(fn,'is a cat')",
                "    else :",
                "        print(fn,'is a dog')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "ASSIGN = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "ASSIGN = list_cat_fn + list_dog_fn",
                "ASSIGN = [layer.output for layer in model.layers[1:]]",
                "ASSIGN = keras.models.Model(inputs=model.input, outputs=succ_out)",
                "ASSIGN = load_img(choice(list_fn), target_size=(150,150))",
                "ASSIGN = img_to_array(img)",
                "ASSIGN  = ASSIGN.reshape((1,) + ASSIGN.shape)",
                "ASSIGN = ASSIGN path",
                "ASSIGN = viz_model.predict(x)",
                "ASSIGN = [layer.name for layer in model.layers]",
                "for layer_name, feature_map in zip(ASSIGN, ASSIGN):",
                "if len(feature_map.shape) == 4:",
                "ASSIGN = feature_map.shape[-1]",
                "ASSIGN    = feature_map.shape[ 1]",
                "ASSIGN = np.zeros((size, size * n_features))",
                "for i in range(ASSIGN):",
                "ASSIGN = feature_map[0, :, :, i]",
                "ASSIGN -= ASSIGN.mean()",
                "ASSIGN= x.std ()",
                "ASSIGN *= 64",
                "ASSIGN += 128",
                "ASSIGN = np.clip(ASSIGN, 0, 255).astype('uint8')",
                "ASSIGN[:, i * ASSIGN : (i + 1) * ASSIGN] = ASSIGN",
                "ASSIGN = 20. path",
                "plt.figure( figsize=(ASSIGN * ASSIGN, ASSIGN) )",
                "plt.title ( layer_name )",
                "plt.grid ( False )",
                "plt.imshow( ASSIGN, aspect='auto', cmap='viridis' )"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Representation of the image based on model",
                "from keras.preprocessing.image import load_img, img_to_array",
                "import keras",
                "from random import choice",
                "list_cat_fn = [os.path.join(val_cats_dir, fn) for fn in sample(val_cat_fn, 5)]",
                "list_dog_fn = [os.path.join(val_dogs_dir, fn) for fn in sample(val_dog_fn, 5)]",
                "list_fn = list_cat_fn + list_dog_fn",
                "succ_out = [layer.output for layer in model.layers[1:]]",
                "",
                "# Define viz model",
                "viz_model = keras.models.Model(inputs=model.input, outputs=succ_out)",
                "",
                "# Pick one image randomly",
                "img = load_img(choice(list_fn), target_size=(150,150))",
                "x = img_to_array(img)",
                "x   = x.reshape((1,) + x.shape)  ",
                "x = x / 255",
                "",
                "# Predicting",
                "successive_feature_maps = viz_model.predict(x)",
                "",
                "# List of layer name",
                "layer_names = [layer.name for layer in model.layers]",
                "",
                "# Plot the visualization on each layer",
                "for layer_name, feature_map in zip(layer_names, successive_feature_maps):",
                "  ",
                "  if len(feature_map.shape) == 4:",
                "    ",
                "    #-------------------------------------------",
                "    # Just do this for the conv / maxpool layers, not the fully-connected layers",
                "    #-------------------------------------------",
                "    n_features = feature_map.shape[-1]  # number of features in the feature map",
                "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)",
                "    ",
                "    # We will tile our images in this matrix",
                "    display_grid = np.zeros((size, size * n_features))",
                "    ",
                "    #-------------------------------------------------",
                "    # Postprocess the feature to be visually palatable",
                "    #-------------------------------------------------",
                "    for i in range(n_features):",
                "      x  = feature_map[0, :, :, i]",
                "      x -= x.mean()",
                "      x /= x.std ()",
                "      x *=  64",
                "      x += 128",
                "      x  = np.clip(x, 0, 255).astype('uint8')",
                "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid",
                "",
                "    #-----------------",
                "    # Display the grid",
                "    #-----------------",
                "",
                "    scale = 20. / n_features",
                "    plt.figure( figsize=(scale * n_features, scale) )",
                "    plt.title ( layer_name )",
                "    plt.grid  ( False )",
                "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN = history.history[ 'val_accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN = history.history['ASSIGN' ]",
                "ASSIGN   = list(range(len(acc)))",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss')",
                "ASSIGN.set_title('Training and Validation Loss')",
                "ASSIGN.legend() ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc')",
                "ASSIGN.set_title('Training and Validation Accuracy')",
                "ASSIGN.legend() ;",
                "plt.tight_layout()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Set the variable ",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo      = list(range(len(acc)))",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;",
                "",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN = history.history[ 'val_accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN = history.history['ASSIGN' ]",
                "ASSIGN  = range(len(acc))",
                "plt.plot ( ASSIGN, ASSIGN )",
                "plt.plot ( ASSIGN, ASSIGN )",
                "plt.title ('Training and validation accuracy')",
                "plt.figure()",
                "plt.plot ( ASSIGN, ASSIGN )",
                "plt.plot ( ASSIGN, ASSIGN )",
                "plt.title ('Training and validation ASSIGN' )"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#-----------------------------------------------------------",
                "# Retrieve a list of list results on training and test data",
                "# sets for each training epoch",
                "#-----------------------------------------------------------",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "",
                "epochs   = range(len(acc)) # Get number of epochs",
                "",
                "#------------------------------------------------",
                "# Plot training and validation accuracy per epoch",
                "#------------------------------------------------",
                "plt.plot  ( epochs,     acc )",
                "plt.plot  ( epochs, val_acc )",
                "plt.title ('Training and validation accuracy')",
                "plt.figure()",
                "",
                "#------------------------------------------------",
                "# Plot training and validation loss per epoch",
                "#------------------------------------------------",
                "plt.plot  ( epochs,     loss )",
                "plt.plot  ( epochs, val_loss )",
                "plt.title ('Training and validation loss'   )"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = time.time()",
                "ASSIGN = 'path'",
                "ASSIGN = zipfile.ZipFile(train_zip, 'r')",
                "ASSIGN.extractall()",
                "ASSIGN.close()",
                "print('Unzip Train Completed')",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)",
                "ASSIGN = time.time()",
                "ASSIGN = 'path'",
                "ASSIGN = zipfile.ZipFile(test_zip, 'r')",
                "ASSIGN.extractall()",
                "ASSIGN.close()",
                "print('Unzip Test Completed')",
                "ASSIGN = time.time()",
                "print('Time Used :',(ASSIGN-ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load full data",
                "import zipfile",
                "import time",
                "",
                "# Unzip train data",
                "start = time.time()",
                "",
                "train_zip = '/kaggle/input/dogs-vs-cats/train.zip'",
                "train_ref = zipfile.ZipFile(train_zip, 'r')",
                "train_ref.extractall()",
                "train_ref.close()",
                "print('Unzip Train Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)",
                "",
                "# Unzip test data",
                "start = time.time()",
                "",
                "test_zip = '/kaggle/input/dogs-vs-cats/test1.zip'",
                "test_ref = zipfile.ZipFile(test_zip, 'r')",
                "test_ref.extractall()",
                "test_ref.close()",
                "print('Unzip Test Completed')",
                "",
                "end = time.time()",
                "print('Time Used :',(end-start)/60)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "os.mkdir(TRAINING_DIR)",
                "os.mkdir(TRAINING_CAT_DIR)",
                "os.mkdir(TRAINING_DOG_DIR)",
                "os.mkdir(VALIDATION_DIR)",
                "os.mkdir(VALIDATION_CAT_DIR)",
                "os.mkdir(VALIDATION_DOG_DIR)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make directory for ImageGenerator",
                "import os",
                "TRAINING_DIR = '/kaggle/training/'",
                "TRAINING_CAT_DIR = '/kaggle/training/cat/'",
                "TRAINING_DOG_DIR = '/kaggle/training/dog/'",
                "os.mkdir(TRAINING_DIR)",
                "os.mkdir(TRAINING_CAT_DIR)",
                "os.mkdir(TRAINING_DOG_DIR)",
                "",
                "VALIDATION_DIR = '/kaggle/validation/'",
                "VALIDATION_CAT_DIR = '/kaggle/validation/cat/'",
                "VALIDATION_DOG_DIR = '/kaggle/validation/dog/'",
                "os.mkdir(VALIDATION_DIR)",
                "os.mkdir(VALIDATION_CAT_DIR)",
                "os.mkdir(VALIDATION_DOG_DIR)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = os.listdir(SOURCE_DIR)",
                "ASSIGN = [fn for fn in list_fn if 'cat' in fn]",
                "ASSIGN = [fn for fn in list_fn if 'dog' in fn]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### List file name in train dataset",
                "SOURCE_DIR = '/kaggle/working/train/'",
                "list_fn = os.listdir(SOURCE_DIR)",
                "list_cat_fn = [fn for fn in list_fn if 'cat' in fn]",
                "list_dog_fn = [fn for fn in list_fn if 'dog' in fn]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = ['cat','dog']",
                "ASSIGN = [list_cat_fn, list_dog_fn]",
                "for i,c in enumerate(tqdm(ASSIGN)) :",
                "ASSIGN = list_name_fn[i]",
                "ASSIGN = random.sample(list_class_fn, len(list_class_fn))",
                "ASSIGN = pure_random[:int(SPLIT_PROP*len(pure_random))]",
                "ASSIGN = pure_random[int(SPLIT_PROP*len(pure_random)):]",
                "for f in ASSIGN :",
                "copyfile(os.path.join(SOURCE_DIR,f), os.path.join(TRAIN_CLASS_DIR,f))",
                "for f in ASSIGN :",
                "copyfile(os.path.join(SOURCE_DIR,f), os.path.join(VALID_CLASS_DIR,f))",
                "del pure_random, random_train, random_val"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Split data",
                "import random",
                "from shutil import copyfile",
                "SPLIT_PROP = 0.8",
                "list_class = ['cat','dog']",
                "list_name_fn = [list_cat_fn, list_dog_fn]",
                "",
                "for i,c in enumerate(tqdm(list_class)) :",
                "            ",
                "    # Splitting",
                "    list_class_fn = list_name_fn[i]",
                "    pure_random = random.sample(list_class_fn, len(list_class_fn))",
                "    random_train = pure_random[:int(SPLIT_PROP*len(pure_random))]",
                "    random_val = pure_random[int(SPLIT_PROP*len(pure_random)):]",
                "    ",
                "    # Insert into new train dir",
                "    TRAIN_CLASS_DIR = os.path.join(TRAINING_DIR,c)",
                "    for f in random_train :",
                "        copyfile(os.path.join(SOURCE_DIR,f), os.path.join(TRAIN_CLASS_DIR,f))",
                "        ",
                "    # Insert into new valid dir",
                "    VALID_CLASS_DIR = os.path.join(VALIDATION_DIR,c)",
                "    for f in random_val :",
                "        copyfile(os.path.join(SOURCE_DIR,f), os.path.join(VALID_CLASS_DIR,f))",
                "        ",
                "    del pure_random, random_train, random_val"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = ResNet50(weights='imagenet',",
                "ASSIGN=False,",
                "ASSIGN=(HEIGHT, WIDTH, 3))",
                "ASSIGN = tf.keras.applications.resnet50.preprocess_input",
                "ASSIGN = tf.keras.applications.resnet50.decode_predictions"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Make pre train model",
                "from tensorflow.keras.applications.resnet50 import ResNet50",
                "from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions",
                "",
                "HEIGHT = 150",
                "WIDTH = 150",
                "base_model = ResNet50(weights='imagenet', ",
                "                 include_top=False, ",
                "                 input_shape=(HEIGHT, WIDTH, 3))",
                "",
                "prec_input = tf.keras.applications.resnet50.preprocess_input",
                "decode = tf.keras.applications.resnet50.decode_predictions"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define parameter",
                "BATCH_SIZE=10",
                "EPOCHS = 10"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input)",
                "ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input)",
                "ASSIGN = train_datagen.flow_from_directory(TRAINING_DIR,",
                "ASSIGN=BATCH_SIZE,",
                "ASSIGN='bicubic',",
                "ASSIGN='categorical',",
                "ASSIGN=True,",
                "ASSIGN=(HEIGHT, WIDTH))",
                "ASSIGN = val_datagen.flow_from_directory(VALIDATION_DIR,",
                "ASSIGN=BATCH_SIZE,",
                "ASSIGN='bicubic',",
                "ASSIGN='categorical',",
                "ASSIGN=False,",
                "ASSIGN=(HEIGHT, WIDTH))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Preprocess the image",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "",
                "# Define generator",
                "train_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)",
                "val_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)",
                "",
                "# Define flow for train gen",
                "train_gen = train_datagen.flow_from_directory(TRAINING_DIR,",
                "                                              batch_size=BATCH_SIZE,",
                "                                              interpolation='bicubic',",
                "                                              class_mode='categorical',",
                "                                              shuffle=True,",
                "                                              target_size=(HEIGHT, WIDTH))",
                "",
                "# Define flow for validation gen",
                "val_gen = val_datagen.flow_from_directory(VALIDATION_DIR,",
                "                                          batch_size=BATCH_SIZE,",
                "                                          interpolation='bicubic',",
                "                                          class_mode='categorical',",
                "                                          shuffle=False,",
                "                                          target_size=(HEIGHT, WIDTH))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = []",
                "ASSIGN = 2",
                "ASSIGN = 0.2",
                "def make_model() :",
                "for l in base_model.layers :",
                "l.trainable = False",
                "ASSIGN = Sequential()",
                "ASSIGN.add(base_model)",
                "ASSIGN.add(GlobalAveragePooling2D())",
                "ASSIGN.add(Flatten())",
                "for fc in ASSIGN:",
                "ASSIGN.add(Dense(fc, activation=swish))",
                "ASSIGN.add(Dropout(ASSIGN))",
                "ASSIGN.add(Dense(ASSIGN, activation='softmax'))",
                "ASSIGN.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])",
                "return model",
                "ASSIGN = make_model()",
                "ASSIGN.summary()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Add layer and dont train the layer before",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish",
                "fc_layers = []",
                "num_classes = 2",
                "dropout = 0.2",
                "",
                "def make_model() :",
                "    ",
                "    # Freeze layer",
                "    for l in base_model.layers :",
                "        l.trainable = False",
                "",
                "    # Make new model",
                "    model = Sequential()",
                "    model.add(base_model)",
                "    model.add(GlobalAveragePooling2D())",
                "    model.add(Flatten())",
                "    for fc in fc_layers:",
                "",
                "        # New FC layer, random init",
                "        model.add(Dense(fc, activation=swish))",
                "        model.add(Dropout(dropout))",
                "",
                "    model.add(Dense(num_classes, activation='softmax'))",
                "",
                "    # Compile model",
                "    model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])",
                "    ",
                "    return model",
                "",
                "",
                "model = make_model()",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = model.fit_generator(train_gen,",
                "ASSIGN=EPOCHS,",
                "ASSIGN=1,",
                "ASSIGN=val_gen)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "history = model.fit_generator(train_gen,",
                "                              epochs=EPOCHS,",
                "                              verbose=1,",
                "                              validation_data=val_gen)",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = tf.keras.models.Sequential([",
                "tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),",
                "tf.keras.layers.MaxPooling2D(2,2),",
                "tf.keras.layers.Conv2D(32, (3,3), activation='relu'),",
                "tf.keras.layers.MaxPooling2D(2,2),",
                "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),",
                "tf.keras.layers.MaxPooling2D(2,2),",
                "tf.keras.layers.Flatten(),",
                "tf.keras.layers.Dense(512, activation='relu'),",
                "tf.keras.layers.Dense(2, activation='softmax')",
                "])",
                "ASSIGN.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS",
                "# USE AT LEAST 3 CONVOLUTION LAYERS",
                "model = tf.keras.models.Sequential([",
                "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),",
                "    tf.keras.layers.MaxPooling2D(2,2),",
                "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),",
                "    tf.keras.layers.MaxPooling2D(2,2), ",
                "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), ",
                "    tf.keras.layers.MaxPooling2D(2,2),",
                "    tf.keras.layers.Flatten(), ",
                "    tf.keras.layers.Dense(512, activation='relu'), ",
                "    tf.keras.layers.Dense(2, activation='softmax')  ",
                "",
                "# YOUR CODE HERE",
                "])",
                "",
                "model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = model.fit_generator(train_gen,",
                "ASSIGN=EPOCHS,",
                "ASSIGN=1,",
                "ASSIGN=val_gen)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "history = model.fit_generator(train_gen,",
                "                              epochs=EPOCHS,",
                "                              verbose=1,",
                "                              validation_data=val_gen)",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get data",
                "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip",
                "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip -O /tmp/validation-horse-or-human.zip "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = 'path'",
                "ASSIGN = zipfile.ZipFile(data_zip, 'r')",
                "ASSIGN.extractall('path')",
                "ASSIGN.close()",
                "print('Unzip Train Data Completed')",
                "ASSIGN = 'path'",
                "ASSIGN = zipfile.ZipFile(valid_zip, 'r')",
                "ASSIGN.extractall('path')",
                "ASSIGN.close()",
                "print('Unzip Valid Data Completed')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Extract the zip file",
                "import zipfile",
                "import time",
                "",
                "# Unzip train data",
                "data_zip = '/tmp/horse-or-human.zip'",
                "data_ref = zipfile.ZipFile(data_zip, 'r')",
                "data_ref.extractall('/training')",
                "data_ref.close()",
                "print('Unzip Train Data Completed')",
                "",
                "# Unzip valid data",
                "valid_zip = '/tmp/validation-horse-or-human.zip'",
                "valid_ref = zipfile.ZipFile(valid_zip, 'r')",
                "valid_ref.extractall('/validating')",
                "valid_ref.close()",
                "print('Unzip Valid Data Completed')",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print('Total obs on horse class in train :',len(os.listdir(TRAIN_HORSE_DIR)))",
                "print('Total obs on human class in train :',len(os.listdir(TRAIN_HUMAN_DIR)))",
                "print('Total obs on horse class in validation :',len(os.listdir(VAL_HORSE_DIR)))",
                "print('Total obs on human class in validation :',len(os.listdir(VAL_HUMAN_DIR)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Check the proportion of classes in train and valid dataset",
                "TRAIN_HORSE_DIR = '/training/horses'",
                "TRAIN_HUMAN_DIR = '/training/humans'",
                "VAL_HORSE_DIR = '/validating/horses'",
                "VAL_HUMAN_DIR = '/validating/humans'",
                "",
                "print('Total obs on horse class in train :',len(os.listdir(TRAIN_HORSE_DIR)))",
                "print('Total obs on human class in train :',len(os.listdir(TRAIN_HUMAN_DIR)))",
                "print('Total obs on horse class in validation :',len(os.listdir(VAL_HORSE_DIR)))",
                "print('Total obs on human class in validation :',len(os.listdir(VAL_HUMAN_DIR)))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "https:path\\",
                "-O path",
                "ASSIGN = InceptionV3(weights=None,",
                "ASSIGN=False,",
                "ASSIGN=(INPUT_SIZE[0], INPUT_SIZE[1], 3))",
                "ASSIGN = 'path'",
                "ASSIGN.load_weights(ASSIGN)",
                "ASSIGN.trainable = False",
                "ASSIGN.summary()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define pretrained model",
                "from tensorflow.keras.applications.inception_v3 import InceptionV3",
                "from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions",
                "BATCH_SIZE = 20",
                "INPUT_SIZE = (150,150)",
                "",
                "# Get local weight",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\",
                "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5",
                "    ",
                "# Define inception model",
                "base_model = InceptionV3(weights=None, ",
                "                         include_top=False, ",
                "                         input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))",
                "",
                "# Load local weight",
                "local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'",
                "base_model.load_weights(local_weight_file)",
                "",
                "# Freeze all layer",
                "base_model.trainable = False",
                "",
                "# Summary of the model",
                "base_model.summary()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = base_model.get_layer('mixed7')",
                "ASSIGN = last_layer.output",
                "print('Last layer shape :',ASSIGN.output_shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get specific layer from pre-trained model ",
                "last_layer = base_model.get_layer('mixed7')",
                "last_output = last_layer.output",
                "",
                "print('Last layer shape :',last_layer.output_shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = ImageDataGenerator(rescale = 1.path,",
                "ASSIGN = 40,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = True)",
                "ASSIGN = train_datagen.flow_from_directory(TRAIN_DIR,",
                "ASSIGN = BATCH_SIZE,",
                "ASSIGN = 'binary',",
                "ASSIGN = INPUT_SIZE)",
                "ASSIGN = ImageDataGenerator(rescale = 1path)",
                "ASSIGN = val_datagen.flow_from_directory(VALID_DIR,",
                "ASSIGN = BATCH_SIZE,",
                "ASSIGN = 'binary',",
                "ASSIGN = INPUT_SIZE)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Set the image generator",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "TRAIN_DIR = '/training'",
                "VALID_DIR = '/validating'",
                "",
                "# Make image generator for training dataset",
                "train_datagen = ImageDataGenerator(rescale = 1./255.,",
                "                                   rotation_range = 40,",
                "                                   width_shift_range = 0.2,",
                "                                   height_shift_range = 0.2,",
                "                                   shear_range = 0.2,",
                "                                   zoom_range = 0.2,",
                "                                   horizontal_flip = True)",
                "",
                "train_gen = train_datagen.flow_from_directory(TRAIN_DIR,",
                "                                              batch_size = BATCH_SIZE,",
                "                                              class_mode = 'binary', ",
                "                                              target_size = INPUT_SIZE)  ",
                "",
                "# Make image generator for validation dataset",
                "val_datagen = ImageDataGenerator(rescale = 1/255. )",
                "",
                "val_gen =  val_datagen.flow_from_directory(VALID_DIR,",
                "                                            batch_size  = BATCH_SIZE,",
                "                                            class_mode  = 'binary', ",
                "                                            target_size = INPUT_SIZE)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Flatten()(last_output)",
                "ASSIGN = Dense(1024, activation='relu')(ASSIGN)",
                "ASSIGN = Dropout(0.2)(ASSIGN)",
                "ASSIGN = Dense(1, activation='sigmoid')(ASSIGN)",
                "ASSIGN = Model(base_model.input, x)",
                "ASSIGN.compile(optimizer = RMSprop(lr=0.0001),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define new model",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish\\",
                "",
                "# model = Sequential()",
                "# model.add(base_model)",
                "# model.add(Flatten())",
                "# model.add(Dense(1024, activation='relu'))",
                "# model.add(Dropout(0.2))",
                "# model.add(Dense(1, activation='sigmoid'))",
                "",
                "# Use this when you not using the whole pre trained model",
                "x = Flatten()(last_output)",
                "x = Dense(1024, activation='relu')(x)",
                "x = Dropout(0.2)(x)",
                "x = Dense(1, activation='sigmoid')(x)",
                "model = Model(base_model.input, x)",
                "",
                "# Compile model",
                "model.compile(optimizer = RMSprop(lr=0.0001),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                ""
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "def __init__(self, threshold) :",
                "self.thres = threshold",
                "def on_epoch_end(self, epoch, logs={}) :",
                "if (logs.get('accuracy')>self.thres) :",
                "print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "self.model.stop_training=True",
                "ASSIGN = EarlyStop(0.99)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define early callback based on metrics",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.99)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = model.fit_generator(train_gen,",
                "ASSIGN=val_gen,",
                "ASSIGN=100,",
                "ASSIGN=50,",
                "ASSIGN=EPOCHS,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train model",
                "EPOCHS = 99",
                "history = model.fit_generator(train_gen,",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=100,",
                "                              validation_steps=50,",
                "                              epochs=EPOCHS,",
                "                              callbacks=callbacks,",
                "                              verbose=1)",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "def get_data(path) :",
                "ASSIGN = pd.read_csv(path)",
                "ASSIGN = to_categorical(np.array(df['ASSIGN']))",
                "ASSIGN = []",
                "for i in range(len(ASSIGN)) :",
                "ASSIGN = np.array(np.split(df.iloc[i,1:].values, 28))",
                "ASSIGN.append(ASSIGN)",
                "return np.array(image), label"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Function to read the data",
                "def get_data(path) :",
                "    # Read the csv",
                "    df = pd.read_csv(path)",
                "    ",
                "    # Get all the label",
                "    from tensorflow.keras.utils import to_categorical",
                "    label = to_categorical(np.array(df['label']))",
                "    ",
                "    # Get array of image",
                "    image = []",
                "    for i in range(len(df)) :",
                "        split_img = np.array(np.split(df.iloc[i,1:].values, 28))",
                "        image.append(split_img)",
                "    ",
                "    return np.array(image), label",
                "    "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = get_data('..path')",
                "ASSIGN = get_data('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get data",
                "train_img, train_label = get_data('../input/sign-language-mnist/sign_mnist_train.csv')",
                "val_img, val_label = get_data('../input/sign-language-mnist/sign_mnist_test.csv')",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Train image shape :',train_img.shape)",
                "print('Train label shape :',train_label.shape)",
                "print('Valid image shape :',val_img.shape)",
                "print('Valid label shape :',val_label.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Shape of the data",
                "print('Train image shape :',train_img.shape)",
                "print('Train label shape :',train_label.shape)",
                "print('Valid image shape :',val_img.shape)",
                "print('Valid label shape :',val_label.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = np.expand_dims(ASSIGN, 3)",
                "ASSIGN = ImageDataGenerator(rescale = 1.path,",
                "ASSIGN = 40,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = 0.2,",
                "ASSIGN = True)",
                "ASSIGN = train_datagen.flow(x=train_img, y=train_label,",
                "ASSIGN = BATCH_SIZE)",
                "ASSIGN = np.expand_dims(ASSIGN, 3)",
                "ASSIGN = ImageDataGenerator(rescale = 1path)",
                "ASSIGN = val_datagen.flow(x=val_img, y=val_label,",
                "ASSIGN = BATCH_SIZE)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make the image generator",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator",
                "BATCH_SIZE = 32",
                "INPUT_SIZE = (28,28)",
                "TRAIN_DIR = '/training'",
                "VALID_DIR = '/validating'",
                "",
                "# Make image generator for training dataset",
                "train_img = np.expand_dims(train_img, 3)",
                "train_datagen = ImageDataGenerator(rescale = 1./255.,",
                "                                   rotation_range = 40,",
                "                                   width_shift_range = 0.2,",
                "                                   height_shift_range = 0.2,",
                "                                   shear_range = 0.2,",
                "                                   zoom_range = 0.2,",
                "                                   horizontal_flip = True)",
                "",
                "train_gen = train_datagen.flow(x=train_img, y=train_label,",
                "                              batch_size = BATCH_SIZE)  ",
                "",
                "# Make image generator for validation dataset",
                "val_img = np.expand_dims(val_img, 3)",
                "val_datagen = ImageDataGenerator(rescale = 1/255. )",
                "",
                "val_gen = val_datagen.flow(x=val_img, y=val_label,",
                "                              batch_size = BATCH_SIZE) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(64, (3,3), activation='relu', input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 1)))",
                "ASSIGN.add(MaxPooling2D(2,2))",
                "ASSIGN.add(Conv2D(128, (3,3), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(2,2))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(256, activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(25, activation='softmax'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'categorical_crossentropy',",
                "ASSIGN = 'accuracy')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define new model",
                "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, MaxPooling2D, Conv2D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "from tensorflow.keras.activations import swish\\",
                "",
                "model = Sequential()",
                "model.add(Conv2D(64, (3,3), activation='relu', input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 1)))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Conv2D(128, (3,3), activation='relu'))",
                "model.add(MaxPooling2D(2,2))",
                "model.add(Flatten())",
                "model.add(Dense(256, activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(25, activation='softmax'))",
                "",
                "# Compile model",
                "model.compile(optimizer = Adam(),",
                "              loss = 'categorical_crossentropy',",
                "              metrics = 'accuracy')",
                ""
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "def __init__(self, threshold) :",
                "self.thres = threshold",
                "def on_epoch_end(self, epoch, logs={}) :",
                "if (logs.get('val_accuracy')>self.thres) :",
                "print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "self.model.stop_training=True",
                "ASSIGN = EarlyStop(0.9)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define early callback based on metrics",
                "class EarlyStop(tf.keras.callbacks.Callback) :",
                "    ",
                "    def __init__(self, threshold) :",
                "        self.thres = threshold",
                "        ",
                "    def on_epoch_end(self, epoch, logs={}) :",
                "        if (logs.get('val_accuracy')>self.thres) :",
                "            print('\\nReached',self.thres*100,'Accuracy so stop train!')",
                "            self.model.stop_training=True",
                "            ",
                "callbacks = EarlyStop(0.9)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = model.fit_generator(train_gen,",
                "ASSIGN=val_gen,",
                "ASSIGN=train_img.shape[0] path,",
                "ASSIGN=val_img.shape[0] path,",
                "ASSIGN=EPOCHS,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train model",
                "EPOCHS = 50",
                "history = model.fit_generator(train_gen,",
                "                              validation_data=val_gen,",
                "                              steps_per_epoch=train_img.shape[0] // BATCH_SIZE,",
                "                              validation_steps=val_img.shape[0] // BATCH_SIZE,",
                "                              epochs=EPOCHS,",
                "                              callbacks=callbacks,",
                "                              verbose=1)",
                ""
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "model.evaluate(val_img, val_label)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Evaluate model",
                "model.evaluate(val_img, val_label)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN = history.history[ 'val_accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN = history.history['ASSIGN' ]",
                "ASSIGN  = range(len(acc))",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss')",
                "ASSIGN.set_title('Training and Validation Loss')",
                "ASSIGN.legend() ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc')",
                "ASSIGN.set_title('Training and Validation Accuracy')",
                "ASSIGN.legend() ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Get the metrics and loss",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo   = range(len(acc)) # Get number of epochs",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the acccuracy",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\",
                "    -O /tmp/bbc-text.csv",
                ""
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = list(data['category'])",
                "ASSIGN = list(data['text'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load data",
                "data = pd.read_csv('/tmp/bbc-text.csv')",
                "label = list(data['category'])",
                "sentences_raw = list(data['text'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(len(sentences_raw))",
                "print(sentences_raw[0])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Print the first expected output",
                "print(len(sentences_raw))",
                "print(sentences_raw[0])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "ASSIGN = []",
                "for t in tqdm(sentences_raw) :",
                "ASSIGN = word_tokenize(t)",
                "ASSIGN = [i for i in tokenize_text if i not in stopwords]",
                "ASSIGN.append((\" \").join(ASSIGN))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Remove stopwords from data",
                "from nltk.tokenize import word_tokenize",
                "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "sentences = []",
                "",
                "for t in tqdm(sentences_raw) :",
                "    tokenize_text = word_tokenize(t)",
                "    list_text = [i for i in tokenize_text if i not in stopwords]",
                "    sentences.append((\" \").join(list_text))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = Tokenizer(oov_token='<OOV>',",
                ")",
                "ASSIGN.fit_on_texts(sentences)",
                "ASSIGN = tokenizer.ASSIGN",
                "print(len(ASSIGN))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Tokenize the text data",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer = Tokenizer(oov_token='<OOV>',",
                "                      #num_words to specify maximum number of token based on frequency",
                "                     )",
                "",
                "# Get the token dict from data",
                "tokenizer.fit_on_texts(sentences)",
                "word_index = tokenizer.word_index",
                "print(len(word_index))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = tokenizer.texts_to_sequences(sentences)",
                "ASSIGN = pad_sequences(sequences, padding='post', truncating='pre', maxlen=None)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Pad the text data",
                "sequences = tokenizer.texts_to_sequences(sentences)",
                "padded = pad_sequences(sequences, padding='post', truncating='pre', maxlen=None)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(padded[0])",
                "print(padded.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Print the second output",
                "print(padded[0])",
                "print(padded.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = Tokenizer()",
                "ASSIGN.fit_on_texts(label)",
                "ASSIGN = tokenizer_label.word_index",
                "ASSIGN = tokenizer_label.texts_to_sequences(label)",
                "print(ASSIGN)",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Tokenize the label data",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer_label = Tokenizer()",
                "",
                "# Get the token dict from data",
                "tokenizer_label.fit_on_texts(label)",
                "label_word_index = tokenizer_label.word_index",
                "label_seq = tokenizer_label.texts_to_sequences(label)",
                "",
                "print(label_seq)",
                "print(label_word_index)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path"
            ],
            "output_type": "stream",
            "content_old": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\",
                "    -O /tmp/bbc-text.csv",
                ""
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = list(data['category'])",
                "ASSIGN = list(data['text'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load data",
                "data = pd.read_csv('/tmp/bbc-text.csv')",
                "label = list(data['category'])",
                "sentences_raw = list(data['text'])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "ASSIGN = []",
                "for t in tqdm(sentences_raw) :",
                "ASSIGN = word_tokenize(t)",
                "ASSIGN = [i for i in tokenize_text if i not in stopwords]",
                "ASSIGN.append((\" \").join(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "### Remove stopwords from data",
                "from nltk.tokenize import word_tokenize",
                "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]",
                "sentences = []",
                "",
                "for t in tqdm(sentences_raw) :",
                "    tokenize_text = word_tokenize(t)",
                "    list_text = [i for i in tokenize_text if i not in stopwords]",
                "    sentences.append((\" \").join(list_text))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "lab_tokenizer.word_index"
            ],
            "output_type": "execute_result",
            "content_old": [
                "lab_tokenizer.word_index"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define variable for tokenizing and modelling",
                "# Reduce this parameter to reduce overfitting",
                "VOCAB_SIZE = 1000",
                "EMBEDDING_DIM = 16",
                "MAX_LENGTH = 120",
                "",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'post'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = int(TRAIN_PROP * len(sentences))",
                "ASSIGN = sentences[:train_size]",
                "ASSIGN = label[:train_size]",
                "ASSIGN = sentences[train_size:]",
                "ASSIGN = label[train_size:]",
                "print(ASSIGN)",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "### Split dataset",
                "train_size = int(TRAIN_PROP * len(sentences))",
                "train_sentences = sentences[:train_size]",
                "train_label = label[:train_size]",
                "val_sentences = sentences[train_size:]",
                "val_label = label[train_size:]",
                "",
                "print(train_size)",
                "print(len(train_sentences))",
                "print(len(train_label))",
                "print(len(val_sentences))",
                "print(len(val_label))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)",
                "ASSIGN.fit_on_texts(train_sentences)",
                "ASSIGN = text_tokenizer.ASSIGN",
                "ASSIGN = text_tokenizer.texts_to_sequences(train_sentences)",
                "ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Tokenize the train sentences",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "text_tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)",
                "",
                "# Get the token dict from data",
                "text_tokenizer.fit_on_texts(train_sentences)",
                "word_index = text_tokenizer.word_index",
                "",
                "# Pad the data",
                "train_sequences = text_tokenizer.texts_to_sequences(train_sentences)",
                "train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = text_tokenizer.texts_to_sequences(val_sentences)",
                "ASSIGN = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Pad the validation data",
                "val_sequences = text_tokenizer.texts_to_sequences(val_sentences)",
                "val_padded = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Tokenizer()",
                "ASSIGN.fit_on_texts(train_label)",
                "ASSIGN = np.array(lab_tokenizer.texts_to_sequences(train_label))",
                "ASSIGN = np.array(lab_tokenizer.texts_to_sequences(val_label))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Tokenize the label",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "lab_tokenizer = Tokenizer()",
                "",
                "# Get the token dict from data",
                "lab_tokenizer.fit_on_texts(train_label)",
                "",
                "# Get label sequences",
                "train_label_seq = np.array(lab_tokenizer.texts_to_sequences(train_label))",
                "val_label_seq = np.array(lab_tokenizer.texts_to_sequences(val_label))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH))",
                "ASSIGN.add(GlobalAveragePooling1D())",
                "ASSIGN.add(Dense(24, activation='relu'))",
                "ASSIGN.add(Dense(6, activation='softmax'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'sparse_categorical_crossentropy',",
                "ASSIGN = 'accuracy')",
                "ASSIGN.summary()"
            ],
            "output_type": "stream",
            "content_old": [
                "### Make simple Embedding MLP model",
                "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH))",
                "model.add(GlobalAveragePooling1D())",
                "model.add(Dense(24, activation='relu'))",
                "model.add(Dense(6, activation='softmax'))",
                "",
                "# Compile model",
                "model.compile(optimizer = Adam(),",
                "              loss = 'sparse_categorical_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Summary of the model",
                "model.summary()",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = 30",
                "ASSIGN = model.fit(train_padded,",
                "train_label_seq,",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, val_label_seq),",
                "ASSIGN=1)"
            ],
            "output_type": "stream",
            "content_old": [
                "### Train the model",
                "num_epochs = 30",
                "history = model.fit(train_padded,",
                "                    train_label_seq,",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, val_label_seq),",
                "                    verbose=1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN = history.history[ 'val_accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN = history.history['ASSIGN' ]",
                "ASSIGN  = range(len(acc))",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss')",
                "ASSIGN.set_title('Training and Validation Loss')",
                "ASSIGN.legend() ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc')",
                "ASSIGN.set_title('Training and Validation Accuracy')",
                "ASSIGN.legend() ;"
            ],
            "output_type": "display_data",
            "content_old": [
                "### Plot the performance of the model",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight') ",
                "sns.set_style('whitegrid')",
                "grid = gridspec.GridSpec(2,1)",
                "",
                "# Get the metrics and loss",
                "acc      = history.history[     'accuracy' ]",
                "val_acc  = history.history[ 'val_accuracy' ]",
                "loss     = history.history[    'loss' ]",
                "val_loss = history.history['val_loss' ]",
                "epo   = range(len(acc)) # Get number of epochs",
                "",
                "# Plot the loss",
                "ax = plt.subplot(grid[0])",
                "ax.plot(epo, loss, label='Train Loss')",
                "ax.plot(epo, val_loss, label='Validation Loss')",
                "ax.set_title('Training and Validation Loss')",
                "ax.legend() ;",
                "",
                "# Plot the acccuracy",
                "ax = plt.subplot(grid[1])",
                "ax.plot(epo, acc, label='Train Acc')",
                "ax.plot(epo, val_acc, label='Validation Acc')",
                "ax.set_title('Training and Validation Accuracy')",
                "ax.legend() ;"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = dict([(value, key) for (key, value) in word_index.items()])",
                "def decode_sentence(text):",
                "return ' '.join([ASSIGN.get(i, '?') for i in text])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make dictionary to reverse the number from tokenizing to text",
                "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])",
                "",
                "def decode_sentence(text):",
                "    return ' '.join([reverse_word_index.get(i, '?') for i in text])",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = model.layers[0]",
                "ASSIGN = e.get_weights()[0]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Get the embedding weight for visualization",
                "e = model.layers[0]",
                "weights = e.get_weights()[0]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = io.open('vecs.tsv', 'w', encoding='utf-8')",
                "ASSIGN = io.open('meta.tsv', 'w', encoding='utf-8')",
                "for word_num in range(1, VOCAB_SIZE):",
                "ASSIGN = reverse_word_index[word_num]",
                "ASSIGN = weights[word_num]",
                "ASSIGN.write(ASSIGN + \"\\n\")",
                "ASSIGN.write('\\t'.join([str(x) for x in ASSIGN]) + \"\\n\")",
                "ASSIGN.close()",
                "ASSIGN.close()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Save the weight",
                "import io",
                "",
                "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')",
                "out_m = io.open('meta.tsv', 'w', encoding='utf-8')",
                "for word_num in range(1, VOCAB_SIZE):",
                "    word = reverse_word_index[word_num]",
                "    embeddings = weights[word_num]",
                "    out_m.write(word + \"\\n\")",
                "    out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")",
                "out_v.close()",
                "out_m.close()",
                "",
                "# Use the file to make embedding visualization at https://projector.tensorflow.org/"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path"
            ],
            "output_type": "stream",
            "content_old": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\",
                "    -O /tmp/training_cleaned.csv"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path', names=['label','id','time','query','handle','text'])",
                "ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True)",
                "ASSIGN = list(data['text'])",
                "ASSIGN = list(data['ASSIGN'].map({0:0, 4:1}))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load the data",
                "data = pd.read_csv('/tmp/training_cleaned.csv', names=['label','id','time','query','handle','text'])",
                "data = data.sample(frac=1).reset_index(drop=True)",
                "sentences = list(data['text'])",
                "label = list(data['label'].map({0:0, 4:1}))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define variable for tokenizing and modelling",
                "EMBEDDING_DIM = 100",
                "MAX_LENGTH = 16",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'post'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = int(TRAIN_PROP * len(sentences))",
                "ASSIGN = sentences[:train_size]",
                "ASSIGN = label[:train_size]",
                "ASSIGN = sentences[train_size:]",
                "ASSIGN = label[train_size:]",
                "print(ASSIGN)",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))",
                "print(len(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "### Split dataset",
                "train_size = int(TRAIN_PROP * len(sentences))",
                "train_sentences = sentences[:train_size]",
                "train_label = label[:train_size]",
                "val_sentences = sentences[train_size:]",
                "val_label = label[train_size:]",
                "",
                "print(train_size)",
                "print(len(train_sentences))",
                "print(len(train_label))",
                "print(len(val_sentences))",
                "print(len(val_label))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Tokenizer(oov_token=OOV_TOK)",
                "ASSIGN.fit_on_texts(train_sentences)",
                "ASSIGN = text_tokenizer.ASSIGN",
                "ASSIGN = text_tokenizer.texts_to_sequences(train_sentences)",
                "ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Tokenize the train sentences",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "text_tokenizer = Tokenizer(oov_token=OOV_TOK)",
                "",
                "# Get the token dict from data",
                "text_tokenizer.fit_on_texts(train_sentences)",
                "word_index = text_tokenizer.word_index",
                "",
                "# Pad the data",
                "train_sequences = text_tokenizer.texts_to_sequences(train_sentences)",
                "train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = text_tokenizer.texts_to_sequences(val_sentences)",
                "ASSIGN = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Pad the validation data",
                "val_sequences = text_tokenizer.texts_to_sequences(val_sentences)",
                "val_padded = pad_sequences(val_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define vocab size",
                "VOCAB_SIZE = len(word_index) + 1"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path",
                "ASSIGN = {};",
                "with open('path') as f:",
                "for line in f:",
                "ASSIGN = line.split();",
                "ASSIGN = values[0];",
                "ASSIGN = np.asarray(values[1:], dtype='float32');",
                "ASSIGN[ASSIGN] = ASSIGN;",
                "ASSIGN = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for ASSIGN, i in word_index.items():",
                "ASSIGN = embeddings_index.get(word);",
                "if ASSIGN is not None:",
                "ASSIGN[i] = ASSIGN;"
            ],
            "output_type": "stream",
            "content_old": [
                "### Get weight for embedding matrix in model",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\",
                "    -O /tmp/glove.6B.100d.txt",
                "    ",
                "# Load the weight",
                "embeddings_index = {};",
                "with open('/tmp/glove.6B.100d.txt') as f:",
                "    for line in f:",
                "        values = line.split();",
                "        word = values[0];",
                "        coefs = np.asarray(values[1:], dtype='float32');",
                "        embeddings_index[word] = coefs;",
                "",
                "# Make weight matrix",
                "embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for word, i in word_index.items():",
                "    embedding_vector = embeddings_index.get(word);",
                "    if embedding_vector is not None:",
                "        embeddings_matrix[i] = embedding_vector;"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "ASSIGN=[embeddings_matrix], trainable=False))",
                "ASSIGN.add(GlobalAveragePooling1D())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "ASSIGN=[embeddings_matrix], trainable=False))",
                "ASSIGN.add(Bidirectional(LSTM(64)))",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "ASSIGN=[embeddings_matrix], trainable=False))",
                "ASSIGN.add(Bidirectional(GRU(64)))",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "ASSIGN=[embeddings_matrix], trainable=False))",
                "ASSIGN.add(Conv1D(64, 5, activation='relu'))",
                "ASSIGN.add(GlobalMaxPooling1D())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make model",
                "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model_simple = Sequential()",
                "model_simple.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_simple.add(GlobalAveragePooling1D())",
                "model_simple.add(Dense(128, activation='relu'))",
                "model_simple.add(Dense(1, activation='sigmoid'))",
                "",
                "model_simple.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single LSTM model",
                "model_single_lstm = Sequential()",
                "model_single_lstm.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_lstm.add(Bidirectional(LSTM(64)))",
                "model_single_lstm.add(Dense(128, activation='relu'))",
                "model_single_lstm.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_lstm.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single GRU",
                "model_single_gru = Sequential()",
                "model_single_gru.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_gru.add(Bidirectional(GRU(64)))",
                "model_single_gru.add(Dense(128, activation='relu'))",
                "model_single_gru.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_gru.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')",
                "",
                "# Make single Conv",
                "model_single_conv = Sequential()",
                "model_single_conv.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model_single_conv.add(Conv1D(64, 5, activation='relu'))",
                "model_single_conv.add(GlobalMaxPooling1D())",
                "model_single_conv.add(Dense(128, activation='relu'))",
                "model_single_conv.add(Dense(1, activation='sigmoid'))",
                "",
                "model_single_conv.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_history(history) :",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN = history.history[ 'val_accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN = history.history['ASSIGN' ]",
                "ASSIGN  = range(len(acc))",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Loss')",
                "ASSIGN.set_title('Training and Validation Loss')",
                "ASSIGN.legend() ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc')",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Validation Acc')",
                "ASSIGN.set_title('Training and Validation Accuracy')",
                "ASSIGN.legend() ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot the performance of the model",
                "def plot_history(history) :",
                "    rcParams['figure.figsize'] = [10,8]",
                "    plt.style.use('fivethirtyeight') ",
                "    sns.set_style('whitegrid')",
                "    grid = gridspec.GridSpec(2,1)",
                "",
                "    # Get the metrics and loss",
                "    acc      = history.history[     'accuracy' ]",
                "    val_acc  = history.history[ 'val_accuracy' ]",
                "    loss     = history.history[    'loss' ]",
                "    val_loss = history.history['val_loss' ]",
                "    epo   = range(len(acc)) # Get number of epochs",
                "",
                "    # Plot the loss",
                "    ax = plt.subplot(grid[0])",
                "    ax.plot(epo, loss, label='Train Loss')",
                "    ax.plot(epo, val_loss, label='Validation Loss')",
                "    ax.set_title('Training and Validation Loss')",
                "    ax.legend() ;",
                "",
                "    # Plot the acccuracy",
                "    ax = plt.subplot(grid[1])",
                "    ax.plot(epo, acc, label='Train Acc')",
                "    ax.plot(epo, val_acc, label='Validation Acc')",
                "    ax.set_title('Training and Validation Accuracy')",
                "    ax.legend() ;"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 5",
                "ASSIGN = model_simple.fit(train_padded,",
                "np.array(train_label),",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, np.array(val_label)),",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train simple model",
                "num_epochs = 5",
                "history_simple = model_simple.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history_simple)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot simple model performance",
                "plot_history(history_simple)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 10",
                "ASSIGN = model_single_lstm.fit(train_padded,",
                "np.array(train_label),",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, np.array(val_label)),",
                "ASSIGN=1)"
            ],
            "output_type": "stream",
            "content_old": [
                "### Train single lstm model",
                "num_epochs = 10",
                "history_single_lstm = model_single_lstm.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history_single_lstm)"
            ],
            "output_type": "display_data",
            "content_old": [
                "### Plot simple model performance",
                "plot_history(history_single_lstm)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 5",
                "ASSIGN = model_single_gru.fit(train_padded,",
                "np.array(train_label),",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, np.array(val_label)),",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train single gru model",
                "num_epochs = 5",
                "history_single_gru = model_single_gru.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history_single_gru)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot simple model performance",
                "plot_history(history_single_gru)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 5",
                "ASSIGN = model_single_conv.fit(train_padded,",
                "np.array(train_label),",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, np.array(val_label)),",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train single conv model",
                "num_epochs = 5",
                "history_single_conv = model_single_conv.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history_single_conv)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot simple model performance",
                "plot_history(history_single_conv)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "ASSIGN=[embeddings_matrix], trainable=False))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Conv1D(64, 5, activation='relu'))",
                "ASSIGN.add(MaxPooling1D(4))",
                "ASSIGN.add(LSTM(64))",
                "ASSIGN.add(Dense(1, activation='sigmoid'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'binary_crossentropy',",
                "ASSIGN = 'accuracy')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make complicated model",
                "from tensorflow.keras.layers import Dense, Embedding, Dropout, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH,",
                "                    weights=[embeddings_matrix], trainable=False))",
                "model.add(Dropout(0.2))",
                "model.add(Conv1D(64, 5, activation='relu'))",
                "model.add(MaxPooling1D(4))",
                "model.add(LSTM(64))",
                "model.add(Dense(1, activation='sigmoid'))",
                "",
                "model.compile(optimizer = Adam(),",
                "              loss = 'binary_crossentropy',",
                "              metrics = 'accuracy')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 5",
                "ASSIGN = model.fit(train_padded,",
                "np.array(train_label),",
                "ASSIGN=num_epochs,",
                "ASSIGN=(val_padded, np.array(val_label)),",
                "ASSIGN=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Train single conv model",
                "num_epochs = 5",
                "history = model.fit(train_padded,",
                "                    np.array(train_label),",
                "                    epochs=num_epochs,",
                "                    validation_data=(val_padded, np.array(val_label)),",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot simple model performance",
                "plot_history(history)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path"
            ],
            "output_type": "stream",
            "content_old": [
                "### Get data",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\",
                "    -O /tmp/sonnets.txt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = open('path').read()",
                "ASSIGN = data.lower().split('\\n')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Load the data",
                "data = open('/tmp/sonnets.txt').read()",
                "corpus = data.lower().split('\\n')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Define variable for tokenizing and modelling",
                "EMBEDDING_DIM = 100",
                "TRUNC_TYPE = 'post'",
                "PADDING_TYPE = 'pre'",
                "OOV_TOK = '<OOV>'",
                "TRAIN_PROP = .8"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = Tokenizer()",
                "ASSIGN.fit_on_texts(corpus)",
                "ASSIGN = tokenizer.ASSIGN",
                "ASSIGN = []",
                "for text in corpus :",
                "ASSIGN = tokenizer.texts_to_sequences([text])[0]",
                "for i in range(1, len(ASSIGN)) :",
                "ASSIGN = token_list[:i+1]",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pad_sequences(train_seq, padding=PADDING_TYPE, maxlen=MAX_LENGTH)",
                "ASSIGN = train_padded[:,:-1], train_padded[:,-1]",
                "ASSIGN = to_categorical(ASSIGN, num_classes=VOCAB_SIZE)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make the input sequences for prediciton word",
                "from tensorflow.keras.preprocessing.text import Tokenizer",
                "from tensorflow.keras.preprocessing.sequence import pad_sequences",
                "tokenizer = Tokenizer()",
                "",
                "# Tokenize the corpus",
                "tokenizer.fit_on_texts(corpus)",
                "word_index = tokenizer.word_index",
                "VOCAB_SIZE = len(word_index) + 1",
                "",
                "# Make input sequences",
                "train_seq = []",
                "for text in corpus :",
                "    token_list = tokenizer.texts_to_sequences([text])[0]",
                "    ",
                "    for i in range(1, len(token_list)) :",
                "        gram = token_list[:i+1]",
                "        train_seq.append(gram)",
                "        ",
                "# Pad the sequences",
                "MAX_LENGTH = np.max([len(seq) for seq in train_seq])",
                "train_padded = pad_sequences(train_seq, padding=PADDING_TYPE, maxlen=MAX_LENGTH)",
                "",
                "# Split to train data and label",
                "train_data, train_label = train_padded[:,:-1], train_padded[:,-1]",
                "",
                "# One hot label",
                "from tensorflow.keras.utils import to_categorical",
                "train_label = to_categorical(train_label, num_classes=VOCAB_SIZE)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "https:path\\",
                "-O path",
                "ASSIGN = {};",
                "with open('path') as f:",
                "for line in f:",
                "ASSIGN = line.split();",
                "ASSIGN = values[0];",
                "ASSIGN = np.asarray(values[1:], dtype='float32');",
                "ASSIGN[ASSIGN] = ASSIGN;",
                "ASSIGN = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for ASSIGN, i in word_index.items():",
                "ASSIGN = embeddings_index.get(word);",
                "if ASSIGN is not None:",
                "ASSIGN[i] = ASSIGN;"
            ],
            "output_type": "stream",
            "content_old": [
                "### Get weight for embedding matrix in model",
                "!wget --no-check-certificate \\",
                "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\",
                "    -O /tmp/glove.6B.100d.txt",
                "    ",
                "# Load the weight",
                "embeddings_index = {};",
                "with open('/tmp/glove.6B.100d.txt') as f:",
                "    for line in f:",
                "        values = line.split();",
                "        word = values[0];",
                "        coefs = np.asarray(values[1:], dtype='float32');",
                "        embeddings_index[word] = coefs;",
                "",
                "# Make weight matrix",
                "embeddings_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM));",
                "for word, i in word_index.items():",
                "    embedding_vector = embeddings_index.get(word);",
                "    if embedding_vector is not None:",
                "        embeddings_matrix[i] = embedding_vector;"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH-1,",
                "ASSIGN=[embeddings_matrix], trainable=True))",
                "ASSIGN.add(Bidirectional(LSTM(256, return_sequences=True)))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Bidirectional(LSTM(128)))",
                "ASSIGN.add(Dense(VOCAB_SIZE path, activation='relu', kernel_regularizer=l2(0.01)))",
                "ASSIGN.add(Dense(VOCAB_SIZE, activation='softmax'))",
                "ASSIGN.compile(optimizer = Adam(),",
                "ASSIGN = 'categorical_crossentropy',",
                "ASSIGN = 'accuracy')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Make model",
                "from tensorflow.keras.layers import Dense, Embedding, TimeDistributed, Dropout, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, LSTM, Bidirectional, GRU",
                "from tensorflow.keras import Sequential, Model",
                "from tensorflow.keras.regularizers import l1, l2",
                "from tensorflow.keras.optimizers import SGD, Adam, RMSprop",
                "from tensorflow.keras.metrics import Precision",
                "",
                "# Make simple embedding model",
                "model = Sequential()",
                "model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH-1,",
                "                    weights=[embeddings_matrix], trainable=True))",
                "model.add(Bidirectional(LSTM(256, return_sequences=True)))",
                "model.add(Dropout(0.2))",
                "model.add(Bidirectional(LSTM(128)))",
                "model.add(Dense(VOCAB_SIZE // 2, activation='relu', kernel_regularizer=l2(0.01)))",
                "model.add(Dense(VOCAB_SIZE, activation='softmax'))",
                "",
                "model.compile(optimizer = Adam(),",
                "              loss = 'categorical_crossentropy',",
                "              metrics = 'accuracy')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 100",
                "ASSIGN = model.fit(train_data,",
                "train_label,",
                "ASSIGN=num_epochs,",
                "ASSIGN=1)"
            ],
            "output_type": "stream",
            "content_old": [
                "### Train model",
                "num_epochs = 100",
                "history = model.fit(train_data,",
                "                    train_label,",
                "                    epochs=num_epochs,",
                "                    verbose=1)",
                "",
                "%time"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_history(history) :",
                "rcParams['figure.figsize'] = [10,8]",
                "plt.style.use('fivethirtyeight')",
                "sns.set_style('whitegrid')",
                "ASSIGN = gridspec.GridSpec(2,1)",
                "ASSIGN   = history.history[   'accuracy' ]",
                "ASSIGN   = history.history[  'ASSIGN' ]",
                "ASSIGN  = range(len(acc))",
                "ASSIGN = plt.subplot(grid[0])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Loss')",
                "ASSIGN.set_title('Training Loss')",
                "ASSIGN.legend() ;",
                "ASSIGN = plt.subplot(grid[1])",
                "ASSIGN.plot(ASSIGN, ASSIGN, label='Train Acc')",
                "ASSIGN.set_title('Training Accuracy')",
                "ASSIGN.legend() ;"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### Plot the performance of the model",
                "def plot_history(history) :",
                "    rcParams['figure.figsize'] = [10,8]",
                "    plt.style.use('fivethirtyeight') ",
                "    sns.set_style('whitegrid')",
                "    grid = gridspec.GridSpec(2,1)",
                "",
                "    # Get the metrics and loss",
                "    acc      = history.history[     'accuracy' ]",
                "    loss     = history.history[    'loss' ]",
                "    epo   = range(len(acc)) # Get number of epochs",
                "",
                "    # Plot the loss",
                "    ax = plt.subplot(grid[0])",
                "    ax.plot(epo, loss, label='Train Loss')",
                "    ax.set_title('Training Loss')",
                "    ax.legend() ;",
                "",
                "    # Plot the acccuracy",
                "    ax = plt.subplot(grid[1])",
                "    ax.plot(epo, acc, label='Train Acc')",
                "    ax.set_title('Training Accuracy')",
                "    ax.legend() ;"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_history(history)"
            ],
            "output_type": "display_data",
            "content_old": [
                "### Plot model performance - LSTM no Bidirection",
                "plot_history(history)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = \"Help me Obi Wan Kenobi, you're my only hope\"",
                "ASSIGN = 100",
                "for _ in range(ASSIGN):",
                "ASSIGN = tokenizer.texts_to_sequences([seed_text])[0]",
                "ASSIGN = pad_sequences([ASSIGN], maxlen=MAX_LENGTH-1, padding='pre')",
                "ASSIGN = model.predict_classes(token_list, verbose=0)",
                "ASSIGN = \"\"",
                "for word, index in tokenizer.word_index.items():",
                "ASSIGN == predicted:",
                "ASSIGN = word",
                "break",
                "ASSIGN += \" \" + ASSIGN",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"",
                "next_words = 100",
                "  ",
                "for _ in range(next_words):",
                "    token_list = tokenizer.texts_to_sequences([seed_text])[0]",
                "    token_list = pad_sequences([token_list], maxlen=MAX_LENGTH-1, padding='pre')",
                "    predicted = model.predict_classes(token_list, verbose=0)",
                "    output_word = \"\"",
                "    for word, index in tokenizer.word_index.items():",
                "        if index == predicted:",
                "            output_word = word",
                "            break",
                "    seed_text += \" \" + output_word",
                "print(seed_text)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"..path\", index_col='Id')",
                "train"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv(\"../input/dataset/train_data.csv\", index_col='Id')",
                "train"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "(train == '?').any()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "(train == '?').any()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[train == '?'].count()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[train == '?'].count()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['workclass'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train['workclass'].value_counts().plot(kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['occupation'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train['occupation'].value_counts().plot(kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train['native.country'].value_counts().plot(kind = 'bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train['native.country'].value_counts().plot(kind = 'bar')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.replace(to_replace = '?', value = 'Private')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train['workclass'] = train['workclass'].replace(to_replace = '?', value = 'Private')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train['native.country'] = train['native.country'].replace(to_replace = '?', value = 'United-States')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train['native.country'] = train['native.country'].replace(to_replace = '?', value = 'United-States')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[train == '?'].count()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[train == '?'].count()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.loc[ASSIGN.occupation != '?']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = train.loc[train.occupation != '?']"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = train.loc[:,'age':'native.country']",
                "ASSIGN = train.income",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtrain = train.loc[:,'age':'native.country']",
                "Ytrain = train.income",
                "Xtrain.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ytrain.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ytrain.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.model_selection import cross_val_score"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn import preprocessing"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.get_dummies(ASSIGN)",
                "Xtrain"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtrain = pd.get_dummies(Xtrain)",
                "Xtrain"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "ASSIGN = KNeighborsClassifier(n_neighbors = 10)",
                "ASSIGN = cross_val_score(knn, Xtrain, Ytrain, cv=10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn = KNeighborsClassifier(n_neighbors = 10)",
                "scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "scores.mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scores.mean()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "ASSIGN = []",
                "for i in range(1,25):",
                "ASSIGN = KNeighborsClassifier(n_neighbors = i)",
                "ASSIGN = cross_val_score(knn, Xtrain, Ytrain, cv=10)",
                "ASSIGN.append(ASSIGN.mean())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scores_array = []",
                "for i in range(1,25):",
                "    knn = KNeighborsClassifier(n_neighbors = i)",
                "    scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)",
                "    scores_array.append(scores.mean())",
                "    ",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(scores_array, 'ro')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.plot(scores_array, 'ro')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('..path', na_values='?', index_col='Id')",
                "test"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test = pd.read_csv('../input/dataset/test_data.csv', na_values='?', index_col='Id')",
                "test"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.get_dummies(test)",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtest = pd.get_dummies(test)",
                "Xtest.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = set( Xtrain.columns ) - set( Xtest.columns )",
                "for c in ASSIGN:",
                "ASSIGN = 0",
                "ASSIGN = ASSIGN[Xtrain.columns]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "##garantindo que ambas tenham a mesma dimenso",
                "missing_cols = set( Xtrain.columns ) - set( Xtest.columns )",
                "for c in missing_cols:",
                "    Xtest[c] = 0",
                "Xtest = Xtest[Xtrain.columns]"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = ASSIGN = KNeighborsClassifier(n_neighbors = 21)",
                "ASSIGN.fit(Xtrain,Ytrain)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn = knn = KNeighborsClassifier(n_neighbors = 21)",
                "knn.fit(Xtrain,Ytrain)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "YtestPred = knn.predict(Xtest)",
                "YtestPred"
            ],
            "output_type": "not_existent",
            "content_old": [
                "YtestPred = knn.predict(Xtest)",
                "YtestPred"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(index = test.index)",
                "ASSIGN = YtestPred"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction = pd.DataFrame(index = test.index)",
                "prediction['income'] = YtestPred"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "prediction"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "prediction.to_csv(\"submition.csv\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction.to_csv(\"submition.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "import numpy as np",
                "from sklearn.model_selection import cross_val_score"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", index_col = 'Id')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv(\"../input/train.csv\", index_col = 'Id')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsRegressor"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = KNeighborsRegressor(38)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn = KNeighborsRegressor(38)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = cross_val_score(knn, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn_score = cross_val_score(knn, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "knn_score.mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn_score.mean()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.linear_model import LassoCV"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = LassoCV(cv = 10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lasso = LassoCV(cv = 10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = cross_val_score(lasso, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lasso_score = cross_val_score(lasso, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "lasso_score.mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lasso_score.mean()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.linear_model import RidgeCV"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = RidgeCV(cv = 10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ridge = RidgeCV(cv = 10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = cross_val_score(ridge, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ridge_score = cross_val_score(ridge, train.drop('median_house_value', axis = 1), train['median_house_value'], cv = 10, scoring = 'r2')"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "ridge_score.mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ridge_score.mean()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np",
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", index_col = 'Id')",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv(\"../input/featdataset/train_data.csv\", index_col = 'Id')",
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "plt.figure(figsize=(15,10))",
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize=(15,10))",
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import sklearn",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.model_selection import cross_val_score"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = train[train.columns[0:57]]",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtrain = train[train.columns[0:57]]",
                "Xtrain.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = train['ham']",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ytrain = train['ham']",
                "Ytrain.head()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = GaussianNB()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB = GaussianNB()",
                ""
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = cross_val_score(NB, Xtrain,Ytrain, cv=10)",
                "scores"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scores = cross_val_score(NB, Xtrain,Ytrain, cv=10)",
                "scores"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path', index_col = 'Id')",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test = pd.read_csv('../input/featdataset/test_features.csv', index_col = 'Id')",
                "test.head()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "NB.fit(Xtrain,Ytrain)",
                "ASSIGN = NB.predict(test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB.fit(Xtrain,Ytrain)",
                "Ytest = NB.predict(test)"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(index = test.index)",
                "ASSIGN = Ytest",
                "ASSIGN.to_csv('submission.csv',index = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pred = pd.DataFrame(index = test.index)",
                "pred['ham'] = Ytest",
                "pred.to_csv('submission.csv',index = True)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 7",
                "np.random.ASSIGN(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt",
                "import numpy as np",
                "import pandas as pd",
                "seed = 7",
                "np.random.seed(seed)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.load(\"..path\")",
                "ASSIGN = np.load(\"..path\")",
                "ASSIGN = np.load(\"..path\")",
                "ASSIGN = np.load(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "tlabel"
            ],
            "output_type": "not_existent",
            "content_old": [
                "tpure = np.load(\"../input/train_images_pure.npy\")",
                "tnoisy = np.load(\"../input/train_images_noisy.npy\")",
                "trotated = np.load(\"../input/train_images_rotated.npy\")",
                "tboth = np.load(\"../input/train_images_both.npy\")",
                "tlabel = pd.read_csv(\"../input/train_labels.csv\")",
                "tlabel"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tpure[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tpure[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tpure[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tpure[3], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.subplot(221)",
                "plt.imshow(tpure[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tpure[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tpure[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tpure[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tnoisy[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tnoisy[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tnoisy[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tnoisy[3], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.subplot(221)",
                "plt.imshow(tnoisy[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tnoisy[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tnoisy[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tnoisy[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for i in range(15,19):",
                "plt.subplot(221+(i%5))",
                "plt.imshow(trotated[i], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for i in range(15,19):",
                "    plt.subplot(221+(i%5))",
                "    plt.imshow(trotated[i], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.subplot(221)",
                "plt.imshow(tboth[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tboth[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tboth[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tboth[3], cmap=plt.get_cmap('gray'))",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.subplot(221)",
                "plt.imshow(tboth[0], cmap=plt.get_cmap('gray'))",
                "plt.subplot(222)",
                "plt.imshow(tboth[1], cmap=plt.get_cmap('gray'))",
                "plt.subplot(223)",
                "plt.imshow(tboth[2], cmap=plt.get_cmap('gray'))",
                "plt.subplot(224)",
                "plt.imshow(tboth[3], cmap=plt.get_cmap('gray'))",
                "# show the plot",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "K.set_image_dim_ordering('th')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from keras.models import Sequential",
                "from keras.layers import Dense",
                "from keras.layers import Dropout",
                "from keras.layers import Flatten",
                "from keras.layers.convolutional import Conv2D",
                "from keras.layers.convolutional import MaxPooling2D",
                "from keras.utils import np_utils",
                "from keras import backend as K",
                "from sklearn.model_selection import train_test_split",
                "from keras.callbacks import EarlyStopping",
                "K.set_image_dim_ordering('th')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def DataPrep(db):",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, 28, 28).astype('float32')",
                "ASSIGN = ASSIGN path",
                "ASSIGN = np_utils.to_categorical(ASSIGN)",
                "return db"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def DataPrep(db):",
                "    db = db.reshape(db.shape[0], 1, 28, 28).astype('float32')",
                "    db = db / 255",
                "    db = np_utils.to_categorical(db)",
                "    return db"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np_utils.to_categorical(ASSIGN['label'])",
                "ASSIGN = DataPrep(ASSIGN)",
                "ASSIGN = DataPrep(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "tlabel = np_utils.to_categorical(tlabel['label'])",
                "tpure = DataPrep(tpure)",
                "trotated = DataPrep(trotated)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def deepCNN():",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))",
                "ASSIGN.add(Conv2D(15, (3, 3), activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dense(50, activation='relu'))",
                "ASSIGN.add(Dense(tlabel.shape[1], activation='softmax'))",
                "ASSIGN.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
                "return model"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def deepCNN():",
                "    # create model",
                "    model = Sequential()",
                "    model.add(Conv2D(30, (5, 5), input_shape=(1, 28, 28), activation='relu'))",
                "    model.add(Conv2D(15, (3, 3), activation='relu'))",
                "    model.add(Dropout(0.2))",
                "    model.add(Flatten())",
                "    model.add(Dense(128, activation='relu'))",
                "    model.add(Dense(50, activation='relu'))",
                "    model.add(Dense(tlabel.shape[1], activation='softmax'))",
                "    # Compile model",
                "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])",
                "    return model"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = deepCNN()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "CNNmodel = deepCNN()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = [EarlyStopping(monitor = 'val_loss', patience = 2)]",
                "Xtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(tpure,tlabel, test_size = 0.2)",
                "CNNmodel.fit(Xtrain, Ytrain, validation_data=(Xvalidation,Yvalidation), epochs=20,",
                "ASSIGN=200, verbose=1, callbacks = callbacks)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "callbacks = [EarlyStopping(monitor = 'val_loss', patience = 2)]",
                "Xtrain,Xvalidation,Ytrain,Yvalidation = train_test_split(tpure,tlabel, test_size = 0.2)",
                "CNNmodel.fit(Xtrain, Ytrain, validation_data=(Xvalidation,Yvalidation), epochs=20, ",
                "          batch_size=200, verbose=1, callbacks = callbacks)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv(\"../input/train.csv\")",
                "test = pd.read_csv(\"../input/test.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"idhogar\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"idhogar\"]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"parentesco1\"].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"parentesco1\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train.drop(train[train[\"parentesco1\"] == 0].index)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_only_heads = train.drop(train[train[\"parentesco1\"] == 0].index)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_only_heads[\"parentesco1\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_only_heads[\"parentesco1\"]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train_only_heads.dropna(thresh=len(train_only_heads[\"parentesco1\"])path, axis=\"columns\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_hna = train_only_heads.dropna(thresh=len(train_only_heads[\"parentesco1\"])/2, axis=\"columns\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_hna = train_hna.dropna()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train_hna"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_hna"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train_hna.drop(['Target','Id','idhogar','dependency','edjefe','edjefa'] ,axis = 'columns')",
                "ASSIGN = train_hna.Target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtrain_h = train_hna.drop(['Target','Id','idhogar','dependency','edjefe','edjefa'] ,axis = 'columns')",
                "Ytrain_h = train_hna.Target"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.model_selection import cross_val_score"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = KNeighborsClassifier(n_neighbors=10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn = KNeighborsClassifier(n_neighbors=10)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "scores.mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scores.mean()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "train_model"
            ],
            "content": [
                "ASSIGN = []",
                "for i in range(50):",
                "ASSIGN = KNeighborsClassifier(n_neighbors=i+1)",
                "ASSIGN = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=5)",
                "ASSIGN.append(ASSIGN.mean())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "score_array = []",
                "for i in range(50):",
                "    knn = KNeighborsClassifier(n_neighbors=i+1)",
                "    scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=5)",
                "    score_array.append(scores.mean())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(score_array, 'ro')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.plot(score_array, 'ro')"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = KNeighborsClassifier(n_neighbors=33)",
                "ASSIGN.fit(Xtrain_h, Ytrain_h)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn = KNeighborsClassifier(n_neighbors=33)",
                "knn.fit(Xtrain_h, Ytrain_h)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = test.drop(['Id','idhogar','dependency','edjefe','edjefa','rez_esc', 'v18q1', 'v2a1'] ,axis = 'columns')",
                "Xtest"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtest = test.drop(['Id','idhogar','dependency','edjefe','edjefa','rez_esc', 'v18q1', 'v2a1'] ,axis = 'columns')",
                "Xtest"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.fillna(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Xtest = Xtest.fillna(0)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = knn.predict(Xtest)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pred = knn.predict(Xtest)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.DataFrame(test.Id)",
                "ASSIGN = pred",
                "prediction"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction = pd.DataFrame(test.Id)",
                "prediction['Target'] = pred",
                "prediction"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "prediction.to_csv(\"submition.csv\",index = False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction.to_csv(\"submition.csv\",index = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path', index_col=\"ID\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv('/kaggle/input/cs-challenge/training_set.csv', index_col=\"ID\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna(axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = train.dropna(axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN = train.columns.to_list()",
                "ASSIGN = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "column_list = column_list = train.columns.to_list()\n",
                "non_redundant_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "non_redundant_cols"
            ],
            "output_type": "execute_result",
            "content_old": [
                "non_redundant_cols"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=train"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#train_non_re = train[non_redundant_cols]\n",
                "train_non_re=train"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def ind_max(l):",
                "M=l[0]",
                "ASSIGN=0",
                "for i in range(1,len(l)):",
                "if l[i]>M:",
                "M=l[i]",
                "ASSIGN=i",
                "return ind"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def ind_max(l):\n",
                "    M=l[0]\n",
                "    ind=0\n",
                "    for i in range(1,len(l)):\n",
                "        if l[i]>M:\n",
                "            M=l[i]\n",
                "            ind=i\n",
                "    return ind"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=[-5+ipath(21)]",
                "ASSIGN.remove(0.0)",
                "ASSIGN=[]",
                "for col in column_list:",
                "if col != 'MAC_CODE':",
                "ASSIGN=[]",
                "for p in ASSIGN:",
                "if (p%1==0 or not any(train[col]<0)) and (p>0 or not any(train[col]==0)):",
                "ASSIGN.append(abs(train_non_re['TARGET'].ASSIGN(train_non_re[col]**p)))",
                "else:",
                "ASSIGN.append(0)",
                "ASSIGN=pows[ind_max(corr)]",
                "ASSIGN.append(ASSIGN)",
                "train_non_re[col] = np.power(train_non_re[col], ASSIGN)",
                "res"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#squared_cols = []\n",
                "pows=[-5+i/2 for i in range(21)]\n",
                "pows.remove(0.0)\n",
                "res=[]\n",
                "#for col in non_redundant_cols:\n",
                "for col in column_list:\n",
                "    if col != 'MAC_CODE':\n",
                "        corr=[]\n",
                "        for p in pows:\n",
                "            if (p%1==0 or not any(train[col]<0)) and (p>0 or not any(train[col]==0)):                \n",
                "                    corr.append(abs(train_non_re['TARGET'].corr(train_non_re[col]**p)))\n",
                "            else:\n",
                "                corr.append(0)\n",
                "        p=pows[ind_max(corr)]\n",
                "        res.append(p)\n",
                "        train_non_re[col] = np.power(train_non_re[col], p)\n",
                "res\n",
                "            \n",
                "            \n",
                "            #cor1 = abs(train_non_re['TARGET'].corr(train_non_re[col]))\n",
                "            #cor2 = abs(train_non_re['TARGET'].corr(train_non_re[col]**2))\n",
                "            #if(cor2 > cor1):\n",
                "                #train_non_re[col] = np.power(train_non_re[col], 2)\n",
                "                #squared_cols.append(col)\n",
                "        "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ColumnTransformer([",
                "('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])],",
                "ASSIGN = StandardScaler())",
                "ASSIGN = RidgeCV(cv=5)",
                "ASSIGN = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
                "from sklearn.linear_model import RidgeCV\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "\n",
                "\n",
                "col_transformer = ColumnTransformer([\n",
                "    ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])],\n",
                "    remainder = StandardScaler())\n",
                "\n",
                "reg = RidgeCV(cv=5)\n",
                "\n",
                "pipe = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train_non_re"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_non_re"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "Xtr, Xte, ytr,  yte = train_test_split(train_non_re.drop('TARGET', axis=1), train_non_re['TARGET'], test_size=0.2)",
                "pipe.fit(Xtr,ytr)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "Xtr, Xte, ytr,  yte = train_test_split(train_non_re.drop('TARGET', axis=1), train_non_re['TARGET'], test_size=0.2)\n",
                "\n",
                "pipe.fit(Xtr,ytr)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "mean_absolute_error(yte, pipe.predict(Xte))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "mean_absolute_error(yte, pipe.predict(Xte))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])"
            ],
            "output_type": "stream",
            "content_old": [
                "pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path', index_col=\"ID\")",
                "ASSIGN = ASSIGN[[x for x in column_list if x != 'TARGET']]",
                "for col in squared_cols:",
                "ASSIGN = np.power(ASSIGN,2)",
                "ASSIGN = pipe.ASSIGN(test)"
            ],
            "output_type": "error",
            "content_old": [
                "test = pd.read_csv('/kaggle/input/cs-challenge/test_set.csv', index_col=\"ID\")\n",
                "#test = test[[x for x in non_redundant_cols if x != 'TARGET']]\n",
                "test = test[[x for x in column_list if x != 'TARGET']]\n",
                "\n",
                "for col in squared_cols:\n",
                "    test[col] = np.power(test[col],2)\n",
                "\n",
                "predict = pipe.predict(test)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = predict",
                "test['TARGET'].to_csv(\"squared_ridge.csv\")"
            ],
            "output_type": "error",
            "content_old": [
                "test['TARGET'] = predict\n",
                "test['TARGET'].to_csv(\"squared_ridge.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", index_col=\"ID\").drop(\"MAC_CODE\", axis = 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv(\"../input/cs-challenge/training_set.csv\", index_col=\"ID\").drop(\"MAC_CODE\", axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna(axis = 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = train.dropna(axis = 1)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "SETUP",
                "ASSIGN= StandardScaler()",
                "ASSIGN.fit(train)",
                "ASSIGN = pd.DataFrame(scaler.transform(ASSIGN), columns = ASSIGN.columns, index = ASSIGN.index)",
                "train"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "scaler= StandardScaler()\n",
                "scaler.fit(train)\n",
                "train = pd.DataFrame(scaler.transform(train), columns = train.columns, index = train.index)\n",
                "train"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = linear_model.LassoCV(cv=5, random_state=0, max_iter=10000).fit(train.drop(\"TARGET\",axis=1), train[\"TARGET\"])",
                "print(ASSIGN.score(train.drop(,axis=1), train[]))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn import linear_model\n",
                "\n",
                "lasso_reg = linear_model.LassoCV(cv=5, random_state=0, max_iter=10000).fit(train.drop(\"TARGET\",axis=1), train[\"TARGET\"])\n",
                "print(lasso_reg.score(train.drop(\"TARGET\",axis=1), train[\"TARGET\"]))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.figure(figsize=(20,5))",
                "plt.xticks(rotation = 'vertical')",
                "plt.bar(train.drop(\"TARGET\", axis=1).columns, lasso_reg.coef_)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(20,5))\n",
                "plt.xticks(rotation = 'vertical')\n",
                "plt.bar(train.drop(\"TARGET\", axis=1).columns, lasso_reg.coef_)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN = train.columns.to_list()",
                "ASSIGN = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "column_list = column_list = train.columns.to_list()\n",
                "base_cols = [x for x in column_list if x.find('_max') == -1 and x.find('_min') == -1 and x.find('_c') == -1]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train[base_cols]",
                "train_base"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_base = train[base_cols]\n",
                "train_base"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = linear_model.LassoCV(cv=5, random_state=0, max_iter=20000, fit_intercept=True,normalize = True).fit(train_base.drop(\"TARGET\",axis=1), train_base[\"TARGET\"])",
                "print(ASSIGN.score(train_base.drop(,axis=1), train_base[]))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn import linear_model\n",
                "\n",
                "lasso_reg_base = linear_model.LassoCV(cv=5, random_state=0, max_iter=20000, fit_intercept=True,normalize = True).fit(train_base.drop(\"TARGET\",axis=1), train_base[\"TARGET\"])\n",
                "print(lasso_reg_base.score(train_base.drop(\"TARGET\",axis=1), train_base[\"TARGET\"]))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.figure(figsize=(20,5))",
                "plt.xticks(rotation = 'vertical')",
                "plt.bar(train_base.drop(\"TARGET\", axis=1).columns, lasso_reg_base.coef_)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "plt.figure(figsize=(20,5))\n",
                "plt.xticks(rotation = 'vertical')\n",
                "plt.bar(train_base.drop(\"TARGET\", axis=1).columns, lasso_reg_base.coef_)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", index_col = \"ID\").drop(\"MAC_CODE\",axis=1)",
                "ASSIGN = np.ones(len(test.index))",
                "ASSIGN = pd.DataFrame(scaler.transform(ASSIGN[train.columns.to_list()]), index=ASSIGN.index, columns=train.columns)",
                "ASSIGN = lasso_reg.predict(test[train.columns.to_list()].drop(\"TARGET\", axis=1))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test = pd.read_csv(\"../input/cs-challenge/test_set.csv\", index_col = \"ID\").drop(\"MAC_CODE\",axis=1)\n",
                "test[\"TARGET\"] = np.ones(len(test.index))\n",
                "#test[train.columns.to_list()]\n",
                "test = pd.DataFrame(scaler.transform(test[train.columns.to_list()]), index=test.index, columns=train.columns)\n",
                "p1 = lasso_reg.predict(test[train.columns.to_list()].drop(\"TARGET\", axis=1))"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = p1",
                "ASSIGN = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']",
                "ASSIGN.to_csv('ASSIGN.csv')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test['TARGET'] = p1\n",
                "a1 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']\n",
                "a1.to_csv('a1.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = lasso_reg_base.predict(test[train_base.columns.to_list()].drop(\"TARGET\", axis=1))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "p2 = lasso_reg_base.predict(test[train_base.columns.to_list()].drop(\"TARGET\", axis=1))"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = p2",
                "ASSIGN = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']",
                "ASSIGN.to_csv('ASSIGN.csv')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test[\"TARGET\"] = p2\n",
                "a2 = pd.DataFrame(scaler.inverse_transform(test[train.columns.to_list()]), index=test.index, columns=test.columns)['TARGET']\n",
                "a2.to_csv('a2.csv')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful modules",
                "import fuzzywuzzy",
                "from fuzzywuzzy import process",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "with open(\"..path(30-November-2017).csv\", 'rb') as rawdata:",
                "ASSIGN = chardet.detect(rawdata.read(100000))",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(100000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path(30-November-2017).csv\",",
                "ASSIGN='Windows-1252')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# read in our dat",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", ",
                "                              encoding='Windows-1252')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# convert to lower case",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = suicide_attacks['Province'].unique()",
                "ASSIGN.sort()",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. ",
                "provincia = suicide_attacks['Province'].unique()",
                "provincia.sort()",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "",
                "",
                "# Then convert the column to lowercase and remove any trailing white spaces",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "matches"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "ASSIGN = df[column].unique()",
                "ASSIGN = fuzzywuzzy.process.extract(string_to_match, strings,",
                "ASSIGN=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "ASSIGN = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "ASSIGN = df[column].isin(close_matches)",
                "df.loc[ASSIGN, column] = string_to_match",
                "print()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# function to replace rows in the provided column of the provided dataframe",
                "# that match the provided string above the provided ratio with the provided string",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "    # get a list of unique strings",
                "    strings = df[column].unique()",
                "    ",
                "    # get the top 10 closest matches to our input string",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, ",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "    # only get matches with a ratio > 90",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "",
                "    # get the rows of all the close matches in our dataframe",
                "    rows_with_matches = df[column].isin(close_matches)",
                "",
                "    # replace all rows with close matches with the input matches ",
                "    df.loc[rows_with_matches, column] = string_to_match",
                "    ",
                "    # let us know the function's done",
                "    print(\"All done!\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should",
                "matches = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "# be the same city. Correct the dataframe so that they are.",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# for Box-Cox Transformation",
                "from scipy import stats",
                "",
                "# for min_max scaling",
                "from mlxtend.preprocessing import minmax_scaling",
                "",
                "# plotting modules",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "# read in all our data",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.random.exponential(size = 1000)",
                "ASSIGN = minmax_scaling(original_data, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# generate 1000 data points randomly drawn from an exponential distribution",
                "original_data = np.random.exponential(size = 1000)",
                "",
                "# mix-max scale the data between 0 and 1",
                "scaled_data = minmax_scaling(original_data, columns = [0])",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = stats.boxcox(original_data)",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# normalize the exponential data with boxcox",
                "normalized_data = stats.boxcox(original_data)",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_data[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.usd_goal_real",
                "ASSIGN = minmax_scaling(usd_goal, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# select the usd_goal_real column",
                "usd_goal = kickstarters_2017.usd_goal_real",
                "",
                "# scale the goals from 0 to 1",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.ASSIGN",
                "ASSIGN = minmax_scaling(goal, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! ",
                "",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?",
                "goal = kickstarters_2017.goal",
                "goal_scaled = minmax_scaling(goal, columns = [0])",
                "fig_goal, ax_goal=plt.subplots(1,2)",
                "",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(goal_scaled, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.usd_pledged_real > 0",
                "ASSIGN = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "ASSIGN = stats.boxcox(positive_pledges)[0]",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.pledged > 0",
                "ASSIGN = kickstarters_2017.pledged.loc[index_of_positive]",
                "ASSIGN = stats.boxcox(positive_pledges_x)[0]",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! ",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?",
                "",
                "index_of_positive = kickstarters_2017.pledged > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges_x = kickstarters_2017.pledged.loc[index_of_positive]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges_x = stats.boxcox(positive_pledges_x)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges_x, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges_x, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = '..path'",
                "ASSIGN = pd.read_csv(iowa_file_path)",
                "ASSIGN = home_data.SalePrice",
                "ASSIGN = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']",
                "ASSIGN = home_data[features]",
                "train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1)",
                "ASSIGN = DecisionTreeRegressor(random_state=1)",
                "ASSIGN.fit(train_X, train_y)",
                "ASSIGN = iowa_model.predict(val_X)",
                "ASSIGN = mean_absolute_error(val_predictions, val_y)",
                "print(.format(ASSIGN))",
                "binder.bind(globals())",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Code you have previously used to load data\n",
                "import pandas as pd\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "\n",
                "\n",
                "# Path of the file to read\n",
                "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
                "\n",
                "home_data = pd.read_csv(iowa_file_path)\n",
                "# Create target object and call it y\n",
                "y = home_data.SalePrice\n",
                "# Create X\n",
                "features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
                "X = home_data[features]\n",
                "\n",
                "# Split into validation and training data\n",
                "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
                "\n",
                "# Specify Model\n",
                "iowa_model = DecisionTreeRegressor(random_state=1)\n",
                "# Fit Model\n",
                "iowa_model.fit(train_X, train_y)\n",
                "\n",
                "# Make validation predictions and calculate mean absolute error\n",
                "val_predictions = iowa_model.predict(val_X)\n",
                "val_mae = mean_absolute_error(val_predictions, val_y)\n",
                "print(\"Validation MAE: {:,.0f}\".format(val_mae))\n",
                "\n",
                "# Set up code checking\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.machine_learning.ex5 import *\n",
                "print(\"\\nSetup complete\")"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):",
                "ASSIGN = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)",
                "ASSIGN.fit(train_X, train_y)",
                "ASSIGN = model.predict(val_X)",
                "ASSIGN = mean_absolute_error(val_y, preds_val)",
                "return(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
                "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
                "    model.fit(train_X, train_y)\n",
                "    preds_val = model.predict(val_X)\n",
                "    mae = mean_absolute_error(val_y, preds_val)\n",
                "    return(mae)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#importando bibliotecas\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing\n",
                "import matplotlib.pyplot as plt # visualization\n",
                "import seaborn as sns # visualization\n",
                "from scipy import stats\n",
                "from scipy.stats import norm \n",
                "import warnings \n",
                "warnings.filterwarnings('ignore') #ignore warnings\n",
                "\n",
                "%matplotlib inline\n",
                "import gc"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv(\"C:path\")"
            ],
            "output_type": "error",
            "content_old": [
                "stores=pd.read_csv(\"C:/Users/sony/OneDrive/Documentos/teste_ds_ze/stores.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "stores"
            ],
            "output_type": "error",
            "content_old": [
                "stores"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\ASSIGN.csv\")"
            ],
            "output_type": "error",
            "content_old": [
                "features = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\features.csv\")\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "features"
            ],
            "output_type": "error",
            "content_old": [
                "features"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\ASSIGN.csv\")"
            ],
            "output_type": "error",
            "content_old": [
                "test = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\test.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "test"
            ],
            "output_type": "error",
            "content_old": [
                "test"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\ASSIGN.csv\")"
            ],
            "output_type": "error",
            "content_old": [
                "train = pd.read_csv(\"C:\\\\Users\\\\sony\\\\OneDrive\\\\Documentos\\\\train.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train"
            ],
            "output_type": "error",
            "content_old": [
                "train"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, train.shape)",
                "print(, test.shape)",
                "print(, (round(train.shape[0]*100path(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100path(train.shape[0]+test.shape[0]))))"
            ],
            "output_type": "error",
            "content_old": [
                "print(\"the structure of train data is \", train.shape)\n",
                "print(\"the structure of test  data is \", test.shape)\n",
                "print(\"the ratio of train data : test data is \", (round(train.shape[0]*100/(train.shape[0]+test.shape[0])),100-round(train.shape[0]*100/(train.shape[0]+test.shape[0]))))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.merge(stores, on='Store', how='left')",
                "ASSIGN.head()"
            ],
            "output_type": "error",
            "content_old": [
                "train=train.merge(stores, on='Store', how='left')\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.merge(stores, features)",
                "dataset"
            ],
            "output_type": "error",
            "content_old": [
                "dataset  =  pd.merge(stores, features) \n",
                "dataset"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset)"
            ],
            "output_type": "error",
            "content_old": [
                "sns.pairplot(dataset)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def scatter(dataset, column):",
                "plt.figure()",
                "plt.scatter(dataset[column] , dataset['weeklySales'])",
                "plt.ylabel('weeklySales')",
                "plt.xlabel(column)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def scatter(dataset, column):\n",
                "    plt.figure()\n",
                "    plt.scatter(dataset[column] , dataset['weeklySales'])\n",
                "    plt.ylabel('weeklySales')\n",
                "    plt.xlabel(column)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def scatter(dataset, column):",
                "plt.figure()",
                "plt.scatter(dataset[column] , dataset['weeklySales'])",
                "plt.ylabel('weeklySales')",
                "plt.xlabel(column)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def scatter(dataset, column):\n",
                "    plt.figure()\n",
                "    plt.scatter(dataset[column] , dataset['weeklySales'])\n",
                "    plt.ylabel('weeklySales')\n",
                "    plt.xlabel(column)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize=(18, 14))",
                "ASSIGN = dataset.ASSIGN()",
                "ASSIGN = plt.pcolor(corr)",
                "plt.yticks(np.arange(0.5, len(ASSIGN.index), 1), ASSIGN.index)",
                "plt.xticks(np.arange(0.5, len(ASSIGN.columns), 1), ASSIGN.columns)",
                "ASSIGN.colorbar(ASSIGN)"
            ],
            "output_type": "error",
            "content_old": [
                "fig = plt.figure(figsize=(18, 14))\n",
                "corr = dataset.corr()\n",
                "c = plt.pcolor(corr)\n",
                "plt.yticks(np.arange(0.5, len(corr.index), 1), corr.index)\n",
                "plt.xticks(np.arange(0.5, len(corr.columns), 1), corr.columns)\n",
                "fig.colorbar(c)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset, vars=['weeklySales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ],
            "output_type": "error",
            "content_old": [
                "sns.pairplot(dataset, vars=['weeklySales', 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(dataset, vars=[ 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ],
            "output_type": "error",
            "content_old": [
                "sns.pairplot(dataset, vars=[ 'Fuel_Price', 'Size', 'CPI', 'Dept', 'Temperature', 'Unemployment'])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=pd.Series(train['ASSIGN'].unique())",
                "ASSIGN=pd.Series(train['ASSIGN'].unique())",
                "ASSIGN=pd.Series(train['ASSIGN'].unique())",
                "ASSIGN=pd.Series(train['ASSIGN'].unique())",
                "ASSIGN=pd.Series(train['ASSIGN'].unique())"
            ],
            "output_type": "error",
            "content_old": [
                "Year=pd.Series(train['Year'].unique())\n",
                "Week=pd.Series(train['Week'].unique())\n",
                "Month=pd.Series(train['Month'].unique())\n",
                "Day=pd.Series(train['Day'].unique())\n",
                "n_days=pd.Series(train['n_days'].unique())"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, stores.shape)",
                "print(, stores['Store'].unique())",
                "print(, stores['Type'].unique())"
            ],
            "output_type": "error",
            "content_old": [
                "print(\"the shape of stores data set is\", stores.shape)\n",
                "print(\"the unique value of store is\", stores['Store'].unique())\n",
                "print(\"the unique value of Type is\", stores['Type'].unique())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(stores.head())",
                "ASSIGN=stores.groupby('Type')",
                "print(ASSIGN.describe()['Size'].round(2))"
            ],
            "output_type": "error",
            "content_old": [
                "print(stores.head())\n",
                "grouped=stores.groupby('Type')\n",
                "print(grouped.describe()['Size'].round(2))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')",
                "ASSIGN=['A store','B store','C store']",
                "ASSIGN=grouped.describe()['Size'].round(1)",
                "ASSIGN=[(22path(17+6+22))*100,(17path(17+6+22))*100,(6path(17+6+22))*100]",
                "ASSIGN = plt.subplots(1,1, figsize=(10,10))",
                "ASSIGN={'edgecolor':'black',",
                "'linewidth':2}",
                "ASSIGN = {'fontsize':30}",
                "axes.pie(ASSIGN,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=(0.02,0,0),",
                "ASSIGN='%1.1f%%',",
                "ASSIGN=0.6,",
                "ASSIGN=1.2,",
                "ASSIGN=wprops,",
                "ASSIGN=tprops,",
                "ASSIGN=0.8,",
                "ASSIGN=(0.5,0.5))",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "plt.style.use('ggplot')\n",
                "labels=['A store','B store','C store']\n",
                "sizes=grouped.describe()['Size'].round(1)\n",
                "sizes=[(22/(17+6+22))*100,(17/(17+6+22))*100,(6/(17+6+22))*100] # convert to the proportion\n",
                "\n",
                "\n",
                "fig, axes = plt.subplots(1,1, figsize=(10,10))\n",
                "\n",
                "wprops={'edgecolor':'black',\n",
                "      'linewidth':2}\n",
                "\n",
                "tprops = {'fontsize':30}\n",
                "\n",
                "\n",
                "axes.pie(sizes,\n",
                "        labels=labels,\n",
                "        explode=(0.02,0,0),\n",
                "        autopct='%1.1f%%',\n",
                "        pctdistance=0.6,\n",
                "        labeldistance=1.2,\n",
                "        wedgeprops=wprops,\n",
                "        textprops=tprops,\n",
                "        radius=0.8,\n",
                "        center=(0.5,0.5))\n",
                "plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([stores['Type'], stores['Size']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Type', y='Size', data=data)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([stores['Type'], stores['Size']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Type', y='Size', data=data)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Type'], train['Weekly_Sales']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Type', y='Weekly_Sales', data=data, showfliers=False)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Type'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Type', y='Weekly_Sales', data=data, showfliers=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')",
                "ASSIGN=plt.figure()",
                "ASSIGN=fig.add_subplot(111)",
                "ASSIGN.scatter(train['Size'],train['Weekly_Sales'], alpha=0.5)",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "plt.style.use('ggplot')\n",
                "\n",
                "fig=plt.figure()\n",
                "ax=fig.add_subplot(111)\n",
                "\n",
                "ax.scatter(train['Size'],train['Weekly_Sales'], alpha=0.5)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=stores['Type'].unique()",
                "plt.style.use('ggplot')",
                "ASSIGN=plt.figure(figsize=(10,5))",
                "ASSIGN=fig.add_subplot(111)",
                "for t in ASSIGN:",
                "ASSIGN=train.loc[train['Type']==t, 'Size']",
                "ASSIGN=train.loc[train['Type']==t, 'Weekly_Sales']",
                "ASSIGN.scatter(ASSIGN,ASSIGN,alpha=0.5, label=t)",
                "ASSIGN.set_title('Scatter plot size and sales by store type')",
                "ASSIGN.set_xlabel('Size')",
                "ASSIGN.set_ylabel('Weekly_Sales')",
                "ASSIGN.legend(loc='higher right',fontsize=12)",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "types=stores['Type'].unique()\n",
                "\n",
                "plt.style.use('ggplot')\n",
                "\n",
                "fig=plt.figure(figsize=(10,5))\n",
                "ax=fig.add_subplot(111)\n",
                "\n",
                "for t in types:\n",
                "    x=train.loc[train['Type']==t, 'Size']\n",
                "    y=train.loc[train['Type']==t, 'Weekly_Sales']\n",
                "    \n",
                "    ax.scatter(x,y,alpha=0.5, label=t)\n",
                "\n",
                "ax.set_title('Scatter plot size and sales by store type')\n",
                "ax.set_xlabel('Size')\n",
                "ax.set_ylabel('Weekly_Sales')\n",
                "\n",
                "ax.legend(loc='higher right',fontsize=12)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Store'], train['Weekly_Sales'], train['Type']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(25, 8))",
                "ASSIGN = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"Type\")"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Store'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 8))\n",
                "fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"Type\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Store'], train['Weekly_Sales'], train['IsHoliday']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(25, 8))",
                "ASSIGN = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Store'], train['Weekly_Sales'], train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 8))\n",
                "fig = sns.boxplot(x='Store', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(25, 10))",
                "ASSIGN = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 10))\n",
                "fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(10, 50))",
                "ASSIGN = sns.boxplot(y='Dept', x='Weekly_Sales', data=data, showfliers=False, hue=\"Type\",orient=\"h\")"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(10, 50))\n",
                "fig = sns.boxplot(y='Dept', x='Weekly_Sales', data=data, showfliers=False, hue=\"Type\",orient=\"h\") "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Dept'], train['Weekly_Sales'], train['IsHoliday']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(25, 10))",
                "ASSIGN = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Dept'], train['Weekly_Sales'], train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(25, 10))\n",
                "fig = sns.boxplot(x='Dept', y='Weekly_Sales', data=data, showfliers=False, hue=\"IsHoliday\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('ggplot')",
                "ASSIGN = plt.subplots(1,2, figsize = (20,5))",
                "fig.subplots_adjust(wspace=1, hspace=1)",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)",
                "ASSIGN=train[['IsHoliday','Weekly_Sales']]",
                "ASSIGN=[sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==True],sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==False]]",
                "ASSIGN=['Holiday','Not Holiday']",
                "ASSIGN={'color':'",
                "'linewidth': 2,",
                "'linestyle':'-'}",
                "ASSIGN={'color' : '",
                "'marker' : 'o',",
                "'markerfacecolor': '",
                "'markeredgecolor':'white',",
                "'markersize' : 3,",
                "'linestyle' : 'None',",
                "'linewidth' : 0.1}",
                "axes[0].boxplot(ASSIGN,ASSIGN=ASSIGN, patch_artist = 'Patch',",
                "ASSIGN=True,",
                "ASSIGN=flierprop,",
                "ASSIGN=medianprop)",
                "axes[1].boxplot(ASSIGN,ASSIGN=ASSIGN, patch_artist = 'Patch',",
                "ASSIGN=True,",
                "ASSIGN=flierprop,",
                "ASSIGN=medianprop)",
                "axes[1].set_ylim(-6000,80000)",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "plt.style.use('ggplot')\n",
                "fig, axes = plt.subplots(1,2, figsize = (20,5))\n",
                "fig.subplots_adjust(wspace=1, hspace=1)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "sales_holiday=train[['IsHoliday','Weekly_Sales']]\n",
                "target=[sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==True],sales_holiday['Weekly_Sales'].loc[sales_holiday['IsHoliday']==False]]\n",
                "labels=['Holiday','Not Holiday']\n",
                "\n",
                "#median\n",
                "medianprop={'color':'#2196F3',\n",
                "            'linewidth': 2,\n",
                "            'linestyle':'-'}\n",
                "\n",
                "# outliers\n",
                "\n",
                "flierprop={'color' : '#EC407A',\n",
                "          'marker' : 'o',\n",
                "          'markerfacecolor': '#2196F3',\n",
                "          'markeredgecolor':'white',\n",
                "          'markersize' : 3,\n",
                "          'linestyle' : 'None',\n",
                "          'linewidth' : 0.1}\n",
                "\n",
                "\n",
                "\n",
                "axes[0].boxplot(target,labels=labels, patch_artist = 'Patch',\n",
                "                  showmeans=True,\n",
                "                  flierprops=flierprop,\n",
                "                  medianprops=medianprop)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "axes[1].boxplot(target,labels=labels, patch_artist = 'Patch',\n",
                "                  showmeans=True,\n",
                "                  flierprops=flierprop,\n",
                "                  medianprops=medianprop)\n",
                "\n",
                "axes[1].set_ylim(-6000,80000)\n",
                "\n",
                "plt.show()\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))",
                "print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1))"
            ],
            "output_type": "error",
            "content_old": [
                "print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))\n",
                "print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "error",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Month'], train['Weekly_Sales']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Month'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Month'], train['Weekly_Sales'],train['IsHoliday']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='IsHoliday')"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Month'], train['Weekly_Sales'],train['IsHoliday']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='IsHoliday')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Month'], train['Weekly_Sales'],train['Type']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='Type')"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Month'], train['Weekly_Sales'],train['Type']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Month', y=\"Weekly_Sales\", data=data, showfliers=False, hue='Type')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Year'], train['Weekly_Sales']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "ASSIGN = sns.boxplot(x='Year', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Year'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "fig = sns.boxplot(x='Year', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.concat([train['Week'], train['Weekly_Sales']], axis=1)",
                "ASSIGN = plt.subplots(figsize=(20, 6))",
                "ASSIGN = sns.boxplot(x='Week', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ],
            "output_type": "error",
            "content_old": [
                "data = pd.concat([train['Week'], train['Weekly_Sales']], axis=1)\n",
                "f, ax = plt.subplots(figsize=(20, 6))\n",
                "fig = sns.boxplot(x='Week', y=\"Weekly_Sales\", data=data, showfliers=False)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "sns.distplot(train['Weekly_Sales'])"
            ],
            "output_type": "error",
            "content_old": [
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.distplot(train['Weekly_Sales'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, train['Weekly_Sales'].skew()) #skewness",
                "print(, train['Weekly_Sales'].kurt()) #kurtosis"
            ],
            "output_type": "error",
            "content_old": [
                "print(\"Skewness: \", train['Weekly_Sales'].skew()) #skewness\n",
                "print(\"Kurtosis: \", train['Weekly_Sales'].kurt()) #kurtosis"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train['Weekly_Sales'].min()"
            ],
            "output_type": "error",
            "content_old": [
                "train['Weekly_Sales'].min()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize = (10,5))",
                "ASSIGN.add_subplot(1,2,1)",
                "ASSIGN = stats.probplot(train.loc[train['Weekly_Sales']>0,'Weekly_Sales'], plot=plt)",
                "ASSIGN.add_subplot(1,2,2)",
                "ASSIGN = stats.probplot(np.log1p(train.loc[train['Weekly_Sales']>0,'Weekly_Sales']), plot=plt)"
            ],
            "output_type": "error",
            "content_old": [
                "fig = plt.figure(figsize = (10,5))\n",
                "\n",
                "fig.add_subplot(1,2,1)\n",
                "res = stats.probplot(train.loc[train['Weekly_Sales']>0,'Weekly_Sales'], plot=plt)\n",
                "\n",
                "fig.add_subplot(1,2,2)\n",
                "res = stats.probplot(np.log1p(train.loc[train['Weekly_Sales']>0,'Weekly_Sales']), plot=plt)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()['Weekly_Sales']"
            ],
            "output_type": "error",
            "content_old": [
                "train.describe()['Weekly_Sales']"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=train[train['Weekly_Sales']>0]",
                "ASSIGN=train[train['Weekly_Sales']<=0]",
                "ASSIGN = np.log1p(train_over_zero['Weekly_Sales'])",
                "ASSIGN = plt.subplots(figsize=(8, 6))",
                "sns.distplot(ASSIGN)"
            ],
            "output_type": "error",
            "content_old": [
                "train_over_zero=train[train['Weekly_Sales']>0]\n",
                "train_below_zero=train[train['Weekly_Sales']<=0]\n",
                "sales_over_zero = np.log1p(train_over_zero['Weekly_Sales'])\n",
                "#histogram\n",
                "f, ax = plt.subplots(figsize=(8, 6))\n",
                "sns.distplot(sales_over_zero)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, sales_over_zero.skew()) #skewness",
                "print(, sales_over_zero.kurt()) #kurtosis"
            ],
            "output_type": "error",
            "content_old": [
                "print(\"Skewness: \", sales_over_zero.skew()) #skewness\n",
                "print(\"Kurtosis: \", sales_over_zero.kurt()) #kurtosis"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=train.groupby(['Dept','Date']).mean().round(0).reset_index()",
                "print(ASSIGN.shape)",
                "print(ASSIGN.head())",
                "ASSIGN=grouped[['Dept','Date','Weekly_Sales']]",
                "ASSIGN=train['Dept'].unique()",
                "ASSIGN.sort()",
                "ASSIGN=dept[0:20]",
                "ASSIGN=dept[20:40]",
                "ASSIGN=dept[40:60]",
                "ASSIGN=dept[60:]",
                "ASSIGN = plt.subplots(2,2,figsize=(20,10))",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Dept']==i]",
                "ax[0,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales')",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Dept']==i]",
                "ax[0,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales')",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Dept']==i]",
                "ax[1,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales')",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Dept']==i]",
                "ax[1,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'],label='Dept_1_mean_sales')",
                "ax[0,0].set_title('Mean sales record by department(0~19)')",
                "ax[0,1].set_title('Mean sales record by department(20~39)')",
                "ax[1,0].set_title('Mean sales record by department(40~59)')",
                "ax[1,1].set_title('Mean sales record by department(60~)')",
                "ax[0,0].set_ylabel('Mean sales')",
                "ax[0,0].set_xlabel('Date')",
                "ax[0,1].set_ylabel('Mean sales')",
                "ax[0,1].set_xlabel('Date')",
                "ax[1,0].set_ylabel('Mean sales')",
                "ax[1,0].set_xlabel('Date')",
                "ax[1,1].set_ylabel('Mean sales')",
                "ax[1,1].set_xlabel('Date')",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "grouped=train.groupby(['Dept','Date']).mean().round(0).reset_index()\n",
                "print(grouped.shape)\n",
                "print(grouped.head())\n",
                "data=grouped[['Dept','Date','Weekly_Sales']]\n",
                "\n",
                "\n",
                "dept=train['Dept'].unique()\n",
                "dept.sort()\n",
                "dept_1=dept[0:20]\n",
                "dept_2=dept[20:40]\n",
                "dept_3=dept[40:60]\n",
                "dept_4=dept[60:]\n",
                "\n",
                "fig, ax = plt.subplots(2,2,figsize=(20,10))\n",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "for i in dept_1 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n",
                "\n",
                "for i in dept_2 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[0,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')\n",
                "    \n",
                "for i in dept_3 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[1,0].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')    \n",
                "\n",
                "for i in dept_4 :\n",
                "    data_1=data[data['Dept']==i]\n",
                "    ax[1,1].plot(data_1['Date'], data_1['Weekly_Sales'],label='Dept_1_mean_sales')        \n",
                "    \n",
                "ax[0,0].set_title('Mean sales record by department(0~19)')\n",
                "ax[0,1].set_title('Mean sales record by department(20~39)')\n",
                "ax[1,0].set_title('Mean sales record by department(40~59)')\n",
                "ax[1,1].set_title('Mean sales record by department(60~)')\n",
                "\n",
                "\n",
                "ax[0,0].set_ylabel('Mean sales')\n",
                "ax[0,0].set_xlabel('Date')\n",
                "ax[0,1].set_ylabel('Mean sales')\n",
                "ax[0,1].set_xlabel('Date')\n",
                "ax[1,0].set_ylabel('Mean sales')\n",
                "ax[1,0].set_xlabel('Date')\n",
                "ax[1,1].set_ylabel('Mean sales')\n",
                "ax[1,1].set_xlabel('Date')\n",
                "\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN=train.groupby(['Store','Date']).mean().round(0).reset_index()",
                "grouped.shape",
                "ASSIGN.head()",
                "ASSIGN=grouped[['Store','Date','Weekly_Sales']]",
                "type(ASSIGN)",
                "ASSIGN=train['Store'].unique()",
                "ASSIGN.sort()",
                "ASSIGN=store[0:5]",
                "ASSIGN=store[5:10]",
                "ASSIGN=store[10:15]",
                "ASSIGN=store[15:20]",
                "ASSIGN=store[20:25]",
                "ASSIGN=store[25:30]",
                "ASSIGN=store[30:35]",
                "ASSIGN=store[35:40]",
                "ASSIGN=store[40:]",
                "ASSIGN = plt.subplots(5,2,figsize=(20,15))",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[0,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[0,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[1,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[1,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[2,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[2,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[3,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[3,1].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "for i in ASSIGN :",
                "ASSIGN=data[data['Store']==i]",
                "ax[4,0].plot(ASSIGN['Date'], ASSIGN['Weekly_Sales'])",
                "ax[0,0].set_title('Mean sales record by ASSIGN(0~4)')",
                "ax[0,1].set_title('Mean sales record by ASSIGN(5~9)')",
                "ax[1,0].set_title('Mean sales record by ASSIGN(10~14)')",
                "ax[1,1].set_title('Mean sales record by ASSIGN(15~19)')",
                "ax[2,0].set_title('Mean sales record by ASSIGN(20~24)')",
                "ax[2,1].set_title('Mean sales record by ASSIGN(25~29)')",
                "ax[3,0].set_title('Mean sales record by ASSIGN(30~34)')",
                "ax[3,1].set_title('Mean sales record by ASSIGN(35~39)')",
                "ax[4,0].set_title('Mean sales record by ASSIGN(40~)')",
                "ax[0,0].set_ylabel('Mean sales')",
                "ax[0,0].set_xlabel('Date')",
                "ax[0,1].set_ylabel('Mean sales')",
                "ax[0,1].set_xlabel('Date')",
                "ax[1,0].set_ylabel('Mean sales')",
                "ax[1,0].set_xlabel('Date')",
                "ax[1,1].set_ylabel('Mean sales')",
                "ax[1,1].set_xlabel('Date')",
                "ax[2,0].set_ylabel('Mean sales')",
                "ax[2,0].set_xlabel('Date')",
                "ax[2,1].set_ylabel('Mean sales')",
                "ax[2,1].set_xlabel('Date')",
                "ax[3,0].set_ylabel('Mean sales')",
                "ax[3,0].set_xlabel('Date')",
                "ax[3,1].set_ylabel('Mean sales')",
                "ax[3,1].set_xlabel('Date')",
                "ax[4,0].set_ylabel('Mean sales')",
                "ax[4,0].set_xlabel('Date')",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "grouped=train.groupby(['Store','Date']).mean().round(0).reset_index()\n",
                "grouped.shape\n",
                "grouped.head()\n",
                "\n",
                "data=grouped[['Store','Date','Weekly_Sales']]\n",
                "type(data)\n",
                "\n",
                "\n",
                "store=train['Store'].unique()\n",
                "store.sort()\n",
                "store_1=store[0:5]\n",
                "store_2=store[5:10]\n",
                "store_3=store[10:15]\n",
                "store_4=store[15:20]\n",
                "store_5=store[20:25]\n",
                "store_6=store[25:30]\n",
                "store_7=store[30:35]\n",
                "store_8=store[35:40]\n",
                "store_9=store[40:]\n",
                "\n",
                "fig, ax = plt.subplots(5,2,figsize=(20,15))\n",
                "\n",
                "fig.subplots_adjust(wspace=0.5, hspace=0.5)\n",
                "fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\n",
                "\n",
                "for i in store_1 :\n",
                "    data_1=data[data['Store']==i]\n",
                "    ax[0,0].plot(data_1['Date'], data_1['Weekly_Sales'])\n",
                "    \n",
                "for i in store_2 :\n",
                "    data_2=data[data['Store']==i]\n",
                "    ax[0,1].plot(data_2['Date'], data_2['Weekly_Sales'])\n",
                "    \n",
                "for i in store_3 :\n",
                "    data_3=data[data['Store']==i]\n",
                "    ax[1,0].plot(data_3['Date'], data_3['Weekly_Sales'])\n",
                "\n",
                "for i in store_4 :\n",
                "    data_4=data[data['Store']==i]\n",
                "    ax[1,1].plot(data_4['Date'], data_4['Weekly_Sales'])\n",
                "    \n",
                "for i in store_5 :\n",
                "    data_5=data[data['Store']==i]\n",
                "    ax[2,0].plot(data_5['Date'], data_5['Weekly_Sales'])  \n",
                "\n",
                "for i in store_6 :\n",
                "    data_6=data[data['Store']==i]\n",
                "    ax[2,1].plot(data_6['Date'], data_6['Weekly_Sales'])  \n",
                "\n",
                "for i in store_7 :\n",
                "    data_7=data[data['Store']==i]\n",
                "    ax[3,0].plot(data_7['Date'], data_7['Weekly_Sales'])      \n",
                "\n",
                "for i in store_8 :\n",
                "    data_8=data[data['Store']==i]\n",
                "    ax[3,1].plot(data_8['Date'], data_8['Weekly_Sales'])     \n",
                "    \n",
                "for i in store_9 :\n",
                "    data_9=data[data['Store']==i]\n",
                "    ax[4,0].plot(data_9['Date'], data_9['Weekly_Sales'])     \n",
                "\n",
                "    \n",
                "ax[0,0].set_title('Mean sales record by store(0~4)')\n",
                "ax[0,1].set_title('Mean sales record by store(5~9)')\n",
                "ax[1,0].set_title('Mean sales record by store(10~14)')\n",
                "ax[1,1].set_title('Mean sales record by store(15~19)')\n",
                "ax[2,0].set_title('Mean sales record by store(20~24)')\n",
                "ax[2,1].set_title('Mean sales record by store(25~29)')\n",
                "ax[3,0].set_title('Mean sales record by store(30~34)')\n",
                "ax[3,1].set_title('Mean sales record by store(35~39)')\n",
                "ax[4,0].set_title('Mean sales record by store(40~)')\n",
                "\n",
                "\n",
                "\n",
                "ax[0,0].set_ylabel('Mean sales')\n",
                "ax[0,0].set_xlabel('Date')\n",
                "ax[0,1].set_ylabel('Mean sales')\n",
                "ax[0,1].set_xlabel('Date')\n",
                "ax[1,0].set_ylabel('Mean sales')\n",
                "ax[1,0].set_xlabel('Date')\n",
                "ax[1,1].set_ylabel('Mean sales')\n",
                "ax[1,1].set_xlabel('Date')\n",
                "ax[2,0].set_ylabel('Mean sales')\n",
                "ax[2,0].set_xlabel('Date')\n",
                "ax[2,1].set_ylabel('Mean sales')\n",
                "ax[2,1].set_xlabel('Date')\n",
                "ax[3,0].set_ylabel('Mean sales')\n",
                "ax[3,0].set_xlabel('Date')\n",
                "ax[3,1].set_ylabel('Mean sales')\n",
                "ax[3,1].set_xlabel('Date')\n",
                "ax[4,0].set_ylabel('Mean sales')\n",
                "ax[4,0].set_xlabel('Date')\n",
                "\n",
                "\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "error",
            "content_old": [
                "import pandas as pd\n",
                "features = pd.read_csv(\"../input/features.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "error",
            "content_old": [
                "import pandas as pd\n",
                "test = pd.read_csv(\"../input/test.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "error",
            "content_old": [
                "import pandas as pd\n",
                "train = pd.read_csv(\"../input/train.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Importando as funes necessrias\n",
                "from __future__ import print_function\n",
                "\n",
                "import tensorflow as tf \n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense\n",
                "from tensorflow.keras.optimizers import RMSprop\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import LabelEncoder"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Lendo o dataset e imprimindo uma amostra\n",
                "breast_cancer = pd.read_csv('../input/data.csv')\n",
                "breast_cancer.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "breast_cancer.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Descrevendo as informaes dos tipos de dados do dataset\n",
                "breast_cancer.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "breast_cancer.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Verificando o tamanho do dataset (569 linhas / 33 colunas)\n",
                "breast_cancer.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "breast_cancer.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Gerando estatsticas descritivas do dataset que apresentam um resumo dos dados, excluindo valores nulos\n",
                "breast_cancer.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "breast_cancer.groupby('diagnosis').size()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Contagem de registros agrupados pela coluna 'diagnosis', que representa o diagnstico (B = Benigno / M = Maligno)\n",
                "breast_cancer.groupby('diagnosis').size()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "breast_cancer.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Contagem de registros nulos por coluna\n",
                "breast_cancer.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = breast_cancer.columns[2:-1]",
                "ASSIGN = breast_cancer[feature_names]",
                "ASSIGN = breast_cancer.diagnosis"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# As colunas 'id' e 'Unnamed: 32' no so teis para a anlise e sero descartadas \n",
                "feature_names = breast_cancer.columns[2:-1]\n",
                "x = breast_cancer[feature_names]\n",
                "# A coluna 'diagnosis'  a caracterstica que vamos prever\n",
                "y = breast_cancer.diagnosis"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = LabelEncoder()",
                "ASSIGN = class_le.fit_transform(breast_cancer.diagnosis.values)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Transforma os dados da coluna 'diagnosis' para valores binrios (M = 1 / B = 0)\n",
                "class_le = LabelEncoder()\n",
                "y = class_le.fit_transform(breast_cancer.diagnosis.values)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.heatmap(",
                "ASSIGN=x.corr(),",
                "ASSIGN=True,",
                "ASSIGN='.2f',",
                "ASSIGN='RdYlGn'",
                ")",
                "ASSIGN = plt.gcf()",
                "ASSIGN.set_size_inches(20, 16)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Gera uma matriz de correlao (heatmap) que fornece informaes teis sobre a relao entre cada varivel do conjunto de dados\n",
                "sns.heatmap(\n",
                "    data=x.corr(),\n",
                "    annot=True,\n",
                "    fmt='.2f',\n",
                "    cmap='RdYlGn'\n",
                ")\n",
                "\n",
                "fig = plt.gcf()\n",
                "fig.set_size_inches(20, 16)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train_test_split(",
                "x,",
                "y,",
                "ASSIGN=42,",
                "ASSIGN=0.32",
                ")",
                "print(x_train.shape, y_train.shape)",
                "print(x_test.shape, y_test.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Obtendo os conjuntos de treino e teste, separando 32% do conjunto de dados para teste (test_size=0.32) e o restante para treino\n",
                "x_train, x_test, y_train, y_test = train_test_split(\n",
                "    x,\n",
                "    y,\n",
                "    random_state=42,\n",
                "    test_size=0.32\n",
                ")\n",
                "\n",
                "print(x_train.shape, y_train.shape)\n",
                "print(x_test.shape, y_test.shape)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 64",
                "ASSIGN = 2",
                "ASSIGN = 200",
                "ASSIGN = ASSIGN.astype('float32')",
                "ASSIGN = ASSIGN.astype('float32')",
                "ASSIGN = tf.keras.utils.to_categorical(ASSIGN, num_classes)",
                "ASSIGN = tf.keras.utils.to_categorical(ASSIGN, num_classes)",
                "ASSIGN = Sequential()",
                "ASSIGN.add(tf.keras.layers.Dense(100, input_dim=30, activation='sigmoid'))",
                "ASSIGN.add(tf.keras.layers.Dense(25, input_dim=30, activation='relu'))",
                "ASSIGN.add(tf.keras.layers.Dense(2, activation='softmax'))",
                "ASSIGN.summary()",
                "ASSIGN.compile(loss='categorical_crossentropy',",
                "ASSIGN=RMSprop(0.0001),",
                "ASSIGN=['accuracy'])",
                "ASSIGN = model.fit(x_train, y_train,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=1,",
                "ASSIGN=(x_test, y_test))",
                "ASSIGN = model.evaluate(x_test, y_test, verbose=1)",
                "print('Test loss:', ASSIGN[0])",
                "print('Test accuracy:', ASSIGN[1])",
                "plt.figure()",
                "plt.plot(np.arange(0,ASSIGN), ASSIGN.history[\"loss\"], label=\"train_loss\")",
                "plt.plot(np.arange(0,ASSIGN), ASSIGN.history[\"val_loss\"], label=\"val_loss\")",
                "plt.plot(np.arange(0,ASSIGN), ASSIGN.history[\"acc\"], label=\"train_acc\")",
                "plt.plot(np.arange(0,ASSIGN), ASSIGN.history[\"val_acc\"], label=\"val_acc\")",
                "plt.title(\"Acurcia\")",
                "plt.xlabel(\"pocas #\")",
                "plt.ylabel(\"Losspath\")",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Implementao da rede neural, utilizando 2 classes (diagnosis) e 200 pocas\n",
                "batch_size = 64\n",
                "num_classes = 2\n",
                "epochs = 200\n",
                "\n",
                "# Transformando os dados de entrada para float32\n",
                "x_train = x_train.astype('float32')\n",
                "x_test = x_test.astype('float32')\n",
                "\n",
                "# Convertendo os vetores das classes em matrizes de classificao binrias\n",
                "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
                "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
                "\n",
                "# Definio da arquitetura do modelo\n",
                "model = Sequential()\n",
                "# Camadas do modelo\n",
                "model.add(tf.keras.layers.Dense(100, input_dim=30, activation='sigmoid'))\n",
                "model.add(tf.keras.layers.Dense(25, input_dim=30, activation='relu'))\n",
                "model.add(tf.keras.layers.Dense(2, activation='softmax'))\n",
                "\n",
                "# Fim - Definio da arquitetura do modelo\n",
                "\n",
                "model.summary()\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer=RMSprop(0.0001),\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "# Treinamento do modelo \n",
                "H = model.fit(x_train, y_train,\n",
                "                    batch_size=batch_size,\n",
                "                    epochs=epochs,\n",
                "                    verbose=1,\n",
                "                    validation_data=(x_test, y_test))\n",
                "\n",
                "# Avaliao do modelo no conjunto de teste\n",
                "score = model.evaluate(x_test, y_test, verbose=1)\n",
                "\n",
                "print('Test loss:', score[0])\n",
                "print('Test accuracy:', score[1])\n",
                "\n",
                "# Plotando 'loss' e 'accuracy' para os datasets 'train' e 'test'\n",
                "plt.figure()\n",
                "plt.plot(np.arange(0,epochs), H.history[\"loss\"], label=\"train_loss\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"acc\"], label=\"train_acc\")\n",
                "plt.plot(np.arange(0,epochs), H.history[\"val_acc\"], label=\"val_acc\")\n",
                "plt.title(\"Acurcia\")\n",
                "plt.xlabel(\"pocas #\")\n",
                "plt.ylabel(\"Loss/Accuracy\")\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN='path'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import seaborn as sns",
                "import os",
                "import scipy.stats as stats",
                "from sklearn.model_selection import train_test_split ",
                "from sklearn.linear_model import LogisticRegression as LR",
                "import sklearn.metrics as m",
                "",
                "input_folder='/kaggle/input/titanic/'"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(input_folder+'train.csv')",
                "ASSIGN = pd.read_csv(input_folder+'test.csv')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data = pd.read_csv(input_folder+'train.csv')",
                "test_data = pd.read_csv(input_folder+'test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print(train_data.isnull().sum()*100path().count())",
                "sns.heatmap(train_data.isnull(),",
                "ASSIGN=True,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN='plasma')"
            ],
            "output_type": "stream",
            "content_old": [
                "print(train_data.isnull().sum()*100/train_data.isnull().count())",
                "# About 20% of age is missing, about 77% of cabin is missing, 0.22% of embarked is missing",
                "",
                "sns.heatmap(train_data.isnull(),",
                "            annot=True,",
                "            yticklabels=False,",
                "            cbar=False,",
                "            cmap='plasma')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train_data['Sex'] = train_data['Sex'].astype('category')",
                "train_data['Sex'] = train_data['Sex'].cat.codes",
                "sns.heatmap(train_data.corr(),",
                "ASSIGN=True,",
                "ASSIGN=False)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# We need to change sex to a numerical value, otherwise the correlation plot will not include sex",
                "train_data['Sex'] = train_data['Sex'].astype('category')",
                "train_data['Sex'] = train_data['Sex'].cat.codes",
                "",
                "",
                "sns.heatmap(train_data.corr(),",
                "           annot=True,",
                "           cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train_data.filter(['Survived','Pclass','Sex','Age','Fare'])",
                "ASSIGN.dropna(how='any',axis='rows', inplace=True)",
                "ASSIGN = test_data.filter(['Survived','Pclass','Sex','Age','Fare','PassengerId'])",
                "ASSIGN.dropna(how='any',axis='rows', inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_filtered = train_data.filter(['Survived','Pclass','Sex','Age','Fare'])",
                "train_filtered.dropna(how='any',axis='rows', inplace=True)",
                "",
                "test_filtered = test_data.filter(['Survived','Pclass','Sex','Age','Fare','PassengerId'])",
                "test_filtered.dropna(how='any',axis='rows', inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train_filtered['Sex'] = train_filtered['Sex'].astype('category')",
                "train_filtered['Sex'] = train_filtered['Sex'].cat.codes",
                "test_filtered['Sex'] = test_filtered['Sex'].astype('category')",
                "test_filtered['Sex'] = test_filtered['Sex'].cat.codes"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_filtered['Sex'] = train_filtered['Sex'].astype('category')",
                "train_filtered['Sex'] = train_filtered['Sex'].cat.codes",
                "",
                "test_filtered['Sex'] = test_filtered['Sex'].astype('category')",
                "test_filtered['Sex'] = test_filtered['Sex'].cat.codes"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index,",
                "train_filtered.groupby(['Pclass']).sum()['Survived']",
                ")",
                "ASSIGN = sns.jointplot(train_filtered.groupby(['Sex']).sum().index,",
                "train_filtered.groupby(['Sex']).sum()['Survived']",
                ")",
                "ASSIGN = sns.jointplot(train_filtered.groupby(['Age']).sum().index,",
                "train_filtered.groupby(['Age']).sum()['Survived']",
                ")",
                "ASSIGN = sns.jointplot(train_filtered.groupby(['Fare']).sum().index,",
                "train_filtered.groupby(['Fare']).sum()['Survived']",
                ")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Survival by pclass",
                "g = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index,",
                "                  train_filtered.groupby(['Pclass']).sum()['Survived']",
                "    )",
                "",
                "# Survival by sex",
                "g = sns.jointplot(train_filtered.groupby(['Sex']).sum().index,",
                "                  train_filtered.groupby(['Sex']).sum()['Survived']",
                "    )",
                "",
                "# Survival by age",
                "g = sns.jointplot(train_filtered.groupby(['Age']).sum().index,",
                "                  train_filtered.groupby(['Age']).sum()['Survived']",
                "    )",
                "",
                "# Survival by fare",
                "g = sns.jointplot(train_filtered.groupby(['Fare']).sum().index,",
                "                  train_filtered.groupby(['Fare']).sum()['Survived']",
                "    )"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "X_train, X_test, y_train, y_test = train_test_split(",
                "train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33)",
                "ASSIGN = LR().fit(y=y_train,X=X_train)",
                "ASSIGN = model.predict(X_test)",
                "ASSIGN = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual'])",
                "ASSIGN['Survived predicted'] = ASSIGN",
                "ASSIGN=m.confusion_matrix(res['Survived actual'],res['Survived predicted'])",
                "sns.heatmap(ASSIGN,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix')",
                "print('Accuracy: '+str(m.accuracy_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted'])))",
                "print('Precision: '+str(m.precision_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted'])))",
                "print('Recall: '+str(m.recall_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted'])))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Split data into test and train sets based on the train data set",
                "X_train, X_test, y_train, y_test = train_test_split(",
                "    train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33)",
                "",
                "# Train model",
                "model = LR().fit(y=y_train,X=X_train)",
                "",
                "# Predict results",
                "results = model.predict(X_test)",
                "",
                "# Add results to a data frame",
                "res = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual'])",
                "res['Survived predicted'] = results",
                "",
                "# Confusion matrix",
                "confmatrix=m.confusion_matrix(res['Survived actual'],res['Survived predicted'])",
                "sns.heatmap(confmatrix,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix')",
                "#   True negatives (tn)     True positives (tp)",
                "#   False negatives (fn)    False positives (fp)",
                "",
                "print('Accuracy: '+str(m.accuracy_score(res['Survived actual'],res['Survived predicted']))) # percent of accurate classification",
                "print('Precision: '+str(m.precision_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fp), 0 is worst, 1 is best",
                "print('Recall: '+str(m.recall_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fn), 0 is worst, 1 is best"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = LR().fit(y=train_filtered['Survived'],X=train_filtered.drop('Survived', axis=1))",
                "ASSIGN = model.predict(test_filtered.drop('PassengerId', axis=1))",
                "ASSIGN = pd.DataFrame(data=results.tolist(), columns = ['Survived'])",
                "ASSIGN = ASSIGN.astype(int)",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Train model",
                "model = LR().fit(y=train_filtered['Survived'],X=train_filtered.drop('Survived', axis=1))",
                "",
                "# Predict survival for the test set",
                "results = model.predict(test_filtered.drop('PassengerId', axis=1))",
                "",
                "res = pd.DataFrame(data=results.tolist(), columns = ['Survived'])",
                "res['PassengerId'] = test_filtered['PassengerId'].astype(int)",
                "",
                "print(res)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "res.to_csv('results.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "res.to_csv('results.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import seaborn as sb",
                "import matplotlib.pyplot as pl",
                "",
                "%matplotlib inline",
                "",
                "train = pd.read_csv('/kaggle/input/titanic/train.csv')",
                "test  = pd.read_csv('/kaggle/input/titanic/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "test.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "train.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "test.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.isnull().sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def grafico(feature):",
                "ASSIGN = train[train['Survived']==1][feature].value_counts()",
                "ASSIGN = train[train['Survived']==0][feature].value_counts()",
                "ASSIGN = pd.DataFrame([survived,dead])",
                "ASSIGN.index = ['Survived','Dead']",
                "ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def grafico(feature):",
                "    survived = train[train['Survived']==1][feature].value_counts()",
                "    dead = train[train['Survived']==0][feature].value_counts()",
                "    df = pd.DataFrame([survived,dead])",
                "    df.index = ['Survived','Dead']",
                "    df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Sex')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Sex')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Pclass')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Pclass')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('SibSp')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('SibSp')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Parch')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Parch')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Embarked')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Embarked')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [train, test]",
                "for dataset in ASSIGN:",
                "ASSIGN = ASSIGN.str.extract(' ([A-Za-z]+)\\.', expand=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_test = [train, test]",
                "for dataset in train_test:",
                "    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train['Title'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test['Title'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test['Title'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {\"Mr\": 0,",
                "\"Miss\": 1,",
                "\"Mrs\": 2,",
                "\"Master\": 3,",
                "\"Dr\": 3,",
                "\"Rev\": 3,",
                "\"Col\": 3,",
                "\"Major\": 3,",
                "\"Mlle\": 3,",
                "\"Ms\": 3,",
                "\"Don\": 3,",
                "\"Lady\": 3,",
                "\"Jonkheer\": 3,",
                "\"Countess\": 3,",
                "\"Mme\": 3,",
                "\"Sir\": 3,",
                "\"Capt\": 3}",
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.map(title_map)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "title_map = {\"Mr\": 0,",
                "            \"Miss\": 1,",
                "            \"Mrs\": 2,",
                "            \"Master\": 3,",
                "            \"Dr\": 3,",
                "            \"Rev\": 3,",
                "            \"Col\": 3,",
                "            \"Major\": 3,",
                "            \"Mlle\": 3,",
                "            \"Ms\": 3,",
                "            \"Don\": 3,",
                "            \"Lady\": 3,",
                "            \"Jonkheer\": 3,",
                "            \"Countess\": 3,",
                "            \"Mme\": 3,",
                "            \"Sir\": 3,",
                "            \"Capt\": 3}",
                "for dataset in train_test:",
                "    dataset['Title'] = dataset['Title'].map(title_map)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "test[\"Title\"].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test[\"Title\"].fillna(0, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "grafico('Title')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Title')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train.drop('Name', axis=1, inplace=True)",
                "test.drop('Name', axis=1, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train.drop('Name', axis=1, inplace=True)",
                "test.drop('Name', axis=1, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {\"male\": 0, \"female\": 1}",
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.map(sex_map)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sex_map = {\"male\": 0, \"female\": 1}",
                "for dataset in train_test:",
                "    dataset['Sex'] = dataset['Sex'].map(sex_map)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Sex')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Sex')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(100)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head(100)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)",
                "test[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"Age\"].fillna(train.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)",
                "test[\"Age\"].fillna(test.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "ASSIGN.map(sb.kdeplot, 'Age', shade=True)",
                "ASSIGN.set(xlim=(0, train['Age'].max()))",
                "ASSIGN.add_legend()",
                "pl.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "facet.map(sb.kdeplot, 'Age', shade=True)",
                "facet.set(xlim=(0, train['Age'].max()))",
                "facet.add_legend()",
                "pl.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "dataset.loc[ dataset['Age'] <= 16, 'Age'] =0",
                "dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1",
                "dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2",
                "dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3",
                "dataset.loc[(dataset['Age'] > 62), 'Age'] = 4"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for dataset in train_test:",
                "    dataset.loc[ dataset['Age'] <= 16, 'Age'] =0",
                "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 26), 'Age'] = 1",
                "    dataset.loc[(dataset['Age'] > 26) & (dataset['Age'] <= 36), 'Age'] = 2",
                "    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 62), 'Age'] = 3",
                "    dataset.loc[(dataset['Age'] > 62), 'Age'] = 4"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "grafico('Age')"
            ],
            "output_type": "display_data",
            "content_old": [
                "grafico('Age')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts()",
                "ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "ASSIGN.index = ['1st class', '2nd class', '3rd class']",
                "ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts()",
                "df = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "df.index = ['1st class', '2nd class', '3rd class']",
                "df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.fillna('S')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for dataset in train_test:",
                "    dataset['Embarked'] =  dataset['Embarked'].fillna('S')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {\"S\": 0,",
                "\"C\": 1,",
                "\"Q\": 2}",
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.map(emb_map)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "emb_map = {\"S\": 0,",
                "           \"C\": 1,",
                "           \"Q\": 2}",
                "for dataset in train_test:",
                "    dataset['Embarked'] = dataset['Embarked'].map(emb_map)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)",
                "test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"Fare\"].fillna(train.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)",
                "test[\"Fare\"].fillna(test.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0",
                "dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1",
                "dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2",
                "dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for dataset in train_test:",
                "    dataset.loc[ dataset['Fare'] <= 17, 'Fare'] =0",
                "    dataset.loc[(dataset['Fare'] > 17) & (dataset['Fare'] <= 30), 'Fare'] = 1",
                "    dataset.loc[(dataset['Fare'] > 30) & (dataset['Fare'] <= 100), 'Fare'] = 2",
                "    dataset.loc[(dataset['Fare'] > 100), 'Fare'] = 3"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.Cabin.value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.Cabin.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.str[:1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for dataset in train_test:",
                "    dataset['Cabin'] = dataset['Cabin'].str[:1]"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts()",
                "ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "ASSIGN.index = ['1st class', '2nd class', '3rd class']",
                "ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts()",
                "Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts()",
                "Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts()",
                "df = pd.DataFrame([Pclass1,Pclass2,Pclass3])",
                "df.index = ['1st class', '2nd class', '3rd class']",
                "df.plot(kind='bar', stacked=True, figsize=(10,5))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {\"A\": 0,",
                "\"B\": 0.4,",
                "\"C\": 0.8,",
                "\"D\": 1.2,",
                "\"E\": 1.6,",
                "\"F\": 2.0,",
                "\"G\": 2.4,",
                "\"T\": 2.8}",
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.map(cab_map)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cab_map = {\"A\": 0,",
                "           \"B\": 0.4,",
                "           \"C\": 0.8,",
                "           \"D\": 1.2,",
                "           \"E\": 1.6,",
                "           \"F\": 2.0,",
                "           \"G\": 2.4,",
                "           \"T\": 2.8}",
                "for dataset in train_test:",
                "    dataset['Cabin'] = dataset['Cabin'].map(cab_map)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)",
                "test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"Cabin\"].fillna(train.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)",
                "test[\"Cabin\"].fillna(test.groupby(\"Pclass\")[\"Cabin\"].transform(\"median\"), inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN + ASSIGN + 1",
                "ASSIGN = ASSIGN + ASSIGN + 1"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]  + 1",
                "test[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]  + 1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "ASSIGN.map(sb.kdeplot, 'FamilySize', shade= True)",
                "ASSIGN.set(xlim=(0, train['FamilySize'].max()))",
                "ASSIGN.add_legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "facet = sb.FacetGrid(train, hue=\"Survived\", aspect=4)",
                "facet.map(sb.kdeplot, 'FamilySize', shade= True)",
                "facet.set(xlim=(0, train['FamilySize'].max()))",
                "facet.add_legend()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}",
                "for dataset in train_test:",
                "ASSIGN = ASSIGN.map(family_map)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "family_map = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4}",
                "for dataset in train_test:",
                "    dataset['FamilySize'] = dataset['FamilySize'].map(family_map)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ['Ticket', 'SibSp', 'Parch']",
                "ASSIGN = ASSIGN.drop(features_drop, axis=1)",
                "ASSIGN = ASSIGN.drop(features_drop, axis=1)",
                "ASSIGN = ASSIGN.drop(['PassengerId'], axis=1)",
                "ASSIGN = train.drop('Survived', axis=1)",
                "ASSIGN = train['Survived']",
                "train_data.shape, target.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "features_drop = ['Ticket', 'SibSp', 'Parch']",
                "train = train.drop(features_drop, axis=1)",
                "test = test.drop(features_drop, axis=1)",
                "train = train.drop(['PassengerId'], axis=1)",
                "train_data = train.drop('Survived', axis=1)",
                "target = train['Survived']",
                "",
                "train_data.shape, target.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_data.head(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsClassifier",
                "from sklearn.tree import DecisionTreeClassifier",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.naive_bayes import GaussianNB",
                "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis",
                "from sklearn import linear_model",
                "from sklearn.svm import SVC",
                "",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "train.info()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = KFold(n_splits=10, shuffle=True, random_state=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import KFold",
                "from sklearn.model_selection import cross_val_score",
                "k_fold = KFold(n_splits=10, shuffle=True, random_state=0)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = KNeighborsClassifier(n_neighbors = 13)",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100, 2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = KNeighborsClassifier(n_neighbors = 13)",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = DecisionTreeClassifier()",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100, 2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = DecisionTreeClassifier()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = RandomForestClassifier(n_estimators=13)",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100, 2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = RandomForestClassifier(n_estimators=13)",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = GaussianNB()",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100, 2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = GaussianNB()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100, 2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = SVC()",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100,2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = SVC()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = QuadraticDiscriminantAnalysis()",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100,2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = QuadraticDiscriminantAnalysis()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1, scoring=scoring)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = linear_model.LinearRegression()",
                "ASSIGN = 'accuracy'",
                "ASSIGN = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1)",
                "print(ASSIGN)",
                "round(np.mean(ASSIGN)*100,2)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = linear_model.LinearRegression()",
                "scoring = 'accuracy'",
                "score = cross_val_score(clf, train_data, target, cv=k_fold, n_jobs=1)",
                "print(score)",
                "round(np.mean(score)*100,2)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = SVC()",
                "ASSIGN.fit(train_data, target)",
                "ASSIGN = test.drop(\"PassengerId\", axis=1).copy()",
                "ASSIGN = clf.predict(test_data)",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = clf.predict(test_data2)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "clf = SVC()",
                "",
                "clf.fit(train_data, target)",
                "",
                "test_data = test.drop(\"PassengerId\", axis=1).copy()",
                "",
                "prediction = clf.predict(test_data)",
                "",
                "test_data2 = pd.read_csv('/kaggle/input/testes/teste.csv')",
                "prediction2 = clf.predict(test_data2)",
                "",
                "print(prediction2)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({",
                "\"PassengerId\": test[\"PassengerId\"],",
                "\"Survived\": prediction",
                "})",
                "ASSIGN.to_csv('path', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "submission = pd.DataFrame({",
                "        \"PassengerId\": test[\"PassengerId\"],",
                "        \"Survived\": prediction",
                "    })",
                "",
                "submission.to_csv('/kaggle/working/submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "submission = pd.read_csv('/kaggle/working/submission.csv')",
                "submission.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np \n",
                "import pandas as pd \n",
                "import tensorflow as tf\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "import math\n",
                "from sklearn import preprocessing, model_selection\n",
                "from sklearn.metrics import mean_squared_error\n",
                "import xgboost as xgb\n",
                "import scipy.stats as stats\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_belem = pd.read_csv(\"../input/temperature-timeseries-for-some-brazilian-cities/station_belem.csv\")\n",
                "df_curitiba = pd.read_csv(\"../input/temperature-timeseries-for-some-brazilian-cities/station_curitiba.csv\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(df_belem.shape, df_curitiba.shape)"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Questo 1\n",
                "display(df_belem.shape, df_curitiba.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "df_belem.set_index('YEAR',inplace=True)",
                "df_curitiba.set_index('YEAR',inplace=True)",
                "display(df_belem.head())",
                "display(df_curitiba.head())"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Questo 2\n",
                "df_belem.set_index('YEAR',inplace=True)\n",
                "df_curitiba.set_index('YEAR',inplace=True)\n",
                "display(df_belem.head())\n",
                "display(df_curitiba.head())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(25,25))",
                "df_belem.hist()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#exerccio 3\n",
                "#plota o histograma dos valores \n",
                "#vemos que h vrios outliers prximos de 1000 que provavelmente no so valores de temperatura vlidos\n",
                "plt.figure(figsize=(25,25))\n",
                "#df_belem.boxplot()\n",
                "df_belem.hist()\n",
                "\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(df_belem['JAN'].value_counts())",
                "display(df_curitiba['JAN'].value_counts())"
            ],
            "output_type": "display_data",
            "content_old": [
                "#mostra a quantidade de valores nicos no ms de janeiro\n",
                "#verificando os valores nicos confirmamos que o valor 999.90  o nico outlier\n",
                "display(df_belem['JAN'].value_counts())\n",
                "display(df_curitiba['JAN'].value_counts())"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_belem.replace(999.90,np.nan)",
                "ASSIGN = ASSIGN.fillna(ASSIGN.mean())",
                "display(ASSIGN)",
                "ASSIGN = df_curitiba.replace(999.90,np.nan)",
                "ASSIGN = ASSIGN.fillna(ASSIGN.mean())",
                "display(ASSIGN)"
            ],
            "output_type": "display_data",
            "content_old": [
                "#exerccio 4\n",
                "#para tratar os outliers, podemos excluir os dados ausentes (999.90) ou substitu-lo pela mdia do ano anterior e posterior.\n",
                "#adotarei a soluo de substituir os nulos pela mdia.\n",
                "\n",
                "\n",
                "#cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento\n",
                "df_belem_t = df_belem.replace(999.90,np.nan)\n",
                "#substitui os valores nulos restantes pela mdia do ano anterior e posterior\n",
                "df_belem_t = df_belem_t.fillna(df_belem_t.mean())\n",
                "display(df_belem_t)\n",
                "\n",
                "#cria um novo dataset transformando o outlier em nulo para aplicao das funes de tratamento\n",
                "df_curitiba_t = df_curitiba.replace(999.90,np.nan)\n",
                "#substitui os valores nulos restantes pela mdia do ano anterior e posterior\n",
                "df_curitiba_t = df_curitiba_t.fillna(df_curitiba_t.mean())\n",
                "display(df_curitiba_t)\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "ASSIGN=plt.figure()",
                "ASSIGN=fig.add_axes([0,0,1,1])",
                "ASSIGN.scatter(df_curitiba_t.index, df_curitiba_t.JUL, color='r')",
                "ASSIGN.scatter(df_belem_t.index, df_belem_t.JUL, color='b')",
                "ASSIGN.set_xlabel('Ano')",
                "ASSIGN.set_ylabel('Temperatura (C)')",
                "ASSIGN.legend([\"Curitiba - Julho\", \"Belm - Julho\"])",
                "ASSIGN.set_title('scatter plot')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#exerccio 5\n",
                "plt.figure(figsize=(10,10))\n",
                "fig=plt.figure()\n",
                "ax=fig.add_axes([0,0,1,1])\n",
                "ax.scatter(df_curitiba_t.index, df_curitiba_t.JUL, color='r')\n",
                "ax.scatter(df_belem_t.index, df_belem_t.JUL, color='b')\n",
                "ax.set_xlabel('Ano')\n",
                "ax.set_ylabel('Temperatura (C)')\n",
                "ax.legend([\"Curitiba - Julho\", \"Belm - Julho\"])\n",
                "ax.set_title('scatter plot')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "display(df_curitiba_t['JUL'].describe())",
                "display(df_belem_t['JUL'].describe())",
                "stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])"
            ],
            "output_type": "display_data",
            "content_old": [
                "#questo 6\n",
                "display(df_curitiba_t['JUL'].describe())\n",
                "display(df_belem_t['JUL'].describe())\n",
                "stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN'])",
                "ASSIGN['A1'] = ASSIGN['JAN'].shift(1)",
                "ASSIGN['A2'] = ASSIGN['JAN'].shift(2)",
                "ASSIGN['A3'] = ASSIGN['JAN'].shift(3)",
                "ASSIGN = ASSIGN.dropna()",
                "display(ASSIGN.head())",
                "X_train, X_test, y_train, y_test = model_selection.train_test_split(ASSIGN.drop(columns=['JAN']),ASSIGN['JAN'],test_size=0.25, random_state=33)"
            ],
            "output_type": "display_data",
            "content_old": [
                "#exerccio 7\n",
                "df_curitiba_jan = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN'])\n",
                "#cria o dataset de previso com os valores dos 3 anos anteriores\n",
                "df_curitiba_jan['A1'] = df_curitiba_jan['JAN'].shift(1)\n",
                "df_curitiba_jan['A2'] = df_curitiba_jan['JAN'].shift(2)\n",
                "df_curitiba_jan['A3'] = df_curitiba_jan['JAN'].shift(3)\n",
                "#dropa os primeiros anos (que no tem anos anteriores para montar o dataset)\n",
                "df_curitiba_jan = df_curitiba_jan.dropna()\n",
                "display(df_curitiba_jan.head())\n",
                "#separa em conjuntos de teste e treinamento\n",
                "X_train, X_test, y_train, y_test = model_selection.train_test_split(df_curitiba_jan.drop(columns=['JAN']),df_curitiba_jan['JAN'],test_size=0.25, random_state=33)\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = xgb.XGBRegressor()",
                "ASSIGN.fit(X_train,y_train)",
                "ASSIGN = model.predict(data=X_train)",
                "ASSIGN = model.predict(data=X_test)"
            ],
            "output_type": "stream",
            "content_old": [
                "#realiza regresso com o regressor de gradient boosting XGBoost\n",
                "#ele frequentemente apresenta resultados iniciais melhores que uma rede neural sem ajustes\n",
                "model = xgb.XGBRegressor()\n",
                "model.fit(X_train,y_train)\n",
                "p_train = model.predict(data=X_train)\n",
                "p_test = model.predict(data=X_test)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = math.sqrt(mean_squared_error(p_train, y_train))",
                "print('Pontuao para o treinamento: %.2f RMSE' % (ASSIGN))",
                "ASSIGN = math.sqrt(mean_squared_error(p_test, y_test))",
                "print('Pontuao para o teste: %.2f RMSE' % (ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "#calcula os erros de previso\n",
                "trainScore = math.sqrt(mean_squared_error(p_train, y_train))\n",
                "print('Pontuao para o treinamento: %.2f RMSE' % (trainScore))\n",
                "testScore = math.sqrt(mean_squared_error(p_test, y_test))\n",
                "print('Pontuao para o teste: %.2f RMSE' % (testScore))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({'YEAR': X_test.index, 'PRED': p_test, 'REAL': y_test}).reset_index(drop=True)",
                "display(ASSIGN.sort_values(['YEAR']).set_index('YEAR'))",
                "plt.figure(figsize=(10,10))",
                "ASSIGN=plt.figure()",
                "ASSIGN=fig.add_axes([0,0,1,1])",
                "ASSIGN.scatter(ASSIGN['YEAR'],ASSIGN['PRED'] , color='r')",
                "ASSIGN.scatter(ASSIGN['YEAR'],ASSIGN['REAL'] , color='b')",
                "ASSIGN.set_xlabel('Ano')",
                "ASSIGN.set_ylabel('Temperatura (C)')",
                "ASSIGN.legend([\"Curitiba - Janeiro - Previsto\", \"Curitiba - Janeiro - Real\"])",
                "ASSIGN.set_title('scatter plot')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#plota o resultado previsto em relao ao real\n",
                "df_plot = pd.DataFrame({'YEAR': X_test.index, 'PRED': p_test, 'REAL': y_test}).reset_index(drop=True)\n",
                "display(df_plot.sort_values(['YEAR']).set_index('YEAR'))\n",
                "plt.figure(figsize=(10,10))\n",
                "fig=plt.figure()\n",
                "ax=fig.add_axes([0,0,1,1])\n",
                "ax.scatter(df_plot['YEAR'],df_plot['PRED'] , color='r')\n",
                "ax.scatter(df_plot['YEAR'],df_plot['REAL'] , color='b')\n",
                "ax.set_xlabel('Ano')\n",
                "ax.set_ylabel('Temperatura (C)')\n",
                "ax.legend([\"Curitiba - Janeiro - Previsto\", \"Curitiba - Janeiro - Real\"])\n",
                "ax.set_title('scatter plot')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "sns.set_style(\"darkgrid\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Imports for Exploratory Data Analysis\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Imports for Machine Learning Models\n",
                "import string\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.pipeline import Pipeline\n",
                "\n",
                "# Validation\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "# Set the style of the plots' background\n",
                "sns.set_style(\"darkgrid\")\n",
                "\n",
                "# Show the plot in the same window as the notebook\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs = pd.read_csv(\"/kaggle/input/real-or-fake-fake-jobposting-prediction/fake_job_postings.csv\")\n",
                "jobs.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(12,8))",
                "sns.heatmap(jobs.isnull(), cmap=\"coolwarm\", yticklabels=False, cbar=False)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(12,8))\n",
                "sns.heatmap(jobs.isnull(), cmap=\"coolwarm\", yticklabels=False, cbar=False)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "jobs.drop(columns=[\"department\", \"salary_range\", \"benefits\"], inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "jobs.drop(columns=[\"department\", \"salary_range\", \"benefits\"], inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "jobs.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [\"company_profile\", \"description\", \"requirements\"]",
                "for col in ASSIGN:",
                "jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 0), col] = \"none\"",
                "jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 1), col] = \"missing\""
            ],
            "output_type": "not_existent",
            "content_old": [
                "# List with the columns to check the length of the text\n",
                "feature_lst = [\"company_profile\", \"description\", \"requirements\"]\n",
                "\n",
                "# For loop to treat the missing values in the columns of the feature_lst.\n",
                "for col in feature_lst:\n",
                "    # If the job post is real, change the missing values to \"none\"\n",
                "    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 0), col] = \"none\"\n",
                "    \n",
                "    # If the job post is fake, change the missing values to \"missing\"\n",
                "    jobs.loc[(jobs[col].isnull()) & (jobs[\"fraudulent\"] == 1), col] = \"missing\""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for num,col in enumerate(feature_lst):",
                "jobs[str(num)] = jobs[col].apply(len)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# For loop to create new columns with the lengths of the ones in the feature_lst\n",
                "for num,col in enumerate(feature_lst):\n",
                "    jobs[str(num)] = jobs[col].apply(len)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.rename({\"0\": \"profile_length\", \"1\": \"description_length\", \"2\": \"requirements_length\"}, axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Rename the new columns created above\n",
                "jobs = jobs.rename({\"0\": \"profile_length\", \"1\": \"description_length\", \"2\": \"requirements_length\"}, axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs.isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "jobs[\"fraudulent\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs[\"fraudulent\"].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(data=jobs[[\"fraudulent\", \"profile_length\", \"description_length\", \"requirements_length\"]],",
                "ASSIGN=\"fraudulent\", height=2, aspect=2);"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.pairplot(data=jobs[[\"fraudulent\", \"profile_length\", \"description_length\", \"requirements_length\"]],\n",
                "             hue=\"fraudulent\", height=2, aspect=2);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)",
                "ASSIGN = ASSIGN.map(plt.hist, \"profile_length\", bins=40)",
                "ASSIGN = profile_grid.ASSIGN.flatten()",
                "ASSIGN[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)",
                "ASSIGN[1].set_title(\"Fraudulent (1)\", fontsize=14)",
                "ASSIGN[0].set_ylabel(\"Count\", fontsize=14)",
                "for ax in ASSIGN:",
                "ax.set_xlabel(\"Profile Text Length\", fontsize=14)"
            ],
            "output_type": "display_data",
            "content_old": [
                "profile_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "profile_grid = profile_grid.map(plt.hist, \"profile_length\", bins=40)\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = profile_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Profile Text Length\", fontsize=14)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)",
                "ASSIGN = ASSIGN.map(plt.hist, \"description_length\", bins=40)",
                "ASSIGN = description_grid.ASSIGN.flatten()",
                "ASSIGN[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)",
                "ASSIGN[1].set_title(\"Fraudulent (1)\", fontsize=14)",
                "ASSIGN[0].set_ylabel(\"Count\", fontsize=14)",
                "for ax in ASSIGN:",
                "ax.set_xlabel(\"Description Text Length\", fontsize=14)"
            ],
            "output_type": "display_data",
            "content_old": [
                "description_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "description_grid = description_grid.map(plt.hist, \"description_length\", bins=40)\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = description_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non-Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Description Text Length\", fontsize=14)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)",
                "ASSIGN = ASSIGN.map(plt.hist, \"requirements_length\", bins=40)",
                "ASSIGN = requirements_grid.ASSIGN.flatten()",
                "ASSIGN[0].set_title(\"Non Fraudulent (0)\", fontsize=14)",
                "ASSIGN[1].set_title(\"Fraudulent (1)\", fontsize=14)",
                "ASSIGN[0].set_ylabel(\"Count\", fontsize=14)",
                "for ax in ASSIGN:",
                "ax.set_xlabel(\"Requirements Text Length\", fontsize=14)"
            ],
            "output_type": "display_data",
            "content_old": [
                "requirements_grid = sns.FacetGrid(jobs, col=\"fraudulent\", aspect=1.5, height=4, sharey=False)\n",
                "requirements_grid = requirements_grid.map(plt.hist, \"requirements_length\", bins=40)\n",
                "\n",
                "# Another option. Makes less obviuos which axes is to be labelled\n",
                "#requirements_grid.set_axis_labels(\"Requirement Length\", \"Count\")\n",
                "\n",
                "# Flatten the axes. Create an iterator\n",
                "axes = requirements_grid.axes.flatten()\n",
                "\n",
                "# Title\n",
                "axes[0].set_title(\"Non Fraudulent (0)\", fontsize=14)\n",
                "axes[1].set_title(\"Fraudulent (1)\", fontsize=14)\n",
                "\n",
                "# Labels\n",
                "axes[0].set_ylabel(\"Count\", fontsize=14)\n",
                "for ax in axes:\n",
                "    ax.set_xlabel(\"Requirements Text Length\", fontsize=14)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.catplot(x=\"has_company_logo\", hue=\"fraudulent\", data=jobs, kind=\"count\", aspect=2, height=4);",
                "plt.xlabel(\"Company Logo\", fontsize=14)",
                "plt.xticks([0, 1], (\"Has\", \"Doesn't have\"), fontsize=12)",
                "plt.ylabel(\"Count\", fontsize=14);"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.catplot(x=\"has_company_logo\", hue=\"fraudulent\", data=jobs, kind=\"count\", aspect=2, height=4);\n",
                "\n",
                "plt.xlabel(\"Company Logo\", fontsize=14)\n",
                "plt.xticks([0, 1], (\"Has\", \"Doesn't have\"), fontsize=12)\n",
                "plt.ylabel(\"Count\", fontsize=14);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1, 2, figsize=(18,8))",
                "ASSIGN = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[0])",
                "axes[0].set_xlabel(\"Employment Type\", fontsize=15)",
                "axes[0].set_ylabel(\"Count\", fontsize=15)",
                "axes[0].set_title(\"Employment Type Count\", fontsize=15)",
                "axes[0].legend(\"\")",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.0f}\".format(p.get_height()),",
                "(p.get_x() + p.get_width() path, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 12),",
                "ASSIGN='offset points')",
                "ASSIGN = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[1])",
                "axes[1].set_xlabel(\"Employment Type\", fontsize=15)",
                "axes[1].set_ylim((0, 1500))",
                "axes[1].set_ylabel(\"\")",
                "axes[1].set_title(\"Employment Type Count Zoom\", fontsize=15)",
                "axes[1].legend(title=\"Fraudulent\", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Create a 1 by 2 figure and axes\n",
                "fig, axes = plt.subplots(1, 2, figsize=(18,8))\n",
                "\n",
                "# Plot a countplot on the first axes\n",
                "employ = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[0])\n",
                "axes[0].set_xlabel(\"Employment Type\", fontsize=15)\n",
                "axes[0].set_ylabel(\"Count\", fontsize=15)\n",
                "axes[0].set_title(\"Employment Type Count\", fontsize=15)\n",
                "axes[0].legend(\"\")\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in employ.patches:\n",
                "    employ.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 12),\n",
                "                        textcoords='offset points')\n",
                "\n",
                "#############################################################\n",
                "\n",
                "# Plot a countplot on the second axes\n",
                "employ_zoom = sns.countplot(x=jobs[\"employment_type\"].dropna(), hue=jobs[\"fraudulent\"], palette=\"Set1\", ax=axes[1])\n",
                "axes[1].set_xlabel(\"Employment Type\", fontsize=15)\n",
                "axes[1].set_ylim((0, 1500))\n",
                "axes[1].set_ylabel(\"\")\n",
                "axes[1].set_title(\"Employment Type Count Zoom\", fontsize=15)\n",
                "axes[1].legend(title=\"Fraudulent\", title_fontsize=14, fontsize=12, bbox_to_anchor=(1.2, 0.6));"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "jobs.columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "jobs.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = jobs[\"company_profile\"]",
                "ASSIGN = jobs[\"fraudulent\"]",
                "X1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X1_profile = jobs[\"company_profile\"]\n",
                "y1 = jobs[\"fraudulent\"]\n",
                "X1_profile_train, X1_profile_test, y1_train, y1_test = train_test_split(X1_profile, y1, test_size=0.2, random_state=42)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def text_process(text):",
                "ASSIGN = [char for char in text if char not in string.punctuation]",
                "ASSIGN = \"\".join(ASSIGN)",
                "return [word for word in ASSIGN.split() if word.lower() not in stopwords.words(\"english\")]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def text_process(text):\n",
                "    # Remove the punctuation\n",
                "    nopunc = [char for char in text if char not in string.punctuation]\n",
                "    \n",
                "    # Join the list of characters to form strings\n",
                "    nopunc = \"\".join(nopunc)\n",
                "    \n",
                "    # Remove stopwords\n",
                "    return [word for word in nopunc.split() if word.lower() not in stopwords.words(\"english\")]"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = Pipeline([(\"bow no func\", CountVectorizer()),",
                "(\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n",
                "                       (\"NB_classifier\", MultinomialNB())])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_pipeline.fit(X1_profile_train, y1_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "NB_pipeline.fit(X1_profile_train, y1_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = NB_pipeline.predict(X1_profile_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_pred = NB_pipeline.predict(X1_profile_test)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(classification_report(y1_test, NB_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(classification_report(y1_test, NB_pred))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "print(confusion_matrix(y1_test, NB_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(confusion_matrix(y1_test, NB_pred))"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),",
                "(\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_func_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n",
                "                            (\"NB_classifier\", MultinomialNB())])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = jobs[\"company_profile\"]",
                "ASSIGN = jobs[\"fraudulent\"]",
                "X2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X2_profile = jobs[\"company_profile\"]\n",
                "y2 = jobs[\"fraudulent\"]\n",
                "X2_profile_train, X2_profile_test, y2_train, y2_test = train_test_split(X2_profile, y2, test_size=0.2, random_state=42)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_func_pipeline.fit(X2_profile_train, y2_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "NB_func_pipeline.fit(X2_profile_train, y2_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = NB_func_pipeline.predict(X2_profile_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_func_pred = NB_func_pipeline.predict(X2_profile_test)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(classification_report(y2_test, NB_func_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(classification_report(y2_test, NB_func_pred))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "print(confusion_matrix(y2_test, NB_func_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(confusion_matrix(y2_test, NB_func_pred))"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = Pipeline([(\"bow no func\", CountVectorizer()),",
                "(\"tfidf\", TfidfTransformer()),",
                "(\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_tfidf_pipeline = Pipeline([(\"bow no func\", CountVectorizer()),\n",
                "                              (\"tfidf\", TfidfTransformer()),\n",
                "                              (\"NB_classifier\", MultinomialNB())])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = jobs[\"company_profile\"]",
                "ASSIGN = jobs[\"fraudulent\"]",
                "X3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X3_profile = jobs[\"company_profile\"]\n",
                "y3 = jobs[\"fraudulent\"]\n",
                "X3_profile_train, X3_profile_test, y3_train, y3_test = train_test_split(X3_profile, y3, test_size=0.2, random_state=42)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_tfidf_pipeline.fit(X3_profile_train, y3_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "NB_tfidf_pipeline.fit(X3_profile_train, y3_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = NB_tfidf_pipeline.predict(X3_profile_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_tfidf_pred = NB_tfidf_pipeline.predict(X3_profile_test)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(classification_report(y3_test, NB_tfidf_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(classification_report(y3_test, NB_tfidf_pred))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "print(confusion_matrix(y3_test, NB_tfidf_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(confusion_matrix(y3_test, NB_tfidf_pred))"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),",
                "(\"tfidf\", TfidfTransformer()),",
                "(\"NB_classifier\", MultinomialNB())])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_func_tfidf_pipeline = Pipeline([(\"bow with func\", CountVectorizer(analyzer=text_process)),\n",
                "                              (\"tfidf\", TfidfTransformer()),\n",
                "                              (\"NB_classifier\", MultinomialNB())])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = jobs[\"company_profile\"]",
                "ASSIGN = jobs[\"fraudulent\"]",
                "X4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(ASSIGN, ASSIGN, test_size=0.2, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X4_profile = jobs[\"company_profile\"]\n",
                "y4 = jobs[\"fraudulent\"]\n",
                "X4_profile_train, X4_profile_test, y4_train, y4_test = train_test_split(X4_profile, y4, test_size=0.2, random_state=42)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "NB_func_tfidf_pipeline.fit(X4_profile_train, y4_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = NB_func_tfidf_pipeline.predict(X4_profile_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "NB_func_tfidf_pred = NB_func_tfidf_pipeline.predict(X4_profile_test)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(classification_report(y4_test, NB_func_tfidf_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(classification_report(y4_test, NB_func_tfidf_pred))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "print(confusion_matrix(y4_test, NB_func_tfidf_pred))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(confusion_matrix(y4_test, NB_func_tfidf_pred))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "sns.set_style(\"darkgrid\")",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "# Important imports for the analysis of the dataset\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "sns.set_style(\"darkgrid\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN.head(8)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Create the dataframe and check the first 8 rows\n",
                "app_df = pd.read_csv(\"/kaggle/input/17k-apple-app-store-strategy-games/appstore_games.csv\")\n",
                "app_df.head(8)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = app_df.drop(columns=['URL', 'Subtitle', 'Icon URL'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Dropping columns that I will not use for this analysis\n",
                "app_df_cut = app_df.drop(columns=['URL', 'Subtitle', 'Icon URL'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "app_df_cut.info()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.sort_values(by=\"User Rating Count\", ascending=False)",
                "ASSIGN.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Most reviewed app\n",
                "#app_df_cut.iloc[app_df_cut[\"User Rating Count\"].idxmax()]\n",
                "\n",
                "# A better way of seeing the most reviwed apps \n",
                "app_df_cut = app_df_cut.sort_values(by=\"User Rating Count\", ascending=False)\n",
                "app_df_cut.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull()),",
                "[\"Average User Rating\", \"User Rating Count\"]] = 0"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Get the columns \"User Rating Count\" and \"Average User Rating\" where they are both equal to NaN and set the\n",
                "# values to 0.\n",
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull()),\n",
                "               [\"Average User Rating\", \"User Rating Count\"]] = 0"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull())]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are any other missing values in those columns\n",
                "app_df_cut.loc[(app_df_cut[\"User Rating Count\"].isnull()) | (app_df_cut[\"Average User Rating\"].isnull())]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull(),",
                "\"In-app Purchases\"] = 0"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Get the column \"In-app Purchases\" where the value is NaN and set it to zero\n",
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull(),\n",
                "               \"In-app Purchases\"] = 0"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are any NaN value in the \"In-app Purchases\" column\n",
                "app_df_cut.loc[app_df_cut[\"In-app Purchases\"].isnull()]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[(app_df_cut[\"ID\"] == 0) | (app_df_cut[\"ID\"].isnull()),",
                "\"ID\"]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are missing or 0 ID's\n",
                "app_df_cut.loc[(app_df_cut[\"ID\"] == 0) | (app_df_cut[\"ID\"].isnull()),\n",
                "              \"ID\"]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(app_df_cut[\"ID\"]) - len(app_df_cut[\"ID\"].unique())"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check for duplicates in the ID column\n",
                "len(app_df_cut[\"ID\"]) - len(app_df_cut[\"ID\"].unique())\n",
                "\n",
                "# The number of unique values is lower than the total amount of ID's, therefore there are duplicates among them."
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "app_df_cut.drop_duplicates(subset=\"ID\", inplace=True)",
                "app_df_cut.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Drop every duplicate ID row\n",
                "app_df_cut.drop_duplicates(subset=\"ID\", inplace=True)\n",
                "app_df_cut.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut[(app_df_cut[\"Size\"].isnull()) | (app_df_cut['Size'] == 0)]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are null values in the Size column\n",
                "app_df_cut[(app_df_cut[\"Size\"].isnull()) | (app_df_cut['Size'] == 0)]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "app_df_cut.drop([16782], axis=0, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Drop the only row in which the game has no size\n",
                "app_df_cut.drop([16782], axis=0, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "app_df_cut[\"Size\"] = round(app_df_cut[\"Size\"]path)",
                "app_df_cut.head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Convert the size to MB\n",
                "app_df_cut[\"Size\"] = round(app_df_cut[\"Size\"]/1000000)\n",
                "app_df_cut.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[\"Price\"].isnull()].index)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Drop the row with NaN values in the \"Price\" column\n",
                "app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[\"Price\"].isnull()].index)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[app_df_cut[\"Price\"].isnull()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are any null values on the price column\n",
                "app_df_cut.loc[app_df_cut[\"Price\"].isnull()]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[\"Languages\"].isnull()].index)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Drop the rows with NaN values in the \"Languages\" column\n",
                "app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[\"Languages\"].isnull()].index)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.loc[app_df_cut[\"Languages\"].isnull()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Check if there are any null values on the \"Languages\" column\n",
                "app_df_cut.loc[app_df_cut[\"Languages\"].isnull()]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_cut.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "app_df_cut.info()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "app_df_cut.to_csv(\"app_df_clean.csv\", index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "app_df_cut.to_csv(\"app_df_clean.csv\", index=False)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"ASSIGN.csv\")",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "app_df_clean = pd.read_csv(\"app_df_clean.csv\")\n",
                "app_df_clean.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "app_df_clean[\"Original Release Date\"] = pd.to_datetime(app_df_clean[\"Original Release Date\"])",
                "app_df_clean[\"Current Version Release Date\"] = pd.to_datetime(app_df_clean[\"Current Version Release Date\"])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Transform the the string dates into datetime objects\n",
                "app_df_clean[\"Original Release Date\"] = pd.to_datetime(app_df_clean[\"Original Release Date\"])\n",
                "app_df_clean[\"Current Version Release Date\"] = pd.to_datetime(app_df_clean[\"Current Version Release Date\"])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "app_df_clean.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "app_df_clean.info()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year)",
                "ASSIGN = app_df_clean[\"Size\"]",
                "ASSIGN = sns.color_palette(\"muted\")",
                "ASSIGN = sns.swarmplot(x=years, y=ASSIGN, palette=palette)",
                "ASSIGN.set_ylabel(\"Size (in MB)\", fontsize=16)",
                "ASSIGN.set_xlabel(\"Original Release Date\", fontsize=16)",
                "ASSIGN.set_title(\"Time Evolution of the Apps' Sizes\", fontsize=20)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "years = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year)\n",
                "size = app_df_clean[\"Size\"]\n",
                "\n",
                "# Plot a swarmplot\n",
                "palette = sns.color_palette(\"muted\")\n",
                "size = sns.swarmplot(x=years, y=size, palette=palette)\n",
                "size.set_ylabel(\"Size (in MB)\", fontsize=16)\n",
                "size.set_xlabel(\"Original Release Date\", fontsize=16)\n",
                "size.set_title(\"Time Evolution of the Apps' Sizes\", fontsize=20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = sns.color_palette(\"inferno_r\")",
                "ASSIGN = sns.countplot(x=years, data=app_df_clean, palette=palette1)",
                "ASSIGN.set_xlabel(\"Year of Release\", fontsize=16)",
                "ASSIGN.set_ylabel(\"Amount\", fontsize=16)",
                "ASSIGN.set_title(\"Quantity of Apps per Year\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{}\".format(p.get_height()),",
                "(p.get_x() + p.get_width() path, p.get_height() + 40),",
                "ASSIGN=\"center\", ha=\"center\", fontsize=16)"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Plot a countplot\n",
                "palette1 = sns.color_palette(\"inferno_r\")\n",
                "apps_per_year = sns.countplot(x=years, data=app_df_clean, palette=palette1)\n",
                "apps_per_year.set_xlabel(\"Year of Release\", fontsize=16)\n",
                "apps_per_year.set_ylabel(\"Amount\", fontsize=16)\n",
                "apps_per_year.set_title(\"Quantity of Apps per Year\", fontsize=20)\n",
                "\n",
                "# Write the height of each bar on top of them\n",
                "for p in apps_per_year.patches:\n",
                "    apps_per_year.annotate(\"{}\".format(p.get_height()),\n",
                "                          (p.get_x() + p.get_width() / 2, p.get_height() + 40),\n",
                "                          va=\"center\", ha=\"center\", fontsize=16)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = [year for year in range(2014,2019)]",
                "for year in ASSIGN:",
                "ASSIGN = app_df_clean[\"Original Release Date\"].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()",
                "ASSIGN = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year == year).sum()",
                "print(\"In {year}, {percentage}% games were produced from August to December.\"",
                ".format(year=year,",
                "ASSIGN=round((from_Augustpath)*100, 1)))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Make a list of years from 2014 to 2018\n",
                "years_lst = [year for year in range(2014,2019)]\n",
                "\n",
                "#For loop to get a picture of the amount of games produced from August to December\n",
                "for year in years_lst:\n",
                "    from_August = app_df_clean[\"Original Release Date\"].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()\n",
                "    total = app_df_clean[\"Original Release Date\"].apply(lambda date: date.year == year).sum()\n",
                "    print(\"In {year}, {percentage}% games were produced from August to December.\"\n",
                "          .format(year=year,\n",
                "                  percentage=round((from_August/total)*100, 1)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = app_df_clean[\"Price\"]",
                "ASSIGN = sns.light_palette(\"green\", reverse=True)",
                "ASSIGN = sns.countplot(x=price, palette=palette2)",
                "ASSIGN.set_xlabel(\"Price (in US dollars)\", fontsize=16)",
                "ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12, rotation=45)",
                "ASSIGN.set_ylabel(\"Amount\", fontsize=16)",
                "ASSIGN.set_title(\"Quantity of Each App per Price\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "price_vis.annotate(\"{:.0f}\".format(p.get_height()), # Text that will appear on the screen",
                "(p.get_x() + p.get_width() path+ 0.1, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 10),",
                "ASSIGN='offset points')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "price = app_df_clean[\"Price\"]\n",
                "\n",
                "# Plot a Countplot\n",
                "palette2 = sns.light_palette(\"green\", reverse=True)\n",
                "price_vis = sns.countplot(x=price, palette=palette2)\n",
                "price_vis.set_xlabel(\"Price (in US dollars)\", fontsize=16)\n",
                "price_vis.set_xticklabels(price_vis.get_xticklabels(), fontsize=12, rotation=45)\n",
                "price_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "price_vis.set_title(\"Quantity of Each App per Price\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in price_vis.patches:\n",
                "    price_vis.annotate(\"{:.0f}\".format(p.get_height()), # Text that will appear on the screen\n",
                "                       (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()), # (x, y) has to be a tuple\n",
                "                       ha='center', va='center', fontsize=14, color='black', xytext=(0, 10), # Customizations\n",
                "                       textcoords='offset points')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = app_df_clean[\"In-app Purchases\"].str.split(\",\").apply(lambda lst: len(lst))",
                "ASSIGN = sns.color_palette(\"BuGn_r\", 23)",
                "ASSIGN = sns.stripplot(x=price, y=in_app_purchases, palette=palette3)",
                "ASSIGN.set_xlabel(\"Game Price (in US dollars)\", fontsize=16)",
                "ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12, rotation=45)",
                "ASSIGN.set_ylabel(\"In-app Purchases Available\", fontsize=16)",
                "ASSIGN.set_title(\"Quantity of In-app Purchases per Game Price\", fontsize=20)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Variables\n",
                "in_app_purchases = app_df_clean[\"In-app Purchases\"].str.split(\",\").apply(lambda lst: len(lst))\n",
                "\n",
                "# Plot a stripplot\n",
                "palette3 = sns.color_palette(\"BuGn_r\", 23)\n",
                "in_app_purchases_vis = sns.stripplot(x=price, y=in_app_purchases, palette=palette3)\n",
                "in_app_purchases_vis.set_xlabel(\"Game Price (in US dollars)\", fontsize=16)\n",
                "in_app_purchases_vis.set_xticklabels(in_app_purchases_vis.get_xticklabels(), fontsize=12, rotation=45)\n",
                "in_app_purchases_vis.set_ylabel(\"In-app Purchases Available\", fontsize=16)\n",
                "in_app_purchases_vis.set_title(\"Quantity of In-app Purchases per Game Price\", fontsize=20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = sns.color_palette(\"BuPu_r\")",
                "ASSIGN = sns.countplot(app_df_clean.iloc[:200][\"Price\"], palette=palette4)",
                "ASSIGN.set_xlabel(\"Price (in US dollars)\", fontsize=16)",
                "ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), fontsize=12)",
                "ASSIGN.set_ylabel(\"Amount\", fontsize=16)",
                "ASSIGN.set_title(\"Quantity of Each App per Price\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.0f}\".format(p.get_height()),",
                "(p.get_x() + p.get_width() path, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8),",
                "ASSIGN='offset points')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot a distribution of the top 200 apps by their price\n",
                "\n",
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Plot a Countplot\n",
                "palette4 = sns.color_palette(\"BuPu_r\")\n",
                "top_prices = sns.countplot(app_df_clean.iloc[:200][\"Price\"], palette=palette4)\n",
                "top_prices.set_xlabel(\"Price (in US dollars)\", fontsize=16)\n",
                "top_prices.set_xticklabels(top_prices.get_xticklabels(), fontsize=12)\n",
                "top_prices.set_ylabel(\"Amount\", fontsize=16)\n",
                "top_prices.set_title(\"Quantity of Each App per Price\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in top_prices.patches:\n",
                "    top_prices.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = app_df_clean[app_df_clean[\"Price\"] > 0]",
                "ASSIGN = len(paid)",
                "ASSIGN = app_df_clean[app_df_clean[\"Price\"] == 0]",
                "ASSIGN = len(free)",
                "ASSIGN = plt.subplots(1, 2, figsize=(16,10))",
                "ASSIGN = sns.countplot(x=\"Average User Rating\", data=free, ax=axes[0])",
                "ASSIGN.set_xlabel(\"Average User Rating\", fontsize=16)",
                "ASSIGN.set_ylabel(\"Amount\", fontsize=16)",
                "ASSIGN.set_title(\"Free Apps\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.1f}%\".format(100 * (p.get_height()path)),",
                "(p.get_x() + p.get_width() path+ 0.1, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8),",
                "ASSIGN='offset points')",
                "ASSIGN = sns.countplot(x=\"Average User Rating\", data=paid, ax=axes[1])",
                "ASSIGN.set_xlabel(\"Average User Rating\", fontsize=16)",
                "ASSIGN.set_ylabel(\" \", fontsize=16)",
                "ASSIGN.set_title(\"Paid Apps\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.1f}%\".format(100 * (p.get_height()path)),",
                "(p.get_x() + p.get_width() path+ 0.1, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8),",
                "ASSIGN='offset points')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Create the DataFrames needed\n",
                "paid = app_df_clean[app_df_clean[\"Price\"] > 0]\n",
                "total_paid = len(paid)\n",
                "free = app_df_clean[app_df_clean[\"Price\"] == 0]\n",
                "total_free = len(free)\n",
                "\n",
                "# Make the figure and the axes (1 row, 2 columns)\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16,10))\n",
                "\n",
                "# Free apps countplot\n",
                "free_vis = sns.countplot(x=\"Average User Rating\", data=free, ax=axes[0])\n",
                "free_vis.set_xlabel(\"Average User Rating\", fontsize=16)\n",
                "free_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "free_vis.set_title(\"Free Apps\", fontsize=20)\n",
                "\n",
                "# Display the percentages on top of the bars\n",
                "for p in free_vis.patches:\n",
                "     free_vis.annotate(\"{:.1f}%\".format(100 * (p.get_height()/total_free)),\n",
                "                       (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')\n",
                "    \n",
                "# Paid apps countplot\n",
                "paid_vis = sns.countplot(x=\"Average User Rating\", data=paid, ax=axes[1])\n",
                "paid_vis.set_xlabel(\"Average User Rating\", fontsize=16)\n",
                "paid_vis.set_ylabel(\" \", fontsize=16)\n",
                "paid_vis.set_title(\"Paid Apps\", fontsize=20)\n",
                "\n",
                "# Display the percentages on top of the bars\n",
                "for p in paid_vis.patches:\n",
                "    paid_vis.annotate(\"{:.1f}%\".format(100 * (p.get_height()/total_paid)),\n",
                "                      (p.get_x() + p.get_width() / 2 + 0.1, p.get_height()),\n",
                "                       ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                       textcoords='offset points')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = sns.color_palette(\"BuGn_r\")",
                "ASSIGN = sns.countplot(x=app_df_clean[\"Age Rating\"], order=[\"4+\", \"9+\", \"12+\", \"17+\"], palette=palette5)",
                "ASSIGN.set_xlabel(\"Age Rating\", fontsize=16)",
                "ASSIGN.set_ylabel(\"Amount\", fontsize=16)",
                "ASSIGN.set_title(\"Amount of Games per Age Restriction\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.0f}\".format(p.get_height()),",
                "(p.get_x() + p.get_width() path, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=14, color='black', xytext=(0, 8),",
                "ASSIGN='offset points')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "# Make a countplot\n",
                "palette5 = sns.color_palette(\"BuGn_r\")\n",
                "age_vis = sns.countplot(x=app_df_clean[\"Age Rating\"], order=[\"4+\", \"9+\", \"12+\", \"17+\"], palette=palette5)\n",
                "age_vis.set_xlabel(\"Age Rating\", fontsize=16)\n",
                "age_vis.set_ylabel(\"Amount\", fontsize=16)\n",
                "age_vis.set_title(\"Amount of Games per Age Restriction\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in age_vis.patches:\n",
                "    age_vis.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                        ha='center', va='center', fontsize=14, color='black', xytext=(0, 8),\n",
                "                        textcoords='offset points')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "app_df_clean[\"numLang\"] = app_df_clean[\"Languages\"].apply(lambda x: len(x.split(\",\")))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Create a new column that contains the amount of languages that app has available\n",
                "app_df_clean[\"numLang\"] = app_df_clean[\"Languages\"].apply(lambda x: len(x.split(\",\")))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,10))",
                "ASSIGN = app_df_clean.loc[app_df_clean[\"numLang\"] <= 25, \"numLang\"]",
                "ASSIGN = sns.color_palette(\"PuBuGn_r\")",
                "ASSIGN = sns.countplot(x=lang, data=app_df_clean, palette=palette6)",
                "ASSIGN.set_xlabel(\"Quantity of Languages\", fontsize=16)",
                "ASSIGN.set_ylabel(\"Amount of Games\", fontsize=16)",
                "ASSIGN.set_title(\"Quantity of Languages Available per Game\", fontsize=20)",
                "for p in ASSIGN.patches:",
                "ASSIGN.annotate(\"{:.0f}\".format(p.get_height()),",
                "(p.get_x() + p.get_width() path+ .1, p.get_height()),",
                "ASSIGN='center', va='center', fontsize=12, color='black', xytext=(0, 12),",
                "ASSIGN='offset points')"
            ],
            "output_type": "display_data",
            "content_old": [
                "#Make the figure\n",
                "plt.figure(figsize=(16,10))\n",
                "\n",
                "#Variables\n",
                "lang = app_df_clean.loc[app_df_clean[\"numLang\"] <= 25, \"numLang\"]\n",
                "\n",
                "#Plot a countplot\n",
                "palette6 = sns.color_palette(\"PuBuGn_r\")\n",
                "numLang_vis = sns.countplot(x=lang, data=app_df_clean, palette=palette6)\n",
                "numLang_vis.set_xlabel(\"Quantity of Languages\", fontsize=16)\n",
                "numLang_vis.set_ylabel(\"Amount of Games\", fontsize=16)\n",
                "numLang_vis.set_title(\"Quantity of Languages Available per Game\", fontsize=20)\n",
                "\n",
                "# Write the height of the bars on top\n",
                "for p in numLang_vis.patches:\n",
                "    numLang_vis.annotate(\"{:.0f}\".format(p.get_height()), \n",
                "                        (p.get_x() + p.get_width() / 2. + .1, p.get_height()),\n",
                "                        ha='center', va='center', fontsize=12, color='black', xytext=(0, 12),\n",
                "                        textcoords='offset points')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] == \"EN\")])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Amount of games that have only the English language\n",
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] == \"EN\")])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] != \"EN\")])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Amount of games that have only one language and is not English\n",
                "len(app_df_clean[(app_df_clean[\"numLang\"] == 1) & (app_df_clean[\"Languages\"] != \"EN\")])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = '..path'",
                "ASSIGN = '..path'",
                "ASSIGN = 'submission.csv'",
                "ASSIGN = pd.read_csv(train_file_path)",
                "ASSIGN = pd.read_csv(test_file_path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_file_path = '../input/train.csv' ",
                "test_file_path = '../input/test.csv' ",
                "submission_file_path = 'submission.csv' ",
                "",
                "train = pd.read_csv(train_file_path)",
                "test = pd.read_csv(test_file_path)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train.fillna(0)",
                "test.fillna(0)",
                "train.dtypes"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train.fillna(0)",
                "test.fillna(0)",
                "",
                "train.dtypes"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']",
                "ASSIGN = train['Survived']",
                "ASSIGN = train[data_predictors]",
                "ASSIGN = test[data_predictors]",
                "print(ASSIGN.head())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data_predictors = ['Pclass', 'Sex', 'Age', 'SibSp', 'Fare']",
                "",
                "train_y = train['Survived']",
                "train_X = train[data_predictors]",
                "test_X = test[data_predictors]",
                "",
                "print(test_X.head())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.get_dummies(train_X)",
                "ASSIGN = pd.get_dummies(test_X)",
                "ASSIGN = ASSIGN.fillna(0)",
                "train_x"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_x = pd.get_dummies(train_X)",
                "test_x = pd.get_dummies(test_X)",
                "train_x = train_x.fillna(0)",
                "train_x"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = GradientBoostingRegressor()",
                "ASSIGN.fit(train_x, train_y)",
                "ASSIGN = plot_partial_dependence(plot_model,",
                "ASSIGN=[1, 3],",
                "X=train_x,",
                "ASSIGN=['Pclass', 'Age', 'SibSp', 'Fare'],",
                "ASSIGN=10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence",
                "from sklearn.ensemble import GradientBoostingRegressor",
                "",
                "plot_model = GradientBoostingRegressor()",
                "plot_model.fit(train_x, train_y)",
                "dep_plots = plot_partial_dependence(plot_model,       ",
                "                                   features=[1, 3], ",
                "                                   X=train_x,            # raw predictors data.",
                "                                   feature_names=['Pclass', 'Age', 'SibSp', 'Fare'], # labels on graphs",
                "                                   grid_resolution=10) # number of values to plot on x axis"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = make_pipeline(Imputer(), XGBRegressor())",
                "ASSIGN.fit(train_x, train_y)",
                "ASSIGN = cross_val_score(data_model, train_x, train_y, scoring='neg_mean_absolute_error')",
                "print(ASSIGN)",
                "print('Mean Absolute Error %2f' %(-1 * ASSIGN.mean()))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from xgboost import XGBRegressor",
                "from sklearn.preprocessing import Imputer",
                "from sklearn.pipeline import make_pipeline",
                "from sklearn.model_selection import cross_val_score",
                "",
                "data_model = make_pipeline(Imputer(), XGBRegressor())",
                "data_model.fit(train_x, train_y)",
                "",
                "scores = cross_val_score(data_model, train_x, train_y, scoring='neg_mean_absolute_error')",
                "print(scores)",
                "print('Mean Absolute Error %2f' %(-1 * scores.mean()))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = data_model.predict(test_x)",
                "print('Estimated survivors: ' + str(ASSIGN.astype(int).sum()) + ' passengers')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "prediction = data_model.predict(test_x)",
                "print('Estimated survivors: ' + str(prediction.astype(int).sum()) + ' passengers')"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = test.assign(Survived=prediction.astype(int))",
                "ASSIGN.to_csv(submission_file_path,sep=',',columns=['PassengerId', 'Survived'], index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "result = test.assign(Survived=prediction.astype(int))",
                "result.to_csv(submission_file_path,sep=',',columns=['PassengerId', 'Survived'], index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np\n",
                "import seaborn as sns\n",
                "import pandas as pd\n",
                "\n",
                "import sklearn\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import AdaBoostClassifier\n",
                "\n",
                "from sklearn.model_selection import cross_val_score\n",
                "from sklearn import preprocessing\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"path\", na_values = \"?\")",
                "adult_train.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_train = pd.read_csv(\"/kaggle/input/adult-pmr3508/train_data.csv\", na_values = \"?\")\n",
                "adult_train.shape\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"path\", na_values = \"?\")",
                "adult_test.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_test = pd.read_csv(\"/kaggle/input/adult-pmr3508/test_data.csv\", na_values = \"?\")\n",
                "adult_test.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_train.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.describe(exclude = [np.number])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_train.describe(exclude = [np.number])"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = adult_train.dropna()",
                "n_adult.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "n_adult = adult_train.dropna()\n",
                "n_adult.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = adult_train.describe(exclude = [np.number]).columns",
                "ASSIGN = adult_train[cat].apply(pd.Categorical)",
                "for col in ASSIGN:",
                "adult_train[col + \"_cat\"] = ASSIGN[col].ASSIGN.codes",
                "ASSIGN = adult_test[cat[:-1]].apply(pd.Categorical)",
                "for col in ASSIGN[:-1]:",
                "adult_test[col + \"_cat\"] = ASSIGN[col].ASSIGN.codes"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cat = adult_train.describe(exclude = [np.number]).columns\n",
                "\n",
                "categoricAdult = adult_train[cat].apply(pd.Categorical)\n",
                "\n",
                "for col in cat:\n",
                "    adult_train[col + \"_cat\"] = categoricAdult[col].cat.codes\n",
                "categoricTestAdult = adult_test[cat[:-1]].apply(pd.Categorical)\n",
                "\n",
                "for col in cat[:-1]:\n",
                "    adult_test[col + \"_cat\"] = categoricTestAdult[col].cat.codes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "adult_train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "adult_train.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(adult_train, vars=[\"age\", \"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\",",
                "\"hours.per.week\"], hue=\"income\", diag_kws={'bw':\"1.0\"}, corner=True)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.pairplot(adult_train, vars=[\"age\", \"fnlwgt\", \"education.num\", \"capital.gain\", \"capital.loss\", \n",
                "                          \"hours.per.week\"], hue=\"income\", diag_kws={'bw':\"1.0\"}, corner=True)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "adult_train[\"native.country\"].value_counts().plot(kind=\"pie\", figsize = (8,8))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "adult_train[\"native.country\"].value_counts().plot(kind=\"pie\", figsize = (8,8))\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = adult_train.copy()",
                "ASSIGN = LabelEncoder()",
                "ASSIGN[\"income\"] = ASSIGN.fit_transform(ASSIGN['income'])",
                "plt.figure(figsize=(10,10))",
                "ASSIGN = np.zeros_like(adult_copy.corr(), dtype=np.bool)",
                "ASSIGN[np.triu_indices_from(ASSIGN)] = True",
                "sns.heatmap(ASSIGN.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, ASSIGN=ASSIGN)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "adult_copy = adult_train.copy()\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "le = LabelEncoder()\n",
                "adult_copy[\"income\"] = le.fit_transform(adult_copy['income'])\n",
                "\n",
                "#heat map:\n",
                "plt.figure(figsize=(10,10))\n",
                "mask = np.zeros_like(adult_copy.corr(), dtype=np.bool)\n",
                "mask[np.triu_indices_from(mask)] = True\n",
                "sns.heatmap(adult_copy.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, mask=mask)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(nrows = 2, ncols = 2)",
                "plt.tight_layout(pad = .4, w_pad = .5, h_pad = 1.)",
                "adult_train.groupby(['sex', 'income']).size().unstack().plot(kind = 'bar', stacked = True, ax = axes[0, 0], figsize = (20, 15))",
                "ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack()",
                "ASSIGN = adult_train.groupby('relationship').size()",
                "ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']]",
                "ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[0, 1])",
                "ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack()",
                "ASSIGN = adult_train.groupby('education').size()",
                "ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']]",
                "ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[1, 0])",
                "ASSIGN = adult_train.groupby(['ASSIGN', 'income']).size().unstack()",
                "ASSIGN = adult_train.groupby('occupation').size()",
                "ASSIGN = ASSIGN.sort_values('sum', ascending = False)[['<=50K', '>50K']]",
                "ASSIGN.plot(kind = 'bar', stacked = True, ax = axes[1, 1])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 2)\n",
                "plt.tight_layout(pad = .4, w_pad = .5, h_pad = 1.)\n",
                "\n",
                "adult_train.groupby(['sex', 'income']).size().unstack().plot(kind = 'bar', stacked = True, ax = axes[0, 0], figsize = (20, 15))\n",
                "\n",
                "relationship = adult_train.groupby(['relationship', 'income']).size().unstack()\n",
                "relationship['sum'] = adult_train.groupby('relationship').size()\n",
                "relationship = relationship.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "relationship.plot(kind = 'bar', stacked = True, ax = axes[0, 1])\n",
                "\n",
                "education = adult_train.groupby(['education', 'income']).size().unstack()\n",
                "education['sum'] = adult_train.groupby('education').size()\n",
                "education = education.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "education.plot(kind = 'bar', stacked = True, ax = axes[1, 0])\n",
                "\n",
                "occupation = adult_train.groupby(['occupation', 'income']).size().unstack()\n",
                "occupation['sum'] = adult_train.groupby('occupation').size()\n",
                "occupation = occupation.sort_values('sum', ascending = False)[['<=50K', '>50K']]\n",
                "occupation.plot(kind = 'bar', stacked = True, ax = axes[1, 1])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']",
                "ASSIGN= ['occupation', 'relationship', 'sex','education']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "princ_num_colum= ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
                "princ_cat_colum= ['occupation', 'relationship', 'sex','education']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = adult_train[princ_num_colum + princ_cat_colum]",
                "ASSIGN = adult_train[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]",
                "ASSIGN = adult_test[princ_num_colum + princ_cat_colum]",
                "ASSIGN = adult_test[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]",
                "ASSIGN = adult_train.income"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train = adult_train[princ_num_colum + princ_cat_colum]\n",
                "numX_train = adult_train[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]\n",
                "\n",
                "X_test = adult_test[princ_num_colum + princ_cat_colum]\n",
                "numXadul_test = adult_test[princ_num_colum + list(map(lambda x: x + \"_cat\", princ_cat_colum))]\n",
                "\n",
                "Yadult = adult_train.income"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = {}",
                "ASSIGN = 0.0",
                "for k in range(30, 35):",
                "ASSIGN = KNeighborsClassifier(k, metric = 'manhattan')",
                "ASSIGN = np.mean(cross_val_score(knn, numX_train, Yadult, cv = 10))",
                "if ASSIGN > ASSIGN:",
                "ASSIGN = k",
                "ASSIGN = score",
                "ASSIGN = knn",
                "ASSIGN['KNN'].fit(numX_train, Yadult)",
                "print(.format(ASSIGN, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "classifiers = {}\n",
                "scores = 0.0\n",
                "\n",
                "\n",
                "for k in range(30, 35):\n",
                "    knn = KNeighborsClassifier(k, metric = 'manhattan')\n",
                "    score = np.mean(cross_val_score(knn, numX_train, Yadult, cv = 10))\n",
                "    \n",
                "    if score > scores:\n",
                "        bestK = k\n",
                "        scores = score\n",
                "        classifiers['KNN'] = knn\n",
                "\n",
                "        \n",
                "classifiers['KNN'].fit(numX_train, Yadult)\n",
                "        \n",
                "print(\"Best acc: {}, K = {}\".format(scores, bestK))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = classifiers['KNN'].predict(numXadul_test)"
            ],
            "output_type": "stream",
            "content_old": [
                "%%time\n",
                "\n",
                "predictions = classifiers['KNN'].predict(numXadul_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({'Id' : list(range(len(predictions)))})",
                "ASSIGN = pd.DataFrame({'ASSIGN' : predictions})",
                "ASSIGN = income"
            ],
            "output_type": "not_existent",
            "content_old": [
                "id_index = pd.DataFrame({'Id' : list(range(len(predictions)))})\n",
                "income = pd.DataFrame({'income' : predictions})\n",
                "result = income"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "result.to_csv(\"submission.csv\", index = True, index_label = 'Id')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "result.to_csv(\"submission.csv\", index = True, index_label = 'Id')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful character encoding module",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = \"This is the euro symbol: \"",
                "type(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# start with a string",
                "before = \"This is the euro symbol: \"",
                "",
                "# check to see what datatype it is",
                "type(before)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = before.encode(\"utf-8\", errors = \"replace\")",
                "type(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# encode it to a different encoding, replacing characters that raise errors",
                "after = before.encode(\"utf-8\", errors = \"replace\")",
                "",
                "# check the type",
                "type(after)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "after"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# take a look at what the bytes look like",
                "after"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(after.decode())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# convert it back to utf-8",
                "print(after.decode(\"utf-8\"))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(after.decode())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# try to decode our bytes with the ascii encoding",
                "print(after.decode(\"ascii\"))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = \"This is the euro symbol: \"",
                "ASSIGN = before.encode(\"ascii\", errors = \"replace\")",
                "print(ASSIGN.decode())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# start with a string",
                "before = \"This is the euro symbol: \"",
                "",
                "# encode it to a different encoding, replacing characters that raise errors",
                "after = before.encode(\"ascii\", errors = \"replace\")",
                "",
                "# convert it back to utf-8",
                "print(after.decode(\"ascii\"))",
                "",
                "# We've lost the original underlying byte string! It's been ",
                "# replaced with the underlying byte string for the unknown character :("
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = \"i'll try the recommended $, #,  and  and see what happens.\"",
                "ASSIGN = my_text.encode(\"ascii\", errors = \"replace\")",
                "print(ASSIGN.decode())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Try encoding and decoding different symbols to ASCII and",
                "# see what happens. I'd recommend $, #,  and  but feel free to",
                "# try other characters. What happens? When would this cause problems?",
                "",
                "# start with a string",
                "my_text = \"i'll try the recommended $, #,  and   and see what happens.\"",
                "",
                "# encode it to a different encoding, replacing characters that raise errors",
                "my_text_encoded = my_text.encode(\"ascii\", errors = \"replace\")",
                "",
                "# convert it back to utf-8",
                "print(my_text_encoded.decode(\"ascii\"))",
                ""
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# try to read in a file not in UTF-8",
                "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "with open(\"..path\", 'rb') as rawdata:",
                "ASSIGN = chardet.detect(rawdata.read(10000))",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/kickstarter-projects/ks-projects-201801.csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(10000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", encoding='Windows-1252')",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# read in the file with the encoding detected by chardet",
                "kickstarter_2016 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201612.csv\", encoding='Windows-1252')",
                "",
                "# look at the first few lines",
                "kickstarter_2016.head()"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN=[1,10,100,1000,10000,100000,1000000]",
                "with open(\"..path\", 'rb') as rawdata1:",
                "for i in ASSIGN:",
                "ASSIGN = chardet.detect(rawdata1.read(i))",
                "print (i, ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your Turn! Trying to read in this file gives you an error. Figure out",
                "# what the correct encoding should be and read in the file. :)",
                "",
                "read_times=[1,10,100,1000,10000,100000,1000000]",
                "with open(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", 'rb') as rawdata1:",
                "    for i in read_times:",
                "        result1 = chardet.detect(rawdata1.read(i))",
                "        print (i, result1)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", encoding='Windows-1252')",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "police_killings = pd.read_csv(\"../input/fatal-police-shootings-in-the-us/PoliceKillingsUS.csv\", encoding='Windows-1252')",
                "",
                "# look at the first few lines",
                "police_killings.head()"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# save our file (will be saved as UTF-8 by default!)",
                "kickstarter_2016.to_csv(\"ks-projects-201801-utf8.csv\")"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "police_killings.to_csv(\"PoliceKillingsUS-utf8.csv\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Save out a version of the police_killings dataset with UTF-8 encoding ",
                "police_killings.to_csv(\"PoliceKillingsUS-utf8.csv\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path(v4).csv\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# read in all our data",
                "nfl_data = pd.read_csv(\"../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv\")",
                "sf_permits = pd.read_csv(\"../input/building-permit-applications-data/Building_Permits.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0) "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "nfl_data.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look at a few rows of the nfl_data file. I can see a handful of missing data already!",
                "nfl_data.sample(5)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sf_permits.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# your turn! Look at a couple of rows from the sf_permits dataset. Do you notice any missing data?",
                "",
                "# your code goes here :)",
                "",
                "sf_permits.sample(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = nfl_data.isnull().sum()",
                "ASSIGN[0:10]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the number of missing data points per column",
                "missing_values_count = nfl_data.isnull().sum()",
                "",
                "# look at the # of missing points in the first ten columns",
                "missing_values_count[0:10]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = np.product(nfl_data.shape)",
                "ASSIGN = missing_values_count.sum()",
                "(total_missingpath) * 100"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# how many total missing values do we have?",
                "total_cells = np.product(nfl_data.shape)",
                "total_missing = missing_values_count.sum()",
                "",
                "# percent of data that is missing",
                "(total_missing/total_cells) * 100"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = sf_permits.isnull().sum()",
                "ASSIGN = np.product(sf_permits.shape)",
                "ASSIGN = missing_values_count_sf_permits.sum()",
                "(total_missing_sf_permitspath) * 100"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# your turn! Find out what percent of the sf_permits dataset is missing",
                "",
                "# get the number of missing data points per column for the sf_permits",
                "missing_values_count_sf_permits = sf_permits.isnull().sum()",
                "",
                "# count the total cells of sf_permits and its missing data",
                "total_cells_sf_permits = np.product(sf_permits.shape)",
                "total_missing_sf_permits = missing_values_count_sf_permits.sum()",
                "",
                "# percent of data that is missing on sf_permits",
                "(total_missing_sf_permits/total_cells_sf_permits) * 100"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "missing_values_count[0:10]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look at the # of missing points in the first ten columns",
                "missing_values_count[0:10]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format((sf_permits['Street Number Suffix'].isnull().sum()path['Street Number Suffix'].shape[0])*100))",
                "print(.format((sf_permits['Zipcode'].isnull().sum()path['Zipcode'].shape[0])*100))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look for the percentage of missing data in order to have some idea of it.",
                "print(\"Percentage of missinf field 'Street Number Suffix': {0:.2f} %\".format((sf_permits['Street Number Suffix'].isnull().sum()/sf_permits['Street Number Suffix'].shape[0])*100))",
                "print(\"Percentageof missinf field 'Zipcode': {0:.2f} %\".format((sf_permits['Zipcode'].isnull().sum()/sf_permits['Zipcode'].shape[0])*100))",
                "#sf_permits['Zipcode']",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "nfl_data.dropna()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# remove all the rows that contain a missing value",
                "nfl_data.dropna()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = nfl_data.dropna(axis=1)",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# remove all columns with at least one missing value",
                "columns_with_na_dropped = nfl_data.dropna(axis=1)",
                "columns_with_na_dropped.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print( % nfl_data.shape[1])",
                "print( % columns_with_na_dropped.shape[1])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# just how much data did we lose?",
                "print(\"Columns in original dataset: %d \\n\" % nfl_data.shape[1])",
                "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped.shape[1])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sf_permits.dropna()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Try removing all the rows from the sf_permits dataset that contain missing values. How many are left?",
                "# remove all the rows that contain a missing value",
                "sf_permits.dropna()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = sf_permits.dropna(axis=1)",
                "print( % sf_permits.shape[1])",
                "print( % ASSIGN.shape[1])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Now try removing all the columns with empty values. Now how much of your data is left?",
                "columns_with_na_dropped_sf = sf_permits.dropna(axis=1)",
                "",
                "# just how much data did we lose?",
                "print(\"Columns in original dataset: %d \\n\" % sf_permits.shape[1])",
                "print(\"Columns with na's dropped: %d\" % columns_with_na_dropped_sf.shape[1])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = nfl_data.loc[:, 'EPA':'Season'].head()",
                "subset_nfl_data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get a small subset of the NFL dataset",
                "subset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()",
                "subset_nfl_data"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "subset_nfl_data.fillna(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# replace all NA's with 0",
                "subset_nfl_data.fillna(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(\"0\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# replace all NA's the value that comes directly after it in the same column, ",
                "# then replace all the reamining na's with 0",
                "subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(\"0\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = sf_permits.fillna(method=\"bfill\", axis=0).fillna(0)",
                "ASSIGN.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Try replacing all the NaN's in the sf_permits data with the one that",
                "# comes directly after it and then ",
                "",
                "sf_permits_sub = sf_permits.fillna(method=\"bfill\", axis=0).fillna(0)",
                "",
                "sf_permits_sub.sample(5)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# helpful modules",
                "import fuzzywuzzy",
                "from fuzzywuzzy import process",
                "import chardet",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "with open(\"..path(30-November-2017).csv\", 'rb') as rawdata:",
                "ASSIGN = chardet.detect(rawdata.read(100000))",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# look at the first ten thousand bytes to guess the character encoding",
                "with open(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", 'rb') as rawdata:",
                "    result = chardet.detect(rawdata.read(100000))",
                "",
                "# check what the character encoding might be",
                "print(result)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path(30-November-2017).csv\",",
                "ASSIGN='Windows-1252')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# read in our dat",
                "suicide_attacks = pd.read_csv(\"../input/PakistanSuicideAttacks Ver 11 (30-November-2017).csv\", ",
                "                              encoding='Windows-1252')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "suicide_attacks.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "suicide_attacks.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# convert to lower case",
                "suicide_attacks['City'] = suicide_attacks['City'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['City'] = suicide_attacks['City'].str.strip()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['Province'].unique()",
                "ASSIGN.sort()",
                "provinces"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Take a look at all the unique values in the \"Province\" column. ",
                "# Then convert the column to lowercase and remove any trailing white spaces",
                "",
                "# get all the unique values in the 'Province' column",
                "provinces = suicide_attacks['Province'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "provinces.sort()",
                "provinces"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()",
                "ASSIGN = suicide_attacks['Province'].unique()",
                "ASSIGN.sort()",
                "provinces"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# convert to lower case",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.lower()",
                "# remove trailing white spaces",
                "suicide_attacks['Province'] = suicide_attacks['Province'].str.strip()",
                "",
                "# get all the unique values in the 'City' column",
                "provinces = suicide_attacks['Province'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "provinces.sort()",
                "provinces"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "matches"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"d.i khan\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "ASSIGN = df[column].unique()",
                "ASSIGN = fuzzywuzzy.process.extract(string_to_match, strings,",
                "ASSIGN=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "ASSIGN = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "ASSIGN = df[column].isin(close_matches)",
                "df.loc[ASSIGN, column] = string_to_match",
                "print()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# function to replace rows in the provided column of the provided dataframe",
                "# that match the provided string above the provided ratio with the provided string",
                "def replace_matches_in_column(df, column, string_to_match, min_ratio = 90):",
                "    # get a list of unique strings",
                "    strings = df[column].unique()",
                "    ",
                "    # get the top 10 closest matches to our input string",
                "    matches = fuzzywuzzy.process.extract(string_to_match, strings, ",
                "                                         limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "    # only get matches with a ratio > 90",
                "    close_matches = [matches[0] for matches in matches if matches[1] >= min_ratio]",
                "",
                "    # get the rows of all the close matches in our dataframe",
                "    rows_with_matches = df[column].isin(close_matches)",
                "",
                "    # replace all rows with close matches with the input matches ",
                "    df.loc[rows_with_matches, column] = string_to_match",
                "    ",
                "    # let us know the function's done",
                "    print(\"All done!\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# use the function we just wrote to replace close matches to \"d.i khan\" with \"d.i khan\"",
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"d.i khan\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "matches"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! It looks like 'kuram agency' and 'kurram agency' should",
                "# be the same city. Correct the dataframe so that they are.",
                "",
                "# get the top 10 closest matches to \"d.i khan\"",
                "matches = fuzzywuzzy.process.extract(\"kuram agency\", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)",
                "",
                "# take a look at them",
                "matches"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\", min_ratio = 94)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=\"kuram agency\", min_ratio = 94)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = suicide_attacks['City'].unique()",
                "ASSIGN.sort()",
                "cities"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get all the unique values in the 'City' column",
                "cities = suicide_attacks['City'].unique()",
                "",
                "# sort them alphabetically and then take a closer look",
                "cities.sort()",
                "cities"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "import seaborn as sns",
                "import datetime",
                "",
                "# read in our data",
                "earthquakes = pd.read_csv(\"../input/earthquake-database/database.csv\")",
                "landslides = pd.read_csv(\"../input/landslide-events/catalog.csv\")",
                "volcanos = pd.read_csv(\"../input/volcanic-eruptions/database.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(landslides['date'].head())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# print the first few rows of the date column",
                "print(landslides['date'].head())"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "landslides['date'].dtype"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# check the data type of our date column",
                "landslides['date'].dtype"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "earthquakes['Date'].dtype"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Check the data type of the Date column in the earthquakes dataframe",
                "# (note the capital 'D' in date!)",
                "earthquakes['Date'].dtype"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%mpath%dpath%y\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# create a new column, date_parsed, with the parsed dates",
                "landslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m/%d/%y\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "landslides['date_parsed'].head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# print the first few rows",
                "landslides['date_parsed'].head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(earthquakes['Date'].dtype))",
                "print(earthquakes['Date'].head())",
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], format = \"%mpath%dpath%Y\")",
                "earthquakes['date_parsed'].head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Create a new column, date_parsed, in the earthquakes",
                "# dataset that has correctly parsed dates in it. (Don't forget to ",
                "# double-check that the dtype is correct!)",
                "print(\"the data type is '{}' \".format(earthquakes['Date'].dtype))",
                "print(earthquakes['Date'].head())",
                "",
                "#since the date format is supposebly american i will assume it is month/day/year",
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], format = \"%m/%d/%Y\")",
                "earthquakes['date_parsed'].head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "earthquakes.iloc[3370:3390]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#check larger range-found it!",
                "earthquakes.iloc[3370:3390]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], infer_datetime_format=True)",
                "earthquakes['date_parsed'].head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "earthquakes['date_parsed']=pd.to_datetime(earthquakes['Date'], infer_datetime_format=True)",
                "earthquakes['date_parsed'].head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = landslides['date'].dt.day"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# try to get the day of the month from the date column",
                "day_of_month_landslides = landslides['date'].dt.day"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = landslides['date_parsed'].dt.day"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the day of the month from the date_parsed column",
                "day_of_month_landslides = landslides['date_parsed'].dt.day"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = earthquakes['date_parsed'].dt.day"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! get the day of the month from the date_parsed column",
                "",
                "# Didn'understand much, i will do for earthquakes",
                "day_of_month_earthquakes = earthquakes['date_parsed'].dt.day"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna()",
                "sns.distplot(ASSIGN, kde=False, bins=31)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# remove na's",
                "day_of_month_landslides = day_of_month_landslides.dropna()",
                "",
                "# plot the day of the month",
                "sns.distplot(day_of_month_landslides, kde=False, bins=31)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna()",
                "sns.distplot(ASSIGN, kde=False, bins=31)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! Plot the days of the month from your",
                "# earthquake dataset and make sure they make sense.",
                "",
                "day_of_month_earthquakes = day_of_month_earthquakes.dropna()",
                "sns.distplot(day_of_month_earthquakes, kde=False, bins=31)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "volcanos['Last Known Eruption'].sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "volcanos['Last Known Eruption'].sample(5)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "np.random.seed(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# modules we'll use",
                "import pandas as pd",
                "import numpy as np",
                "",
                "# for Box-Cox Transformation",
                "from scipy import stats",
                "",
                "# for min_max scaling",
                "from mlxtend.preprocessing import minmax_scaling",
                "",
                "# plotting modules",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "# read in all our data",
                "kickstarters_2017 = pd.read_csv(\"../input/kickstarter-projects/ks-projects-201801.csv\")",
                "",
                "# set seed for reproducibility",
                "np.random.seed(0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.random.exponential(size = 1000)",
                "ASSIGN = minmax_scaling(original_data, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# generate 1000 data points randomly drawn from an exponential distribution",
                "original_data = np.random.exponential(size = 1000)",
                "",
                "# mix-max scale the data between 0 and 1",
                "scaled_data = minmax_scaling(original_data, columns = [0])",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = stats.boxcox(original_data)",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# normalize the exponential data with boxcox",
                "normalized_data = stats.boxcox(original_data)",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(original_data, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_data[0], ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "kickstarters_2017.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#a little glimpse at the data",
                "",
                "kickstarters_2017.sample(5)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.usd_goal_real",
                "ASSIGN = minmax_scaling(usd_goal, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# select the usd_goal_real column",
                "usd_goal = kickstarters_2017.usd_goal_real",
                "",
                "# scale the goals from 0 to 1",
                "scaled_data = minmax_scaling(usd_goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.usd_goal_real, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_data, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.ASSIGN",
                "ASSIGN = minmax_scaling(goal, columns = [0])",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! ",
                "",
                "# We just scaled the \"usd_goal_real\" column. What about the \"goal\" column?",
                "goal = kickstarters_2017.goal",
                "",
                "# scale the goals from 0 to 1",
                "scaled_goal = minmax_scaling(goal, columns = [0])",
                "",
                "# plot the original & scaled data together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(kickstarters_2017.goal, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(scaled_goal, ax=ax[1])",
                "ax[1].set_title(\"Scaled data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.usd_pledged_real > 0",
                "ASSIGN = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "ASSIGN = stats.boxcox(positive_pledges)[0]",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# get the index of all positive pledges (Box-Cox only takes postive values)",
                "index_of_positive_pledges = kickstarters_2017.usd_pledged_real > 0",
                "",
                "# get only positive pledges (using their indexes)",
                "positive_pledges = kickstarters_2017.usd_pledged_real.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledges = stats.boxcox(positive_pledges)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledges, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledges, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = kickstarters_2017.pledged > 0",
                "ASSIGN = kickstarters_2017.pledged.loc[index_of_positive_pledges]",
                "ASSIGN = stats.boxcox(positive_pledged)[0]",
                "ASSIGN=plt.subplots(1,2)",
                "sns.distplot(ASSIGN, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(ASSIGN, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Your turn! ",
                "# We looked as the usd_pledged_real column. What about the \"pledged\" column? Does it have the same info?",
                "index_of_positive_pledged = kickstarters_2017.pledged > 0",
                "",
                "# get only positive pledged (using their indexes)",
                "positive_pledged = kickstarters_2017.pledged.loc[index_of_positive_pledges]",
                "",
                "# normalize the pledges (w/ Box-Cox)",
                "normalized_pledged = stats.boxcox(positive_pledged)[0]",
                "",
                "# plot both together to compare",
                "fig, ax=plt.subplots(1,2)",
                "sns.distplot(positive_pledged, ax=ax[0])",
                "ax[0].set_title(\"Original Data\")",
                "sns.distplot(normalized_pledged, ax=ax[1])",
                "ax[1].set_title(\"Normalized data\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "sns.set(rc={'figure.figsize':(20, 15)})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# ML\n",
                "# for transformers creation\n",
                "from sklearn.base import BaseEstimator, TransformerMixin\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "\n",
                "# models and metrics\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV\n",
                "from sklearn.metrics import classification_report\n",
                "# distributions for random search\n",
                "from scipy.stats import randint, expon, reciprocal\n",
                "\n",
                "sns.set(rc={'figure.figsize':(20, 15)})"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "def save(model, cv_info, classification_report, name=\"model\", cv_scores=None):",
                "ASSIGN = {",
                "\"cv_info\": cv_info, \"classification_report\": classification_report, \"model\": model, \"cv_scores\": cv_scores",
                "}",
                "joblib.dump(ASSIGN, \"path\" + name + \".pkl\")",
                "def load(name=\"model\", verbose=True, with_metadata=False):",
                "ASSIGN = joblib.load(\"path\" + name + \".pkl\")",
                "if verbose:",
                "print()",
                "[print(.format(key=key, val=val)) for key, val in ASSIGN[].items()]",
                "print()",
                "print(ASSIGN[])",
                "if not with_metadata:",
                "return ASSIGN[\"model\"]",
                "else:",
                "return _model"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import joblib  # for saving models from skikit-learn\n",
                "\n",
                "# some utils for saving and reading later\n",
                "def save(model, cv_info, classification_report, name=\"model\", cv_scores=None):\n",
                "    _model = {\n",
                "        \"cv_info\": cv_info, \"classification_report\": classification_report, \"model\": model, \"cv_scores\": cv_scores\n",
                "    }\n",
                "    joblib.dump(_model, \"/kaggle/working/\" + name + \".pkl\")\n",
                "\n",
                "\n",
                "def load(name=\"model\", verbose=True, with_metadata=False):\n",
                "    _model = joblib.load(\"/kaggle/working/\" + name + \".pkl\")\n",
                "    if verbose:\n",
                "        print(\"\\nLoading model with the following info:\\n\")\n",
                "        [print(\"{key}: {val}\".format(key=key, val=val)) for key, val in _model[\"cv_info\"].items()]\n",
                "        print(\"\\nClassification Report:\\n\")\n",
                "        print(_model[\"classification_report\"])\n",
                "    if not with_metadata:\n",
                "        return _model[\"model\"]\n",
                "    else:\n",
                "        return _model"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dirname, _, filenames in os.walk('..path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "for dirname, _, filenames in os.walk('../input/titanic'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train = pd.read_csv('../input/titanic/train.csv')\n",
                "test = pd.read_csv('../input/titanic/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "train.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Survived\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train[\"Survived\"].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Sex\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train[\"Sex\"].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Embarked\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train[\"Embarked\"].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"Pclass\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# train[\"Name\"].value_counts() as imagined are uniques\n",
                "# train[\"Ticket\"].value_counts() is not unique, but has some few repetitions\n",
                "train[\"Pclass\"].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "train.profile_report()"
            ],
            "output_type": "display_data",
            "content_old": [
                "import pandas_profiling \n",
                "\n",
                "train.profile_report()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "train.hist(figsize=(20, 15))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "train.hist(figsize=(20, 15))\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train.corr()",
                "ASSIGN[\"Survived\"]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "corr_matrix = train.corr()\n",
                "corr_matrix[\"Survived\"]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\",",
                "ASSIGN=train, palette=\"muted\", split=True)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# hypothesis for feature engineering \"women and children first\"\n",
                "sns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\",\n",
                "                    data=train, palette=\"muted\", split=True)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN path* 15",
                "train[[\"Age\", \"Survived\"]].groupby(['Age']).mean()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Hypotesis: since the survived looks like \"bimodal\" near 15 years, we should try to see \n",
                "# the correlation of categorizing if the passenger is less than 15 years\n",
                "\n",
                "train[\"Age\"] = train[\"Age\"] // 15 * 15\n",
                "train[[\"Age\", \"Survived\"]].groupby(['Age']).mean()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ASSIGN + ASSIGN",
                "train[[\"RelativesOnboard\", \"Survived\"]].groupby([\"RelativesOnboard\"]).mean()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train[\"RelativesOnboard\"] = train[\"SibSp\"] + train[\"Parch\"]\n",
                "# train[[\"RelativesOnboard\", \"Survived\"]].groupby(['RelativesOnboard']).mean()\n",
                "train[[\"RelativesOnboard\", \"Survived\"]].groupby([\"RelativesOnboard\"]).mean()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# count total family members look to have a better discrimination on survival rate\n",
                "# since number of siblings is nearer to the mean survival rate 38% \n",
                "train[[\"SibSp\", \"Survived\"]].groupby(['SibSp']).mean()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train[[\"Parch\", \"Survived\"]].groupby(['Parch']).mean()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.corr()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Age grouped by 15 years have near 0 correlation, but some groups have more survival rate than others\n",
                "# this only means that the relation of age groups and survival rate are non-linear\n",
                "train.corr()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "class DataFrameSelector(BaseEstimator, TransformerMixin):",
                "def __init__(self, attribute_names):",
                "self.attribute_names = attribute_names",
                "def fit(self, X, y=None):",
                "return self",
                "def transform(self, X):",
                "return X[self.attribute_names]",
                "class MostFrequentImputer(BaseEstimator, TransformerMixin):",
                "def fit(self, X, y=None):",
                "self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],",
                "ASSIGN=X.columns)",
                "return self",
                "def transform(self, X, y=None):",
                "return X.fillna(self.most_frequent_)",
                "class AgeGrouper(BaseEstimator, TransformerMixin):",
                "def __init__(self, new_attribute=\"AgeGrp\", attribute_name=\"Age\", group_scale=15, del_originals=True):",
                "self.group_scale = group_scale",
                "self.attribute_name = attribute_name",
                "self.new_attribute = new_attribute",
                "self.del_originals = del_originals",
                "def fit(self, X, y=None):",
                "self.age_groups = X[self.attribute_name] path* self.group_scale",
                "return self",
                "def transform(self, X, y=None):",
                "X[self.new_attribute] = self.age_groups",
                "if self.del_originals:",
                "X.drop(columns=self.attribute_name, axis=1, inplace=True)",
                "return X",
                "class AtributesAdder(BaseEstimator, TransformerMixin):",
                "def __init__(self, new_attribute=\"RelativesOnboard\", attribute_names=[\"SibSp\", \"Parch\"], del_originals=True):",
                "self.attribute_names = attribute_names",
                "self.final_attr = 0",
                "self.new_attribute = new_attribute",
                "self.del_originals = del_originals",
                "def fit(self, X, y=None):",
                "for attr in self.attribute_names:",
                "self.final_attr += X[attr]",
                "return self",
                "def transform(self, X, y=None):",
                "X[self.new_attribute] = self.final_attr",
                "if self.del_originals:",
                "X.drop(columns=self.attribute_names, axis=1, inplace=True)",
                "return X"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Transformers created by https://github.com/ageron/handson-ml2\n",
                "\n",
                "# this transformers we will choose which attributes, late numerical and categorical\n",
                "# to use some input strategies\n",
                "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, attribute_names):\n",
                "        self.attribute_names = attribute_names\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        return self\n",
                "\n",
                "    def transform(self, X):\n",
                "        return X[self.attribute_names]\n",
                "\n",
                "class MostFrequentImputer(BaseEstimator, TransformerMixin):\n",
                "    def fit(self, X, y=None):\n",
                "        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n",
                "                                        index=X.columns)\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        return X.fillna(self.most_frequent_)\n",
                "\n",
                "\n",
                "class AgeGrouper(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, new_attribute=\"AgeGrp\", attribute_name=\"Age\", group_scale=15, del_originals=True):\n",
                "        self.group_scale = group_scale\n",
                "        self.attribute_name = attribute_name\n",
                "        self.new_attribute = new_attribute\n",
                "        self.del_originals = del_originals\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        self.age_groups = X[self.attribute_name] // self.group_scale * self.group_scale\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        X[self.new_attribute] = self.age_groups\n",
                "        if self.del_originals:\n",
                "            X.drop(columns=self.attribute_name, axis=1, inplace=True)\n",
                "        return X\n",
                "\n",
                "\n",
                "class AtributesAdder(BaseEstimator, TransformerMixin):\n",
                "    def __init__(self, new_attribute=\"RelativesOnboard\", attribute_names=[\"SibSp\", \"Parch\"], del_originals=True):\n",
                "        self.attribute_names = attribute_names\n",
                "        self.final_attr = 0\n",
                "        self.new_attribute = new_attribute\n",
                "        self.del_originals = del_originals\n",
                "\n",
                "    def fit(self, X, y=None):\n",
                "        for attr in self.attribute_names:\n",
                "            self.final_attr += X[attr]\n",
                "        return self\n",
                "\n",
                "    def transform(self, X, y=None):\n",
                "        X[self.new_attribute] = self.final_attr\n",
                "        if self.del_originals:\n",
                "            X.drop(columns=self.attribute_names, axis=1, inplace=True)\n",
                "        return X"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "(\"select_numeric\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])),",
                "(\"age_grouper\", AgeGrouper(attribute_name=\"Age\", group_scale=15)),",
                "(\"total_relatives\", AtributesAdder(attribute_names=[\"SibSp\", \"Parch\"], del_originals=True)),",
                "(\"imputer\", SimpleImputer(strategy=\"median\")),",
                "])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Numerical Pipeline\n",
                "num_pipeline = Pipeline([\n",
                "        (\"select_numeric\", DataFrameSelector([\"Age\", \"Fare\", \"SibSp\", \"Parch\"])),\n",
                "        (\"age_grouper\", AgeGrouper(attribute_name=\"Age\", group_scale=15)),\n",
                "        (\"total_relatives\", AtributesAdder(attribute_names=[\"SibSp\", \"Parch\"], del_originals=True)),\n",
                "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
                "    ])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "(\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),",
                "(\"imputer\", MostFrequentImputer()),",
                "(\"cat_encoder\", OneHotEncoder(sparse=False)),",
                "])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Categorical Pipeline\n",
                "cat_pipeline = Pipeline([\n",
                "        (\"select_cat\", DataFrameSelector([\"Pclass\", \"Sex\", \"Embarked\"])),\n",
                "        (\"imputer\", MostFrequentImputer()),\n",
                "        (\"cat_encoder\", OneHotEncoder(sparse=False)),\n",
                "    ])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = FeatureUnion(transformer_list=[",
                "(\"num_pipeline\", num_pipeline),",
                "(\"cat_pipeline\", cat_pipeline),",
                "])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.pipeline import FeatureUnion\n",
                "preprocess_pipeline = FeatureUnion(transformer_list=[\n",
                "        (\"num_pipeline\", num_pipeline),\n",
                "        (\"cat_pipeline\", cat_pipeline),\n",
                "    ])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = preprocess_pipeline.fit_transform(train)",
                "ASSIGN = train[\"Survived\"]",
                "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(",
                "ASSIGN=0.3, random_state=42)"
            ],
            "output_type": "stream",
            "content_old": [
                "X_train = preprocess_pipeline.fit_transform(train)\n",
                "y_train = train[\"Survived\"]\n",
                "\n",
                "X_train_val, X_test_val, y_train_val, y_test_val = train_test_split(\n",
                "        X_train, y_train, test_size=0.3, random_state=42)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.zeros_like(corr_matrix)",
                "ASSIGN[np.triu_indices_from(ASSIGN)] = True",
                "with sns.axes_style(\"white\"):",
                "ASSIGN = plt.subplots(figsize=(7, 5))",
                "ASSIGN = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True, cmap=\"YlGnBu\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "mask = np.zeros_like(corr_matrix)\n",
                "mask[np.triu_indices_from(mask)] = True\n",
                "with sns.axes_style(\"white\"):\n",
                "    f, ax = plt.subplots(figsize=(7, 5))\n",
                "    ax = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True, cmap=\"YlGnBu\")"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = {",
                "\"KNeighborsClassifier\": KNeighborsClassifier(),",
                "\"RandomForest\": RandomForestClassifier(),",
                "\"SVM\": SVC(),",
                "}",
                "ASSIGN = {",
                "\"KNeighborsClassifier\": {",
                "\"n_neighbors\": randint(low=1, high=30),",
                "},",
                "\"RandomForest\": {",
                "\"n_estimators\": randint(low=1, high=200),",
                "\"max_features\": randint(low=1, high=8),",
                "},",
                "\"SVM\": {",
                "\"kernel\": [\"linear\", \"rbf\"],",
                "\"C\": reciprocal(0.1, 200000),",
                "\"gamma\": expon(scale=1.0),",
                "}",
                "}"
            ],
            "output_type": "not_existent",
            "content_old": [
                "models = {\n",
                "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
                "    \"RandomForest\": RandomForestClassifier(),\n",
                "    \"SVM\": SVC(),\n",
                "}\n",
                "\n",
                "randomized_params = {\n",
                "    \"KNeighborsClassifier\": {\n",
                "        \"n_neighbors\": randint(low=1, high=30),\n",
                "    },\n",
                "    \"RandomForest\": {\n",
                "        \"n_estimators\": randint(low=1, high=200),\n",
                "        \"max_features\": randint(low=1, high=8),\n",
                "    },\n",
                "    \"SVM\": {\n",
                "        \"kernel\": [\"linear\", \"rbf\"],\n",
                "        \"C\": reciprocal(0.1, 200000),\n",
                "        \"gamma\": expon(scale=1.0),\n",
                "    }\n",
                "}"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = \"accuracy\"",
                "for model_name in models.keys():",
                "ASSIGN = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100,",
                "ASSIGN=ASSIGN, cv=5, verbose=2, random_state=42, n_jobs=-1)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10,",
                "ASSIGN=ASSIGN, verbose=0, n_jobs=-1)",
                "ASSIGN = scores.mean()",
                "ASSIGN = scores.std()",
                "ASSIGN = grid.score(X_test_val, y_test_val)",
                "ASSIGN = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores,",
                "'CV Mean': ASSIGN, 'CV STDEV': ASSIGN}",
                "ASSIGN = grid.best_estimator_.fit(X_train_val, y_train_val)",
                "ASSIGN.score(X_test_val, y_test_val)",
                "ASSIGN = clf.predict(X_test_val)",
                "ASSIGN = classification_report(y_test_val, y_pred)",
                "save(ASSIGN, ASSIGN, ASSIGN, name=\"titanic_\"+model_name+\"_02\", cv_scores=ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "scoring = \"accuracy\"\n",
                "\n",
                "\n",
                "for model_name in models.keys():\n",
                "    grid = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100,\n",
                "                                  scoring=scoring, cv=5, verbose=2, random_state=42,  n_jobs=-1)\n",
                "    grid.fit(X_train, y_train)\n",
                "\n",
                "    scores = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10,\n",
                "                             scoring=scoring, verbose=0, n_jobs=-1)\n",
                "\n",
                "    CV_scores = scores.mean()\n",
                "    STDev = scores.std()\n",
                "    Test_scores = grid.score(X_test_val, y_test_val)\n",
                "\n",
                "    cv_score = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores,\n",
                "                'CV Mean': CV_scores, 'CV STDEV': STDev}\n",
                "\n",
                "    clf = grid.best_estimator_.fit(X_train_val, y_train_val)\n",
                "    clf.score(X_test_val, y_test_val)\n",
                "    y_pred = clf.predict(X_test_val)\n",
                "    clf_report = classification_report(y_test_val, y_pred)\n",
                "    save(grid, cv_score, clf_report, name=\"titanic_\"+model_name+\"_02\", cv_scores=scores)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = load(\"titanic_KNeighborsClassifier_02\", with_metadata=True)"
            ],
            "output_type": "stream",
            "content_old": [
                "knn_grid = load(\"titanic_KNeighborsClassifier_02\", with_metadata=True)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = load(\"titanic_SVM_02\", with_metadata=True)"
            ],
            "output_type": "stream",
            "content_old": [
                "svc_grid = load(\"titanic_SVM_02\", with_metadata=True)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = load(\"titanic_RandomForest_02\", with_metadata=True)"
            ],
            "output_type": "stream",
            "content_old": [
                "random_forest_grid = load(\"titanic_RandomForest_02\", with_metadata=True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(8, 4))",
                "plt.plot([1]*10, knn_grid[\"cv_scores\"], \".\")",
                "plt.plot([2]*10, svc_grid[\"cv_scores\"], \".\")",
                "plt.plot([3]*10, random_forest_grid[\"cv_scores\"], \".\")",
                "plt.boxplot([knn_grid[\"cv_scores\"], svc_grid[\"cv_scores\"], random_forest_grid[\"cv_scores\"]], labels=(\"KNN\", \"SVM\", \"Random Forest\"))",
                "plt.ylabel(\"Accuracy\", fontsize=14)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(8, 4))\n",
                "plt.plot([1]*10, knn_grid[\"cv_scores\"], \".\")\n",
                "plt.plot([2]*10, svc_grid[\"cv_scores\"], \".\")\n",
                "plt.plot([3]*10, random_forest_grid[\"cv_scores\"], \".\")\n",
                "plt.boxplot([knn_grid[\"cv_scores\"], svc_grid[\"cv_scores\"], random_forest_grid[\"cv_scores\"]], labels=(\"KNN\", \"SVM\", \"Random Forest\"))\n",
                "plt.ylabel(\"Accuracy\", fontsize=14)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = random_forest_grid[\"model\"].best_estimator_.fit(X_train, y_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "random_forest = random_forest_grid[\"model\"].best_estimator_.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = preprocess_pipeline.fit_transform(test)"
            ],
            "output_type": "stream",
            "content_old": [
                "X_test = preprocess_pipeline.fit_transform(test)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = random_forest.predict(X_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_pred = random_forest.predict(X_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(columns=['PassengerId', 'Survived'])",
                "ASSIGN['PassengerId'] = test['PassengerId']",
                "ASSIGN['Survived'] = y_pred"
            ],
            "output_type": "not_existent",
            "content_old": [
                "submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\n",
                "submission_df['PassengerId'] = test['PassengerId']\n",
                "submission_df['Survived'] = y_pred"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "submission_df.to_csv(\"path\", header=True, index=False)",
                "submission_df.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "submission_df.to_csv(\"/kaggle/working/titanic_02.csv\", header=True, index=False)\n",
                "submission_df.head(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = test_final"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.impute import SimpleImputer",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from sklearn.ensemble import RandomForestRegressor",
                "from sklearn.metrics import mean_squared_error",
                "from sklearn.model_selection import train_test_split",
                "import math",
                "import os",
                "print(os.listdir(\"../input\"))",
                "df = pd.read_csv(\"../input/train.csv\")",
                "test_final = pd.read_csv(\"../input/test.csv\")",
                "test_final_id = test_final"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def Filterdataset (dataset):",
                "ASSIGN = ASSIGN.copy()",
                "ASSIGN['has_alley'] = df['Alley'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "ASSIGN = ASSIGN.fillna(value= {'Alley':'No alley access'})",
                "ASSIGN['has_BsmtQual'] = df['BsmtQual'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "ASSIGN = ASSIGN.fillna(value= {'BsmtQual':'No Basement'})",
                "ASSIGN['has_BsmtCond'] = df['BsmtCond'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "ASSIGN = ASSIGN.fillna(value= {'BsmtCond':'No Basement'})",
                "ASSIGN = ASSIGN - ASSIGN",
                "ASSIGN = ASSIGN - ASSIGN",
                "ASSIGN = (ASSIGN + ASSIGN + ASSIGN + ASSIGN).astype('float32')",
                "ASSIGN = SimpleImputer(missing_values=np.nan, strategy='most_frequent')",
                "ASSIGN = pd.get_dummies(ASSIGN, drop_first=True)",
                "ASSIGN = dataset",
                "ASSIGN = imp_mean.fit_transform(ASSIGN)",
                "ASSIGN = pd.DataFrame(data = ASSIGN, index = datasetc.index, columns = datasetc.columns)",
                "if 'Id' in ASSIGN.columns:",
                "ASSIGN = ASSIGN.drop(['Id'], axis=1)",
                "if 'SalePrice' in ASSIGN.columns:",
                "ASSIGN = ASSIGN.drop(['SalePrice'], axis=1)",
                "return dataset"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def Filterdataset (dataset):    ",
                "    dataset = dataset.copy()",
                "    ",
                "    dataset['has_alley'] = df['Alley'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'Alley':'No alley access'})",
                "    dataset['has_BsmtQual'] = df['BsmtQual'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'BsmtQual':'No Basement'})",
                "    dataset['has_BsmtCond'] = df['BsmtCond'].fillna(0).apply(lambda _: 0 if _ == 0 else 1)",
                "    dataset = dataset.fillna(value= {'BsmtCond':'No Basement'})",
                "    dataset['Age'] = dataset['YrSold'] - dataset['YearBuilt']",
                "    dataset['AgeSinceRemode'] = dataset['YrSold'] - dataset['YearRemodAdd']",
                "    dataset['WholeArea'] = (dataset['GrLivArea'] + dataset['GarageArea'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']).astype('float32')",
                "    #dataset = dataset.select_dtypes(include=['float64','int'])",
                "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='most_frequent')",
                "    dataset = pd.get_dummies(dataset, drop_first=True)",
                "    datasetc = dataset",
                "    dataset = imp_mean.fit_transform(dataset)",
                "    dataset = pd.DataFrame(data = dataset, index = datasetc.index, columns = datasetc.columns)",
                "    if 'Id' in dataset.columns:",
                "        dataset = dataset.drop(['Id'], axis=1)",
                "    if 'SalePrice' in dataset.columns:",
                "        dataset = dataset.drop(['SalePrice'], axis=1)",
                "    return dataset"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df['SalePrice']",
                "ASSIGN = df.drop('SalePrice',axis=1)",
                "X_train, X_test, y_train, y_test = train_test_split( ASSIGN, ASSIGN, test_size=0.10, random_state=0)",
                "ASSIGN = Filterdataset(ASSIGN)",
                "ASSIGN = Filterdataset(ASSIGN)",
                "ASSIGN = Filterdataset(ASSIGN)",
                "ASSIGN = []",
                "for c in ASSIGN.ASSIGN:",
                "if c in ASSIGN.ASSIGN:",
                "if c in ASSIGN.ASSIGN:",
                "ASSIGN.append(c)",
                "ASSIGN = ASSIGN[columns]",
                "ASSIGN = ASSIGN[columns]",
                "ASSIGN = ASSIGN[columns]",
                "ASSIGN = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100)",
                "ASSIGN.fit(ASSIGN, np.log(y_train))",
                "print(len(ASSIGN.ASSIGN), len(ASSIGN.ASSIGN), len(ASSIGN.ASSIGN))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y = df['SalePrice']",
                "X = df.drop('SalePrice',axis=1)",
                "",
                "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.10, random_state=0)",
                "",
                "X_train = Filterdataset(X_train)",
                "X_test = Filterdataset(X_test)",
                "test_final = Filterdataset(test_final)",
                "columns = []",
                "for c in X_train.columns:",
                "    if c in X_test.columns:",
                "        if c in test_final.columns:",
                "            columns.append(c)",
                "X_train = X_train[columns]",
                "test_final = test_final[columns]",
                "X_test = X_test[columns]",
                "regr = RandomForestRegressor(max_depth=50, random_state=0, n_estimators=100)",
                "regr.fit(X_train, np.log(y_train))",
                "print(len(X_train.columns), len(X_test.columns), len(test_final.columns))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = regr.predict(X_train)",
                "ASSIGN = regr.predict(X_test)",
                "ASSIGN = np.exp(regr.predict(test_final))",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_pred_train = regr.predict(X_train)",
                "y_pred_test = regr.predict(X_test)",
                "y_pred_final = np.exp(regr.predict(test_final))",
                "print(y_pred_final)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "def Mrmse(y_true,y_pred):",
                "ASSIGN = np.log(ASSIGN)",
                "ASSIGN = math.sqrt(mean_squared_error(y_true, y_pred))",
                "return rmse"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def Mrmse(y_true,y_pred):",
                "    y_true = np.log(y_true)",
                "    #y_pred = np.log(y_pred)",
                "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))",
                "    return rmse"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(Mrmse(y_train,y_pred_train))",
                "print(Mrmse(y_test,y_pred_test))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(Mrmse(y_train,y_pred_train))",
                "print(Mrmse(y_test,y_pred_test))"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final})",
                "ASSIGN.to_csv('submission.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "my_submission = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final})",
                "# you could use any filename. We choose submission here",
                "my_submission.to_csv('submission.csv', index=False)",
                "#my_submission"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df = pd.read_csv('../input/attendancemarks/AttendanceMarksSA.csv')\n",
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df['MSE']",
                "ASSIGN = df['ESE']",
                "sns.scatterplot(ASSIGN,ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "x = df['MSE']\n",
                "y = df['ESE']\n",
                "sns.scatterplot(x,y)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0.01",
                "ASSIGN = 10000",
                "ASSIGN = float(len(x))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "b0 = 0\n",
                "b1 = 0\n",
                "alpha = 0.01\n",
                "count = 10000\n",
                "n = float(len(x))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "for i in range(count):",
                "ASSIGN = b1*x + b0",
                "ASSIGN = ASSIGN - (alphapath)*sum(x*(y_bar-y))",
                "ASSIGN = ASSIGN - (alphapath)*sum(y_bar-y)",
                "print(ASSIGN,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "for i in range(count):\n",
                "    y_bar = b1*x + b0\n",
                "    b1 = b1 - (alpha/n)*sum(x*(y_bar-y))\n",
                "    b0 = b0 - (alpha/n)*sum(y_bar-y)\n",
                "        \n",
                "print(b0,b1)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = b1*x + b0",
                "plt.scatter(x,y)",
                "plt.plot([min(x),max(x)],[min(ASSIGN),max(ASSIGN)],color='red')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "y_bar = b1*x + b0\n",
                "\n",
                "plt.scatter(x,y)\n",
                "plt.plot([min(x),max(x)],[min(y_bar),max(y_bar)],color='red') #regression line\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "VALIDATION",
                "def RSE(y_true,y_predict):",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = math.sqrt(RSSpath(len(y_true)-2))",
                "return rse",
                "ASSIGN = RSE(df['ESE'],y_bar)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "import math\n",
                "def RSE(y_true,y_predict):\n",
                "    y_true = np.array(y_true)\n",
                "    y_predict = np.array(y_predict)\n",
                "    RSS = np.sum(np.square(y_true-y_predict))\n",
                "    \n",
                "    rse = math.sqrt(RSS/(len(y_true)-2))\n",
                "    return rse\n",
                "\n",
                "rse = RSE(df['ESE'],y_bar)\n",
                "print(rse)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.linear_model import LinearRegression"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.array(df['MSE']).reshape(-1,1)",
                "ASSIGN = np.array(df['ESE']).reshape(-1,1)",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(ASSIGN,ASSIGN)",
                "print(ASSIGN.coef_)",
                "print(ASSIGN.intercept_)",
                "ASSIGN = lr.predict(X)",
                "ASSIGN = RSE(Y,yp)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "X = np.array(df['MSE']).reshape(-1,1)\n",
                "Y = np.array(df['ESE']).reshape(-1,1)\n",
                "\n",
                "lr = LinearRegression()\n",
                "lr.fit(X,Y)\n",
                "\n",
                "print(lr.coef_)\n",
                "print(lr.intercept_)\n",
                "\n",
                "yp = lr.predict(X)\n",
                "rse = RSE(Y,yp)\n",
                "\n",
                "print(rse)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline\n",
                "import seaborn as sns\n",
                "import time"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "DATA_DIR = \"../input\"\n",
                "\n",
                "ID_COLUMN = 'Id'\n",
                "TARGET_COLUMN = 'Response'\n",
                "\n",
                "SEED = 0\n",
                "CHUNKSIZE = 10000\n",
                "NROWS = 50000\n",
                "\n",
                "TRAIN_NUMERIC = \"{0}/train_numeric.csv\".format(DATA_DIR)\n",
                "TRAIN_DATE = \"{0}/train_date.csv\".format(DATA_DIR)\n",
                "\n",
                "TEST_NUMERIC = \"{0}/test_numeric.csv\".format(DATA_DIR)\n",
                "TEST_DATE = \"{0}/test_date.csv\".format(DATA_DIR)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)",
                "ASSIGN = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)",
                "ASSIGN = -1",
                "ASSIGN = -1",
                "ASSIGN = -1",
                "ASSIGN = -1"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "\n",
                "train = pd.read_csv(TRAIN_NUMERIC, usecols=[ID_COLUMN, TARGET_COLUMN], nrows=NROWS)\n",
                "test = pd.read_csv(TEST_NUMERIC, usecols=[ID_COLUMN], nrows=NROWS)\n",
                "\n",
                "train[\"StartTime\"] = -1\n",
                "test[\"StartTime\"] = -1\n",
                "train[\"EndTime\"] = -1\n",
                "test[\"EndTime\"] = -1"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 0",
                "print ('ASSIGN',)",
                "ASSIGN = pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE, iterator=True)",
                "ASSIGN = pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE, iterator=True)",
                "for i in range(int(NROWSpath) + 1):",
                "ASSIGN = train_reader.get_chunk()",
                "ASSIGN = test_reader.get_chunk()",
                "ASSIGN = np.setdiff1d(tr.columns, [ID_COLUMN])",
                "ASSIGN = tr[feats].min(axis=1).values",
                "ASSIGN = te[feats].min(axis=1).values",
                "ASSIGN = tr[feats].max(axis=1).values",
                "ASSIGN = te[feats].max(axis=1).values",
                "train.loc[train.Id.isin(ASSIGN.Id), 'StartTime'] = ASSIGN",
                "test.loc[test.Id.isin(ASSIGN.Id), 'StartTime'] = ASSIGN",
                "train.loc[train.Id.isin(ASSIGN.Id), 'EndTime'] = ASSIGN",
                "test.loc[test.Id.isin(ASSIGN.Id), 'EndTime'] = ASSIGN",
                "ASSIGN += CHUNKSIZE",
                "print (ASSIGN,)",
                "if ASSIGN >= NROWS:",
                "break"
            ],
            "output_type": "stream",
            "content_old": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "\n",
                "nrows = 0\n",
                "print ('nrows',)\n",
                "\n",
                "train_reader = pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE, iterator=True)\n",
                "test_reader = pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE, iterator=True)\n",
                "for i in range(int(NROWS/CHUNKSIZE) + 1):\n",
                "    tr = train_reader.get_chunk()\n",
                "    te = test_reader.get_chunk()\n",
                "\n",
                "#for tr, te in zip(pd.read_csv(TRAIN_DATE, chunksize=CHUNKSIZE), pd.read_csv(TEST_DATE, chunksize=CHUNKSIZE)):\n",
                "    feats = np.setdiff1d(tr.columns, [ID_COLUMN])    \n",
                "\n",
                "    stime_tr = tr[feats].min(axis=1).values\n",
                "    stime_te = te[feats].min(axis=1).values\n",
                "\n",
                "    etime_tr = tr[feats].max(axis=1).values\n",
                "    etime_te = te[feats].max(axis=1).values\n",
                "    \n",
                "    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n",
                "    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n",
                "\n",
                "    train.loc[train.Id.isin(tr.Id), 'EndTime'] = etime_tr\n",
                "    test.loc[test.Id.isin(te.Id), 'EndTime'] = etime_te\n",
                "    \n",
                "    nrows += CHUNKSIZE\n",
                "    print (nrows,)\n",
                "    if nrows >= NROWS:\n",
                "        break"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train.shape[0]",
                "ASSIGN = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)",
                "ASSIGN['Duration'] = ASSIGN['EndTime'] - ASSIGN['StartTime']",
                "ASSIGN['magic1'] = ASSIGN[ID_COLUMN].diff().fillna(9999999).astype(int)",
                "ASSIGN['magic2'] = ASSIGN[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)",
                "ASSIGN = ASSIGN.sort_values(by=['StartTime', 'Id'], ascending=True)",
                "ASSIGN['magic3'] = ASSIGN[ID_COLUMN].diff().fillna(9999999).astype(int)",
                "ASSIGN['magic4'] = ASSIGN[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)",
                "ASSIGN = ASSIGN.sort_values(by=['index']).drop(['index'], axis=1)",
                "ASSIGN = train_test.iloc[:ntrain, :]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Borrowed from Faron's Road to 4 kernel\n",
                "#HAD to change the names so they are easy to type. what can I say \\_()_/\n",
                "\n",
                "ntrain = train.shape[0]\n",
                "train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n",
                "\n",
                "train_test['Duration'] = train_test['EndTime'] - train_test['StartTime']\n",
                "\n",
                "train_test['magic1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
                "train_test['magic2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
                "\n",
                "train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n",
                "\n",
                "train_test['magic3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
                "train_test['magic4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
                "\n",
                "train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\n",
                "train = train_test.iloc[:ntrain, :]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def twoplot(df, col, xaxis=None):",
                "''' scatter plot a feature split into response values as two subgraphs '''",
                "if col not in df.columns.values:",
                "print('ERROR: %s not a column' % col)",
                "ASSIGN = pd.DataFrame(index = df.index)",
                "ASSIGN = ASSIGN",
                "ASSIGN = ASSIGN if xaxis else df.index",
                "ASSIGN = ASSIGN",
                "ASSIGN = sns.FacetGrid(ndf, col=\"Response\", hue=\"Response\")",
                "ASSIGN.map(plt.scatter, xaxis, col, alpha=.7, s=1)",
                "ASSIGN.add_legend();",
                "del ndf"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def twoplot(df, col, xaxis=None):\n",
                "    ''' scatter plot a feature split into response values as two subgraphs '''\n",
                "    if col not in df.columns.values:\n",
                "        print('ERROR: %s not a column' % col)\n",
                "    ndf = pd.DataFrame(index = df.index)\n",
                "    ndf[col] = df[col]\n",
                "    ndf[xaxis] = df[xaxis] if xaxis else df.index\n",
                "    ndf['Response'] = df['Response']\n",
                "    \n",
                "    g = sns.FacetGrid(ndf, col=\"Response\", hue=\"Response\")\n",
                "    g.map(plt.scatter, xaxis, col, alpha=.7, s=1)\n",
                "    g.add_legend();\n",
                "    \n",
                "    del ndf"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic1')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'magic1')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic2')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'magic2')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic3')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'magic3')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'magic4')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'magic4')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'Duration')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'Duration')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'StartTime')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'StartTime')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "twoplot(train, 'EndTime')"
            ],
            "output_type": "display_data",
            "content_old": [
                "twoplot(train, 'EndTime')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!pip install autokeras\n",
                "!pip install natsort"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "import autokeras as ak\n",
                "import numpy as np \n",
                "import pandas as pd \n",
                "from glob import glob\n",
                "from skimage.io import imread\n",
                "import skimage.io as sio\n",
                "import os\n",
                "from natsort import natsorted\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
                "from skimage.transform import resize, rotate\n",
                "import warnings; warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = \"..path\"",
                "ASSIGN = root_dir + \"path\"",
                "ASSIGN = root_dir + \"path\"",
                "ASSIGN = root_dir + \"path\"",
                "ASSIGN = root_dir + \"sample_submission.csv\"",
                "ASSIGN  = pd.read_csv(csv_path)",
                "ASSIGN  = np.array([ imread(train_dir+p)path])",
                "ASSIGN  = df.has_cactus.values"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# train test directories\n",
                "root_dir = \"../input\"\n",
                "train_dir = root_dir + \"/train/train/\"\n",
                "test_dir  = root_dir + \"/test/test/\"\n",
                "csv_path  = root_dir + \"/train.csv\"\n",
                "sub_path  = root_dir + \"sample_submission.csv\"\n",
                "\n",
                "# loading images\n",
                "df   = pd.read_csv(csv_path)\n",
                "x    = np.array([ imread(train_dir+p)/255 for p in df.id.values])\n",
                "y    = df.has_cactus.values"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = train_test_split(x, y, test_size=0.20,stratify=y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# splitting training dataset into train/validation\n",
                "from sklearn.model_selection import train_test_split\n",
                "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.20,stratify=y)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "def display_images(imgs,y=None, y_pred=None):",
                "ASSIGN = imgs.shape[0]",
                "ASSIGN = 5",
                "ASSIGN = n_imagespath",
                "ASSIGN = 1",
                "plt.figure(figsize=(10,6),frameon=False)",
                "for i in range(ASSIGN):",
                "for j in range(ASSIGN):",
                "plt.subplot(ASSIGN, ASSIGN, ASSIGN)",
                "plt.imshow(imgs[ASSIGN-1])",
                "plt.axis(\"off\")",
                "if (y is not None) and (y_pred is not None):",
                "plt.title(\"y=%d | pred=%0.1f\"%(y[ASSIGN-1],y_pred[ASSIGN-1]))",
                "elif y is not None:",
                "plt.title(\"y=%d\"%y[ASSIGN-1])",
                "ASSIGN+=1",
                "plt.tight_layout()",
                "plt.show()",
                "def getProb(model, x):",
                "ASSIGN = model.preprocess(x)",
                "ASSIGN = model.data_transformer.transform_test(xprocessed)",
                "ASSIGN = model.cnn.predict(loader)",
                "ASSIGN  = np.exp(probs[:,1])",
                "ASSIGN = num + np.exp(probs[:,0])",
                "ASSIGN = num path",
                "return probs"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# helper functions here\n",
                "def display_images(imgs,y=None, y_pred=None):\n",
                "    n_images = imgs.shape[0]\n",
                "    n_gridx  = 5\n",
                "    n_gridy  = n_images//n_gridx\n",
                "#     n_grid   = int(np.sqrt(n_images))\n",
                "    k = 1\n",
                "    plt.figure(figsize=(10,6),frameon=False)\n",
                "    for i in range(n_gridy):\n",
                "        for j in range(n_gridx):\n",
                "            plt.subplot(n_gridy, n_gridx, k)\n",
                "            plt.imshow(imgs[k-1])\n",
                "            plt.axis(\"off\")\n",
                "            if (y is not None) and (y_pred is not None):\n",
                "                plt.title(\"y=%d | pred=%0.1f\"%(y[k-1],y_pred[k-1]))\n",
                "            elif y is not None:\n",
                "                plt.title(\"y=%d\"%y[k-1])\n",
                "            k+=1\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "\n",
                "def getProb(model, x):\n",
                "    xprocessed = model.preprocess(x)\n",
                "    loader = model.data_transformer.transform_test(xprocessed)\n",
                "    probs  = model.cnn.predict(loader)\n",
                "    num    = np.exp(probs[:,1])\n",
                "    denom  = num + np.exp(probs[:,0])\n",
                "    probs  = num / denom \n",
                "    return probs"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = 20",
                "ASSIGN = np.random.randint(0,len(x_train),n_samples)",
                "display_images(x_train[ASSIGN], y_train[ASSIGN])"
            ],
            "output_type": "display_data",
            "content_old": [
                "n_samples  = 20\n",
                "idx_sample = np.random.randint(0,len(x_train),n_samples)\n",
                "display_images(x_train[idx_sample], y_train[idx_sample])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = 5",
                "ASSIGN = ak.ImageClassifier(verbose=True, augment=True )",
                "ASSIGN.fit(x_train, y_train, time_limit=4*60*60)"
            ],
            "output_type": "stream",
            "content_old": [
                "runFor = 5 # time in hours\n",
                "model = ak.ImageClassifier(verbose=True, augment=True )\n",
                "model.fit(x_train, y_train, time_limit=4*60*60)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = model.predict(x_train)",
                "ASSIGN = getProb(model, x_train)",
                "print(, accuracy_score(y_train, ASSIGN))",
                "print(, recall_score(y_train, ASSIGN))",
                "print(, precision_score(y_train, ASSIGN))",
                "print(, roc_auc_score(y_train, ASSIGN))",
                "print(, f1_score(y_train, ASSIGN))",
                "ASSIGN = model.predict(x_val)",
                "ASSIGN = getProb(model, x_val)",
                "print(, accuracy_score(y_val, ASSIGN))",
                "print(, recall_score(y_val, ASSIGN))",
                "print(, precision_score(y_val, ASSIGN))",
                "print(,roc_auc_score(y_val, ASSIGN))",
                "print(, f1_score(y_val, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "# model.final_fit(x_train, y_train, x_val, y_val, retrain=False)\n",
                "y_pred = model.predict(x_train)\n",
                "y_prob = getProb(model, x_train)\n",
                "print(\"training   accuracy  = \", accuracy_score(y_train, y_pred))\n",
                "print(\"training   recall    = \", recall_score(y_train, y_pred))\n",
                "print(\"training   precision = \", precision_score(y_train, y_pred))\n",
                "print(\"training   auc score = \", roc_auc_score(y_train, y_prob))\n",
                "print(\"training   f1 score  = \", f1_score(y_train, y_pred))\n",
                "y_pred = model.predict(x_val)\n",
                "y_prob = getProb(model, x_val)\n",
                "print(\"validation accuracy  = \", accuracy_score(y_val, y_pred))\n",
                "print(\"validation recall    = \", recall_score(y_val, y_pred))\n",
                "print(\"validation precision = \", precision_score(y_val, y_pred))\n",
                "print(\"validation auc score = \",roc_auc_score(y_val, y_prob))\n",
                "print(\"validation f1 score  = \", f1_score(y_val, y_pred))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = np.array([ imread(test_dir+p)path])",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = getProb(model, x_test)",
                "ASSIGN['has_cactus'] = ASSIGN",
                "ASSIGN.to_csv('cactus_net_submission.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_test = pd.read_csv('../input/sample_submission.csv')\n",
                "x_test  = np.array([ imread(test_dir+p)/255 for p in df_test.id.values])\n",
                "x_test  = np.array(x_test)\n",
                "\n",
                "# test prediction\n",
                "y_prob_test = getProb(model, x_test)\n",
                "\n",
                "df_test['has_cactus'] = y_prob_test\n",
                "df_test.to_csv('cactus_net_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"..path\")",
                "print(ASSIGN.isnull().sum())",
                "ASSIGN.dropna(inplace=True)",
                "ASSIGN = ASSIGN[ASSIGN['country']!='World']"
            ],
            "output_type": "stream",
            "content_old": [
                "df = pd.read_csv(\"../input/ecological-footprint/EcologicalFootPrint.csv\")\n",
                "print(df.isnull().sum())\n",
                "df.dropna(inplace=True)\n",
                "df = df[df['country']!='World']"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['record'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['record'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ['crop_land','grazing_land','forest_land','fishing_ground','built_up_land','carbon','total']",
                "for columns in ASSIGN:",
                "plt.figure(figsize=(15,10))",
                "ASSIGN = df.groupby('year')[columns].mean()",
                "sns.barplot(ASSIGN.index,ASSIGN.values).set_xticklabels(sns.barplot(ASSIGN.index,ASSIGN.values).get_xticklabels(),rotation=\"90\")",
                "plt.title(\"Year Comparation with \"+columns)",
                "plt.xlabel(columns)",
                "plt.ylabel(\"Count\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "l = ['crop_land','grazing_land','forest_land','fishing_ground','built_up_land','carbon','total']\n",
                "for columns in l:\n",
                "    plt.figure(figsize=(15,10))\n",
                "    every_year = df.groupby('year')[columns].mean()\n",
                "    sns.barplot(every_year.index,every_year.values).set_xticklabels(sns.barplot(every_year.index,every_year.values).get_xticklabels(),rotation=\"90\")\n",
                "    plt.title(\"Year Comparation with \"+columns)\n",
                "    plt.xlabel(columns)\n",
                "    plt.ylabel(\"Count\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, most Carbon Producing Countring\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, most Carbon Producing Countring\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=False)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, lowest Carbon Producing Countring\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, lowest Carbon Producing Countring\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['carbon'].sort_values(ascending=True)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, Lowest Country with Forst Land\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Lowest Country with Forst Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=True)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, Highest Country with Forst Land\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Highest Country with Forst Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['forest_land'].sort_values(ascending=False)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, Highest Country with Build Up Land\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=False)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Highest Country with Build Up Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=False)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,10))",
                "plt.title(\"Total 10, Lowest Country with Build Up Land\")",
                "plt.xlabel(\"Country\")",
                "plt.ylabel(\"Rank\")",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=True)[:10].plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,10))\n",
                "plt.title(\"Total 10, Lowest Country with Build Up Land\")\n",
                "plt.xlabel(\"Country\")\n",
                "plt.ylabel(\"Rank\")\n",
                "df.groupby(['country']).mean()['built_up_land'].sort_values(ascending=True)[:10].plot.bar()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "from pathlib import Path",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# opening train/test csv files",
                "train = pd.read_csv('../input/train.csv')",
                "test = pd.read_csv('../input/test.csv')",
                "train.head(5)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(f)",
                "print(f)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# 10 for 1 ration",
                "print(f\"Train shape : {train.shape}\")",
                "print(f\"Test shape : {test.shape}\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.nunique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Dataset organization",
                "train.nunique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize = (8, 5))",
                "plt.title('Category Distribuition')",
                "sns.distplot(train['landmark_id'])",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize = (8, 5))",
                "plt.title('Category Distribuition')",
                "sns.distplot(train['landmark_id'])",
                "",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(train['landmark_id'].value_counts().head(7))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Top categories",
                "print(train['landmark_id'].value_counts().head(7))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(f)",
                "print(f)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(f\"Median number : {train['landmark_id'].value_counts().median()}\")",
                "print(f\"Mean number : {train['landmark_id'].value_counts().mean()}\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train['landmark_id'].value_counts().describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# More exhaustive description",
                "train['landmark_id'].value_counts().describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "f\"Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}path{len(train['landmark_id'].unique())}\""
            ],
            "output_type": "not_existent",
            "content_old": [
                "f\"Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}/{len(train['landmark_id'].unique())}\""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "def display_category(urls, category_name):",
                "ASSIGN = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"",
                "ASSIGN = ''.join([f\"<img style='{img_style}' src='{u}' path>\" for _, u in urls.head(12).iteritems()])",
                "display(HTML(ASSIGN))",
                "ASSIGN = train['landmark_id'].value_counts().keys()[0]",
                "ASSIGN = train[train['landmark_id'] == category]['url']",
                "display_category(ASSIGN, \"\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from IPython.display import Image",
                "from IPython.core.display import HTML ",
                "",
                "def display_category(urls, category_name):",
                "    img_style = \"width: 180px; margin: 0px; float: left; border: 1px solid black;\"",
                "    images_list = ''.join([f\"<img style='{img_style}' src='{u}' />\" for _, u in urls.head(12).iteritems()])",
                "",
                "    display(HTML(images_list))",
                "",
                "category = train['landmark_id'].value_counts().keys()[0]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")",
                "    ",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = train['landmark_id'].value_counts().keys()[1]",
                "ASSIGN = train[train['landmark_id'] == category]['url']",
                "display_category(ASSIGN, \"\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "category = train['landmark_id'].value_counts().keys()[1]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = train['landmark_id'].value_counts().keys()[2]",
                "ASSIGN = train[train['landmark_id'] == category]['url']",
                "display_category(ASSIGN, \"\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "category = train['landmark_id'].value_counts().keys()[2]",
                "urls = train[train['landmark_id'] == category]['url']",
                "display_category(urls, \"\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "H_T = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "H.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "H.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "H.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H.duplicated()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H.duplicated()]\n",
                "#No duplicates"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H.loc[:,~H.columns.isin(['SalePrice'])].duplicated()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H.loc[:,~H.columns.isin(['SalePrice'])].duplicated()]\n",
                "#No duplicates"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = H.apply(pd.Series.nunique)",
                "ASSIGN[ASSIGN == 1]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Unique_data = H.apply(pd.Series.nunique)\n",
                "Unique_data[Unique_data == 1]\n",
                "#No single unique values in the dataset"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = H.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False)",
                "NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [\"ASSIGN\", \"ASSIGN\"])",
                "NullValues[NullValues.ASSIGN > 0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.drop(['Id','Alley', 'PoolQC', 'Fence','MiscFeature','MiscVal','FireplaceQu','LotFrontage'], axis = 1, inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Deleting columns that have more than 20% missing values and Id column\n",
                "H.drop(['Id','Alley', 'PoolQC', 'Fence','MiscFeature','MiscVal','FireplaceQu','LotFrontage'], axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = H.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False)",
                "NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [\"ASSIGN\", \"ASSIGN\"])",
                "NullValues[NullValues.ASSIGN > 0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]\n",
                "\n",
                "# Missing values exist as there is no garage for these homes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.fillna({'GarageType': 'NoGarage', 'GarageYrBlt': 0, 'GarageFinish': 'NoGarage', 'GarageQual': 'NoGarage','GarageCond': 'NoGarage'} , inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.fillna({'GarageType': 'NoGarage', 'GarageYrBlt': 0, 'GarageFinish': 'NoGarage', 'GarageQual': 'NoGarage','GarageCond': 'NoGarage'} , inplace = True)\n",
                "\n",
                "#Filling appropriate values for nulls "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H[\"GarageArea\"] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "H[H['TotalBsmtSF'] == 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H['TotalBsmtSF'] == 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()\n",
                "\n",
                "# Missing values exist as there is no basement for these homes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[H['TotalBsmtSF'] > 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[H['TotalBsmtSF'] > 0][['BsmtQual', 'BsmtCond','BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']].isnull().sum()\n",
                "\n",
                "# BsmtExposure and BsmtFinType2 have missing values though these homes have a basement"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = H['BsmtExposure'].ASSIGN()[0]",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtExposure'].isnull()), 'BsmtExposure'] = ASSIGN",
                "ASSIGN = H['BsmtFinType2'].ASSIGN()[0]",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtFinType2'].isnull()), 'BsmtFinType2'] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "mode = H['BsmtExposure'].mode()[0]\n",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtExposure'].isnull()), 'BsmtExposure'] = mode\n",
                "\n",
                "mode = H['BsmtFinType2'].mode()[0]\n",
                "H.loc[(H['TotalBsmtSF'] > 0) & (H['BsmtFinType2'].isnull()), 'BsmtFinType2'] = mode\n",
                "\n",
                "#Filling these nulls with mode"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.fillna({'BsmtQual': 'NoBasement', 'BsmtCond': 'NoBasement','BsmtExposure': 'NoBasement', 'BsmtFinType1': 'NoBasement', 'BsmtFinType2': 'NoBasement'} , inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.fillna({'BsmtQual': 'NoBasement', 'BsmtCond': 'NoBasement','BsmtExposure': 'NoBasement', 'BsmtFinType1': 'NoBasement', 'BsmtFinType2': 'NoBasement'} , inplace = True)\n",
                "\n",
                "\n",
                "#Filling appropriate values for other nulls "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = H['Electrical'].ASSIGN()[0]",
                "H['Electrical'].fillna(ASSIGN, inplace = True)",
                "ASSIGN = H['MasVnrType'].ASSIGN()[0]",
                "H['MasVnrType'].fillna(ASSIGN, inplace = True)",
                "ASSIGN = H['MasVnrArea'].ASSIGN()",
                "H['MasVnrArea'].fillna(ASSIGN, inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "mode = H['Electrical'].mode()[0]\n",
                "H['Electrical'].fillna(mode, inplace = True)\n",
                "mode = H['MasVnrType'].mode()[0]\n",
                "H['MasVnrType'].fillna(mode, inplace = True)\n",
                "median = H['MasVnrArea'].median()\n",
                "H['MasVnrArea'].fillna(median, inplace = True)\n",
                "\n",
                "# Filling missing values in MasVnrArea,MasVnrType,Electrical with mode"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = H.isnull().sum().sort_values(ascending = False)",
                "ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False)",
                "NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [\"ASSIGN\", \"ASSIGN\"])",
                "NullValues[NullValues.ASSIGN > 0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Sum = H.isnull().sum().sort_values(ascending = False)\n",
                "Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)\n",
                "NullValues = pd.concat([Sum, Percent], axis = 1, keys = [\"Sum\", \"Percent\"])\n",
                "NullValues[NullValues.Sum > 0]\n",
                "\n",
                "#No null values"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 2011 - ASSIGN",
                "ASSIGN = 2011 - ASSIGN",
                "ASSIGN = 2011 - ASSIGN",
                "ASSIGN = 2011 - ASSIGN",
                "H.loc[H['AgeOfGarage'] > 100 , 'AgeOfGarage'] = 0",
                "H.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis = 1, inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H[\"AgeOfHouse\"] = 2011 - H[\"YearBuilt\"]\n",
                "H[\"AgeOfRemod\"] = 2011 - H[\"YearRemodAdd\"]\n",
                "H['AgeOfSell'] = 2011 - H['YrSold']\n",
                "H['AgeOfGarage'] = 2011 - H['GarageYrBlt']\n",
                "H.loc[H['AgeOfGarage'] > 100 , 'AgeOfGarage'] = 0\n",
                "H.drop(['YearBuilt','YearRemodAdd','YrSold','GarageYrBlt'], axis = 1, inplace = True)\n",
                "\n",
                "#Using age instead of year for better intution and ease"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN + (0.5 * ASSIGN)",
                "ASSIGN = ASSIGN + (0.5 * ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H['BsmtBath'] = H['BsmtFullBath'] + (0.5 * H['BsmtHalfBath'])\n",
                "H['Bath'] = H['FullBath'] + (0.5 * H['HalfBath'])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ASSIGN + ASSIGN + ASSIGN + ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H['TotalPorchArea'] = H['OpenPorchSF'] + H['EnclosedPorch'] + H['3SsnPorch'] + H['ScreenPorch']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['SalePrice','LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']",
                "ASSIGN = ['BsmtBath','Bath','BedroomAbvGr','BldgType','BsmtHalfBath','BsmtFullBath','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Fireplaces','Foundation','FullBath','Functional','GarageCars','GarageFinish','GarageType','HalfBath','Heating','HouseStyle','KitchenAbvGr','LandContour','LandSlope','LotConfig','LotShape','MSSubClass','MSZoning','MasVnrType','MoSold','Neighborhood','PavedDrive','RoofMatl','RoofStyle','SaleCondition','SaleType','Street','TotRmsAbvGrd','Utilities']",
                "ASSIGN = [ \"OverallQual\",\"OverallCond\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",'BsmtCond',\"BsmtExposure\",\"HeatingQC\",\"KitchenQual\",\"GarageQual\",\"GarageCond\", 'BsmtFinType1', 'BsmtFinType2','CentralAir']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "numerical_columns = ['SalePrice','LotArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']\n",
                "categorical_columns = ['BsmtBath','Bath','BedroomAbvGr','BldgType','BsmtHalfBath','BsmtFullBath','Condition1','Condition2','Electrical','Exterior1st','Exterior2nd','Fireplaces','Foundation','FullBath','Functional','GarageCars','GarageFinish','GarageType','HalfBath','Heating','HouseStyle','KitchenAbvGr','LandContour','LandSlope','LotConfig','LotShape','MSSubClass','MSZoning','MasVnrType','MoSold','Neighborhood','PavedDrive','RoofMatl','RoofStyle','SaleCondition','SaleType','Street','TotRmsAbvGrd','Utilities']\n",
                "ordinal_columns = [ \"OverallQual\",\"OverallCond\",\"ExterQual\",\"ExterCond\",\"BsmtQual\",'BsmtCond',\"BsmtExposure\",\"HeatingQC\",\"KitchenQual\",\"GarageQual\",\"GarageCond\", 'BsmtFinType1', 'BsmtFinType2','CentralAir']"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[ordinal_columns]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'NoBasement' : 0})",
                "ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "ASSIGN = ASSIGN.map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})",
                "H['BsmtFinType1'] = H['BsmtFinType1'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})",
                "H['BsmtFinType2'] = H['BsmtFinType2'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})",
                "ASSIGN = ASSIGN.map({'N' : 0, 'Y' : 1})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H['ExterQual'] = H['ExterQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['ExterCond'] = H['ExterCond'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtQual'] = H['BsmtQual'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtCond'] = H['BsmtCond'].map({'NoBasement' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtExposure'] = H['BsmtExposure'].map({'Gd' : 4, 'Av' : 3, 'Mn' : 2, 'No' : 1, 'NoBasement' : 0})\n",
                "H['HeatingQC'] = H['HeatingQC'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['KitchenQual'] = H['KitchenQual'].map({'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['GarageQual'] = H['GarageQual'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['GarageCond'] = H['GarageCond'].map({'NoGarage' : 0, 'NA' : 0, 'Po' : 1, 'Fa': 2, 'TA' : 3, 'Gd': 4 , 'Ex' : 5})\n",
                "H['BsmtFinType1'] = H['BsmtFinType1'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})\n",
                "H['BsmtFinType2'] = H['BsmtFinType2'].map({'GLQ' : 6, 'ALQ' : 5, 'BLQ' : 4, 'Rec' : 3, 'LwQ' : 2, 'Unf' : 1, 'NoBasement' : 0})\n",
                "H['CentralAir'] = H['CentralAir'].map({'N' : 0, 'Y' : 1})"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns].head(5)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[ordinal_columns].head(5)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[ordinal_columns].isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[ordinal_columns].isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "H[categorical_columns].dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[categorical_columns].dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in range(0, len(categorical_columns)):",
                "if (H[categorical_columns[i]].dtype == 'int64') | (H[categorical_columns[i]].dtype == 'float64'):",
                "H[categorical_columns[i]] = H[categorical_columns[i]].apply(str)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for i in range(0, len(categorical_columns)):\n",
                "    if (H[categorical_columns[i]].dtype == 'int64') | (H[categorical_columns[i]].dtype == 'float64'):\n",
                "        H[categorical_columns[i]] = H[categorical_columns[i]].apply(str)\n",
                "        \n",
                "#Changing data type to string/object"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[categorical_columns].head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[categorical_columns].head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BsmtBath\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"BsmtBath\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BsmtBath\"] == '3.0', 'BsmtBath'] = '2.0'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"BsmtBath\"] == '3.0', 'BsmtBath'] = '2.0'\n",
                "\n",
                "#Merging 3 baths to 2 as there is only 1 record"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BedroomAbvGr\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"BedroomAbvGr\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BedroomAbvGr\"] == '8', 'BedroomAbvGr'] = '6'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"BedroomAbvGr\"] == '8', 'BedroomAbvGr'] = '6'\n",
                "#Merging 8 to closer one 6"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"BsmtFullBath\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"BsmtFullBath\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"BsmtFullBath\"] == '3', 'BsmtFullBath'] = '2'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"BsmtFullBath\"] == '3', 'BsmtFullBath'] = '2'\n",
                "#Merging 3 to closer one 2"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Condition1\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Condition1\"].value_counts()\n",
                "# Not merging as they seem to have an importance"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Condition2\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Condition2\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Condition2\"].isin(['PosA','RRAn','RRAe']), 'Condition2'] = 'PosA_RRAn_RRAe'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"Condition2\"].isin(['PosA','RRAn','RRAe']), 'Condition2'] = 'PosA_RRAn_RRAe'\n",
                "#Merging 'PosA','RRAn','RRAe' to one field"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Electrical\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Electrical\"].value_counts()\n",
                "# Not merging as they seem to have some importance"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Exterior1st\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Exterior1st\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Exterior1st\"].isin(['Stone','BrkComm','CBlock','AsphShn','ImStucc']), 'Exterior1st'] = 'Other'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"Exterior1st\"].isin(['Stone','BrkComm','CBlock','AsphShn','ImStucc']), 'Exterior1st'] = 'Other'\n",
                "#Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to other"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Exterior2nd\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Exterior2nd\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Exterior2nd\"].isin(['Stone','Brk Cmn','CBlock','AsphShn','ImStucc','Other']), 'Exterior2nd'] = 'Other'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"Exterior2nd\"].isin(['Stone','Brk Cmn','CBlock','AsphShn','ImStucc','Other']), 'Exterior2nd'] = 'Other'\n",
                "# Renaming 'Stone','BrkComm','CBlock','AsphShn','ImStucc' to Other"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Utilities\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Utilities\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.drop(['Utilities'], axis = 1, inplace = True)",
                "categorical_columns.remove('Utilities')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.drop(['Utilities'], axis = 1, inplace = True)\n",
                "categorical_columns.remove('Utilities')\n",
                "\n",
                "#Droping Utilities as it has 99% data as 1 unique value"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"Heating\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"Heating\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"Heating\"] == 'Floor', 'Heating'] = 'OthW'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"Heating\"] == 'Floor', 'Heating'] = 'OthW'\n",
                "#Merging 'Floor' to 'OthW'"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"RoofMatl\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"RoofMatl\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"RoofMatl\"].isin(['Roll','Membran','Metal','ClyTile']), 'RoofMatl'] = 'Other'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"RoofMatl\"].isin(['Roll','Membran','Metal','ClyTile']), 'RoofMatl'] = 'Other'\n",
                "#Clubbing 'Roll','Membran','Metal','ClyTile' to other"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H[\"TotRmsAbvGrd\"].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H[\"TotRmsAbvGrd\"].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "H.loc[H[\"TotRmsAbvGrd\"] == '2', 'TotRmsAbvGrd'] = '3'",
                "H.loc[H[\"TotRmsAbvGrd\"] == '14', 'TotRmsAbvGrd'] = '12'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "H.loc[H[\"TotRmsAbvGrd\"] == '2', 'TotRmsAbvGrd'] = '3'\n",
                "H.loc[H[\"TotRmsAbvGrd\"] == '14', 'TotRmsAbvGrd'] = '12'\n",
                "#Merging outliers to closer values"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ['SalePrice','LotArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']",
                "plt.figure(figsize=(15,60))",
                "for i in range(0, len(ASSIGN)):",
                "plt.subplot(12,2,(i+1))",
                "sns.distplot(H[ASSIGN[i]])"
            ],
            "output_type": "display_data",
            "content_old": [
                "numerical_columns_1 = ['SalePrice','LotArea','BsmtFinSF1','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea','GarageArea','WoodDeckSF','TotalPorchArea','MasVnrArea','AgeOfGarage', 'AgeOfHouse', 'AgeOfRemod','AgeOfSell']\n",
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(numerical_columns_1)):\n",
                "    plt.subplot(12,2,(i+1))\n",
                "    sns.distplot(H[numerical_columns_1[i]])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "H['SalePrice'], fitted_lambda = stats.boxcox(H['SalePrice'])",
                "H['LotArea'], fitted_lambda = stats.boxcox(H['LotArea'])",
                "H['1stFlrSF'], fitted_lambda = stats.boxcox(H['1stFlrSF'])",
                "H['GrLivArea'], fitted_lambda = stats.boxcox(H['GrLivArea'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from scipy import stats\n",
                "\n",
                "# correcting target variable\n",
                "H['SalePrice'], fitted_lambda = stats.boxcox(H['SalePrice'])\n",
                "\n",
                "#Correcting some normally distributed but skewed numerical data\n",
                "H['LotArea'], fitted_lambda = stats.boxcox(H['LotArea'])\n",
                "H['1stFlrSF'], fitted_lambda = stats.boxcox(H['1stFlrSF'])\n",
                "H['GrLivArea'], fitted_lambda = stats.boxcox(H['GrLivArea'])\n",
                "\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "H['TotalBsmtSF'].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "H['TotalBsmtSF'].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(H['1stFlrSF'])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.distplot(H['1stFlrSF'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(H['GrLivArea'])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.distplot(H['GrLivArea'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,15))",
                "ASSIGN = H[numerical_columns].corr()",
                "sns.heatmap(ASSIGN, annot = True)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(15,15))\n",
                "correlation = H[numerical_columns].corr()\n",
                "sns.heatmap(correlation, annot = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,60))",
                "for i in range(0, len(categorical_columns)):",
                "plt.subplot(20,2,(i+1))",
                "sns.boxplot(data = H, x = categorical_columns[i], y = 'SalePrice'  )"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(categorical_columns)):\n",
                "    plt.subplot(20,2,(i+1))\n",
                "    sns.boxplot(data = H, x = categorical_columns[i], y = 'SalePrice'  )\n",
                "\n",
                "#Plotting all categorical with box plot to ponder and check for any obvious issues with data    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,60))",
                "for i in range(0, len(numerical_columns)):",
                "plt.subplot(12,2,(i+1))",
                "sns.scatterplot(data = H, x = numerical_columns[i], y = 'SalePrice'  )"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,60))\n",
                "for i in range(0, len(numerical_columns)):\n",
                "    plt.subplot(12,2,(i+1))\n",
                "    sns.scatterplot(data = H, x = numerical_columns[i], y = 'SalePrice'  )\n",
                "    \n",
                "#Ploting all numerical data in scatter plots to ponder"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,35))",
                "for i in range(0, len(ordinal_columns)):",
                "plt.subplot(7,2,(i+1))",
                "sns.barplot(data = H, x = ordinal_columns[i], y = 'SalePrice'  )"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,35))\n",
                "for i in range(0, len(ordinal_columns)):\n",
                "    plt.subplot(7,2,(i+1))\n",
                "    sns.barplot(data = H, x = ordinal_columns[i], y = 'SalePrice'  )\n",
                "    \n",
                "#Ploting all ordinal values with Sala price in a bar graph \n",
                "# Sale price doesnt seem to change much with any ordinal data"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "from sklearn import linear_model, metrics\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn.feature_selection import RFE\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = H['SalePrice']",
                "ASSIGN = H.drop(['SalePrice'], axis = 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y = H['SalePrice']\n",
                "\n",
                "X = H.drop(['SalePrice'], axis = 1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "House_Dummies = pd.get_dummies(H[categorical_columns], drop_first = True)",
                "House_Dummies.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "House_Dummies = pd.get_dummies(H[categorical_columns], drop_first = True)\n",
                "House_Dummies.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(categorical_columns)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "len(categorical_columns)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.drop(categorical_columns, axis = 1)",
                "ASSIGN = pd.concat([ASSIGN, House_Dummies], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = X.drop(categorical_columns, axis = 1)\n",
                "X = pd.concat([X, House_Dummies], axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size = 0.7, test_size = 0.3, random_state = 100)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = StandardScaler()",
                "numerical_columns.remove('SalePrice')",
                "X_train[numerical_columns+ordinal_columns] = ASSIGN.fit_transform(X_train[numerical_columns+ordinal_columns])",
                "X_test[numerical_columns+ordinal_columns] = ASSIGN.transform(X_test[numerical_columns+ordinal_columns])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "scaler = StandardScaler()\n",
                "numerical_columns.remove('SalePrice')\n",
                "X_train[numerical_columns+ordinal_columns] = scaler.fit_transform(X_train[numerical_columns+ordinal_columns])\n",
                "X_test[numerical_columns+ordinal_columns] = scaler.transform(X_test[numerical_columns+ordinal_columns])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_train.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_train.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_test.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_test.shape"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = {'alpha': [0.00001,0.00005,0.0001, 0.0005,0.001,0.01, 0.02]}",
                "ASSIGN = Lasso()",
                "ASSIGN = 5",
                "ASSIGN = GridSearchCV(estimator = lasso,",
                "ASSIGN = params,",
                "ASSIGN= 'neg_mean_absolute_error',",
                "ASSIGN = folds,",
                "ASSIGN=True,",
                "ASSIGN = 1)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "stream",
            "content_old": [
                "params = {'alpha': [0.00001,0.00005,0.0001, 0.0005,0.001,0.01, 0.02]}\n",
                "#arams = {'alpha': [0.1, 1,10,100,200,300,500,1000]}\n",
                "\n",
                "\n",
                "lasso = Lasso()\n",
                "\n",
                "\n",
                "folds = 5\n",
                "#Taking 5 folds for Cross validation\n",
                "\n",
                "model_cv = GridSearchCV(estimator = lasso, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "\n",
                "model_cv.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(model_cv.cv_results_)",
                "ASSIGN['param_alpha'] = ASSIGN['param_alpha'].astype('float32')",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_train_score'])",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_test_score'])",
                "plt.xlabel('alpha')",
                "plt.ylabel('Negative Mean Absolute Error')",
                "plt.title(\"Negative Mean Absolute Error and alpha\")",
                "plt.legend(['train score', 'test score'], loc='upper left')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN =0.001",
                "ASSIGN = Lasso(alpha=alpha)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "alpha =0.001\n",
                "lasso = Lasso(alpha=alpha)  \n",
                "lasso.fit(X_train, y_train) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":lasso.coef_})",
                "ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Lasso_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":lasso.coef_})\n",
                "Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "\n",
                "#Feature selection is done by Lasso and features narrowed down to 46"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = lasso.predict(X_test)",
                "ASSIGN = lasso.predict(X_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_test_lasso_predict = lasso.predict(X_test)\n",
                "y_train_lasso_predict = lasso.predict(X_train)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict))",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 100, 1000]}",
                "ASSIGN = Ridge()",
                "ASSIGN = 5",
                "ASSIGN = GridSearchCV(estimator = ridge,",
                "ASSIGN = params,",
                "ASSIGN= 'neg_mean_absolute_error',",
                "ASSIGN = folds,",
                "ASSIGN=True,",
                "ASSIGN = 1)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "params = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 2, 10, 100, 1000]}\n",
                "\n",
                "\n",
                "ridge = Ridge()\n",
                "\n",
                "# cross validation with 5 folds\n",
                "folds = 5\n",
                "model_cv = GridSearchCV(estimator = ridge, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "model_cv.fit(X_train, y_train) "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(model_cv.cv_results_)",
                "ASSIGN['param_alpha'] = ASSIGN['param_alpha'].astype('float32')",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_train_score'])",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_test_score'])",
                "plt.xlabel('alpha')",
                "plt.ylabel('Negative Mean Absolute Error')",
                "plt.title(\"Negative Mean Absolute Error and alpha\")",
                "plt.legend(['train score', 'test score'], loc='upper left')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN =100",
                "ASSIGN = Ridge(alpha=alpha)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "alpha =100\n",
                "ridge = Ridge(alpha=alpha)  \n",
                "ridge.fit(X_train, y_train) "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = ridge.predict(X_test)",
                "ASSIGN = ridge.predict(X_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_test_ridge_predict = ridge.predict(X_test)\n",
                "y_train_ridge_predict = ridge.predict(X_train)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict))",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":ridge.coef_})",
                "ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Ridge_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":ridge.coef_})\n",
                "Ridge_coef[Ridge_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "#No feature selection happened"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = {'alpha': [0,0.0001, 0.0005, 0.001, 0.01]}",
                "ASSIGN = ElasticNet()",
                "ASSIGN = GridSearchCV(estimator = elasticnet,",
                "ASSIGN = params,",
                "ASSIGN= 'neg_mean_absolute_error',",
                "ASSIGN = folds,",
                "ASSIGN=True,",
                "ASSIGN = 1)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "stream",
            "content_old": [
                "params = {'alpha': [0,0.0001, 0.0005, 0.001, 0.01]}\n",
                "\n",
                "elasticnet = ElasticNet()\n",
                "\n",
                "# cross validation\n",
                "model_cv = GridSearchCV(estimator = elasticnet, \n",
                "                        param_grid = params, \n",
                "                        scoring= 'neg_mean_absolute_error', \n",
                "                        cv = folds, \n",
                "                        return_train_score=True,\n",
                "                        verbose = 1)            \n",
                "\n",
                "model_cv.fit(X_train, y_train) "
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(model_cv.cv_results_)",
                "ASSIGN['param_alpha'] = ASSIGN['param_alpha'].astype('float32')",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_train_score'])",
                "plt.plot(ASSIGN['param_alpha'], ASSIGN['mean_test_score'])",
                "plt.xlabel('alpha')",
                "plt.ylabel('Negative Mean Absolute Error')",
                "plt.title(\"Negative Mean Absolute Error and alpha\")",
                "plt.legend(['train score', 'test score'], loc='upper left')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
                "cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n",
                "plt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n",
                "plt.xlabel('alpha')\n",
                "plt.ylabel('Negative Mean Absolute Error')\n",
                "\n",
                "plt.title(\"Negative Mean Absolute Error and alpha\")\n",
                "plt.legend(['train score', 'test score'], loc='upper left')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN =0.001",
                "ASSIGN = ElasticNet(alpha=alpha)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "alpha =0.001\n",
                "elasticnet = ElasticNet(alpha=alpha)  \n",
                "elasticnet.fit(X_train, y_train) "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = elasticnet.predict(X_test)",
                "ASSIGN = elasticnet.predict(X_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_test_elasticnet_predict = elasticnet.predict(X_test)\n",
                "y_train_elasticnet_predict = elasticnet.predict(X_train)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict))",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict))\n",
                "print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":elasticnet.coef_})",
                "ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "elasticnet_coef = pd.DataFrame({\"Feature\":X_train.columns.tolist(),\"Coefficients\":elasticnet.coef_})\n",
                "elasticnet_coef[elasticnet_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).count()\n",
                "#Feature selection happened but not as good as Lasso"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN[ASSIGN['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).reset_index()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Lasso_coef = Lasso_coef[Lasso_coef['Coefficients'] != 0 ].sort_values(by = \"Coefficients\" , ascending = False).reset_index()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Lasso_coef.drop(['index'], axis = 1, inplace = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Lasso_coef.drop(['index'], axis = 1, inplace = True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Lasso_coef.head(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef.tail(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Lasso_coef.tail(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Lasso_coef['Feature'].to_list()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Lasso_coef['Feature'].to_list()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,15))",
                "sns.barplot(x=\"Coefficients\", y=\"Feature\", data=Lasso_coef, palette=\"vlag\")",
                "plt.xlabel(\"Feature Importance\")",
                "plt.tight_layout()"
            ],
            "output_type": "display_data",
            "content_old": [
                "\n",
                "plt.figure(figsize=(15,15))\n",
                "sns.barplot(x=\"Coefficients\", y=\"Feature\", data=Lasso_coef, palette=\"vlag\")\n",
                "plt.xlabel(\"Feature Importance\")\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())",
                "print(os.listdir())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "print(os.listdir(\"../input/A_Z Handwritten Data\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN= \"..path\"",
                "ASSIGN= \"..path\"",
                "ASSIGN=pd.read_csv(data_train_file)",
                "ASSIGN= pd.read_csv(data_test_file)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data_train_file= \"../input/A_Z Handwritten Data/A_Z Handwritten Data.csv\"",
                "data_test_file= \"../input/A_Z Handwritten Data/A_Z Handwritten Data.csv\"",
                "",
                "df_train=pd.read_csv(data_train_file)",
                "df_test= pd.read_csv(data_test_file)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_train.describe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "for i in range(5000,5005):",
                "ASSIGN =np.reshape(df_test[df_test.columns[1:]].iloc[i].valuespath,(28,28))",
                "plt.figure()",
                "plt.title(\"labelled cs {}\".format(df_test[\"0\"].iloc[i]))",
                "plt.imshow(ASSIGN,'gray')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt ",
                "%matplotlib inline",
                "for i in range(5000,5005):",
                "    sample =np.reshape(df_test[df_test.columns[1:]].iloc[i].values/255,(28,28))",
                "    plt.figure()",
                "    plt.title(\"labelled cs {}\".format(df_test[\"0\"].iloc[i]))",
                "    plt.imshow(sample,'gray')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "from scipy.stats import norm",
                "from scipy.stats import skew",
                "from scipy import stats",
                "from sklearn.metrics import mean_squared_error, make_scorer",
                "%matplotlib inline",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('..path', header=0, sep=',')",
                "ASSIGN = ASSIGN.loc[:,'MSSubClass':]",
                "print('Training data columns:', ASSIGN.columns)",
                "print('Training data shape', ASSIGN.shape)",
                "ASSIGN = pd.read_csv('..path', header=0, sep=',')",
                "ASSIGN = ASSIGN.loc[:,'MSSubClass':]",
                "print('Training data columns:', ASSIGN.columns)",
                "print('Test data shape\\n', ASSIGN.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_df = pd.read_csv('../input/train.csv', header=0, sep=',')",
                "train_df = train_df.loc[:,'MSSubClass':]",
                "#print('Training data\\n', train_df.head())",
                "print('Training data columns:', train_df.columns)",
                "print('Training data shape', train_df.shape)",
                "",
                "test_df = pd.read_csv('../input/test.csv', header=0, sep=',')",
                "test_df = test_df.loc[:,'MSSubClass':]",
                "#print('Test data\\n', test_df.head())",
                "print('Training data columns:', test_df.columns)",
                "print('Test data shape\\n', test_df.shape)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Missing data points in every features')",
                "ASSIGN = train_df.isnull().sum().sort_values(ascending=False)",
                "ASSIGN = (train_df.isnull().sum()path().count()).sort_values(ascending=False)",
                "ASSIGN = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])",
                "print(ASSIGN[:20])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Missing data",
                "print('Missing data points in every features')",
                "total = train_df.isnull().sum().sort_values(ascending=False)",
                "percent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)",
                "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])",
                "print(missing_data[:20])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = missing_data[missing_data['Percent'] > 0.0].index",
                "ASSIGN = train_df[missing_data[missing_data['Percent'] <= 0.0].index]",
                "ASSIGN = filr_train_df.dtypes[filr_train_df.dtypes != \"object\"].index # Numerical columns",
                "ASSIGN = filr_train_df.dtypes[filr_train_df.dtypes == \"object\"].index # Categorical columns",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Findout how many of the columns are caregorical or numerical",
                "nan_columns = missing_data[missing_data['Percent'] > 0.0].index",
                "filr_train_df = train_df[missing_data[missing_data['Percent'] <= 0.0].index]",
                "numr_cols = filr_train_df.dtypes[filr_train_df.dtypes != \"object\"].index # Numerical columns",
                "catg_cols = filr_train_df.dtypes[filr_train_df.dtypes == \"object\"].index # Categorical columns",
                "print(catg_cols)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for ind, col in filr_train_df[catg_cols].iteritems():",
                "print(ind, set(list(col)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#dealing with missing data",
                "for ind, col in filr_train_df[catg_cols].iteritems(): # What kind of data in categorical columns",
                "    print(ind, set(list(col)))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print('Number of features:', len(filr_train_df.columns))",
                "ASSIGN = filr_train_df.corr()",
                "ASSIGN = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index",
                "ASSIGN = filr_train_df.loc[:,highly_corr].corr()",
                "print('Number of features with corr:', len(ASSIGN))",
                "plt.figure(figsize=(10,10))",
                "sns.heatmap(ASSIGN, annot=True, square=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Corrilation of numerical features with sales prize",
                "print('Number of features:', len(filr_train_df.columns))",
                "filr_train_df_corr = filr_train_df.corr()",
                "highly_corr = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index # No negative correlation in data",
                "corr_sale = filr_train_df.loc[:,highly_corr].corr()",
                "print('Number of features with corr:', len(corr_sale))",
                "plt.figure(figsize=(10,10))",
                "sns.heatmap(corr_sale, annot=True, square=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna()))",
                "print('Skewness in feature data\\n',ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Now check for skewnwss of numerical features",
                "skewed_feats = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna())) #compute skewness",
                "print('Skewness in feature data\\n',skewed_feats)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "ASSIGN = pd.DataFrame()",
                "for ind, skew in skewed_feats.iteritems():",
                "if (skew > 0.5):",
                "ASSIGN = pd.concat([ASSIGN, np.log1p(filr_train_df[ind])], axis=1)",
                "ASSIGN.append(ind)",
                "else:",
                "ASSIGN = pd.concat([ASSIGN, filr_train_df[ind]], axis=1)",
                "print('Example feature:', ASSIGN[0])",
                "ASSIGN = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]})",
                "ASSIGN.hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correct skewness for features with positive skewness > 0.5 with log1p transformation",
                "LT_columns = []",
                "numLT_train_df = pd.DataFrame()",
                "for ind, skew in skewed_feats.iteritems():",
                "    if (skew > 0.5):",
                "        numLT_train_df = pd.concat([numLT_train_df, np.log1p(filr_train_df[ind])], axis=1)",
                "        LT_columns.append(ind)",
                "    else:",
                "        numLT_train_df = pd.concat([numLT_train_df, filr_train_df[ind]], axis=1)",
                "        ",
                "# Example of skewness",
                "print('Example feature:', LT_columns[0])",
                "skew_ex = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]})",
                "skew_ex.hist()"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "sns.set()",
                "sns.distplot(numLT_train_df[LT_columns[0]], fit=norm);",
                "ASSIGN = plt.figure()",
                "ASSIGN = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Can also look at normal prob plot",
                "#histogram and normal probability plot",
                "sns.set()",
                "sns.distplot(numLT_train_df[LT_columns[0]], fit=norm);",
                "fig = plt.figure()",
                "res = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print(numLT_train_df.shape)",
                "sns.pairplot(numLT_train_df.iloc[:,:5])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(numLT_train_df.shape)",
                "sns.pairplot(numLT_train_df.iloc[:,:5])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "catg_cols",
                "highly_corr",
                "LT_columns"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# What features we want to take",
                "catg_cols # these are our categorical features",
                "highly_corr # numeric features which we have selected based on correlation",
                "LT_columns # numeric features which needs to be transformed"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True)",
                "print(ASSIGN.shape)",
                "print(ASSIGN.head())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Concat test and training for preprocessing",
                "tot_data = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True)",
                "print(tot_data.shape)",
                "print(tot_data.head())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.get_dummies(tot_data[catg_cols])",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Create dummy variable for categorical features",
                "tot_data_cat = pd.get_dummies(tot_data[catg_cols])",
                "tot_data_cat.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = tot_data[highly_corr]",
                "print(ASSIGN.shape)",
                "for cols in LT_columns:",
                "ASSIGN.loc[:,cols] = np.log1p(ASSIGN.loc[:,cols])",
                "print(ASSIGN.shape)",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Log transform numerical data",
                "tot_data_num = tot_data[highly_corr]",
                "print(tot_data_num.shape)",
                "for cols in LT_columns:",
                "    tot_data_num.loc[:,cols] = np.log1p(tot_data_num.loc[:,cols])",
                "print(tot_data_num.shape)",
                "tot_data_num.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.concat([tot_data_cat, tot_data_num],axis=1)",
                "ASSIGN.head()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Combining preprocessed data",
                "tot_data_pro = pd.concat([tot_data_cat, tot_data_num],axis=1)",
                "tot_data_pro.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = tot_data_pro[:1000]",
                "ASSIGN = tot_data_pro[1000:1400]",
                "ASSIGN = ASSIGN.fillna(ASSIGN.mean())",
                "print(ASSIGN.shape, ASSIGN.shape)",
                "ASSIGN = pr_trainData['SalePrice']",
                "X_trainData = ASSIGN.drop('SalePrice', axis=1)",
                "X_testData = ASSIGN.drop('SalePrice', axis=1)",
                "print(X_trainData.shape, X_testData.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Creating matrix for sklern ",
                "pr_trainData = tot_data_pro[:1000]",
                "pr_testData = tot_data_pro[1000:1400]",
                "pr_testData = pr_testData.fillna(pr_testData.mean())",
                "print(pr_trainData.shape, pr_testData.shape)",
                "Y = pr_trainData['SalePrice']",
                "X_trainData = pr_trainData.drop('SalePrice', axis=1)",
                "X_testData = pr_testData.drop('SalePrice', axis=1)",
                "print(X_trainData.shape, X_testData.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = make_scorer(mean_squared_error, greater_is_better = False)",
                "def rmse_cv_train(model):",
                "ASSIGN= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5))",
                "return(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV",
                "from sklearn.model_selection import cross_val_score, train_test_split",
                "",
                "# Define error measure for official scoring : RMSE",
                "scorer = make_scorer(mean_squared_error, greater_is_better = False)",
                "",
                "def rmse_cv_train(model):",
                "    rmse= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5))",
                "    return(rmse)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])",
                "ASSIGN.fit(X_trainData, Y)",
                "ASSIGN = ridge.alpha_",
                "print(, ASSIGN)",
                "print( + str(ASSIGN))",
                "ASSIGN = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,",
                "ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15,",
                "ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4],",
                "ASSIGN = 5)",
                "ASSIGN.fit(X_trainData, Y)",
                "ASSIGN = ridge.alpha_",
                "print(, ASSIGN)",
                "print(, rmse_cv_train(ASSIGN).mean())",
                "ASSIGN = ridge.predict(X_trainData)",
                "ASSIGN = ridge.predict(X_testData)",
                "sns.set()",
                "plt.scatter(ASSIGN, ASSIGN - Y, c = \"blue\", marker = \"o\", label = \"Training data\", ASSIGN=0.7)",
                "plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", ASSIGN=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "plt.scatter(ASSIGN, Y, c = \"blue\", marker = \"o\", label = \"Training data\", ASSIGN=0.7)",
                "plt.scatter(ASSIGN, pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", ASSIGN=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "ASSIGN = pd.Series(ridge.coef_, index = X_trainData.columns)",
                "print( + str(sum(ASSIGN != 0)) + + \\",
                "str(sum(ASSIGN == 0)) + \" features\")",
                "ASSIGN = pd.concat([coefs.sort_values().head(10),",
                "ASSIGN.sort_values().tail(10)])",
                "ASSIGN.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Ridge Model\")",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])",
                "ridge.fit(X_trainData, Y)",
                "alpha = ridge.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Try again for more precision with alphas centered around \" + str(alpha))",
                "ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, ",
                "                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,",
                "                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], ",
                "                cv = 5)",
                "ridge.fit(X_trainData, Y)",
                "alpha = ridge.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())",
                "y_train_rdg = ridge.predict(X_trainData)",
                "y_test_rdg = ridge.predict(X_testData)",
                "# Plot residuals",
                "sns.set()",
                "plt.scatter(y_train_rdg, y_train_rdg - Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "",
                "# Plot predictions",
                "plt.scatter(y_train_rdg, Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_rdg, pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Ridge regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "",
                "# Plot important coefficients",
                "coefs = pd.Series(ridge.coef_, index = X_trainData.columns)",
                "print(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\",
                "      str(sum(coefs == 0)) + \" features\")",
                "imp_coefs = pd.concat([coefs.sort_values().head(10),",
                "                     coefs.sort_values().tail(10)])",
                "imp_coefs.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Ridge Model\")",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],",
                "ASSIGN = 50000, cv = 10)",
                "ASSIGN.fit(X_trainData, Y)",
                "ASSIGN = lasso.alpha_",
                "print(, ASSIGN)",
                "print( + str(ASSIGN))",
                "ASSIGN = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8,",
                "ASSIGN * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05,",
                "ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35,",
                "ASSIGN * 1.4],",
                "ASSIGN = 50000, cv = 10)",
                "ASSIGN.fit(X_trainData, Y)",
                "ASSIGN = lasso.alpha_",
                "print(, ASSIGN)",
                "print(, rmse_cv_train(ASSIGN).mean())",
                "ASSIGN = lasso.predict(X_trainData)",
                "ASSIGN = lasso.predict(X_testData)",
                "plt.scatter(ASSIGN, ASSIGN - Y, c = \"blue\", marker = \"s\", label = \"Training data\", ASSIGN=0.7)",
                "plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", ASSIGN=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "plt.scatter(ASSIGN, Y, c = \"blue\", marker = \"s\", label = \"Training data\", ASSIGN=0.7)",
                "plt.scatter(ASSIGN, pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", ASSIGN=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "ASSIGN = pd.Series(lasso.coef_, index = X_trainData.columns)",
                "print( + str(sum(ASSIGN != 0)) + + \\",
                "str(sum(ASSIGN == 0)) + \" features\")",
                "ASSIGN = pd.concat([coefs.sort_values().head(10),",
                "ASSIGN.sort_values().tail(10)])",
                "ASSIGN.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Lasso Model\")",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], ",
                "                max_iter = 50000, cv = 10)",
                "lasso.fit(X_trainData, Y)",
                "alpha = lasso.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Try again for more precision with alphas centered around \" + str(alpha))",
                "lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, ",
                "                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, ",
                "                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, ",
                "                          alpha * 1.4], ",
                "                max_iter = 50000, cv = 10)",
                "lasso.fit(X_trainData, Y)",
                "alpha = lasso.alpha_",
                "print(\"Best alpha :\", alpha)",
                "",
                "print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())",
                "y_train_las = lasso.predict(X_trainData)",
                "y_test_las = lasso.predict(X_testData)",
                "",
                "# Plot residuals",
                "plt.scatter(y_train_las, y_train_las - Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Residuals\")",
                "plt.legend(loc = \"upper left\")",
                "plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")",
                "plt.show()",
                "",
                "# Plot predictions",
                "plt.scatter(y_train_las, Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)",
                "plt.scatter(y_test_las, pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)",
                "plt.title(\"Linear regression with Lasso regularization\")",
                "plt.xlabel(\"Predicted values\")",
                "plt.ylabel(\"Real values\")",
                "plt.legend(loc = \"upper left\")",
                "plt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")",
                "plt.show()",
                "",
                "# Plot important coefficients",
                "coefs = pd.Series(lasso.coef_, index = X_trainData.columns)",
                "print(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\",
                "      str(sum(coefs == 0)) + \" features\")",
                "imp_coefs = pd.concat([coefs.sort_values().head(10),",
                "                     coefs.sort_values().tail(10)])",
                "imp_coefs.plot(kind = \"barh\")",
                "plt.title(\"Coefficients in the Lasso Model\")",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN=tf.keras.regularizers.l2(0.01)",
                "print(tf.__version__)"
            ],
            "output_type": "stream",
            "content_old": [
                "from __future__ import print_function\n",
                "import keras\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, MaxPool2D, AveragePooling2D\n",
                "import os\n",
                "\n",
                "import numpy as np\n",
                "\n",
                "import seaborn as sns\n",
                "import matplotlib\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import itertools\n",
                "\n",
                "%matplotlib inline\n",
                "# TensorFlow and tf.keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout,Flatten\n",
                "from tensorflow.keras.optimizers import Adam, SGD\n",
                "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
                "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n",
                "%load_ext tensorboard\n",
                "\n",
                "# Helper libraries\n",
                "import numpy as np\n",
                "import random\n",
                "import matplotlib.pyplot as plt\n",
                "import datetime\n",
                "\n",
                "from keras.models import Sequential\n",
                "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\n",
                "from keras.regularizers import l2\n",
                "from keras.regularizers import l1\n",
                "from tensorflow.keras import regularizers\n",
                "\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.callbacks import TensorBoard,EarlyStopping\n",
                "\n",
                "kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
                "\n",
                "print(tf.__version__)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = '..path'",
                "ASSIGN = '..path'",
                "ASSIGN = 150",
                "ASSIGN = 150",
                "ASSIGN = 100",
                "ASSIGN = 32",
                "ASSIGN = 5736",
                "ASSIGN = 2460",
                "ASSIGN = ImageDataGenerator(rescale=1. path,",
                "ASSIGN=40,",
                "ASSIGN=0.2,",
                "ASSIGN=0.2,",
                "ASSIGN=0.2,",
                "ASSIGN=0.2,",
                "ASSIGN=True,",
                "ASSIGN='nearest')",
                "ASSIGN = ImageDataGenerator(rescale=1. path)",
                "ASSIGN = train_datagen.flow_from_directory(train_data_path,",
                "ASSIGN=(img_rows, img_cols),",
                "ASSIGN=ASSIGN,",
                "ASSIGN='categorical')",
                "ASSIGN = test_datagen.flow_from_directory(test_data_path,",
                "ASSIGN=(img_rows, img_cols),",
                "ASSIGN=ASSIGN,",
                "ASSIGN='categorical')"
            ],
            "output_type": "stream",
            "content_old": [
                "import numpy as np\n",
                "from keras import backend as K\n",
                "from keras.models import Sequential\n",
                "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
                "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "\n",
                "#Start\n",
                "train_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/train'\n",
                "test_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/val'\n",
                "img_rows = 150\n",
                "img_cols = 150\n",
                "epochs = 100\n",
                "batch_size = 32\n",
                "num_of_train_samples = 5736\n",
                "num_of_test_samples = 2460\n",
                "\n",
                "#Image Generator\n",
                "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
                "                                   rotation_range=40,\n",
                "                                   width_shift_range=0.2,\n",
                "                                   height_shift_range=0.2,\n",
                "                                   shear_range=0.2,\n",
                "                                   zoom_range=0.2,\n",
                "                                   horizontal_flip=True,\n",
                "                                   fill_mode='nearest')\n",
                "\n",
                "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(train_data_path,\n",
                "                                                    target_size=(img_rows, img_cols),\n",
                "                                                    batch_size=batch_size,\n",
                "                                                    class_mode='categorical')\n",
                "\n",
                "validation_generator = test_datagen.flow_from_directory(test_data_path,\n",
                "                                                        target_size=(img_rows, img_cols),\n",
                "                                                        batch_size=batch_size,\n",
                "                                                        class_mode='categorical')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = image_dataset_from_directory(",
                "ASSIGN=r\"..path\",",
                "ASSIGN = \"inferred\", label_mode = 'int',",
                "ASSIGN = 0.2,",
                "ASSIGN = \"training\",",
                "ASSIGN = 1337,",
                "ASSIGN=(224, 224),",
                "ASSIGN=32",
                ")",
                "plt.figure(figsize=(10, 10))",
                "for images, ASSIGN in ASSIGN.take(1):",
                "for i in range(9):",
                "ASSIGN = plt.subplot(3, 3, i + 1)",
                "plt.imshow(images[i].numpy().astype(\"uint8\"))",
                "plt.title(int(ASSIGN[i]))",
                "plt.axis(\"off\")"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "train_generator_2 = image_dataset_from_directory(\n",
                "    directory=r\"../input/animal1209/animal_dataset_intermediate_new/train_split/train\",\n",
                "    labels = \"inferred\", label_mode = 'int',\n",
                "    validation_split = 0.2,\n",
                "    subset = \"training\",\n",
                "    seed = 1337,\n",
                "    image_size=(224, 224),\n",
                "    batch_size=32\n",
                ")\n",
                "\n",
                "#visualizing the data\n",
                "import matplotlib.pyplot as plt\n",
                "plt.figure(figsize=(10, 10))\n",
                "for images, labels in train_generator_2.take(1):\n",
                "    for i in range(9):\n",
                "        ax = plt.subplot(3, 3, i + 1)\n",
                "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
                "        plt.title(int(labels[i]))\n",
                "        plt.axis(\"off\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN=STEP_SIZE_TRAIN",
                "ASSIGN = tf.keras.optimizers.schedules.InverseTimeDecay(",
                "0.005, decay_steps=ASSIGN*1000,decay_rate=1,staircase=False)",
                "ASSIGN = SGD(lr_schedule)",
                "ASSIGN= SGD(lr = 0.01)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Define optimizer \n",
                "\n",
                "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
                "steps_per_epoch=STEP_SIZE_TRAIN \n",
                "\n",
                "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
                "  0.005, decay_steps=steps_per_epoch*1000,decay_rate=1,staircase=False)\n",
                "\n",
                "optimizer_2 = SGD(lr_schedule)\n",
                "optimizer= SGD(lr = 0.01)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,",
                "ASSIGN=5, min_lr=0.001)",
                "ASSIGN=[reduce_lr]",
                "ASSIGN = EarlyStopping(",
                "ASSIGN='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',",
                "ASSIGN=None, restore_best_weights=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Define  Callbacks \n",
                "\n",
                "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
                "                              patience=5, min_lr=0.001)\n",
                "callbacks=[reduce_lr]\n",
                "\n",
                "earlystopping_callback = EarlyStopping(\n",
                "    monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto',\n",
                "    baseline=None, restore_best_weights=True)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Flatten(input_shape=(224, 224, 3)))",
                "ASSIGN.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))",
                "ASSIGN.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(BatchNormalization())",
                "ASSIGN.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))",
                "ASSIGN.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(BatchNormalization())",
                "ASSIGN.add(Dense(5, activation='softmax'))",
                "ASSIGN.summary()",
                "ASSIGN.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
            ],
            "output_type": "stream",
            "content_old": [
                "#MLP model 62%/42%\n",
                "\n",
                "model3 = Sequential()\n",
                "#input layer size is 784 after flattening\n",
                "model3.add(Flatten(input_shape=(224, 224, 3)))\n",
                "  \n",
                "#hidden layer with 512 neurons\n",
                "model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dropout(0.5))\n",
                "model3.add(BatchNormalization())\n",
                "\n",
                "model3.add(Dense(512, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dense(256, activation='relu',kernel_regularizer=regularizers.l1(0.01)))\n",
                "model3.add(Dropout(0.5))\n",
                "model3.add(BatchNormalization())\n",
                "model3.add(Dense(5, activation='softmax'))\n",
                "\n",
                "model3.summary()\n",
                "\n",
                "\n",
                "# compile model\n",
                "model3.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))",
                "ASSIGN.add(Activation('relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2)))",
                "ASSIGN.add(Convolution2D(32, (3, 3), padding='valid'))",
                "ASSIGN.add(Activation('relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2)))",
                "ASSIGN.add(Convolution2D(64, (3, 3), padding='valid'))",
                "ASSIGN.add(Activation('relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2)))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(64))",
                "ASSIGN.add(Activation('relu'))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(5))",
                "ASSIGN.add(Activation('softmax'))",
                "ASSIGN.summary()",
                "ASSIGN.compile(loss='categorical_crossentropy',",
                "ASSIGN='rmsprop',",
                "ASSIGN=['accuracy'])"
            ],
            "output_type": "stream",
            "content_old": [
                "# CNN model\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Convolution2D(32, (3, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Convolution2D(64, (3, 3), padding='valid'))\n",
                "model.add(Activation('relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
                "\n",
                "model.add(Flatten())\n",
                "model.add(Dense(64))\n",
                "model.add(Activation('relu'))\n",
                "model.add(Dropout(0.5))\n",
                "model.add(Dense(5))\n",
                "model.add(Activation('softmax'))\n",
                "\n",
                "model.summary()\n",
                "\n",
                "model.compile(loss='categorical_crossentropy',\n",
                "              optimizer='rmsprop',\n",
                "              metrics=['accuracy'])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "model.fit_generator(train_generator,",
                "ASSIGN=num_of_train_samples path,",
                "ASSIGN=ASSIGN,",
                "ASSIGN=validation_generator,",
                "ASSIGN=num_of_test_samples path)"
            ],
            "output_type": "stream",
            "content_old": [
                "#Train\n",
                "model.fit_generator(train_generator,\n",
                "                    steps_per_epoch=num_of_train_samples // batch_size,\n",
                "                    epochs=epochs,\n",
                "                    validation_data=validation_generator,\n",
                "                    validation_steps=num_of_test_samples // batch_size)\n",
                "\n"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "model.save_weights('model.h5')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model.save_weights('model.h5')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(tf.__version__)",
                "ASSIGN = \"logspath\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")",
                "ASSIGN = TensorBoard(log_dir=log_dir, histogram_freq=1)"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "# TensorFlow and tf.keras\n",
                "\n",
                "%load_ext tensorboard\n",
                "import datetime\n",
                "print(tf.__version__)\n",
                "\n",
                "# run the tensorboard command to view the visualizations.\n",
                "\n",
                "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
                "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
                "%tensorboard --logdir logs/fit"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "model.evaluate(validation_generator,",
                "ASSIGN=STEP_SIZE_VALID)"
            ],
            "output_type": "stream",
            "content_old": [
                "#Evaluate the model \n",
                "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
                "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
                "model.evaluate(validation_generator,\n",
                "steps=STEP_SIZE_VALID)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = model.predict_generator(validation_generator, num_of_test_samples)",
                "ASSIGN = np.argmax(Y_pred, axis=1)",
                "ASSIGN[200]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "\n",
                "Y_pred = model.predict_generator(validation_generator, num_of_test_samples)\n",
                "y_pred = np.argmax(Y_pred, axis=1)\n",
                "\n",
                "y_pred[200]\n",
                "\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_pred.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_pred.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [labels[i] for i in y_pred]",
                "predictions"
            ],
            "output_type": "execute_result",
            "content_old": [
                "predictions = [labels[i] for i in y_pred]\n",
                "predictions"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train']",
                "print(classification_report(validation_generator.labels, y_pred, ASSIGN=ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "target_names = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train']\n",
                "print(classification_report(validation_generator.labels, y_pred, target_names=target_names))"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0path)",
                "ASSIGN = test_datagen.flow_from_directory(\"..path\",",
                "ASSIGN = 'categorical',",
                "ASSIGN = (150, 150))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Extract the test data => I didnt find a way without creating a new folder on colab\n",
                "\n",
                "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)\n",
                "\n",
                "test_generator = test_datagen.flow_from_directory(\"../input/animal-ori/animal_dataset_intermediate\",\n",
                "                                                    \n",
                "                                                    class_mode = 'categorical', \n",
                "                                                    target_size = (150, 150))"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 9106",
                "ASSIGN = model.predict_generator(test_generator)",
                "ASSIGN = np.argmax(Y_pred_test, axis=1)",
                "y_pred_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#prediction on test data \n",
                "\n",
                "num = 9106\n",
                "\n",
                "Y_pred_test = model.predict_generator(test_generator)\n",
                "y_pred_test = np.argmax(Y_pred_test, axis=1)\n",
                "y_pred_test\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Y_pred_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Y_pred_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = y_pred_test[0:910]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_final = y_pred_test[0:910]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(*y_final, sep = )"
            ],
            "output_type": "stream",
            "content_old": [
                "print(*y_final, sep = \", \")  "
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(y_final)",
                "ASSIGN.columns = [\"prediction\"]",
                "ASSIGN.to_csv(\"prediction_results.csv\")   # the csv file will be saved locally on the same location where this notebook is located."
            ],
            "output_type": "not_existent",
            "content_old": [
                "res = pd.DataFrame(y_final) #preditcions are nothing but the final predictions of your model on input features of your new unseen test data\n",
                " # its important for comparison. Here \"test_new\" is your new test dataset\n",
                "res.columns = [\"prediction\"]\n",
                "res.to_csv(\"prediction_results.csv\")      # the csv file will be saved locally on the same location where this notebook is located."
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "res"
            ],
            "output_type": "execute_result",
            "content_old": [
                "res"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('..path')",
                "im"
            ],
            "output_type": "execute_result",
            "content_old": [
                "im = pd.read_csv('../input/animal-ori/animal_dataset_intermediate/Testing_set_animals.csv')\n",
                "im\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.concat([im,res], axis = 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei = pd.concat([im,res],  axis = 1) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "rei.drop('target', axis = 1)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "rei.drop('target', axis = 1)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN",
                "rei"
            ],
            "output_type": "execute_result",
            "content_old": [
                "rei['animal'] = rei['prediction'] \n",
                "rei"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN =ASSIGN.replace(to_replace=2,value = \"mucca\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei['animal'] =rei['animal'].replace(to_replace=2,value = \"mucca\") "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "rei.head(100)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "rei.head(100)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN =ASSIGN.replace(to_replace=3,value = \"pecora\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei['animal'] =rei['animal'].replace(to_replace=3,value = \"pecora\") "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN =ASSIGN.replace(to_replace=4,value = \"scoiattolo\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "\n",
                "rei['animal'] =rei['animal'].replace(to_replace=4,value = \"scoiattolo\") "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN =ASSIGN.replace(to_replace=0,value = \"elefante\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei['animal'] =rei['animal'].replace(to_replace=0,value = \"elefante\") "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "rei"
            ],
            "output_type": "execute_result",
            "content_old": [
                "rei"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN =ASSIGN.replace(to_replace=1,value = \"farfalla\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei['animal'] =rei['animal'].replace(to_replace=1,value = \"farfalla\") "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "rei.head(200)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "rei.head(200)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "rei.to_csv('final')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "rei.to_csv('final')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_final.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_final.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "binder.bind(globals())",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Set up feedback system\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.sql.ex3 import *\n",
                "print(\"Setup Complete\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = bigquery.Client()",
                "ASSIGN = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")",
                "ASSIGN = client.get_dataset(dataset_ref)",
                "ASSIGN = dataset_ref.table(\"comments\")",
                "ASSIGN = client.get_table(table_ref)",
                "ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()"
            ],
            "output_type": "stream",
            "content_old": [
                "from google.cloud import bigquery\n",
                "\n",
                "# Create a \"Client\" object\n",
                "client = bigquery.Client()\n",
                "\n",
                "# Construct a reference to the \"hacker_news\" dataset\n",
                "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
                "\n",
                "# API request - fetch the dataset\n",
                "dataset = client.get_dataset(dataset_ref)\n",
                "\n",
                "# Construct a reference to the \"comments\" table\n",
                "table_ref = dataset_ref.table(\"comments\")\n",
                "\n",
                "# API request - fetch the table\n",
                "table = client.get_table(table_ref)\n",
                "\n",
                "# Preview the first five lines of the \"comments\" table\n",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from numpy import sort",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import warnings",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "SETUP",
                "ASSIGN = multiprocessing.cpu_count()",
                "n_jobs"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import multiprocessing",
                "",
                "n_jobs = multiprocessing.cpu_count()",
                "n_jobs"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#prediction and Classification Report",
                "from sklearn.metrics import classification_report",
                "",
                "# select features using threshold",
                "from sklearn.feature_selection import SelectFromModel",
                "from sklearn.model_selection import KFold, GridSearchCV, cross_val_score, cross_val_predict",
                "from sklearn.metrics import accuracy_score, recall_score, f1_score",
                "from sklearn.metrics.scorer import make_scorer",
                "",
                "# plot tree, importance",
                "from xgboost import plot_tree, plot_importance",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# load xgboost, test train split",
                "import xgboost as xgb",
                "from sklearn.model_selection import train_test_split, cross_validate",
                "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif",
                "from sklearn.ensemble import RandomForestClassifier",
                "from sklearn.metrics import confusion_matrix"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "import seaborn as sns",
                "import matplotlib.pyplot as plt",
                "",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv('../input/train.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = len(list(df.columns))",
                "num_of_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "num_of_cols = len(list(df.columns))",
                "num_of_cols"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "pd.options.display.max_columns = num_of_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pd.options.display.max_columns = num_of_cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(df)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(df.isnull().sum())",
                "ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# columns with null values",
                "df_isna = pd.DataFrame(df.isnull().sum())",
                "df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = list(df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)].T.columns)",
                "nan_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nan_cols = list(df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)].T.columns)",
                "nan_cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[nan_cols].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[nan_cols].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[nan_cols].sample(3000).describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[nan_cols].sample(3000).describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['parentesco1'].loc[df.parentesco1 == 1].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['parentesco1'].loc[df.parentesco1 == 1].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']path(df))*100"
            ],
            "output_type": "not_existent",
            "content_old": [
                "(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']/len(df))*100"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['idhogar'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# find number of households",
                "",
                "df['idhogar'].describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = list(df['idhogar'].unique())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "house_ids = list(df['idhogar'].unique())"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df.groupby(['idhogar'])['parentesco1'].apply(lambda x: pd.unique(x.values.ravel()).tolist()).reset_index()",
                "len(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "hid_heads = df.groupby(['idhogar'])['parentesco1'].apply(lambda x: pd.unique(x.values.ravel()).tolist()).reset_index()",
                "len(hid_heads)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1'])",
                "ASSIGN.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hid = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1'])",
                "df_hid.sample(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_hid['parentesco1'] = df_hid['parentesco1'].apply(lambda x: ''.join(map(str, x)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hid['parentesco1'] = df_hid['parentesco1'].apply(lambda x: ''.join(map(str, x)))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_hid.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hid.sample(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_hid.loc[df_hid.parentesco1 == '0']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hid.loc[df_hid.parentesco1 == '0']"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = list(df_hid['idhogar'].loc[df_hid.parentesco1 == '0'])",
                "len(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# id's without head!",
                "hid_wo_heads = list(df_hid['idhogar'].loc[df_hid.parentesco1 == '0'])",
                "len(hid_wo_heads)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[df['idhogar'].isin(hid_wo_heads)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hwoh = df[df['idhogar'].isin(hid_wo_heads)]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_hwoh[['idhogar', 'parentesco1','v2a1']]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hwoh[['idhogar', 'parentesco1','v2a1']]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].hist()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df_hwoh['v2a1'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_hwoh['v2a1'].hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_hwoh)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(df_hwoh)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_hwoh['idhogar'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# these 15 households (23 rows) doesn't have a head..",
                "# we should exclude these from analysis and scoring perhaps...",
                "df_hwoh['idhogar'].unique()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a'])",
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a'])",
                "print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, len(df))",
                "ASSIGN = ASSIGN.loc[-ASSIGN['idhogar'].isin(hid_wo_heads)]",
                "print(, len(ASSIGN))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# required dataframe - without households without a head!!",
                "print(\"before removal: \", len(df))",
                "df = df.loc[-df['idhogar'].isin(hid_wo_heads)]",
                "print(\"after removal: \", len(df))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].describe().plot()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].describe().plot()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "gc.collect()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import gc",
                "",
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df['v2a1'].unique())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(df['v2a1'].unique())"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].unique()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].max()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].max()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# remove these two rows...",
                "df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, len(df))",
                "df.drop(df[df.idhogar == '563cc81b7'].index, inplace=True)",
                "print(, len(df))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(\"before removal: \", len(df))",
                "df.drop(df[df.idhogar == '563cc81b7'].index, inplace=True)",
                "print(\"after removal: \", len(df))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].hist()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['v2a1'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['v2a1'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['v18q1'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['v18q1'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['rez_esc'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['rez_esc'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['meaneduc'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['meaneduc'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['SQBmeaned'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['SQBmeaned'])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = list(df.columns)",
                "cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols = list(df.columns)",
                "cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.sample(10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "set(df.dtypes)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "set(df.dtypes)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "for col in cols:",
                "ASSIGN[col] = df[col].dtype"
            ],
            "output_type": "not_existent",
            "content_old": [
                "col_types = {}",
                "",
                "for col in cols:",
                "    col_types[col] = df[col].dtype",
                "    # print(col, df[col].dtype)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(len(col_types))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(len(col_types))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for key in sorted(col_types):",
                "print(key, col_types[key])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for key in sorted(col_types):",
                "    print(key, col_types[key])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "ASSIGN = []",
                "for col in cols:",
                "if df[col].dtype == 'O':",
                "ASSIGN.append(col)",
                "print(col, df[col].dtype)",
                "else:",
                "ASSIGN.append(col)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cat_cols = []",
                "num_cols = []",
                "for col in cols:",
                "    if df[col].dtype == 'O':",
                "        cat_cols.append(col)",
                "        print(col, df[col].dtype)",
                "    else:",
                "        num_cols.append(col)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "cat_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# categorical columns",
                "cat_cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cat_cols].sample(10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[cat_cols].sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(num_cols)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(num_cols)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sorted(num_cols)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# numerical columns",
                "sorted(num_cols)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = sns.PairGrid(df[nan_cols])",
                "ASSIGN = ASSIGN.map_offdiag(plt.scatter)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "g = sns.PairGrid(df[nan_cols])",
                "g = g.map_offdiag(plt.scatter)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ]",
                "ASSIGN = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a',",
                "'tamhog', 'hacdor', 'hacapo', 'r4t3', ]",
                "ASSIGN = ['age', 'agesq', 'female', 'male',]",
                "ASSIGN = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin',",
                "'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',]",
                "ASSIGN = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]",
                "ASSIGN = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',]",
                "ASSIGN = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',]",
                "ASSIGN = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]",
                "ASSIGN = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',]",
                "ASSIGN = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]",
                "ASSIGN = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]",
                "ASSIGN = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',",
                "'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',]",
                "ASSIGN = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother',",
                "'paredpreb', 'paredzinc', 'paredzocalo',]",
                "ASSIGN = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',",
                "'instlevel7', 'instlevel8', 'instlevel9',]",
                "ASSIGN = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',]",
                "ASSIGN = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',",
                "'estadocivil5', 'estadocivil6', 'estadocivil7',]",
                "ASSIGN = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',]",
                "ASSIGN = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',]",
                "ASSIGN = [ 'eviv1', 'eviv2', 'eviv3',]",
                "ASSIGN = [ 'etecho1', 'etecho2', 'etecho3',]",
                "ASSIGN = [ 'epared1', 'epared2', 'epared3',]",
                "ASSIGN = [ 'dis', 'escolari', 'meaneduc',",
                "'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ]",
                "ASSIGN = ['coopele', 'noelec', 'planpri', 'public',]",
                "ASSIGN = cols_electronics+cols_house_details+cols_person_details+\\",
                "ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\\",
                "ASSIGN+ASSIGN+ASSIGN+ASSIGN+\\",
                "ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\\",
                "cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec",
                "len(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols_electronics = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ]",
                "cols_house_details = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a', ",
                "                    'tamhog', 'hacdor', 'hacapo', 'r4t3', ]",
                "cols_person_details = ['age', 'agesq', 'female', 'male',]",
                "cols_SQ = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin', ",
                "           'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',]",
                "cols_water = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]",
                "",
                "cols_h = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',]",
                "cols_r = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',]",
                "cols_tip = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]",
                "cols_roof = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',]",
                "cols_floor = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]",
                "cols_sanitary = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]",
                "cols_parents = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',",
                "                'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',]",
                "cols_outside_wall = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother', ",
                "              'paredpreb', 'paredzinc', 'paredzocalo',]",
                "cols_instlevel = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',",
                "                  'instlevel7', 'instlevel8', 'instlevel9',]",
                "cols_lugar = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',]",
                "cols_estadoc = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', ",
                "                'estadocivil5', 'estadocivil6', 'estadocivil7',]",
                "cols_elim = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',]",
                "cols_energ = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',]",
                "cols_eviv = [ 'eviv1', 'eviv2', 'eviv3',]",
                "cols_etech = [ 'etecho1', 'etecho2', 'etecho3',]",
                "cols_pared = [ 'epared1', 'epared2', 'epared3',]",
                "cols_unknown = [ 'dis', 'escolari', 'meaneduc', ",
                "                'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ]",
                "cols_elec = ['coopele', 'noelec', 'planpri', 'public',]",
                "",
                "total_features = cols_electronics+cols_house_details+cols_person_details+\\",
                "cols_SQ+cols_water+cols_h+cols_r+cols_tip+cols_roof+\\",
                "cols_floor+cols_sanitary+cols_parents+cols_outside_wall+\\",
                "cols_instlevel+cols_lugar+cols_estadoc+cols_elim+cols_energ+\\",
                "cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df[cols_electronics].plot.area()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[cols_electronics].plot.area()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['Target'].unique()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = cols_electronics.append('Target')",
                "df[cols_electronics].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols_electronics_target = cols_electronics.append('Target')",
                "df[cols_electronics].corr()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "cols_electronics.remove('Target')",
                "cols_electronics"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols_electronics.remove('Target')",
                "cols_electronics"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.groupby('Target')[cols_electronics].sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.groupby('Target')[cols_electronics].sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamhog'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['tamhog'].unique()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['tamhog','r4t3', 'tamviv']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# high correlation between ",
                "# no. of persons in the household,",
                "# persons living in the household ",
                "# and size of the household",
                "# we can use any one...!!",
                "df[['tamhog','r4t3', 'tamviv']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['r4t3','tamviv']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['r4t3','tamviv']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "total_features.remove('r4t3')",
                "total_features.remove('tamhog')",
                "total_features.remove('tamviv')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "total_features.remove('r4t3')",
                "total_features.remove('tamhog')",
                "total_features.remove('tamviv')",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['escolari'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['escolari'].unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['escolari'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['escolari'].hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['escolari'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['escolari'].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['escolari'].plot.line()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['escolari'].plot.line()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df.escolari)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df.escolari)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[num_cols].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "correlations = df[num_cols].corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.zeros_like(correlations, dtype=np.bool)",
                "ASSIGN[np.triu_indices_from(ASSIGN)] = True",
                "ASSIGN = plt.subplots(figsize=(17, 13))",
                "ASSIGN = sns.diverging_palette(220, 10, as_cmap=True)",
                "sns.heatmap(correlations, ASSIGN=ASSIGN, ASSIGN=ASSIGN, vmax=.3, center=0,",
                "ASSIGN=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# correlation heatmap masking",
                "mask = np.zeros_like(correlations, dtype=np.bool)",
                "mask[np.triu_indices_from(mask)] = True",
                "",
                "f, ax = plt.subplots(figsize=(17, 13))",
                "cmap = sns.diverging_palette(220, 10, as_cmap=True)",
                "",
                "sns.heatmap(correlations, mask=mask, cmap=cmap, vmax=.3, center=0,",
                "square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[num_cols].corrwith(df.escolari, axis=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# difficult to look into the above one",
                "es_corr = df[num_cols].corrwith(df.escolari, axis=0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for x,y in zip(num_cols, list(es_corr)):",
                "if (y >= 0.75) or (y < -0.6):",
                "print(x,y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for x,y in zip(num_cols, list(es_corr)):",
                "    if (y >= 0.75) or (y < -0.6):",
                "        print(x,y)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[num_cols].corrwith(df.SQBescolari, axis=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sqbes_corr = df[num_cols].corrwith(df.SQBescolari, axis=0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for x,y in zip(num_cols, list(sqbes_corr)):",
                "if (y >= 0.5) or (y < -0.6):",
                "print(x,y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for x,y in zip(num_cols, list(sqbes_corr)):",
                "    if (y >= 0.5) or (y < -0.6):",
                "        print(x,y)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total_features.remove('escolari')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "total_features.remove('escolari')",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['overcrowding'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['overcrowding'].hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['overcrowding'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['overcrowding'].unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.plot.scatter(x='Target', y='overcrowding')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.plot.scatter(x='Target', y='overcrowding')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.groupby('Target').overcrowding.value_counts().unstack().plot.bar()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.groupby('Target').overcrowding.value_counts().unstack().plot.bar()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['Target'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['Target'].unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['Target'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['Target'].hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "nan_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "nan_cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[nan_cols].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# filling missing values",
                "",
                "df[nan_cols].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in nan_cols:",
                "if col != 'v2a1':",
                "print(col, df[col].unique())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for col in nan_cols:",
                "    if col != 'v2a1':",
                "        print(col, df[col].unique())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.regplot(df['meaneduc'],df['SQBmeaned'], order=2)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# there's a clear quadratic relation between meaneduc and SQBmeaned",
                "# hence, we can ignore either one of these..say, meaneduc",
                "sns.regplot(df['meaneduc'],df['SQBmeaned'], order=2)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['meaneduc'].fillna(0, inplace=True)",
                "df['SQBmeaned'].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# filling na values in meaneduc and SQBmeaned",
                "df['meaneduc'].fillna(0, inplace=True)",
                "df['SQBmeaned'].fillna(0, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "total_features.remove('meaneduc')",
                "total_features"
            ],
            "output_type": "not_existent",
            "content_old": [
                "total_features.remove('meaneduc')",
                "",
                "total_features"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v18q','v18q1','idhogar']].loc[df.v18q1.isna()].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# we can fill v18q1 (household tablets) with 0 as individual tablet count is 0 for all such columns",
                "df[['v18q','v18q1','idhogar']].loc[df.v18q1.isna()].describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['v18q1'] = df['v18q'].groupby(df['idhogar']).transform('sum')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v18q1'] = df['v18q'].groupby(df['idhogar']).transform('sum')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.sample(7)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.sample(7)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(df.isnull().sum())",
                "ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ff = pd.DataFrame(df.isnull().sum())",
                "ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# rez_esc - years behind in school",
                "df['rez_esc'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].isnull().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['rez_esc'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df) - df['rez_esc'].isnull().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# only these many rows has values for years behind school",
                "len(df) - df['rez_esc'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isnull().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df) - df['v2a1'].isnull().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# only these many rows has values for income",
                "len(df) - df['v2a1'].isnull().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# number of rows where income and rez_esc has values",
                "len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# how many rows with nan values for both income and rez_esc ",
                "len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)])"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# how many rows with nan values for either income or rez_esc ",
                "len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['rez_esc'].hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['rez_esc','v2a1']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['rez_esc','v2a1']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','rez_esc']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['Target','rez_esc']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','rez_esc']].fillna(0).corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['Target','rez_esc']].fillna(0).corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','Target']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','Target']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','Target']].fillna(0).corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','Target']].fillna(0).corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['rez_esc'].unique()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['rez_esc'].unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['rez_esc'])",
                "sns.kdeplot(df['rez_esc'].fillna(0))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['rez_esc'])",
                "sns.kdeplot(df['rez_esc'].fillna(0))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['v2a1'])",
                "sns.kdeplot(df['v2a1'].fillna(0))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize=(13,7))",
                "sns.kdeplot(df['v2a1'])",
                "sns.kdeplot(df['v2a1'].fillna(0))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df['rez_esc'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "ASSIGN = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "ASSIGN.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "ASSIGN.ax_joint.collections[0].set_alpha(0)",
                "ASSIGN.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x, y = df['rez_esc'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df['rez_esc'].fillna(0), df['v2a1'].fillna(0)",
                "sns.jointplot(ASSIGN, data=df, kind=\"kde\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x, y = df['rez_esc'].fillna(0), df['v2a1'].fillna(0)",
                "sns.jointplot(x, y, data=df, kind=\"kde\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['rez_esc'].fillna(0, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['rez_esc'].fillna(0, inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(df.isnull().sum())",
                "ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ff = pd.DataFrame(df.isnull().sum())",
                "ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df['Target'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "ASSIGN = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "ASSIGN.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "ASSIGN.ax_joint.collections[0].set_alpha(0)",
                "ASSIGN.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x, y = df['Target'], df['v2a1']",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df['Target'], df['v2a1'].fillna(0)",
                "plt.figure(figsize=(23,17))",
                "ASSIGN = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "ASSIGN.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "ASSIGN.ax_joint.collections[0].set_alpha(0)",
                "ASSIGN.set_axis_labels(\"$X$\", \"$Y$\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x, y = df['Target'], df['v2a1'].fillna(0)",
                "plt.figure(figsize=(23,17))",
                "g = sns.jointplot(x, y, data=df, kind=\"kde\", color=\"m\")",
                "g.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1, marker=\"+\")",
                "g.ax_joint.collections[0].set_alpha(0)",
                "g.set_axis_labels(\"$X$\", \"$Y$\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['Target'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['Target'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.groupby('Target').count()['v2a1']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.groupby('Target').count()['v2a1']"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(15,7))",
                "df.groupby('Target').count()['v2a1'].plot(ax=ax)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.groupby('Target').count()['v2a1'].plot(ax=ax)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby('Target').count()['v2a1'].plot(ax=ax)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby('Target').count()['v2a1'].plot(ax=ax)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(15,7))",
                "df.groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, ax = plt.subplots(figsize=(15,7))",
                "df.fillna(0).groupby(['Target','hhsize']).count()['v2a1'].unstack().plot(ax=ax)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['hhsize'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['hhsize'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['hogar_total'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['hogar_total'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total_features.remove('hogar_total')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# use hhsize, ignore 'hogar_total',",
                "total_features.remove('hogar_total')",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['hhsize','hogar_adul']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['hhsize','hogar_adul']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['hhsize','Target']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['hhsize','Target']].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['Target','hogar_adul']].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['Target','hogar_adul']].corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_adul'])",
                "sns.kdeplot(df['hhsize'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['hogar_adul'])",
                "sns.kdeplot(df['hhsize'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_adul'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_adul'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "max(df['hogar_adul']), max(df['hogar_total'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "max(df['hogar_adul']), max(df['hogar_total'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_nin'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['hogar_total'])",
                "sns.kdeplot(df['hogar_nin'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['male'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['male'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['female'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['female'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total_features.remove('female')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# removing female",
                "total_features.remove('female')",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['r4t3'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['r4t3'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamhog'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['tamhog'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tamviv'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['tamviv'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(17,13))",
                "sns.kdeplot(df['tamviv'])",
                "sns.kdeplot(df['tamhog'])",
                "sns.kdeplot(df['r4t3'])",
                "sns.kdeplot(df['hhsize'])",
                "sns.kdeplot(df['hogar_total'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.figure(figsize=(17,13))",
                "sns.kdeplot(df['tamviv'])",
                "sns.kdeplot(df['tamhog'])",
                "sns.kdeplot(df['r4t3'])",
                "sns.kdeplot(df['hhsize'])",
                "sns.kdeplot(df['hogar_total'])",
                "#sns.kdeplot(df['hogar_adul'])",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "total_features.remove('r4t3')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# removing 'r4t3', as 'hhsize' is of almost same distribution",
                "total_features.remove('r4t3')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['dependency'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['dependency'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['dependency'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['dependency'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['SQBdependency'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['SQBdependency'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['SQBdependency'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['SQBdependency'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "cat_cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cat_cols"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefe'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['edjefe'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefa'].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['edjefa'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefe'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['edjefe'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['edjefa'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['edjefa'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.loc[df.edjefa == 'yes', 'edjefa'] = 1",
                "df.loc[df.edjefa == 'no', 'edjefa'] = 0"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.loc[df.edjefa == 'yes', 'edjefa'] = 1",
                "df.loc[df.edjefa == 'no', 'edjefa'] = 0",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.loc[df.edjefe == 'yes', 'edjefe'] = 1",
                "df.loc[df.edjefe == 'no', 'edjefe'] = 0"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.loc[df.edjefe == 'yes', 'edjefe'] = 1",
                "df.loc[df.edjefe == 'no', 'edjefe'] = 0",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['edjefa','edjefe']].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['edjefa','edjefe']].describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df[['edjefa','edjefe']] = df[['edjefa','edjefe']].apply(pd.to_numeric)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['edjefa','edjefe']] = df[['edjefa','edjefe']].apply(pd.to_numeric)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df[['edjefa','edjefe']].dtypes"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['edjefa','edjefe']].dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "total_features.append('edjefa')",
                "total_features.append('edjefe')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "total_features.append('edjefa')",
                "total_features.append('edjefe')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "cols_water"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols_water"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cols_water].describe()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[cols_water].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[cols_water].corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[cols_water].corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguadentro'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['abastaguadentro'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguafuera'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['abastaguafuera'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['abastaguano'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['abastaguano'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_water].sum().reset_index()",
                "df_water_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_water_target = df.groupby('Target')[cols_water].sum().reset_index()",
                "df_water_target"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "722+1496+1133+5844"
            ],
            "output_type": "not_existent",
            "content_old": [
                "722+1496+1133+5844"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_water_target.corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_water_target.corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "total_features.remove('abastaguano')",
                "total_features.remove('abastaguafuera')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "total_features.remove('abastaguano')",
                "total_features.remove('abastaguafuera')",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['pisocemento'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# cols_floor",
                "# ",
                "# ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',]",
                "",
                "df['pisocemento'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_floor].sum().reset_index()",
                "df_floor_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_floor_target = df.groupby('Target')[cols_floor].sum().reset_index()",
                "df_floor_target"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "total_features.remove('pisonatur')",
                "total_features.remove('pisonotiene')",
                "total_features.remove('pisoother')",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# removing these features -> inc by 0.002",
                "total_features.remove('pisonatur')",
                "total_features.remove('pisonotiene')",
                "total_features.remove('pisoother')",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_outside_wall].sum().reset_index()",
                "df_wall_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_wall_target = df.groupby('Target')[cols_outside_wall].sum().reset_index()",
                "df_wall_target"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredblolad'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['paredblolad'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['paredpreb'])",
                "sns.kdeplot(df['paredmad'])",
                "sns.kdeplot(df['paredzocalo'])",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_roof].sum().reset_index()",
                "df_roof_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_roof_target = df.groupby('Target')[cols_roof].sum().reset_index()",
                "df_roof_target"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techozinc'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['techozinc'])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techozinc'])",
                "sns.kdeplot(df['techoentrepiso'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['techozinc'])",
                "sns.kdeplot(df['techoentrepiso'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['techoentrepiso'])",
                "sns.kdeplot(df['techocane'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['techoentrepiso'])",
                "sns.kdeplot(df['techocane'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_sanitary].sum().reset_index()",
                "df_sani_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',]",
                "",
                "df_sani_target = df.groupby('Target')[cols_sanitary].sum().reset_index()",
                "df_sani_target"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['sanitario1'])",
                "sns.kdeplot(df['sanitario6'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['sanitario1'])",
                "sns.kdeplot(df['sanitario6'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['sanitario3'])",
                "sns.kdeplot(df['sanitario2'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['sanitario3'])",
                "sns.kdeplot(df['sanitario2'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(total_features)",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.groupby('Target')[cols_tip].sum().reset_index()",
                "df_tipo_target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# cols_tip ",
                "# ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]",
                "",
                "df_tipo_target = df.groupby('Target')[cols_tip].sum().reset_index()",
                "df_tipo_target"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi2'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['tipovivi2'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi1'])",
                "sns.kdeplot(df['tipovivi3'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['tipovivi1'])",
                "sns.kdeplot(df['tipovivi3'])",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.kdeplot(df['tipovivi5'])",
                "sns.kdeplot(df['tipovivi4'])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sns.kdeplot(df['tipovivi5'])",
                "sns.kdeplot(df['tipovivi4'])",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isna().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].isna().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['tipovivi3'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['tipovivi3'].value_counts()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.line()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.line()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.hist()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].loc[df.parentesco1 == 1].plot.hist()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df['v2a1'].fillna(120000, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].fillna(120000, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['v2a1'].isna().sum()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['v2a1'].isna().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "'Target' in total_features"
            ],
            "output_type": "not_existent",
            "content_old": [
                "'Target' in total_features"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[total_features], df['Target']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X, y = df[total_features], df['Target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 42",
                "ASSIGN = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, ASSIGN=ASSIGN, random_state=ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Split the dataset into train and Test",
                "seed = 42",
                "test_size = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = xgb.XGBClassifier(n_jobs=n_jobs)",
                "model1"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Train the XGboost Model for Classification",
                "model1 = xgb.XGBClassifier(n_jobs=n_jobs)",
                "model1"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = model1.fit(X_train, y_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model1 = model1.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5, n_jobs=n_jobs)",
                "model2"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model2 = xgb.XGBClassifier(n_estimators=100, max_depth=8, learning_rate=0.1, subsample=0.5, n_jobs=n_jobs)",
                "model2"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = model2.fit(X_train, y_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model2 = model2.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train_model1.predict(X_test)",
                "ASSIGN = train_model2.predict(X_test)",
                "print('Model 1 XGboost Report %r' % (classification_report(y_test, ASSIGN)))",
                "print('Model 2 XGboost Report %r' % (classification_report(y_test, ASSIGN)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# predictions",
                "pred1 = train_model1.predict(X_test)",
                "pred2 = train_model2.predict(X_test)",
                "",
                "print('Model 1 XGboost Report %r' % (classification_report(y_test, pred1)))",
                "print('Model 2 XGboost Report %r' % (classification_report(y_test, pred2)))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print( % (accuracy_score(y_test, pred1) * 100))",
                "print( % (accuracy_score(y_test, pred2) * 100))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(\"Accuracy for model 1: %.2f\" % (accuracy_score(y_test, pred1) * 100))",
                "print(\"Accuracy for model 2: %.2f\" % (accuracy_score(y_test, pred2) * 100))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = xgb.XGBClassifier(",
                "ASSIGN =0.1,",
                "ASSIGN=1000,",
                "ASSIGN=5,",
                "ASSIGN=1,",
                "ASSIGN=0,",
                "ASSIGN=0.8,",
                "ASSIGN=0.8,",
                "ASSIGN= 'binary:logistic',",
                "ASSIGN=ASSIGN,",
                "ASSIGN=1,",
                "ASSIGN=27)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Let's do a little Gridsearch, Hyperparameter Tunning",
                "model3 = xgb.XGBClassifier(",
                " learning_rate =0.1,",
                " n_estimators=1000,",
                " max_depth=5,",
                " min_child_weight=1,",
                " gamma=0,",
                " subsample=0.8,",
                " colsample_bytree=0.8,",
                " objective= 'binary:logistic',",
                " n_jobs=n_jobs,",
                " scale_pos_weight=1,",
                " seed=27)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = model3.fit(X_train, y_train)",
                "ASSIGN = train_model3.predict(X_test)",
                "print( % (accuracy_score(y_test, ASSIGN) * 100))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model3 = model3.fit(X_train, y_train)",
                "pred3 = train_model3.predict(X_test)",
                "print(\"Accuracy for model 3: %.2f\" % (accuracy_score(y_test, pred3) * 100))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {",
                "'n_estimators': [100],",
                "'max_depth': [6, 9],",
                "'subsample': [0.9, 1.0],",
                "'colsample_bytree': [0.9, 1.0],",
                "}"
            ],
            "output_type": "not_existent",
            "content_old": [
                "parameters = {",
                "    'n_estimators': [100],",
                "    'max_depth': [6, 9],",
                "    'subsample': [0.9, 1.0],",
                "    'colsample_bytree': [0.9, 1.0],",
                "}",
                ""
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = GridSearchCV(model3,",
                "ASSIGN=n_jobs,",
                "ASSIGN=\"neg_log_loss\",",
                "ASSIGN=3)",
                "grid"
            ],
            "output_type": "not_existent",
            "content_old": [
                "grid = GridSearchCV(model3,",
                "                    parameters, n_jobs=n_jobs,",
                "                    scoring=\"neg_log_loss\",",
                "                    cv=3)",
                "grid"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(23, 17))",
                "plot_importance(model3, ax=ax)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, ax = plt.subplots(figsize=(23, 17))",
                "plot_importance(model3, ax=ax)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['estadocivil1','instlevel9','techocane','parentesco10','v14a',",
                "'parentesco11','parentesco5','paredother','parentesco7','noelec',",
                "'elimbasu4','elimbasu6']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "less_imp_features = ['estadocivil1','instlevel9','techocane','parentesco10','v14a',",
                "                     'parentesco11','parentesco5','paredother','parentesco7','noelec',",
                "                     'elimbasu4','elimbasu6']",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# before removing less important features",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "for f in less_imp_features:",
                "if f in total_features:",
                "total_features.remove(f)",
                "len(total_features)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for f in less_imp_features:",
                "    if f in total_features:",
                "        total_features.remove(f)",
                "",
                "len(total_features)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df[total_features], df['Target']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X, y = df[total_features], df['Target']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 43",
                "ASSIGN = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, ASSIGN=ASSIGN, random_state=ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Split the dataset into train and Test",
                "seed = 43",
                "test_size = 0.3",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = model3.fit(X_train, y_train)",
                "ASSIGN = train_model5.predict(X_test)",
                "print( % (accuracy_score(y_test, ASSIGN) * 100))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model5 = model3.fit(X_train, y_train)",
                "pred5 = train_model5.predict(X_test)",
                "print(\"Accuracy for model 5: %.2f\" % (accuracy_score(y_test, pred5) * 100))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = train_model5.fit(X, y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model6 = train_model5.fit(X, y)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "train_model6"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model6"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = sort(model3.feature_importances_)",
                "thresholds"
            ],
            "output_type": "not_existent",
            "content_old": [
                "thresholds = sort(model3.feature_importances_)",
                "thresholds"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "thresholds.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "thresholds.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "np.unique(thresholds).shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "np.unique(thresholds).shape"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = RandomForestClassifier(n_jobs=n_jobs)",
                "model4"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model4 = RandomForestClassifier(n_jobs=n_jobs)",
                "model4"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = model4.fit(X_train, y_train)",
                "ASSIGN = train_model4.predict(X_test)",
                "print( % (accuracy_score(y_test, ASSIGN) * 100))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_model4 = model4.fit(X_train, y_train)",
                "pred4 = train_model4.predict(X_test)",
                "print(\"Accuracy for model 4: %.2f\" % (accuracy_score(y_test, pred4) * 100))",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_test, pred4)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "confusion_matrix(y_test, pred4)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_test = pd.read_csv('../input/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(df_test)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_test.sample(10)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_test.sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "len(df_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "len(df_test)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_test.loc[df_test.edjefa == 'yes', 'edjefa'] = 1",
                "df_test.loc[df_test.edjefa == 'no', 'edjefa'] = 0",
                "df_test.loc[df_test.edjefe == 'yes', 'edjefe'] = 1",
                "df_test.loc[df_test.edjefe == 'no', 'edjefe'] = 0",
                "df_test[['edjefa','edjefe']] = df_test[['edjefa','edjefe']].apply(pd.to_numeric)",
                "df_test[['edjefa','edjefe']].dtypes"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_test.loc[df_test.edjefa == 'yes', 'edjefa'] = 1",
                "df_test.loc[df_test.edjefa == 'no', 'edjefa'] = 0",
                "",
                "df_test.loc[df_test.edjefe == 'yes', 'edjefe'] = 1",
                "df_test.loc[df_test.edjefe == 'no', 'edjefe'] = 0",
                "df_test[['edjefa','edjefe']] = df_test[['edjefa','edjefe']].apply(pd.to_numeric)",
                "df_test[['edjefa','edjefe']].dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df_test[total_features]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_actual_test = df_test[total_features]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_actual_test.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_actual_test.shape"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train_model6.predict(X_actual_test)",
                "pred_actual"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pred_actual = train_model6.predict(X_actual_test)",
                "pred_actual"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "pred_actual.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pred_actual.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(df['Id'], pred_actual).reset_index()",
                "ASSIGN.columns = ['Target','Id']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final = pd.DataFrame(df['Id'], pred_actual).reset_index()",
                "df_final.columns = ['Target','Id']",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df_final.columns.tolist()",
                "cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols = df_final.columns.tolist()",
                "cols"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN[-1:] + ASSIGN[:-1]",
                "cols"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cols = cols[-1:] + cols[:-1]",
                "cols",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = ASSIGN[cols]",
                "ASSIGN.head(7)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final = df_final[cols]",
                "df_final.head(7)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "df_final.index.name = None",
                "df_final.head(7)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final.index.name = None",
                "df_final.head(7)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_final['Target'].value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final['Target'].value_counts()",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_final[cols].sample(4)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final[cols].sample(4)"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "df_final[cols].to_csv('sample_submission.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df_final[cols].to_csv('sample_submission.csv', index=False)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "os.listdir('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "os.listdir('../input/')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "ASSIGN=\"github_repos\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# import package with helper functions ",
                "import bq_helper",
                "",
                "# create a helper object for this dataset",
                "github = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",",
                "                                              dataset_name=\"github_repos\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = (\"\"\"",
                "-- Select all the columns we want in our joined table",
                "SELECT L.license, COUNT(sf.path) AS number_of_files",
                "FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "-- Table to merge into sample_files",
                "INNER JOIN `bigquery-public-data.github_repos.licenses` as L",
                "ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "GROUP BY L.license",
                "ORDER BY number_of_files DESC",
                "\"\"\")",
                "ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# You can use two dashes (--) to add comments in SQL",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT L.license, COUNT(sf.path) AS number_of_files",
                "        FROM `bigquery-public-data.github_repos.sample_files` as sf",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.licenses` as L ",
                "            ON sf.repo_name = L.repo_name -- what columns should we join on?",
                "        GROUP BY L.license",
                "        ORDER BY number_of_files DESC",
                "        \"\"\")",
                "",
                "file_count_by_license = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(file_count_by_license)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# print out all the returned results",
                "print(file_count_by_license)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "github.list_tables()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#view the list of tables inside the big query",
                "github.list_tables()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "github.head(\"sample_commits\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#lets view the a couple of lines from the table using head",
                "github.head(\"sample_commits\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "github.head(\"sample_files\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#lets view the a couple of lines from the table using head",
                "github.head(\"sample_files\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (\"\"\"",
                "-- Select all the columns we want in our joined table",
                "SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python",
                "FROM `bigquery-public-data.github_repos.sample_commits` as sc",
                "-- Table to merge into sample_files",
                "INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf",
                "ON sc.repo_name = sf.repo_name -- what columns should we join on?",
                "WHERE sf.path LIKE '%.py'",
                "GROUP BY sf.repo_name",
                "ORDER BY number_of_commits_in_python DESC",
                "\"\"\")",
                "print(github.estimate_query_size(ASSIGN))",
                "ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#How many commits (recorded in the \"sample_commits\" table) have been made in repos written in the Python programming language? (I'm looking for the number of commits per repo for all the repos written in Python.",
                "",
                "query = (\"\"\"",
                "        -- Select all the columns we want in our joined table",
                "        SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python",
                "        FROM `bigquery-public-data.github_repos.sample_commits` as sc",
                "        -- Table to merge into sample_files",
                "        INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf ",
                "            ON sc.repo_name =  sf.repo_name -- what columns should we join on?",
                "        WHERE sf.path LIKE '%.py'",
                "        GROUP BY sf.repo_name",
                "        ORDER BY number_of_commits_in_python DESC",
                "        \"\"\")",
                "",
                "print(github.estimate_query_size(query))",
                "file_count_by_python_files = github.query_to_pandas_safe(query, max_gb_scanned=6)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "file_count_by_python_files"
            ],
            "output_type": "not_existent",
            "content_old": [
                "file_count_by_python_files"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os",
                "import torch",
                "import torchvision",
                "import tarfile",
                "import torch.nn as nn",
                "import numpy as np",
                "import torch.nn.functional as F",
                "from torchvision.datasets.utils import download_url",
                "from torchvision.datasets import ImageFolder",
                "from torch.utils.data import DataLoader",
                "import torchvision.transforms as tt",
                "from torch.utils.data import random_split",
                "from torchvision.utils import make_grid",
                "import matplotlib.pyplot as plt",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN='course_project'"
            ],
            "output_type": "not_existent",
            "content_old": [
                "project_name='course_project'"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = '..path'",
                "print(os.listdir(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "data_dir = '../input/russian-handwritten-letters'",
                "",
                "print(os.listdir(data_dir))",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from torchvision.datasets import ImageFolder",
                "from torchvision.transforms import ToTensor"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ImageFolder(data_dir, transform=ToTensor())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "dataset = ImageFolder(data_dir, transform=ToTensor())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "VALIDATION",
                "ASSIGN = dataset[0]",
                "print(img.shape, label)",
                "img"
            ],
            "output_type": "stream",
            "content_old": [
                "img, label = dataset[0]",
                "print(img.shape, label)",
                "img"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(dataset.classes)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(dataset.classes)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "def show_example(img, label):",
                "print('Label: ', dataset.classes[label], +str(label)+)",
                "plt.imshow(img.permute(1, 2, 0))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt",
                "",
                "def show_example(img, label):",
                "    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")",
                "    plt.imshow(img.permute(1, 2, 0))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "show_example(*dataset[0])"
            ],
            "output_type": "stream",
            "content_old": [
                "show_example(*dataset[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))",
                "ASSIGN = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),",
                "tt.RandomHorizontalFlip(),",
                "tt.ToTensor(),",
                "tt.Normalize(*ASSIGN,inplace=True)])",
                "ASSIGN = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Data transforms (normalization & data augmentation)",
                "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))",
                "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'), ",
                "                         tt.RandomHorizontalFlip(), ",
                "                         tt.ToTensor(), ",
                "                         tt.Normalize(*stats,inplace=True)])",
                "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 400"
            ],
            "output_type": "not_existent",
            "content_old": [
                "batch_size = 400"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 5000",
                "ASSIGN = len(dataset) - val_size",
                "ASSIGN = random_split(dataset, [train_size, val_size])",
                "len(train_ds), len(val_ds)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "val_size = 5000",
                "train_size = len(dataset) - val_size",
                "",
                "train_ds, val_ds = random_split(dataset, [train_size, val_size])",
                "len(train_ds), len(val_ds)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)",
                "ASSIGN = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# PyTorch data loaders",
                "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)",
                "valid_dl = DataLoader(val_ds, batch_size*2, num_workers=3, pin_memory=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 42",
                "torch.manual_seed(ASSIGN);"
            ],
            "output_type": "not_existent",
            "content_old": [
                "random_seed = 42",
                "torch.manual_seed(random_seed);"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = 784",
                "ASSIGN = 256",
                "ASSIGN = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)",
                "ASSIGN = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "image_size = 784",
                "hidden_size = 256",
                "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)",
                "val_dl = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "SETUP",
                "def show_batch(dl):",
                "for images, labels in dl:",
                "ASSIGN = plt.subplots(figsize=(12, 12))",
                "ax.set_xticks([]); ax.set_yticks([])",
                "ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))",
                "break"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from torchvision.utils import make_grid",
                "",
                "def show_batch(dl):",
                "    for images, labels in dl:",
                "        fig, ax = plt.subplots(figsize=(12, 12))",
                "        ax.set_xticks([]); ax.set_yticks([])",
                "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))",
                "        break"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "show_batch(train_dl)"
            ],
            "output_type": "display_data",
            "content_old": [
                "show_batch(train_dl)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "def get_default_device():",
                "\"\"\"Pick GPU if available, else CPU\"\"\"",
                "if torch.cuda.is_available():",
                "return torch.device('cuda')",
                "else:",
                "return torch.device('cpu')",
                "def to_device(data, device):",
                "\"\"\"Move tensor(s) to chosen device\"\"\"",
                "if isinstance(data, (list,tuple)):",
                "return [to_device(x, device) for x in data]",
                "return data.to(device, non_blocking=True)",
                "class DeviceDataLoader():",
                "\"\"\"Wrap a dataloader to move data to a device\"\"\"",
                "def __init__(self, dl, device):",
                "self.dl = dl",
                "self.device = device",
                "def __iter__(self):",
                "\"\"\"Yield a batch of data after moving it to device\"\"\"",
                "for b in self.dl:",
                "yield to_device(b, self.device)",
                "def __len__(self):",
                "\"\"\"Number of batches\"\"\"",
                "return len(self.dl)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def get_default_device():",
                "    \"\"\"Pick GPU if available, else CPU\"\"\"",
                "    if torch.cuda.is_available():",
                "        return torch.device('cuda')",
                "    else:",
                "        return torch.device('cpu')",
                "    ",
                "def to_device(data, device):",
                "    \"\"\"Move tensor(s) to chosen device\"\"\"",
                "    if isinstance(data, (list,tuple)):",
                "        return [to_device(x, device) for x in data]",
                "    return data.to(device, non_blocking=True)",
                "",
                "class DeviceDataLoader():",
                "    \"\"\"Wrap a dataloader to move data to a device\"\"\"",
                "    def __init__(self, dl, device):",
                "        self.dl = dl",
                "        self.device = device",
                "        ",
                "    def __iter__(self):",
                "        \"\"\"Yield a batch of data after moving it to device\"\"\"",
                "        for b in self.dl: ",
                "            yield to_device(b, self.device)",
                "",
                "    def __len__(self):",
                "        \"\"\"Number of batches\"\"\"",
                "        return len(self.dl)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = get_default_device()",
                "device"
            ],
            "output_type": "execute_result",
            "content_old": [
                "device = get_default_device()",
                "device"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = DeviceDataLoader(ASSIGN, device)",
                "ASSIGN = DeviceDataLoader(ASSIGN, device)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_dl = DeviceDataLoader(train_dl, device)",
                "valid_dl = DeviceDataLoader(valid_dl, device)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "class SimpleResidualBlock(nn.Module):",
                "def __init__(self):",
                "super().__init__()",
                "self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "self.relu1 = nn.ReLU()",
                "self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "self.relu2 = nn.ReLU()",
                "def forward(self, x):",
                "ASSIGN = self.conv1(x)",
                "ASSIGN = self.relu1(ASSIGN)",
                "ASSIGN = self.conv2(ASSIGN)",
                "return self.relu2(out) + x"
            ],
            "output_type": "not_existent",
            "content_old": [
                "class SimpleResidualBlock(nn.Module):",
                "    def __init__(self):",
                "        super().__init__()",
                "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "        self.relu1 = nn.ReLU()",
                "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)",
                "        self.relu2 = nn.ReLU()",
                "        ",
                "    def forward(self, x):",
                "        out = self.conv1(x)",
                "        out = self.relu1(out)",
                "        out = self.conv2(out)",
                "        return self.relu2(out) + x # ReLU can be applied before or after adding the input"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = to_device(SimpleResidualBlock(), device)",
                "for images, labels in train_dl:",
                "ASSIGN = simple_resnet(images)",
                "print(ASSIGN.shape)",
                "break",
                "del simple_resnet, images, labels",
                "torch.cuda.empty_cache()"
            ],
            "output_type": "stream",
            "content_old": [
                "simple_resnet = to_device(SimpleResidualBlock(), device)",
                "",
                "for images, labels in train_dl:",
                "    out = simple_resnet(images)",
                "    print(out.shape)",
                "    break",
                "    ",
                "del simple_resnet, images, labels",
                "torch.cuda.empty_cache()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def accuracy(outputs, labels):",
                "ASSIGN = torch.max(outputs, dim=1)",
                "return torch.tensor(torch.sum(preds == labels).item() path(preds))",
                "class ImageClassificationBase(nn.Module):",
                "def training_step(self, batch):",
                "ASSIGN = batch",
                "ASSIGN = self(images)",
                "ASSIGN = F.cross_entropy(out, labels)",
                "return loss",
                "def validation_step(self, batch):",
                "ASSIGN = batch",
                "ASSIGN = self(images)",
                "ASSIGN = F.cross_entropy(out, labels)",
                "ASSIGN = accuracy(out, labels)",
                "return {'val_loss': ASSIGN.detach(), 'val_acc': ASSIGN}",
                "def validation_epoch_end(self, outputs):",
                "ASSIGN = [x['val_loss'] for x in outputs]",
                "ASSIGN = torch.stack(batch_losses).mean()",
                "ASSIGN = [x['val_acc'] for x in outputs]",
                "ASSIGN = torch.stack(batch_accs).mean()",
                "return {'val_loss': ASSIGN.item(), 'val_acc': ASSIGN.item()}",
                "def epoch_end(self, epoch, result):",
                "print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(",
                "epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def accuracy(outputs, labels):",
                "    _, preds = torch.max(outputs, dim=1)",
                "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))",
                "",
                "class ImageClassificationBase(nn.Module):",
                "    def training_step(self, batch):",
                "        images, labels = batch ",
                "        out = self(images)                  # Generate predictions",
                "        loss = F.cross_entropy(out, labels) # Calculate loss",
                "        return loss",
                "    ",
                "    def validation_step(self, batch):",
                "        images, labels = batch ",
                "        out = self(images)                    # Generate predictions",
                "        loss = F.cross_entropy(out, labels)   # Calculate loss",
                "        acc = accuracy(out, labels)           # Calculate accuracy",
                "        return {'val_loss': loss.detach(), 'val_acc': acc}",
                "        ",
                "    def validation_epoch_end(self, outputs):",
                "        batch_losses = [x['val_loss'] for x in outputs]",
                "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses",
                "        batch_accs = [x['val_acc'] for x in outputs]",
                "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies",
                "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}",
                "    ",
                "    def epoch_end(self, epoch, result):",
                "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(",
                "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def conv_block(in_channels, out_channels, pool=False):",
                "ASSIGN = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),",
                "nn.BatchNorm2d(out_channels),",
                "nn.ReLU(inplace=True)]",
                "if pool: ASSIGN.append(nn.MaxPool2d(2))",
                "return nn.Sequential(*ASSIGN)",
                "class ResNet9(ImageClassificationBase):",
                "def __init__(self, in_channels, num_classes):",
                "super().__init__()",
                "self.conv1 = conv_block(in_channels, 64)",
                "self.conv2 = conv_block(64, 128, pool=True)",
                "self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))",
                "self.conv3 = conv_block(128, 256, pool=True)",
                "self.conv4 = conv_block(256, 512, pool=True)",
                "self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))",
                "self.classifier = nn.Sequential(nn.MaxPool2d(4),",
                "nn.Flatten(),",
                "nn.Linear(512, num_classes))",
                "def forward(self, xb):",
                "ASSIGN = self.conv1(xb)",
                "ASSIGN = self.conv2(ASSIGN)",
                "ASSIGN = self.res1(ASSIGN) + ASSIGN",
                "ASSIGN = self.conv3(ASSIGN)",
                "ASSIGN = self.conv4(ASSIGN)",
                "ASSIGN = self.res2(ASSIGN) + ASSIGN",
                "ASSIGN = self.classifier(ASSIGN)",
                "return out"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def conv_block(in_channels, out_channels, pool=False):",
                "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), ",
                "              nn.BatchNorm2d(out_channels), ",
                "              nn.ReLU(inplace=True)]",
                "    if pool: layers.append(nn.MaxPool2d(2))",
                "    return nn.Sequential(*layers)",
                "",
                "class ResNet9(ImageClassificationBase):",
                "    def __init__(self, in_channels, num_classes):",
                "        super().__init__()",
                "        ",
                "        self.conv1 = conv_block(in_channels, 64)",
                "        self.conv2 = conv_block(64, 128, pool=True)",
                "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))",
                "        ",
                "        self.conv3 = conv_block(128, 256, pool=True)",
                "        self.conv4 = conv_block(256, 512, pool=True)",
                "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))",
                "        ",
                "        self.classifier = nn.Sequential(nn.MaxPool2d(4), ",
                "                                        nn.Flatten(), ",
                "                                        nn.Linear(512, num_classes))",
                "        ",
                "    def forward(self, xb):",
                "        out = self.conv1(xb)",
                "        out = self.conv2(out)",
                "        out = self.res1(out) + out",
                "        out = self.conv3(out)",
                "        out = self.conv4(out)",
                "        out = self.res2(out) + out",
                "        out = self.classifier(out)",
                "        return out"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = to_device(ResNet9(3, 10), device)",
                "model"
            ],
            "output_type": "execute_result",
            "content_old": [
                "model = to_device(ResNet9(3, 10), device)",
                "model"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "@torch.no_grad()",
                "def evaluate(model, val_loader):",
                "model.eval()",
                "ASSIGN = [model.validation_step(batch) for batch in val_loader]",
                "return model.validation_epoch_end(ASSIGN)",
                "def get_lr(optimizer):",
                "for param_group in optimizer.param_groups:",
                "return param_group['lr']",
                "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,",
                "ASSIGN=0, grad_clip=None, opt_func=torch.optim.SGD):",
                "torch.cuda.empty_cache()",
                "ASSIGN = []",
                "ASSIGN = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)",
                "ASSIGN = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs,",
                "ASSIGN=len(train_loader))",
                "for epoch in range(epochs):",
                "model.train()",
                "ASSIGN = []",
                "ASSIGN = []",
                "for batch in train_loader:",
                "ASSIGN = model.training_step(batch)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.backward()",
                "if grad_clip:",
                "nn.utils.clip_grad_value_(model.parameters(), grad_clip)",
                "ASSIGN.step()",
                "ASSIGN.zero_grad()",
                "ASSIGN.append(get_lr(ASSIGN))",
                "ASSIGN.step()",
                "ASSIGN = evaluate(model, val_loader)",
                "ASSIGN['train_loss'] = torch.stack(ASSIGN).mean().item()",
                "ASSIGN = lrs",
                "model.epoch_end(epoch, ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "return history"
            ],
            "output_type": "not_existent",
            "content_old": [
                "@torch.no_grad()",
                "def evaluate(model, val_loader):",
                "    model.eval()",
                "    outputs = [model.validation_step(batch) for batch in val_loader]",
                "    return model.validation_epoch_end(outputs)",
                "",
                "def get_lr(optimizer):",
                "    for param_group in optimizer.param_groups:",
                "        return param_group['lr']",
                "",
                "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, ",
                "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):",
                "    torch.cuda.empty_cache()",
                "    history = []",
                "    ",
                "    # Set up cutom optimizer with weight decay",
                "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)",
                "    # Set up one-cycle learning rate scheduler",
                "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, ",
                "                                                steps_per_epoch=len(train_loader))",
                "    ",
                "    for epoch in range(epochs):",
                "        # Training Phase ",
                "        model.train()",
                "        train_losses = []",
                "        lrs = []",
                "        for batch in train_loader:",
                "            loss = model.training_step(batch)",
                "            train_losses.append(loss)",
                "            loss.backward()",
                "            ",
                "            # Gradient clipping",
                "            if grad_clip: ",
                "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)",
                "            ",
                "            optimizer.step()",
                "            optimizer.zero_grad()",
                "            ",
                "            # Record & update learning rate",
                "            lrs.append(get_lr(optimizer))",
                "            sched.step()",
                "        ",
                "        # Validation phase",
                "        result = evaluate(model, val_loader)",
                "        result['train_loss'] = torch.stack(train_losses).mean().item()",
                "        result['lrs'] = lrs",
                "        model.epoch_end(epoch, result)",
                "        history.append(result)",
                "    return history"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = 8",
                "ASSIGN = 0.01",
                "ASSIGN = 0.1",
                "ASSIGN = 1e-4",
                "ASSIGN = torch.optim.Adam"
            ],
            "output_type": "not_existent",
            "content_old": [
                "epochs = 8",
                "max_lr = 0.01",
                "grad_clip = 0.1",
                "weight_decay = 1e-4",
                "opt_func = torch.optim.Adam"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)"
            ],
            "output_type": "stream",
            "content_old": [
                "%%time",
                "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_accuracies(history):",
                "ASSIGN = [x['val_acc'] for x in history]",
                "plt.plot(ASSIGN, '-x')",
                "plt.xlabel('epoch')",
                "plt.ylabel('accuracy')",
                "plt.title('Accuracy vs. No. of epochs');"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def plot_accuracies(history):",
                "    accuracies = [x['val_acc'] for x in history]",
                "    plt.plot(accuracies, '-x')",
                "    plt.xlabel('epoch')",
                "    plt.ylabel('accuracy')",
                "    plt.title('Accuracy vs. No. of epochs');"
            ]
        },
        {
            "tags": [
                "validate_data",
                "evaluate_model"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [evaluate(model, valid_dl)]",
                "history"
            ],
            "output_type": "error",
            "content_old": [
                "history = [evaluate(model, valid_dl)]",
                "history"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_accuracies(history)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plot_accuracies(history)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_losses(history):",
                "ASSIGN = [x.get('train_loss') for x in history]",
                "ASSIGN = [x['val_loss'] for x in history]",
                "plt.plot(ASSIGN, '-bx')",
                "plt.plot(ASSIGN, '-rx')",
                "plt.xlabel('epoch')",
                "plt.ylabel('loss')",
                "plt.legend(['Training', 'Validation'])",
                "plt.title('Loss vs. No. of epochs');"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def plot_losses(history):",
                "    train_losses = [x.get('train_loss') for x in history]",
                "    val_losses = [x['val_loss'] for x in history]",
                "    plt.plot(train_losses, '-bx')",
                "    plt.plot(val_losses, '-rx')",
                "    plt.xlabel('epoch')",
                "    plt.ylabel('loss')",
                "    plt.legend(['Training', 'Validation'])",
                "    plt.title('Loss vs. No. of epochs');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_losses(history)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plot_losses(history)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plot_lrs(history):",
                "ASSIGN = np.concatenate([x.get('ASSIGN', []) for x in history])",
                "plt.plot(ASSIGN)",
                "plt.xlabel('Batch no.')",
                "plt.ylabel('Learning rate')",
                "plt.title('Learning Rate vs. Batch no.');"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def plot_lrs(history):",
                "    lrs = np.concatenate([x.get('lrs', []) for x in history])",
                "    plt.plot(lrs)",
                "    plt.xlabel('Batch no.')",
                "    plt.ylabel('Learning rate')",
                "    plt.title('Learning Rate vs. Batch no.');"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plot_lrs(history)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plot_lrs(history)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "jovian.commit(project=project_name)",
                "ASSIGN='..path'",
                "jovian.log_dataset(ASSIGN=ASSIGN, val_size=val_size, random_seed=random_seed)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "!pip install jovian --upgrade --quiet",
                "import jovian",
                "jovian.commit(project=project_name)",
                "dataset_url='../input/russian-handwritten-letters'",
                "jovian.log_dataset(dataset_url=dataset_url, val_size=val_size, random_seed=random_seed)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import warnings    #warnings to ignore any kind of warnings that we may recieve.\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def display_all(df):",
                "'''",
                "input: dataframe",
                "description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen",
                "'''",
                "with pd.option_context(\"display.max_rows\",10,\"display.max_columns\",9):  #you might want to change these numbers.",
                "display(df)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def display_all(df):\n",
                "    '''\n",
                "    input: dataframe\n",
                "    description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen\n",
                "    '''\n",
                "    with pd.option_context(\"display.max_rows\",10,\"display.max_columns\",9):  #you might want to change these numbers.\n",
                "        display(df)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=pd.read_csv('..path')",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df=pd.read_csv('../input/diabetes.csv')\n",
                "df.shape"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display_all(df)"
            ],
            "output_type": "display_data",
            "content_old": [
                "display_all(df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def missing_values_table(df):",
                "ASSIGN = df.isnull().sum()",
                "ASSIGN = 100 * df.isnull().sum() path(df)",
                "ASSIGN = pd.concat([mis_val, mis_val_percent], axis=1)",
                "ASSIGN = mis_val_table.rename(",
                "ASSIGN = {0 : 'Missing Values', 1 : '% of Total Values'})",
                "ASSIGN = ASSIGN[",
                "ASSIGN.iloc[:,1] != 0].sort_values(",
                "'% of Total Values', ascending=False).round(1)",
                "print (\"Your selected dataframe has \" + str(df.shape[1]) + \" ASSIGN.\\n\"",
                "\"There are \" + str(ASSIGN.shape[0]) +",
                "\" ASSIGN that have missing values.\")",
                "return mis_val_table_ren_columns"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def missing_values_table(df):\n",
                "        # Total missing values\n",
                "        mis_val = df.isnull().sum()\n",
                "        \n",
                "        # Percentage of missing values\n",
                "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
                "        \n",
                "        # Make a table with the results\n",
                "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
                "        \n",
                "        # Rename the columns\n",
                "        mis_val_table_ren_columns = mis_val_table.rename(\n",
                "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
                "        \n",
                "        # Sort the table by percentage of missing descending\n",
                "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
                "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
                "        '% of Total Values', ascending=False).round(1)\n",
                "        \n",
                "        # Print some summary information\n",
                "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
                "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
                "              \" columns that have missing values.\")\n",
                "        \n",
                "        \n",
                "        return mis_val_table_ren_columns"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "missing_values_table(df)"
            ],
            "output_type": "stream",
            "content_old": [
                "missing_values_table(df)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=['BMI','SkinThickness','BloodPressure','Insulin','Glucose']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "features_with_missing_values=['BMI','SkinThickness','BloodPressure','Insulin','Glucose']\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in features_with_missing_values:",
                "SLICE=SLICE.replace(0,np.median(SLICE.values))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for i in features_with_missing_values:\n",
                "    df[i]=df[i].replace(0,np.median(df[i].values))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=df['Outcome'].values",
                "df.drop(['Outcome'],inplace=True,axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "target=df['Outcome'].values\n",
                "df.drop(['Outcome'],inplace=True,axis=1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=StandardScaler()",
                "ASSIGN=sta.fit_transform(df)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#from sklearn importing standard scalar that will convert the provided dataframe into standardised one.\n",
                "from sklearn.preprocessing import StandardScaler                                              \n",
                "sta=StandardScaler()\n",
                "input=sta.fit_transform(df)    #will give numpy array as output"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train,X_test,y_train,y_test=train_test_split(input,target,test_size=0.1,random_state=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test=train_test_split(input,target,test_size=0.1,random_state=0)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsClassifier"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN=KNeighborsClassifier(n_neighbors=7)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn=KNeighborsClassifier(n_neighbors=7)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "knn.fit(X_train,y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "knn.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "knn.score(X_test,y_test)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "knn.score(X_test,y_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],",
                "ASSIGN.loc[:,'MSSubClass':'SaleCondition']))",
                "ASSIGN = np.log1p(ASSIGN)",
                "ASSIGN = all_data.dtypes[all_data.dtypes != \"object\"].index",
                "ASSIGN = train[numeric_feats].apply(lambda x: skew(x.dropna()))",
                "ASSIGN = ASSIGN[ASSIGN > 0.75]",
                "ASSIGN = ASSIGN.index",
                "ASSIGN[ASSIGN] = np.log1p(ASSIGN[ASSIGN])",
                "ASSIGN = pd.get_dummies(ASSIGN)",
                "ASSIGN = ASSIGN.fillna(ASSIGN.mean())",
                "ASSIGN = all_data[:train.shape[0]]",
                "ASSIGN = all_data[train.shape[0]:]",
                "ASSIGN = train.SalePrice",
                "def rmse_cv(model):",
                "ASSIGN= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))",
                "return(ASSIGN)",
                "ASSIGN = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)",
                "rmse_cv(ASSIGN).mean()",
                "ASSIGN = pd.Series(model_lasso.coef_, index = X_train.columns)",
                "print( + str(sum(ASSIGN != 0)) + + str(sum(ASSIGN == 0)) + )",
                "ASSIGN = np.expm1(model_lasso.predict(X_test))",
                "ASSIGN = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_preds})",
                "ASSIGN.to_csv(\"ridge_sol.csv\", index = False)"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from scipy.stats import skew\n",
                "\n",
                "train = pd.read_csv(\"../input/train.csv\")\n",
                "test = pd.read_csv(\"../input/test.csv\")\n",
                "\n",
                "all_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n",
                "                      test.loc[:,'MSSubClass':'SaleCondition']))\n",
                "\n",
                "train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n",
                "numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n",
                "\n",
                "skewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
                "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
                "skewed_feats = skewed_feats.index\n",
                "\n",
                "all_data[skewed_feats] = np.log1p(all_data[skewed_feats])\n",
                "all_data = pd.get_dummies(all_data)\n",
                "all_data = all_data.fillna(all_data.mean())\n",
                "\n",
                "X_train = all_data[:train.shape[0]]\n",
                "X_test = all_data[train.shape[0]:]\n",
                "y = train.SalePrice\n",
                "\n",
                "from sklearn.linear_model import LassoCV\n",
                "from sklearn.model_selection import cross_val_score\n",
                "\n",
                "def rmse_cv(model):\n",
                "    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n",
                "    return(rmse)\n",
                "\n",
                "model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\n",
                "rmse_cv(model_lasso).mean()\n",
                "\n",
                "coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n",
                "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")\n",
                "\n",
                "lasso_preds = np.expm1(model_lasso.predict(X_test))\n",
                "\n",
                "solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_preds})\n",
                "solution.to_csv(\"ridge_sol.csv\", index = False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=pd.read_csv(\"path\")",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import pandas as pd \n",
                "trd=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "trd.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=pd.read_csv(\"path\")",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import pandas as pd \n",
                "tsd=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "tsd.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=trd.loc[trd.YrSold<2008][trd.SaleCondition=='Normal']",
                "ASSIGN= len(w)path(trd)",
                "print(,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "w=trd.loc[trd.YrSold<2008][trd.SaleCondition=='Normal']\n",
                "#print(w)\n",
                "rw= len(w)/len(trd)\n",
                "print(\"percentage of houses are for sale before 2008 are normal is : \",rw)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=trd.loc[trd.LotArea<10000][trd.SaleCondition=='Normal']",
                "ASSIGN= len(x)path(trd)",
                "print(,ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "x=trd.loc[trd.LotArea<10000][trd.SaleCondition=='Normal']\n",
                "#print(w)\n",
                "rw= len(x)/len(trd)\n",
                "print(\"percentage of houses are having area less than 10000 m^2 are normal is : \",rw)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=trd.loc[trd.MSSubClass>60]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "value=trd.loc[trd.MSSubClass>60]\n",
                "print(value)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model"
            ],
            "content": [
                "SETUP",
                "\"\"\"author    s_agnik1511\"\"\"",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = trd.SalePrice",
                "ASSIGN = ['LotArea']",
                "ASSIGN = trd[predictor_cols]",
                "ASSIGN = RandomForestRegressor()",
                "ASSIGN.fit(ASSIGN, ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "\"\"\"author    s_agnik1511\"\"\"\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "trd = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "train_y = trd.SalePrice\n",
                "predictor_cols = ['LotArea']\n",
                "train_X = trd[predictor_cols]\n",
                "my_model = RandomForestRegressor()\n",
                "my_model.fit(train_X, train_y)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "trd.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "trd.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "\"\"\"author s_agnik1511\"\"\"",
                "ASSIGN = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})",
                "ASSIGN.to_csv('submission_sagnik.csv', index=False)"
            ],
            "output_type": "error",
            "content_old": [
                "\"\"\"author s_agnik1511\"\"\"\n",
                "import pandas as pd\n",
                "my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predicted_prices})\n",
                "my_submission.to_csv('submission_sagnik.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "SETUP",
                "ASSIGN=pd.read_csv(\"path\")",
                "k.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import pandas as pd\n",
                "k=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "k.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "ASSIGN=tsd.predict(\"path\")",
                "SLICE=ASSIGN",
                "Submission.to_csv(\"submission_sagnik123\",index=False)"
            ],
            "output_type": "error",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "m=tsd.predict(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "Submission[\"taregt\"]=m\n",
                "Submission.to_csv(\"submission_sagnik123\",index=False)\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "\"\"\"author s_agnik1511\"\"\"",
                "ASSIGN=pd.read_csv('path')",
                "ASSIGN=test[predictor_cols]",
                "ASSIGN=my_model.predict(test_X)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "\"\"\"author s_agnik1511\"\"\"\n",
                "test=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\n",
                "test_X=test[predictor_cols]\n",
                "predicted_prices=my_model.predict(test_X)\n",
                "print(predicted_prices)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN=LinearRegression()",
                "ASSIGN=pd.read_csv(\"path\")",
                "ASSIGN.fit(ASSIGN)",
                "ASSIGN.predict(ASSIGN)"
            ],
            "output_type": "error",
            "content_old": [
                "from sklearn.linear_model import LinearRegression\n",
                "import pandas as pd\n",
                "Model=LinearRegression()\n",
                "x_test=pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "Model.fit(x_test)\n",
                "Model.predict(x_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "import pandas as pd # for reading the data frame\n",
                "import numpy as np # for numerical calculation\n",
                "import matplotlib.pyplot as plt # use for visualization\n",
                "import seaborn as sns   # mostly used for statistical visualization \n",
                "%matplotlib inline      # used for inline ploting\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")",
                "print(, train.shape) #  rows : 1459 columns : 81",
                "print(, test.shape)  # rows : 1459 columns : 80"
            ],
            "output_type": "stream",
            "content_old": [
                "train = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n",
                "test = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n",
                "\n",
                "print(\"Shape of train: \", train.shape) #  rows : 1459 columns : 81\n",
                "print(\"Shape of test: \", test.shape)  # rows : 1459 columns : 80"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train.head(20)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train.head(20)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.head(10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.concat((train, test))",
                "ASSIGN = df",
                "print(, ASSIGN.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "df = pd.concat((train, test)) # here we concat the test and train data set\n",
                "temp_df = df\n",
                "print(\"Shape of df: \", df.shape)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "temp_df.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "temp_df.head() # by default its selected 5 rows and all the columns"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "temp_df.tail()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "temp_df.tail() # for vewig the last five rows"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "pd.set_option(\"display.max_columns\",2000)  # used for viewing all the columns at onces",
                "pd.set_option(\"display.max_rows\",85)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# To show the all columns\n",
                "pd.set_option(\"display.max_columns\",2000)  # used for viewing all the columns at onces\n",
                "pd.set_option(\"display.max_rows\",85)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.head() "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "df.info()   # lets view the information about our data set like find the data types of our columns"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.describe() # used for finding the describtion about the data set like,  mean , standard deviation given below"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.select_dtypes(include=['int64', 'float64']).columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.select_dtypes(include=['int64', 'float64']).columns  # extracrt the columns whose dtype is intege and float"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.select_dtypes(include=['object']).columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.select_dtypes(include=['object']).columns  # find the columns whose dtype is object"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.set_index(\"Id\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Set index as Id column\n",
                "df = df.set_index(\"Id\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,9))",
                "sns.heatmap(df.isnull())"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Show the null values using heatmap\n",
                "plt.figure(figsize=(16,9))\n",
                "sns.heatmap(df.isnull())       \n",
                "\n",
                "# useing heat map we can see the missing values ... the white stripes indicates the missing values \n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.isnull().sum()   # from this we can see which columns has how many missig values  like LotFrontsge has 486 missing vales"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.isnull().sum()path[0]*100",
                "null_percent"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Get the percentages of null value\n",
                "null_percent = df.isnull().sum()/df.shape[0]*100\n",
                "null_percent\n",
                "\n",
                "\n",
                "# from this we can say LotFrontage has 16 % and Alley has 93 % missing vslues"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = null_percent[null_percent > 20].keys()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "col_for_drop = null_percent[null_percent > 20].keys() # if the null value % 20 or > 20 so need to drop it"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.drop(col_for_drop, \"columns\")",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# drop columns\n",
                "df = df.drop(col_for_drop, \"columns\")\n",
                "df.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df.isnull().sum()path[0]*100",
                "null_percent"
            ],
            "output_type": "execute_result",
            "content_old": [
                "null_percent = df.isnull().sum()/df.shape[0]*100\n",
                "null_percent # shows values which has less than 20 % missing values"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for i in df.columns:",
                "print(i +  + str(len(df[i].unique())))"
            ],
            "output_type": "stream",
            "content_old": [
                "# find the unique value count\n",
                "for i in df.columns:\n",
                "    print(i + \"\\t\" + str(len(df[i].unique())))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for i in df.columns:",
                "print(.format(i, len(df[i].unique()), df[i].unique()))"
            ],
            "output_type": "stream",
            "content_old": [
                "# find unique values of each column\n",
                "for i in df.columns:\n",
                "    print(\"Unique value of:>>> {} ({})\\n{}\\n\".format(i, len(df[i].unique()), df[i].unique()))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train[\"SalePrice\"].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Describe the target \n",
                "train[\"SalePrice\"].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,8))",
                "ASSIGN = sns.distplot(train[\"SalePrice\"])"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plot the distplot of target\n",
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"SalePrice\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(25,25))",
                "ASSIGN = sns.heatmap(train.corr(), cmap = \"coolwarm\", annot=True, linewidth=2)"
            ],
            "output_type": "display_data",
            "content_old": [
                "# correlation heatmap\n",
                "plt.figure(figsize=(25,25))\n",
                "ax = sns.heatmap(train.corr(), cmap = \"coolwarm\", annot=True, linewidth=2)\n",
                "\n",
                "# here we use piearson corelation"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train.corr()",
                "ASSIGN = hig_corr.index[abs(hig_corr[\"SalePrice\"]) >= 0.5]",
                "hig_corr_features"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# correlation heatmap of higly correlated features with SalePrice\n",
                "hig_corr = train.corr()\n",
                "hig_corr_features = hig_corr.index[abs(hig_corr[\"SalePrice\"]) >= 0.5]\n",
                "hig_corr_features\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,8))",
                "ASSIGN = sns.distplot(train[\"LotFrontage\"])"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"LotFrontage\"])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,8))",
                "ASSIGN = sns.distplot(train[\"MSSubClass\"])"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(10,8))\n",
                "bar = sns.distplot(train[\"MSSubClass\"])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [1, 2, 3, 4, 5, 6]",
                "for x in ASSIGN:",
                "print (x)"
            ],
            "output_type": "stream",
            "content_old": [
                "mylist = [1, 2, 3, 4, 5, 6]\n",
                "\n",
                "# Printing out each number one by one in a for-loop\n",
                "for x in mylist:\n",
                "    print (x)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 0",
                "for x in mylist:",
                "ASSIGN=ASSIGN+x",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Your notebook already knows about mylist. Sum its values by adding the code below this comment.\n",
                "total = 0\n",
                "\n",
                "# This is showing cumulative addition\n",
                "for x in mylist:\n",
                "    total=total+x\n",
                "    print(total)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = \"This is a test string for HCDE 530\"",
                "ASSIGN=s.split()",
                "for x in ASSIGN:",
                "print (x)"
            ],
            "output_type": "stream",
            "content_old": [
                "s = \"This is a test string for HCDE 530\"\n",
                "# Add your code below\n",
                "y=s.split()\n",
                "\n",
                "# For each loop, the word that comes out of the variable \"y\" goes on a new line\n",
                "for x in y:\n",
                "    print (x)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [1, 2, 3, 4, 5, 6]",
                "ASSIGN[3]=\"four\"",
                "print (ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Add your code here\n",
                "mylist = [1, 2, 3, 4, 5, 6]\n",
                "\n",
                "# This will replace the fourth item in this list with \"four\"\n",
                "mylist[3]=\"four\"\n",
                "print (mylist)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))",
                "ASSIGN = open('path', 'r')",
                "for f in ASSIGN:",
                "print (f.rstrip())",
                "ASSIGN.close()"
            ],
            "output_type": "stream",
            "content_old": [
                "# The code below allows you to access your Kaggle data files\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "fname = open('/kaggle/input/testtext/test.txt', 'r')\n",
                "\n",
                "# Add your code below\n",
                "for f in fname:\n",
                "    # This will print out each line\n",
                "    print (f.rstrip())\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [\"Akita\",\"Alaskan Malamute\",\"Australian shepherd\",\"Basset hound\",\"Beagle\",\"Boston terrier\",\"Bulldog\",\"Chihuahua\",\"Cocker Spaniel\",\"Collie\",\"French Bulldog\",\"Golden Retriever\",\"Great Dane\",\"Poodle\",\"Russell Terrier\",\"Scottish Terrier\",\"Siberian Husky\",\"Skye terrier\",\"Smooth Fox terrier\",\"Terrier\",\"Whippet\"]",
                "for dogs in ASSIGN:",
                "if (dogs.find(\"Terrier\") != -1):",
                "print (dogs.find(\"Terrier\"))",
                "elif (dogs.find(\"terrier\") != -1):",
                "print(dogs.find())",
                "else:",
                "print(-1)"
            ],
            "output_type": "stream",
            "content_old": [
                "dogList = [\"Akita\",\"Alaskan Malamute\",\"Australian shepherd\",\"Basset hound\",\"Beagle\",\"Boston terrier\",\"Bulldog\",\"Chihuahua\",\"Cocker Spaniel\",\"Collie\",\"French Bulldog\",\"Golden Retriever\",\"Great Dane\",\"Poodle\",\"Russell Terrier\",\"Scottish Terrier\",\"Siberian Husky\",\"Skye terrier\",\"Smooth Fox terrier\",\"Terrier\",\"Whippet\"]\n",
                "\n",
                "# Add your code below\n",
                "for dogs in dogList:\n",
                "    # If the word \"Terrier\" (uppercase) is in this item, print the character number of where it starts\n",
                "    if (dogs.find(\"Terrier\") != -1):\n",
                "        print (dogs.find(\"Terrier\"))\n",
                "     # If the word \"terrier\" (lowercase) is in this item, print the character number of where it starts\n",
                "    elif (dogs.find(\"terrier\") != -1):\n",
                "        print(dogs.find(\"terrier\"))\n",
                "    # If neither \"Terrier\" nor \"terrier\" are in the item, just print -1\n",
                "    else:\n",
                "        print(-1)\n",
                "                                               \n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [0,1,1,0,1,1,0]",
                "for num in ASSIGN:",
                "ASSIGN==1:",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "binList = [0,1,1,0,1,1,0]\n",
                "\n",
                "# Add your code below\n",
                "for num in binList:\n",
                "    # If the item in the list is \"1\" then print \"One\"\n",
                "    if num==1:\n",
                "        print(\"One\")\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for dogs in dogList:",
                "if (dogs.find(\"Bulldog\") != -1):",
                "print(dogs)"
            ],
            "output_type": "stream",
            "content_old": [
                "for dogs in dogList:\n",
                "    # If the item in dogList has \"Bulldog\" then print the name of the dog. Otherwise, don't print anything.\n",
                "    if (dogs.find(\"Bulldog\") != -1):\n",
                "        print(dogs)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = open('path', 'r')",
                "for f in ASSIGN:",
                "ASSIGN=len(f)+ASSIGN",
                "ASSIGN=len(f[0])+ASSIGN",
                "ASSIGN=(len(f.split()))+ASSIGN",
                "print('%d characters'%ASSIGN)",
                "print('%d lines'%ASSIGN)",
                "print('%d words'%ASSIGN)",
                "ASSIGN.close()"
            ],
            "output_type": "stream",
            "content_old": [
                "numChars = 0\n",
                "numLines = 0\n",
                "numWords = 0\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "# make sure to upload your test.txt file first\n",
                "fname = open('/kaggle/input/testtext/test.txt', 'r')\n",
                "\n",
                "# Add your code below to read each line in the file, count the number of characters, lines, and words\n",
                "# updating the numChars, numLines, and numWords variables.\n",
                "for f in fname:\n",
                "    \n",
                "    # Cumulative addition of the length of each word\n",
                "    numChars=len(f)+numChars\n",
                "    # Cumulative addition of the first letter in each line\n",
                "    numLines=len(f[0])+numLines\n",
                "    # Cumulative addition of items in each line's list\n",
                "    numWords=(len(f.split()))+numWords\n",
                "    \n",
                "# output code below is provided for you; you should not edit this\n",
                "\n",
                "print('%d characters'%numChars)\n",
                "print('%d lines'%numLines)\n",
                "print('%d words'%numWords)\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = open('path', 'r')",
                "for f in ASSIGN:",
                "ASSIGN=len(f)+ASSIGN",
                "ASSIGN=len(f[0])+ASSIGN",
                "ASSIGN=(len(f.split()))+ASSIGN",
                "print('%d characters'%ASSIGN)",
                "print('%d lines'%ASSIGN)",
                "print('%d words'%ASSIGN)",
                "ASSIGN.close()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Add your code below\n",
                "numChars = 0\n",
                "numLines = 0\n",
                "numWords = 0\n",
                "\n",
                "# create a file handle called fname to open and hold the contents of the data file\n",
                "# make sure to upload your test.txt file first\n",
                "fname = open('/kaggle/input/reading/sherlock.txt', 'r')\n",
                "\n",
                "# Add your code below to read each line in the file, count the number of characters, lines, and words\n",
                "# updating the numChars, numLines, and numWords variables.\n",
                "for f in fname:\n",
                "    \n",
                "    # Cumulative addition of the length of each word\n",
                "    numChars=len(f)+numChars\n",
                "    # Cumulative addition of the first letter in each line\n",
                "    numLines=len(f[0])+numLines\n",
                "    # Cumulative addition of items in each line's list\n",
                "    numWords=(len(f.split()))+numWords\n",
                "    \n",
                "# output code below is provided for you; you should not edit this\n",
                "\n",
                "print('%d characters'%numChars)\n",
                "print('%d lines'%numLines)\n",
                "print('%d words'%numWords)\n",
                "\n",
                "# It's good practice to close your file when you are finished. This is in the next line.\n",
                "fname.close()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "py.init_notebook_mode(connected=True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Importing the relevant libraries\n",
                "import IPython.display\n",
                "import pandas as pd\n",
                "import seaborn as sns\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "import plotly.graph_objs as go\n",
                "from matplotlib import pyplot as plt"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\",parse_dates=['date'])",
                "ASSIGN = pd.read_csv(\"..path\", parse_dates=['date'])"
            ],
            "output_type": "stream",
            "content_old": [
                "items = pd.read_csv(\"../input/items.csv\")\n",
                "holiday_events = pd.read_csv(\"../input/holidays_events.csv\")\n",
                "stores = pd.read_csv(\"../input/stores.csv\")\n",
                "oil = pd.read_csv(\"../input/oil.csv\")\n",
                "transactions = pd.read_csv(\"../input/transactions.csv\",parse_dates=['date'])\n",
                "train = pd.read_csv(\"../input/train.csv\", parse_dates=['date'])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(train.shape)",
                "train.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "print(train.shape)\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(train.columns.values, train.isnull().any().values))",
                "print('---')",
                "print(.format(oil.columns.values,oil.isnull().any().values))",
                "print('---')",
                "print(.format(holiday_events.columns.values,holiday_events.isnull().any().values))",
                "print('---')",
                "print(.format(stores.columns.values,stores.isnull().any().values))",
                "print('---')",
                "print(.format(transactions.columns.values,transactions.isnull().any().values))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Check for NULL values in all files\n",
                "print(\"Nulls in train: {0} => {1}\".format(train.columns.values, train.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in oil: {0} => {1}\".format(oil.columns.values,oil.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in holiday_events: {0} => {1}\".format(holiday_events.columns.values,holiday_events.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in stores: {0} => {1}\".format(stores.columns.values,stores.isnull().any().values))\n",
                "print('---')\n",
                "print(\"Nulls in transactions: {0} => {1}\".format(transactions.columns.values,transactions.isnull().any().values))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "oil.head(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# EDA for oil.csv begins here #\n",
                "oil.head(3)"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = go.Scatter(",
                "ASSIGN='Oil prices',",
                "ASSIGN=oil['date'],",
                "ASSIGN=oil['dcoilwtico'].dropna(),",
                "ASSIGN='lines',",
                ")",
                "ASSIGN = [trace]",
                "ASSIGN = go.Layout(",
                "ASSIGN = dict(title = 'Daily Oil price'),",
                "ASSIGN = True)",
                "ASSIGN = go.Figure(data = data, layout = layout)",
                "py.iplot(ASSIGN, filename='pandas-time-series-error-bars')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Plotting graph for oil price trends\n",
                "trace = go.Scatter(\n",
                "    name='Oil prices',\n",
                "    x=oil['date'],\n",
                "    y=oil['dcoilwtico'].dropna(),\n",
                "    mode='lines',\n",
                "   )\n",
                "\n",
                "data = [trace]\n",
                "\n",
                "layout = go.Layout(\n",
                "    yaxis = dict(title = 'Daily Oil price'),\n",
                "    showlegend = True)\n",
                "fig = go.Figure(data = data, layout = layout)\n",
                "py.iplot(fig, filename='pandas-time-series-error-bars')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "holiday_events.head(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# EDA for holiday_events.csv begins here #\n",
                "holiday_events.head(3)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "holiday_events.type.unique()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "holiday_events.type.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('seaborn-white')",
                "ASSIGN = holiday_events.groupby(['locale_name', 'type']).size()",
                "ASSIGN.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10), grid=False)",
                "plt.ylabel('Count of entries')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.style.use('seaborn-white')\n",
                "holiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()\n",
                "holiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10),  grid=False)\n",
                "plt.ylabel('Count of entries')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "items.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# EDA for items.csv begins here\n",
                "items.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index,",
                "items.family.value_counts().values),",
                "ASSIGN = False)))",
                "ASSIGN = go.Bar(",
                "ASSIGN = items.family.value_counts().values,",
                "ASSIGN = items.family.value_counts().index",
                ")",
                "ASSIGN = dict(",
                "ASSIGN='Counts of items per family category',",
                "ASSIGN = 900, height = 600,",
                "ASSIGN=dict(",
                "ASSIGN = True,",
                "ASSIGN = True,",
                "ASSIGN = True",
                "))",
                "ASSIGN = go.Figure(data=[trace2])",
                "ASSIGN['ASSIGN'].update(ASSIGN)",
                "py.iplot(ASSIGN, filename='plots')"
            ],
            "output_type": "display_data",
            "content_old": [
                "# BAR PLOT FOR ITEMS V/S FAMILY TYPE\n",
                "x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, \n",
                "                                         items.family.value_counts().values), \n",
                "                                        reverse = False)))\n",
                "trace2 = go.Bar(\n",
                "    y = items.family.value_counts().values,\n",
                "    x = items.family.value_counts().index\n",
                ")\n",
                "\n",
                "layout = dict(\n",
                "    title='Counts of items per family category',\n",
                "     width = 900, height = 600,\n",
                "    yaxis=dict(\n",
                "        showgrid = True,\n",
                "        showline = True,\n",
                "        showticklabels = True\n",
                "    ))\n",
                "\n",
                "fig1 = go.Figure(data=[trace2])\n",
                "fig1['layout'].update(layout)\n",
                "py.iplot(fig1, filename='plots')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('seaborn-white')",
                "ASSIGN = items.groupby(['family', 'perishable']).size()",
                "ASSIGN.unstack().plot(kind='bar',stacked=True, colormap = 'coolwarm', figsize=(12,10), grid = True)",
                "plt.ylabel('count')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Persihable or not \n",
                "plt.style.use('seaborn-white')\n",
                "fam_perishable = items.groupby(['family', 'perishable']).size()\n",
                "fam_perishable.unstack().plot(kind='bar',stacked=True, colormap = 'coolwarm', figsize=(12,10),  grid = True)\n",
                "plt.ylabel('count')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores.head(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# EDA for stores.csv begins here #\n",
                "stores.head(3)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots()",
                "fig.set_size_inches(8, 8)",
                "ASSIGN = sns.countplot(y = stores['state'], data = stores)"
            ],
            "output_type": "display_data",
            "content_old": [
                "# store distribution across states\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 8)\n",
                "ax = sns.countplot(y = stores['state'], data = stores) "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots()",
                "fig.set_size_inches(8, 8)",
                "ASSIGN = sns.countplot(y = stores['city'], data = stores)"
            ],
            "output_type": "display_data",
            "content_old": [
                "# store distribution across cities\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 8)\n",
                "ax = sns.countplot(y = stores['city'], data = stores) "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores.state.unique()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Unique state names\n",
                "stores.state.unique()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores.city.unique()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Unique state names\n",
                "stores.city.unique()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots()",
                "fig.set_size_inches(8, 5)",
                "ASSIGN = sns.countplot(x = \"type\", data = stores, palette=\"Paired\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Various types of stores and their count\n",
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(8, 5)\n",
                "ax = sns.countplot(x = \"type\", data = stores, palette=\"Paired\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.crosstab(stores.state, stores.type)",
                "ASSIGN.plot.bar(figsize = (12, 6), stacked=True)",
                "plt.legend(title='type')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "ct = pd.crosstab(stores.state, stores.type)\n",
                "ct.plot.bar(figsize = (12, 6), stacked=True)\n",
                "plt.legend(title='type')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.crosstab(stores.city, stores.type)",
                "ASSIGN.plot.bar(figsize = (12, 6), stacked=True)",
                "plt.legend(title='type')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "ct = pd.crosstab(stores.city, stores.type)\n",
                "\n",
                "ct.plot.bar(figsize = (12, 6), stacked=True)\n",
                "plt.legend(title='type')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores.store_nbr.nunique()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# total no. of unique stores \n",
                "stores.store_nbr.nunique()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "stores.cluster.sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# total no. of stores (including non-unique)\n",
                "stores.cluster.sum()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots()",
                "fig.set_size_inches(12, 7)",
                "ASSIGN = sns.countplot(x = \"cluster\", data = stores)"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig, ax = plt.subplots()\n",
                "fig.set_size_inches(12, 7)\n",
                "ax = sns.countplot(x = \"cluster\", data = stores)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "transactions.head(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# EDA for transactions.csv begins here #\n",
                "transactions.head(3)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "print(\"There are {0} transactions\".",
                "format(transactions.shape[0], transactions.shape[1]))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Finding out the no.of transactions (rows)\n",
                "print(\"There are {0} transactions\".\n",
                "      format(transactions.shape[0], transactions.shape[1]))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.style.use('seaborn-white')",
                "plt.figure(figsize=(13,11))",
                "plt.plot(transactions.date.values, transactions.transactions.values, color='grey')",
                "plt.axvline(x='2015-12-23',color='red',alpha=0.3)",
                "plt.axvline(x='2016-12-23',color='red',alpha=0.3)",
                "plt.axvline(x='2014-12-23',color='red',alpha=0.3)",
                "plt.axvline(x='2013-12-23',color='red',alpha=0.3)",
                "plt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--')",
                "plt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--')",
                "plt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--')",
                "plt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--')",
                "plt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--')",
                "plt.ylabel('Transactions per day', fontsize= 16)",
                "plt.xlabel('Date', fontsize= 16)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Time series plot for transaction #\n",
                "plt.style.use('seaborn-white')\n",
                "plt.figure(figsize=(13,11))\n",
                "plt.plot(transactions.date.values, transactions.transactions.values, color='grey')\n",
                "plt.axvline(x='2015-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2016-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2014-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2013-12-23',color='red',alpha=0.3)\n",
                "plt.axvline(x='2013-05-12',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2015-05-10',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2016-05-08',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2014-05-11',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.axvline(x='2017-05-14',color='green',alpha=0.2, linestyle= '--')\n",
                "plt.ylabel('Transactions per day', fontsize= 16)\n",
                "plt.xlabel('Date', fontsize= 16)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(check_output([, ]).decode())"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "sns.set_style(\"whitegrid\")",
                "plt.style.use(\"fivethirtyeight\")",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "%matplotlib inline",
                "import seaborn as sns",
                "sns.set_style(\"whitegrid\")",
                "plt.style.use(\"fivethirtyeight\")",
                "",
                "#Showing full path of datasets",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "        ",
                "",
                "#Disable warnings",
                "import warnings",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv(\"/kaggle/input/weather-dataset-rattle-package/weatherAUS.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Number of rows and columns in our dataset",
                "df.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.columns"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#The 24 columns ",
                "df.columns"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.drop(['RISK_MM'],axis=1,inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#As mentioned in the dataset description , ",
                "#we should exclude the variable Risk-MM when training a binary classification model.",
                "#Not excluding it will leak the answers to your model and reduce its predictability.",
                "",
                "df.drop(['RISK_MM'],axis=1,inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "#Basic Information of dataset",
                "",
                "df.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']",
                "df.drop(columns=ASSIGN,inplace=True,axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Before looking at the description of the data",
                "#We can see that there are few columns with very less data",
                "#Evaporation,Sunshine,Cloud9am,Cloud3pm",
                "#It is better to remove these four columns as it will affect our prediction even if we",
                "#fill the na values...",
                "",
                "#Date and Location is also not required",
                "#As we are predicting rain in australia and not when and where in australia",
                "",
                "",
                "drop_cols = ['Evaporation','Sunshine','Cloud9am','Cloud3pm','Date','Location']",
                "",
                "df.drop(columns=drop_cols,inplace=True,axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "df.info()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Basic description of our data",
                "#Numerical features first",
                "df.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.describe(include='object')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Including Categorical features with include object",
                "df.describe(include='object')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.describe(include='all')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Now including all the features",
                "df.describe(include='all')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isna().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Our dataset consists of 142193 rows and the count for many features is less than 142193.",
                "#This shows presence of Null values.",
                "#Let's look at the null values..",
                "",
                "df.isna().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.skew()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.skew()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [col for col in df.columns if df[col].dtype==\"float64\"]",
                "for col in ASSIGN:",
                "df[col].fillna(df[col].median(),inplace=True)",
                "ASSIGN = [col for col in df.columns if df[col].dtype==\"O\"]",
                "for col in ASSIGN:",
                "df[col].fillna(df[col].mode()[0],inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Filling missing values",
                "",
                "#We can see that there are outliers in our data",
                "#So the best way to fill the na values in our numerical features is with median",
                "#Because median deals the best with outliers",
                "",
                "#Let's separate numerical and categorical",
                "#data type of numerical features is equal to float64",
                "#With the help of following list comprehension we separate the numerical features...",
                "",
                "num = [col for col in df.columns if df[col].dtype==\"float64\"]",
                "",
                "for col in num:",
                "    df[col].fillna(df[col].median(),inplace=True)",
                "    ",
                "cat = [col for col in df.columns if df[col].dtype==\"O\"]",
                "for col in cat:",
                "    df[col].fillna(df[col].mode()[0],inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isna().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Check missing values",
                "df.isna().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.corr().style.background_gradient(cmap=\"Reds\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.corr().style.background_gradient(cmap=\"Reds\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = plt.figure(figsize=(12,12))",
                "sns.heatmap(ASSIGN,annot=True,fmt=\".1f\",linewidths=\"0.1\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#With the use of heatmap",
                "corr = df.corr()",
                "",
                "fig = plt.figure(figsize=(12,12))",
                "sns.heatmap(corr,annot=True,fmt=\".1f\",linewidths=\"0.1\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(num))",
                "print(.format(len(num)))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"Numerical features :: {}\\n\".format(num))",
                "print(\"No of Numerical features :: {}\".format(len(num)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,15))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN=1",
                "ASSIGN = ['Red','Blue','Green','Cyan',",
                "'Red','Blue','Green','Cyan',",
                "'Red','Blue','Green','Cyan']",
                "ASSIGN=0",
                "for col in num:",
                "plt.subplot(3,4,ASSIGN)",
                "ASSIGN = sns.distplot(df[col],color=colors[j])",
                "ASSIGN+=1",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,15))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "colors = ['Red','Blue','Green','Cyan',",
                "         'Red','Blue','Green','Cyan',",
                "         'Red','Blue','Green','Cyan']",
                "j=0",
                "for col in num:",
                "    plt.subplot(3,4,i)",
                "    a1 = sns.distplot(df[col],color=colors[j])",
                "    i+=1",
                "    j+=1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN=1",
                "for col in num:",
                "plt.subplot(6,2,ASSIGN)",
                "ASSIGN = sns.boxplot(data=df,x=\"RainTomorrow\",y=col)",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "for col in num:",
                "    plt.subplot(6,2,i)",
                "    a1 = sns.boxplot(data=df,x=\"RainTomorrow\",y=col)",
                "    i+=1"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "for col in ASSIGN:",
                "Lower_Bound = df[col].quantile(0.25) - (IQR*3)",
                "Upper_Bound = df[col].quantile(0.75) + (IQR*3)",
                "print(.format(col,Lower_Bound,Upper_Bound))",
                "ASSIGN = df[col].min()",
                "ASSIGN = df[col].max()",
                "print(.format(col,ASSIGN,ASSIGN))",
                "if ASSIGN>Upper_Bound:",
                "print(.format(col,Upper_Bound))",
                "elif ASSIGN<Lower_Bound:",
                "print(.format(col,Lower_Bound))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Create a loop that finds the outliers in train and test  and removes it",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "",
                "for col in features_to_examine:",
                "    IQR = df[col].quantile(0.75) - df[col].quantile(0.25) ",
                "    Lower_Bound = df[col].quantile(0.25) - (IQR*3)",
                "    Upper_Bound = df[col].quantile(0.75) + (IQR*3)",
                "    ",
                "    print(\"The outliers in {} feature are values <<< {} and >>> {}\".format(col,Lower_Bound,Upper_Bound))",
                "    ",
                "    minimum = df[col].min()",
                "    maximum = df[col].max()",
                "    print(\"The minimum value in {} is {} and maximum value is {}\".format(col,minimum,maximum))",
                "    ",
                "    if maximum>Upper_Bound:",
                "          print(\"The outliers for {} are value greater than {}\\n\".format(col,Upper_Bound))",
                "    elif minimum<Lower_Bound:",
                "          print(\"The outliers for {} are value smaller than {}\\n\".format(col,Lower_Bound))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN=1",
                "for col in num:",
                "plt.subplot(6,2,ASSIGN)",
                "ASSIGN = sns.barplot(data=df,x=\"RainTomorrow\",y=col)",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "for col in num:",
                "    plt.subplot(6,2,i)",
                "    a1 = sns.barplot(data=df,x=\"RainTomorrow\",y=col)",
                "    i+=1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,5))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN=1",
                "ASSIGN = [\"MaxTemp\",\"Temp9am\",\"Temp3pm\"]",
                "for feature in ASSIGN:",
                "plt.subplot(1,3,ASSIGN)",
                "sns.scatterplot(data=df,x=\"MinTemp\",y=feature,hue=\"RainTomorrow\")",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(15,5))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "i=1",
                "features_list = [\"MaxTemp\",\"Temp9am\",\"Temp3pm\"]",
                "for feature in features_list:",
                "    plt.subplot(1,3,i)",
                "    sns.scatterplot(data=df,x=\"MinTemp\",y=feature,hue=\"RainTomorrow\")",
                "    i+=1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,8))",
                "plt.subplots_adjust(hspace=0.5)",
                "plt.subplot(3,2,1)",
                "sns.scatterplot(data=df,x=\"WindSpeed9am\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "plt.subplot(3,2,2)",
                "sns.scatterplot(data=df,x=\"WindSpeed3pm\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "plt.subplot(3,2,3)",
                "sns.scatterplot(data=df,x=\"Humidity9am\",y=\"Humidity3pm\",hue=\"RainTomorrow\")",
                "plt.subplot(3,2,4)",
                "sns.scatterplot(data=df,x=\"Temp9am\",y=\"Temp3pm\",hue=\"RainTomorrow\")",
                "plt.subplot(3,2,5)",
                "sns.scatterplot(data=df,x=\"MaxTemp\",y=\"Temp9am\",hue=\"RainTomorrow\")",
                "plt.subplot(3,2,6)",
                "sns.scatterplot(data=df,x=\"Humidity3pm\",y=\"Temp3pm\",hue=\"RainTomorrow\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(15,8))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "plt.subplot(3,2,1)",
                "sns.scatterplot(data=df,x=\"WindSpeed9am\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,2)",
                "sns.scatterplot(data=df,x=\"WindSpeed3pm\",y=\"WindGustSpeed\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,3)",
                "sns.scatterplot(data=df,x=\"Humidity9am\",y=\"Humidity3pm\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,4)",
                "sns.scatterplot(data=df,x=\"Temp9am\",y=\"Temp3pm\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,5)",
                "sns.scatterplot(data=df,x=\"MaxTemp\",y=\"Temp9am\",hue=\"RainTomorrow\")",
                "",
                "plt.subplot(3,2,6)",
                "sns.scatterplot(data=df,x=\"Humidity3pm\",y=\"Temp3pm\",hue=\"RainTomorrow\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "cat"
            ],
            "output_type": "execute_result",
            "content_old": [
                "cat"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindGustDir'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['WindGustDir'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindGustDir\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindGustDir\",hue=\"RainTomorrow\");"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindDir9am'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['WindDir9am'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir9am\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir9am\",hue=\"RainTomorrow\");"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['WindDir3pm'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['WindDir3pm'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir3pm\",hue=\"RainTomorrow\");"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig = plt.figure(figsize=(15,5))",
                "sns.countplot(data=df,x=\"WindDir3pm\",hue=\"RainTomorrow\");"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['RainTomorrow'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['RainTomorrow'].value_counts()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(data=df,x=\"RainTomorrow\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(data=df,x=\"RainTomorrow\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=df[['RainTomorrow']]",
                "X=df.drop(['RainTomorrow'],axis=1)",
                "X_train,X_test,y_train,y_test = tts(X,ASSIGN,test_size=0.3,random_state=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split as tts",
                "y=df[['RainTomorrow']]",
                "X=df.drop(['RainTomorrow'],axis=1)",
                "",
                "X_train,X_test,y_train,y_test = tts(X,y,test_size=0.3,random_state=0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_train"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_train"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_test"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "ASSIGN=1",
                "for col in ASSIGN:",
                "plt.subplot(6,2,ASSIGN)",
                "ASSIGN = df[col].hist(bins=10)",
                "ASSIGN.set_xlabel(col)",
                "ASSIGN.set_ylabel('RainTomorrow')",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "#We'll plot these four as subplots ",
                "",
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "i=1",
                "for col in features_to_examine:",
                "    plt.subplot(6,2,i)",
                "    fig = df[col].hist(bins=10)",
                "    fig.set_xlabel(col)",
                "    fig.set_ylabel('RainTomorrow')",
                "    i+=1"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def remove_outliers(df,col,Lower_Bound,Upper_Bound):",
                "ASSIGN = df[col].min()",
                "ASSIGN = df[col].max()",
                "if ASSIGN>Upper_Bound:",
                "return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])",
                "elif ASSIGN<Lower_Bound:",
                "return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def remove_outliers(df,col,Lower_Bound,Upper_Bound):    ",
                "    minimum = df[col].min()",
                "    maximum = df[col].max()",
                "    ",
                "    if maximum>Upper_Bound:",
                "        return np.where(df[col]>Upper_Bound,Upper_Bound,df[col])",
                "          ",
                "    elif minimum<Lower_Bound:",
                "        return np.where(df[col]<Lower_Bound,Lower_Bound,df[col])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for df1 in [X_train,X_test]:",
                "df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)",
                "df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)",
                "df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)",
                "df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for df1 in [X_train,X_test]:",
                "    df1['Rainfall'] = remove_outliers(df1,'Rainfall',-1.799,2.4)",
                "    df1['WindGustSpeed'] = remove_outliers(df1,'WindGustSpeed',-14.0,91.0)",
                "    df1['WindSpeed9am'] = remove_outliers(df1,'WindSpeed9am',-29.0,55.0)",
                "    df1['WindSpeed3pm'] = remove_outliers(df1,'WindSpeed3pm',-20.0,57.0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "ASSIGN = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "ASSIGN=1",
                "for col in ASSIGN:",
                "plt.subplot(6,2,ASSIGN)",
                "ASSIGN = sns.boxplot(data=X_train,y=col)",
                "ASSIGN.set_xlabel(col)",
                "ASSIGN.set_ylabel('RainTomorrow')",
                "ASSIGN+=1"
            ],
            "output_type": "display_data",
            "content_old": [
                "#If we look at their boxplots we can see that the outliers are now capped...",
                "plt.figure(figsize=(15,30))",
                "plt.subplots_adjust(hspace=0.5)",
                "",
                "features_to_examine = ['Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm']",
                "i=1",
                "for col in features_to_examine:",
                "    plt.subplot(6,2,i)",
                "    fig = sns.boxplot(data=X_train,y=col)",
                "    fig.set_xlabel(col)",
                "    fig.set_ylabel('RainTomorrow')",
                "    i+=1"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train[features_to_examine].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Describe helps us understand more about the mean and max values",
                "",
                "X_train[features_to_examine].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test[features_to_examine].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_test[features_to_examine].describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for df2 in [y_train,y_test]:",
                "df2['RainTomorrow'] = df2['RainTomorrow'].replace({\"Yes\":1,",
                "\"No\":0})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Our next step is to encode all the categorical variables.",
                "#first we will convert our target variable",
                "",
                "for df2 in [y_train,y_test]:",
                "    df2['RainTomorrow'] = df2['RainTomorrow'].replace({\"Yes\":1,",
                "                                                    \"No\":0})",
                "",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ce.BinaryEncoder(cols=['RainToday'])",
                "ASSIGN = encoder.fit_transform(ASSIGN)",
                "ASSIGN = encoder.transform(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import category_encoders as ce",
                "",
                "encoder = ce.BinaryEncoder(cols=['RainToday'])",
                "",
                "X_train = encoder.fit_transform(X_train)",
                "",
                "X_test = encoder.transform(X_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.concat([ASSIGN[num],ASSIGN[['RainToday_0','RainToday_1']],",
                "pd.get_dummies(ASSIGN['WindGustDir']),",
                "pd.get_dummies(ASSIGN['WindDir9am']),",
                "pd.get_dummies(ASSIGN['WindDir3pm'])],axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Now we will make our training dataset",
                "",
                "X_train = pd.concat([X_train[num],X_train[['RainToday_0','RainToday_1']],",
                "                    pd.get_dummies(X_train['WindGustDir']),",
                "                    pd.get_dummies(X_train['WindDir9am']),",
                "                    pd.get_dummies(X_train['WindDir3pm'])],axis=1)",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_train.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.concat([ASSIGN[num],ASSIGN[['RainToday_0','RainToday_1']],",
                "pd.get_dummies(ASSIGN['WindGustDir']),",
                "pd.get_dummies(ASSIGN['WindDir9am']),",
                "pd.get_dummies(ASSIGN['WindDir3pm'])],axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Same for testing set",
                "",
                "X_test = pd.concat([X_test[num],X_test[['RainToday_0','RainToday_1']],",
                "                    pd.get_dummies(X_test['WindGustDir']),",
                "                    pd.get_dummies(X_test['WindDir9am']),",
                "                    pd.get_dummies(X_test['WindDir3pm'])],axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_test.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = X_train.columns",
                "ASSIGN = MinMaxScaler()",
                "ASSIGN = scaler.fit_transform(ASSIGN)",
                "ASSIGN = scaler.transform(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#our training and testing set is ready for our model",
                "#But ,before that we need to bring all the features to same scale with feature scaling",
                "#For this we will use MinMaxScaler",
                "#As there our negative values in our dataset and MinMaxScaler scales our data in range -1 to 1.",
                "",
                "cols = X_train.columns",
                "",
                "from sklearn.preprocessing import MinMaxScaler",
                "",
                "scaler = MinMaxScaler()",
                "X_train = scaler.fit_transform(X_train)",
                "X_test = scaler.transform(X_test)",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame(ASSIGN,columns=cols)",
                "ASSIGN = pd.DataFrame(ASSIGN,columns=cols)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train = pd.DataFrame(X_train,columns=cols)",
                "X_test = pd.DataFrame(X_test,columns=cols)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = LogisticRegression(solver='liblinear', random_state=0)",
                "ASSIGN.fit(X_train, y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.linear_model import LogisticRegression",
                "",
                "# instantiate the model",
                "logreg = LogisticRegression(solver='liblinear', random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg.fit(X_train, y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = logreg.predict(X_test)",
                "y_pred_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg.predict(X_test)",
                "",
                "y_pred_test"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "logreg.predict_proba(X_test)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#using predict_proba gives the probability value for the target feature",
                "",
                "logreg.predict_proba(X_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "logreg.predict_proba(X_test)[:,0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#probability of getting no rain (0)",
                "",
                "logreg.predict_proba(X_test)[:,0]"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "logreg.predict_proba(X_test)[:,1]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#probability of getting rain (1)",
                "",
                "logreg.predict_proba(X_test)[:,1]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = accuracy_score(y_test,y_pred_test)",
                "print(.format(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Check accuracy with accuracy_score",
                "",
                "from sklearn.metrics import accuracy_score",
                "",
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = ASSIGN(y_test, y_pred_test)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "#Creating confusion matrix",
                "",
                "from sklearn.metrics import confusion_matrix",
                "",
                "confusion_matrix = confusion_matrix(y_test, y_pred_test)",
                "print(confusion_matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(classification_report(y_test, y_pred_test))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Classification report",
                "from sklearn.metrics import classification_report",
                "",
                "print(classification_report(y_test, y_pred_test))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = logreg.predict(X_train)",
                "y_pred_train"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Comparing train and test accuracy",
                "",
                "y_pred_train = logreg.predict(X_train)",
                "y_pred_train"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = accuracy_score(y_train,y_pred_train)",
                "print(.format(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Check accuracy of our model with train set",
                "",
                "predict_train = accuracy_score(y_train,y_pred_train)",
                "print(\"Accuracy of our model on train set :: {}\".format(predict_train))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(logreg.score(X_test,y_test)))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg.score(X_test,y_test)))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = LogisticRegression(solver='liblinear',C=100, random_state=0)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = logreg100.predict(X_test)",
                "y_pred_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#C=100",
                "",
                "# instantiate the model",
                "logreg100 = LogisticRegression(solver='liblinear',C=100, random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg100.fit(X_train, y_train)",
                "",
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg100.predict(X_test)",
                "",
                "y_pred_test"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = accuracy_score(y_test,y_pred_test)",
                "print(.format(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(logreg100.score(X_test,y_test)))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg100.score(X_test,y_test)))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = ASSIGN(y_test, y_pred_test)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "#Confusion matrix",
                "from sklearn.metrics import confusion_matrix",
                "",
                "confusion_matrix = confusion_matrix(y_test, y_pred_test)",
                "print(confusion_matrix)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(classification_report(y_test, y_pred_test))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Classification report",
                "print(classification_report(y_test, y_pred_test))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = LogisticRegression(solver='liblinear',C=0.01, random_state=0)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = logreg001.predict(X_test)",
                "y_pred_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Let's increase the regularization strength",
                "",
                "#C=0.01",
                "",
                "# instantiate the model",
                "logreg001 = LogisticRegression(solver='liblinear',C=0.01, random_state=0)",
                "",
                "",
                "# fit the model",
                "logreg001.fit(X_train, y_train)",
                "",
                "#Prediction on Xtest",
                "",
                "y_pred_test = logreg001.predict(X_test)",
                "",
                "y_pred_test"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = accuracy_score(y_test,y_pred_test)",
                "print(.format(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "predict_test = accuracy_score(y_test,y_pred_test)",
                "",
                "print(\"Accuracy of model on test set :: {}\".format(predict_test))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(.format(logreg001.score(X_test,y_test)))"
            ],
            "output_type": "stream",
            "content_old": [
                "#Overall Accuracy",
                "",
                "print(\"Accuracy of our model :: {}\".format(logreg001.score(X_test,y_test)))"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = logreg100.predict_proba(X_test)[:, 1]",
                "ASSIGN = logreg100.predict_proba(X_test)[:, 0]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# store the predicted probabilities for class 1 - Probability of rain",
                "",
                "y_pred1 = logreg100.predict_proba(X_test)[:, 1]",
                "y_pred0 = logreg100.predict_proba(X_test)[:, 0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.rcParams['font.size'] = 12",
                "plt.hist(y_pred1, bins = 10)",
                "plt.hist(y_pred0, bins = 10)",
                "plt.title('Histogram of predicted probabilities')",
                "plt.xlim(0,1)",
                "plt.legend('upper left' , labels = ['Rain','No Rain'])",
                "plt.xlabel('Predicted probabilities')",
                "plt.ylabel('Frequency')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# plot histogram of predicted probabilities",
                "",
                "",
                "# adjust the font size ",
                "plt.rcParams['font.size'] = 12",
                "",
                "",
                "# plot histogram with 10 bins",
                "plt.hist(y_pred1, bins = 10)",
                "plt.hist(y_pred0, bins = 10)",
                "",
                "# set the title of predicted probabilities",
                "plt.title('Histogram of predicted probabilities')",
                "",
                "",
                "# set the x-axis limit",
                "plt.xlim(0,1)",
                "",
                "#Set legend",
                "plt.legend('upper left' , labels = ['Rain','No Rain'])",
                "",
                "# set the title",
                "plt.xlabel('Predicted probabilities')",
                "plt.ylabel('Frequency')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")",
                "init_notebook_mode(connected=True)",
                "matplotlib.rc('font', size=20)",
                "matplotlib.rc('axes', titlesize=20)",
                "matplotlib.rc('axes', labelsize=20)",
                "matplotlib.rc('xtick', labelsize=20)",
                "matplotlib.rc('ytick', labelsize=20)",
                "matplotlib.rc('legend', fontsize=20)",
                "matplotlib.rc('figure', titlesize=20)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import gc",
                "import warnings",
                "warnings.filterwarnings(\"ignore\")",
                "",
                "import pandas as pd",
                "import numpy as np",
                "from IPython.display import display",
                "from IPython.core.display import HTML",
                "import plotly.plotly as py",
                "from plotly.offline import init_notebook_mode, iplot",
                "init_notebook_mode(connected=True)",
                "import plotly.graph_objs as go",
                "import matplotlib.pyplot as plt",
                "import matplotlib",
                "#matplotlib.rc['font.size'] = 9.0",
                "matplotlib.rc('font', size=20)",
                "matplotlib.rc('axes', titlesize=20)",
                "matplotlib.rc('axes', labelsize=20)",
                "matplotlib.rc('xtick', labelsize=20)",
                "matplotlib.rc('ytick', labelsize=20)",
                "matplotlib.rc('legend', fontsize=20)",
                "matplotlib.rc('figure', titlesize=20)",
                "import seaborn as sns",
                "",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "def file_len(fname):",
                "ASSIGN = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE,",
                "ASSIGN=subprocess.PIPE)",
                "ASSIGN = p.communicate()",
                "if ASSIGN.returncode != 0:",
                "raise IOError(err)",
                "return int(result.strip().split()[0])",
                "ASSIGN = file_len('..path')",
                "print('Number of ASSIGN in is:', ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import subprocess",
                "#from https://stackoverflow.com/questions/845058/how-to-get-line-count-cheaply-in-python , Olafur's answer",
                "def file_len(fname):",
                "    p = subprocess.Popen(['wc', '-l', fname], stdout=subprocess.PIPE, ",
                "                                              stderr=subprocess.PIPE)",
                "    result, err = p.communicate()",
                "    if p.returncode != 0:",
                "        raise IOError(err)",
                "    return int(result.strip().split()[0])",
                "",
                "lines = file_len('../input/data.csv')",
                "print('Number of lines in \"train.csv\" is:', lines)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)",
                "ASSIGN=np.sort(ASSIGN)",
                "print('lines to skip:', len(ASSIGN))",
                "ASSIGN = pd.read_csv(\"..path\", skiprows=skiplines)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False)",
                "skiplines=np.sort(skiplines)",
                "print('lines to skip:', len(skiplines))",
                "",
                "data = pd.read_csv(\"../input/data.csv\", skiprows=skiplines)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data.sample(5)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.isnull().sum(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data.isnull().sum(0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN={",
                "1:\"Jan\",",
                "2:\"Feb\",",
                "3:\"Mar\",",
                "4:\"Apr\",",
                "5:\"May\",",
                "6:\"June\",",
                "7:\"July\",",
                "8:\"Aug\",",
                "9:\"Sept\",",
                "10:\"Oct\",",
                "11:\"Nov\",",
                "12:\"Dec\"",
                "}",
                "ASSIGN = data.month.apply(lambda x: ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Just a helper module to make visualizations more intuitive",
                "num_to_month={",
                "    1:\"Jan\",",
                "    2:\"Feb\",",
                "    3:\"Mar\",",
                "    4:\"Apr\",",
                "    5:\"May\",",
                "    6:\"June\",",
                "    7:\"July\",",
                "    8:\"Aug\",",
                "    9:\"Sept\",",
                "    10:\"Oct\",",
                "    11:\"Nov\",",
                "    12:\"Dec\"",
                "}",
                "data['month'] = data.month.apply(lambda x: num_to_month[x])"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = data.pivot_table(index='year', columns='month', values='day', aggfunc=len)",
                "ASSIGN = [\"#8B8B00\", \"#8B7E66\", \"#EE82EE\", \"#00C78C\",",
                "\"#00E5EE\", \"#FF6347\", \"#EED2EE\",",
                "\"#63B8FF\", \"#00FF7F\", \"#B9D3EE\",",
                "\"#836FFF\", \"#7D26CD\"]",
                "ASSIGN.loc[:,['Jan','Feb', 'Mar',",
                "'Apr','May','June',",
                "'July','Aug','Sept',",
                "'Oct','Nov','Dec']].plot.bar(stacked=True, figsize=(20,10), color=ASSIGN)",
                "plt.xlabel(\"Years\")",
                "plt.ylabel(\"Ridership\")",
                "plt.legend(loc=10)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pivot = data.pivot_table(index='year', columns='month', values='day', aggfunc=len)",
                "colors = [\"#8B8B00\", \"#8B7E66\", \"#EE82EE\", \"#00C78C\", ",
                "          \"#00E5EE\", \"#FF6347\", \"#EED2EE\", ",
                "          \"#63B8FF\", \"#00FF7F\", \"#B9D3EE\", ",
                "          \"#836FFF\", \"#7D26CD\"]",
                "pivot.loc[:,['Jan','Feb', 'Mar',",
                "            'Apr','May','June',",
                "            'July','Aug','Sept',",
                "            'Oct','Nov','Dec']].plot.bar(stacked=True, figsize=(20,10), color=colors)",
                "plt.xlabel(\"Years\")",
                "plt.ylabel(\"Ridership\")",
                "plt.legend(loc=10)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,2, figsize=(20,7))",
                "ASSIGN = ['",
                "ASSIGN = ax[0].ASSIGN(list(data['gender'].value_counts()),",
                "ASSIGN=list(data.gender.unique()),",
                "ASSIGN='%1.1f%%', shadow=True, startangle=90, colors=colors)",
                "ASSIGN = sns.countplot(x='usertype', data=data, ax=ax[1], color='g', alpha=0.75)",
                "ax[0].set_title(\"Gender Distribution in Ridership\")",
                "ax[1].set_xlabel(\"Type of Rider\")",
                "ax[1].set_ylabel(\"Ridership\")",
                "ax[1].set_title(\"Type of Customers\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "f, ax = plt.subplots(1,2, figsize=(20,7))",
                "colors = ['#66b3ff','#ff9999']",
                "pie = ax[0].pie(list(data['gender'].value_counts()), ",
                "                   labels=list(data.gender.unique()),",
                "                  autopct='%1.1f%%', shadow=True, startangle=90, colors=colors)",
                "count = sns.countplot(x='usertype', data=data, ax=ax[1], color='g', alpha=0.75)",
                "ax[0].set_title(\"Gender Distribution in Ridership\")",
                "ax[1].set_xlabel(\"Type of Rider\")",
                "ax[1].set_ylabel(\"Ridership\")",
                "ax[1].set_title(\"Type of Customers\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "data.usertype.value_counts()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data.usertype.value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = data[['from_station_name','latitude_start','longitude_start']].drop_duplicates(subset='from_station_name')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "station_info = data[['from_station_name','latitude_start','longitude_start']].drop_duplicates(subset='from_station_name')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "station_info.sample(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "station_info.sample(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = list(station_info.latitude_start)",
                "ASSIGN = [str(i) for i in ASSIGN]",
                "ASSIGN = list(station_info.longitude_start)",
                "ASSIGN = [str(i) for i in ASSIGN]",
                "ASSIGN = list(station_info.from_station_name)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lat_list = list(station_info.latitude_start)",
                "lat_list = [str(i) for i in lat_list]",
                "lon_list = list(station_info.longitude_start)",
                "lon_list = [str(i) for i in lon_list]",
                "names = list(station_info.from_station_name)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(HTML(\"\"\"",
                "<div>",
                "<a href=\"https:path~sominwpath?share_key=y6irxkKqSVolnuF0l4w420\" target=\"_blank\" title=\"Chicago Cycle Sharing Stations\" style=\"display: block; text-align: center;\"><img src=\"https:path~sominwpath?share_key=y6irxkKqSVolnuF0l4w420\" alt=\"Chicago Cycle Sharing Stations\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https:path';\" path><path>",
                "<script data-plotly=\"sominw:6\" sharekey-plotly=\"y6irxkKqSVolnuF0l4w420\" src=\"https:path\" async><path>",
                "<path>\"\"\"))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "display(HTML(\"\"\"",
                "<div>",
                "    <a href=\"https://plot.ly/~sominw/6/?share_key=y6irxkKqSVolnuF0l4w420\" target=\"_blank\" title=\"Chicago Cycle Sharing Stations\" style=\"display: block; text-align: center;\"><img src=\"https://plot.ly/~sominw/6.png?share_key=y6irxkKqSVolnuF0l4w420\" alt=\"Chicago Cycle Sharing Stations\" style=\"max-width: 100%;width: 600px;\"  width=\"600\" onerror=\"this.onerror=null;this.src='https://plot.ly/404.png';\" /></a>",
                "    <script data-plotly=\"sominw:6\" sharekey-plotly=\"y6irxkKqSVolnuF0l4w420\" src=\"https://plot.ly/embed.js\" async></script>",
                "</div>\"\"\"))"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!pip install imageio-ffmpeg"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import imageio\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.animation as animation\n",
                "from skimage.transform import resize\n",
                "from IPython.display import HTML\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def display(driving ):",
                "ASSIGN = plt.figure(figsize=(10, 6))",
                "ASSIGN = []",
                "for i in range(len(driving)):",
                "ASSIGN = plt.imshow(driving[i], animated=True)",
                "plt.axis('off')",
                "ASSIGN.append([ASSIGN])",
                "ASSIGN = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)",
                "plt.close()",
                "return ani"
            ],
            "output_type": "not_existent",
            "content_old": [
                "\n",
                " \n",
                "def display(driving ):\n",
                "    fig = plt.figure(figsize=(10, 6))\n",
                "\n",
                "    ims = []\n",
                "    for i in range(len(driving)):\n",
                "        im = plt.imshow(driving[i], animated=True)\n",
                "        plt.axis('off')\n",
                "        ims.append([im])\n",
                "\n",
                "    ani = animation.ArtistAnimation(fig, ims, interval=50, repeat_delay=1000)\n",
                "    plt.close()\n",
                "    return ani\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = imageio.get_reader('..path')",
                "ASSIGN = reader.get_meta_data()['ASSIGN']",
                "ASSIGN = []",
                "try:",
                "for im in ASSIGN:",
                "ASSIGN.append(im)",
                "except RuntimeError:",
                "pass",
                "ASSIGN.close()",
                "ASSIGN = [resize(frame, (256, 256))[..., :3] for frame in ASSIGN]",
                "HTML(display(ASSIGN).to_html5_video())"
            ],
            "output_type": "execute_result",
            "content_old": [
                "reader = imageio.get_reader('../input/digitsinnoise-video/Test.mp4')\n",
                "fps = reader.get_meta_data()['fps']\n",
                "driving_video = []\n",
                "try:\n",
                "    for im in reader:\n",
                "        driving_video.append(im)\n",
                "except RuntimeError:\n",
                "    pass\n",
                "reader.close()\n",
                "\n",
                "driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
                "\n",
                "\n",
                "\n",
                "HTML(display(driving_video).to_html5_video())"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os",
                "import cv2",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "def DisplayImage(image,title,cols=1):",
                "ASSIGN=len(image)+1",
                "ASSIGN=0",
                "plt.figure(figsize=(8, 8))",
                "for i in range(1,ASSIGN):",
                "ASSIGN+=1",
                "plt.subplot(1,cols,ASSIGN),plt.imshow(image[i-1],cmap = 'gray'), plt.title(title[i-1]), plt.axis('off')",
                "if ( i%cols==0):",
                "plt.show(),plt.figure(figsize=(8, 8))",
                "ASSIGN=0"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def DisplayImage(image,title,cols=1):",
                "  ",
                "    import matplotlib.pyplot as plt",
                "    ",
                "    image_no=len(image)+1",
                "    postion=0",
                "",
                "    plt.figure(figsize=(8, 8))",
                "    for i in range(1,image_no):",
                "        postion+=1",
                "        plt.subplot(1,cols,postion),plt.imshow(image[i-1],cmap = 'gray'), plt.title(title[i-1]), plt.axis('off')",
                "       ",
                "        if ( i%cols==0):",
                "            plt.show(),plt.figure(figsize=(8, 8))",
                "            postion=0"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=[]",
                "ASSIGN=[]",
                "for dirname, _, filenames in os.walk('..path'):",
                "for filename in filenames[0:9]:",
                "ASSIGN.append( cv2.imread(dirname+\"path\"+ filename,0))",
                "ASSIGN.append(\"1\")",
                "DisplayImage(ASSIGN,ASSIGN,3)"
            ],
            "output_type": "display_data",
            "content_old": [
                "images=[]",
                "titles=[]",
                "for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):",
                "    for filename in filenames[0:9]:",
                "       ",
                "        images.append( cv2.imread(dirname+\"/\"+ filename,0))",
                "        titles.append(\"1\")",
                "        ",
                "        ",
                "DisplayImage(images,titles,3)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!python -m pip install dtw",
                "",
                "import numpy as np",
                "from dtw import dtw"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.array([1,2,3,4,3,2,1,1,1,2])",
                "ASSIGN = np.array([0,1,1,2,3,4,3,2,1,1])",
                "ASSIGN = lambda s1,s2: np.abs(s1 - s2)",
                "ASSIGN = dtw( s1,s2, dist=manhattan_distance)",
                "print(d)"
            ],
            "output_type": "stream",
            "content_old": [
                "",
                " ",
                "s1 = np.array([1,2,3,4,3,2,1,1,1,2]) ",
                "s2 = np.array([0,1,1,2,3,4,3,2,1,1]) ",
                "",
                "",
                "",
                "manhattan_distance = lambda s1,s2: np.abs(s1 - s2)",
                "",
                "d, cost_matrix, acc_cost_matrix, path = dtw( s1,s2, dist=manhattan_distance)",
                "",
                "print(d)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!pip install dtaidistance",
                "from dtaidistance import dtw",
                "from dtaidistance import dtw_visualisation as dtwvis",
                "import random",
                "import numpy as np"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.arange(0, 20, .5)",
                "ASSIGN = np.sin(x)",
                "ASSIGN = np.sin(x - 1)",
                "random.seed(1)",
                "for idx in range(len(ASSIGN)):",
                "if random.random() < 0.05:",
                "ASSIGN[idx] += (random.random() - 0.5) path",
                "ASSIGN = dtw.warping_paths(s1, s2, window=25, psi=2)",
                "ASSIGN = dtw.ASSIGN(paths)",
                "dtwvis.plot_warpingpaths(ASSIGN, ASSIGN, paths, ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "",
                "x = np.arange(0, 20, .5)",
                "s1 = np.sin(x)",
                "s2 = np.sin(x - 1)",
                "random.seed(1)",
                "for idx in range(len(s2)):",
                "    if random.random() < 0.05:",
                "        s2[idx] += (random.random() - 0.5) / 2",
                "d, paths = dtw.warping_paths(s1, s2, window=25, psi=2)",
                "best_path = dtw.best_path(paths)",
                "dtwvis.plot_warpingpaths(s1, s2, paths, best_path)",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = dtw.ASSIGN(s1, s2)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "distance = dtw.distance(s1, s2)",
                "print(distance)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "!pip install pydub"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "ASSIGN=\".path\"",
                "os.mkdir(ASSIGN)",
                "for dirname, _, filenames in os.walk('..path'):",
                "for filename in filenames:",
                "ASSIGN = \"..path\"+filename",
                "ASSIGN = test_set+os.path.splitext(filename)[0]+\".wav\"",
                "ASSIGN = AudioSegment.from_mp3(src)",
                "ASSIGN = ASSIGN.set_frame_rate(8000)",
                "ASSIGN.export(ASSIGN, format=\"wav\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os",
                "",
                "from os import path",
                "from pydub import AudioSegment",
                "",
                "",
                " ",
                "",
                "test_set=\"./test_set/\"",
                " ",
                "",
                "os.mkdir(test_set) ",
                " ",
                "",
                "for dirname, _, filenames in os.walk('../input/quran-asr-challenge/test_set'):",
                "    for filename in filenames:",
                "        # files                                                                         ",
                "        src = \"../input/quran-asr-challenge/test_set/\"+filename",
                "        dst = test_set+os.path.splitext(filename)[0]+\".wav\"",
                "        # convert wav to mp3                                                            ",
                "        sound = AudioSegment.from_mp3(src)",
                "        sound = sound.set_frame_rate(8000)",
                "        sound.export(dst, format=\"wav\")",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from Levenshtein import distance",
                "import numpy as np",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def get_distance_matrix(str_list):",
                "\"\"\" Construct a levenshtein distance matrix for a list of strings\"\"\"",
                "ASSIGN = np.zeros(shape=(len(str_list), len(str_list)))",
                "print (\"Starting to build distance matrix. This will iterate from 0 till \", len(str_list) )",
                "for i in range(0, len(str_list)):",
                "print (i)",
                "for j in range(i+1, len(str_list)):",
                "ASSIGN[i][j] = distance(str_list[i], str_list[j])",
                "for i in range(0, len(str_list)):",
                "for j in range(0, len(str_list)):",
                "ASSIGN == j:",
                "ASSIGN[i][j] = 0",
                "elif i > j:",
                "ASSIGN[i][j] = ASSIGN[j][i]",
                "return dist_matrix"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "def get_distance_matrix(str_list):",
                "    \"\"\" Construct a levenshtein distance matrix for a list of strings\"\"\"",
                "    dist_matrix = np.zeros(shape=(len(str_list), len(str_list)))",
                "",
                "    print (\"Starting to build distance matrix. This will iterate from 0 till \", len(str_list) )",
                "    for i in range(0, len(str_list)):",
                "        print (i)",
                "        for j in range(i+1, len(str_list)):",
                "                dist_matrix[i][j] = distance(str_list[i], str_list[j]) ",
                "    for i in range(0, len(str_list)):",
                "        for j in range(0, len(str_list)):",
                "            if i == j:",
                "                dist_matrix[i][j] = 0 ",
                "            elif i > j:",
                "                dist_matrix[i][j] = dist_matrix[j][i]",
                "",
                "    return dist_matrix",
                "",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = [",
                "\"part\", \"spartan\"",
                "]",
                "get_distance_matrix(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "str_list = [",
                "    \"part\", \"spartan\"",
                "  ",
                "]",
                "get_distance_matrix(str_list)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np",
                "",
                "import pandas as pd",
                "import matplotlib",
                "import matplotlib.pyplot as plt",
                "from matplotlib import image",
                "import seaborn as sns",
                "%matplotlib inline",
                "",
                "",
                "",
                "from scipy.spatial import distance",
                "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D",
                "from keras.optimizers import RMSprop",
                "from tensorflow.keras.utils  import plot_model, model_to_dot",
                "from keras.preprocessing.image import ImageDataGenerator",
                "from keras.callbacks import EarlyStopping,LearningRateScheduler,ReduceLROnPlateau",
                "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = np.load(DSPATH+\"olivetti_faces.npy\")",
                "ASSIGN = np.load(DSPATH+\"olivetti_faces_target.npy\")",
                "ThiefImage={}",
                "SLICE= image.imread(\"..path\")",
                "SLICE=image.imread(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "DSPATH=\"../input/olivetti-faces/\"",
                "X = np.load(DSPATH+\"olivetti_faces.npy\")",
                "y = np.load(DSPATH+\"olivetti_faces_target.npy\")",
                "",
                " ",
                "ThiefImage={}",
                "ThiefImage[\"False\"]= image.imread(\"../input/thief-images/False.jpg\")",
                "ThiefImage[\"True\"]=image.imread(\"../input/thief-images/True.jpg\")",
                " ",
                "",
                "",
                "",
                " "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ClASSES=np.unique(y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ClASSES=np.unique(y)",
                "# N_CLASSES=len(np.unique(labels))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "StratifiedSplit = StratifiedShuffleSplit( test_size=0.4, random_state=0)",
                "StratifiedSplit.get_n_splits(X, y)",
                "for train_index, test_index in StratifiedSplit.split(X, y):",
                "X_train, X_test, y_train, y_test= X[train_index], X[test_index], y[train_index], y[test_index]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Split into train/val",
                "",
                "StratifiedSplit = StratifiedShuffleSplit( test_size=0.4, random_state=0)",
                "StratifiedSplit.get_n_splits(X, y)",
                "for train_index, test_index in StratifiedSplit.split(X, y):",
                "    X_train, X_test, y_train, y_test= X[train_index], X[test_index], y[train_index], y[test_index]",
                "    ",
                "# X_train, X_test, y_train, y_test = train_test_split(    ",
                "#     X, y, test_size=.40, random_state=42)",
                " "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = sns.countplot(y_train)"
            ],
            "output_type": "display_data",
            "content_old": [
                "import seaborn as sns",
                "g = sns.countplot(y_train)",
                "",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=[]",
                "for i in range(len(X_train)):",
                "ASSIGN.append(X_train[y_train==i].mean(axis = 0))"
            ],
            "output_type": "stream",
            "content_old": [
                "# calculate class mean ",
                "class_mean=[]",
                "for i in range(len(X_train)):",
                "    class_mean.append(X_train[y_train==i].mean(axis = 0))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def ShowTrainingData(showNclasses=5):",
                "if showNclasses>=40:",
                "ASSIGN=ClASSES",
                "ASSIGN=2,4",
                "for i in range(ASSIGN+1):",
                "ASSIGN = plt.subplots(rows,cols )",
                "ASSIGN=0",
                "for face in X_train[y_train==i]:",
                "ASSIGN+=1",
                "ASSIGN==cols:",
                "ASSIGN=5",
                "ASSIGN=plt.subplot(rows,cols,j)",
                "ASSIGN.imshow(face ,'gray' )",
                "ASSIGN = plt.subplot(1,cols,cols)",
                "ASSIGN.imshow( class_mean[i], 'gray' )",
                "plt.xlabel(\"Class \"+str(i)+\" mean \" )",
                "fig.tight_layout(pad=1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Show Data",
                "",
                "def ShowTrainingData(showNclasses=5):",
                "    if showNclasses>=40:",
                "        showNclasses=ClASSES",
                "    rows,cols=2,4",
                "    ",
                "    for i in range(showNclasses+1):",
                "        fig,ax =  plt.subplots(rows,cols )",
                "        j=0",
                "        for face in X_train[y_train==i]:",
                "            j+=1",
                "            if j==cols:",
                "                j=5",
                "            ax=plt.subplot(rows,cols,j)",
                "            ax.imshow(face ,'gray' )",
                "",
                "        ax = plt.subplot(1,cols,cols)",
                "       ",
                "        ax.imshow( class_mean[i], 'gray' )",
                "        plt.xlabel(\"Class \"+str(i)+\" mean \" )",
                "        fig.tight_layout(pad=1.0)",
                "        plt.show()",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def ShowTrainingData2(showNclasses):",
                "if showNclasses>=40:",
                "ASSIGN=ClASSES",
                "ASSIGN=2,4",
                "for i in range(ASSIGN+1):",
                "ASSIGN = plt.figure(figsize=(8, 4))",
                "ASSIGN=0",
                "for face in X_train[y_train==i]:",
                "ASSIGN+=1",
                "ASSIGN==cols:",
                "ASSIGN=5",
                "ASSIGN.add_subplot(rows, cols, ASSIGN)",
                "plt.imshow(face, cmap = plt.get_cmap('gray'))",
                "plt.axis('off')",
                "ASSIGN.add_subplot(1,cols,cols)",
                "plt.imshow(class_mean[i], cmap = plt.get_cmap('gray'))",
                "plt.title(\"class_mean {}\".format(i), fontsize=16)",
                "plt.axis('off')",
                "plt.suptitle(\"There are 6 image for class {}\".format(i), fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Show Data",
                "",
                "def ShowTrainingData2(showNclasses):",
                "    if showNclasses>=40:",
                "        showNclasses=ClASSES",
                "    rows,cols=2,4",
                "    ",
                "    for i in range(showNclasses+1):",
                "        fig = plt.figure(figsize=(8, 4))",
                "    ",
                "        j=0",
                "        for face in X_train[y_train==i]:",
                "            j+=1",
                "            if j==cols:",
                "                j=5",
                "            fig.add_subplot(rows, cols, j)",
                "            plt.imshow(face, cmap = plt.get_cmap('gray'))",
                " ",
                "            plt.axis('off')",
                "",
                "        ",
                "        fig.add_subplot(1,cols,cols)",
                "        plt.imshow(class_mean[i], cmap = plt.get_cmap('gray'))",
                "        plt.title(\"class_mean {}\".format(i), fontsize=16)",
                "        plt.axis('off')",
                " #         fig.tight_layout(pad=1.0)",
                "",
                "",
                "        plt.suptitle(\"There are 6 image for class {}\".format(i), fontsize=15)",
                "        plt.show()",
                "",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def ShowPredictions(predic_Model,ShowNPredictions=5):",
                "if ShowNPredictions>=len(y_predictions):",
                "ShowNPredictions=len(y_predictions)",
                "ASSIGN=1,3",
                "for index, row in y_predictions.iterrows():",
                "if (index>ShowNPredictions):",
                "break",
                "ASSIGN=int(row[\"ASSIGN\"])",
                "ASSIGN=int(row[\"ASSIGN\"])",
                "ASSIGN=int(row[predic_Model+\"_predic\"])",
                "IsTrue=str(row[predic_Model+\"_True\"])",
                "ASSIGN = plt.subplots(rows,cols )",
                "ASSIGN=1",
                "ASSIGN=plt.subplot(rows,cols,j)",
                "ASSIGN.imshow(X_test[ASSIGN] ,'gray' )",
                "plt.xlabel(\"Test Number :\"+str(ASSIGN) )",
                "ASSIGN=2",
                "ASSIGN=plt.subplot(rows,cols,j)",
                "ASSIGN.imshow(class_mean[ASSIGN] ,'gray' )",
                "plt.xlabel(\"Class \"+str(ASSIGN)+\" mean \" )",
                "ASSIGN=3",
                "ASSIGN=plt.subplot(rows,cols,j)",
                "ASSIGN.imshow(ThiefImage[IsTrue] ,'gray' )",
                "plt.xlabel(\"Class \"+str(ASSIGN)+\" mean \" )",
                "fig.tight_layout(pad=2.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def ShowPredictions(predic_Model,ShowNPredictions=5):",
                "    ",
                "    if ShowNPredictions>=len(y_predictions):",
                "        ShowNPredictions=len(y_predictions)",
                "    rows,cols=1,3",
                " ",
                "    for index, row in y_predictions.iterrows():",
                "        if (index>ShowNPredictions):",
                "            break",
                "            ",
                "        x=int(row[\"x\"])",
                "        actually=int(row[\"actually\"])",
                "        y_predic=int(row[predic_Model+\"_predic\"])",
                "        IsTrue=str(row[predic_Model+\"_True\"])",
                "",
                "        fig,ax =  plt.subplots(rows,cols )",
                "        j=1",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(X_test[x] ,'gray' )",
                "        plt.xlabel(\"Test Number :\"+str(x)  )",
                "",
                "        j=2",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(class_mean[y_predic] ,'gray' )",
                "        plt.xlabel(\"Class \"+str(y_predic)+\" mean \" )",
                "",
                "        j=3",
                "        ax=plt.subplot(rows,cols,j)",
                "        ax.imshow(ThiefImage[IsTrue] ,'gray' )",
                "        plt.xlabel(\"Class \"+str(actually)+\" mean \" )",
                "",
                "",
                "",
                "        fig.tight_layout(pad=2.0)",
                "        plt.show()   ",
                "   "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ShowTrainingData2(5)"
            ],
            "output_type": "display_data",
            "content_old": [
                "ShowTrainingData2(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=np.array([(i,y_test[i],c,distance.euclidean(X_test[i].flatten() , class_mean[c].flatten() )) for c in ClASSES for i in range(len(X_test))])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "distanceTable=np.array([(i,y_test[i],c,distance.euclidean(X_test[i].flatten() , class_mean[c].flatten() )) for c in ClASSES  for i in range(len(X_test))])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "distanceTable"
            ],
            "output_type": "execute_result",
            "content_old": [
                "distanceTable"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.T"
            ],
            "output_type": "not_existent",
            "content_old": [
                "distanceTable=distanceTable.T",
                "# distanceTable.shape=(4,6400)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = {'x': distanceTable[0], 'actually':distanceTable[1],'KNN_predic':distanceTable[2],'distance':distanceTable[3]}",
                "ASSIGN= pd.DataFrame(data=d)",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                " ",
                "d = {'x': distanceTable[0], 'actually':distanceTable[1],'KNN_predic':distanceTable[2],'distance':distanceTable[3]}",
                "df= pd.DataFrame(data=d)",
                "df.head()",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df[df.x==0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df[df.x==0]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=pd.merge(df ,df.groupby([\"x\",\"actually\"]).distance.min(), how = 'inner', on=[\"x\",\"actually\",\"distance\"])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_predictions=pd.merge(df ,df.groupby([\"x\",\"actually\"]).distance.min(), how = 'inner',  on=[\"x\",\"actually\",\"distance\"])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_predictions[\"KNN_True\"]=y_predictions[\"KNN_predic\"]==y_predictions[\"actually\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_predictions[\"KNN_True\"]=y_predictions[\"KNN_predic\"]==y_predictions[\"actually\"]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.nonzero(y_predictions[\"KNN_True\"].values==1)[0]",
                "ASSIGN = np.nonzero(y_predictions[\"KNN_True\"].values==0)[0]",
                "print(len(ASSIGN),)",
                "print(len(ASSIGN),)"
            ],
            "output_type": "stream",
            "content_old": [
                "correct_predictions = np.nonzero(y_predictions[\"KNN_True\"].values==1)[0]",
                "incorrect_predictions = np.nonzero(y_predictions[\"KNN_True\"].values==0)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "print()",
                "ShowPredictions(\"KNN\",5)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"KNN_predic\")",
                "print(\"=============\")",
                "",
                "ShowPredictions(\"KNN\",5)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.reshape(-1,64,64,1)",
                "ASSIGN = ASSIGN.reshape(-1,64,64,1)",
                "print(,ASSIGN.shape,,y_train.shape)",
                "print(, ASSIGN.shape,,y_test.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "X_train = X_train.reshape(-1,64,64,1)",
                "X_test = X_test.reshape(-1,64,64,1)",
                " ",
                "",
                "print(\"X_train shape: \",X_train.shape,\"y_train shape: \",y_train.shape)",
                "print(\"x_test shape: \", X_test.shape,\"y_test shape: \",y_test.shape)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(filters = 20, kernel_size = (5,5),padding = 'Same',",
                "ASSIGN ='relu', input_shape = (64,64,1)))",
                "ASSIGN.add(MaxPool2D(pool_size=(2,2)))",
                "ASSIGN.add(Dropout(0.25))",
                "ASSIGN.add(Conv2D(filters = 50, kernel_size = (6,6),padding = 'Same',",
                "ASSIGN ='relu'))",
                "ASSIGN.add(MaxPool2D(pool_size=(2,2)))",
                "ASSIGN.add(Dropout(0.25))",
                "ASSIGN.add(Conv2D(filters = 150, kernel_size = (5,5),padding = 'Same',",
                "ASSIGN ='relu', input_shape = (64,64,1)))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(256, ASSIGN = \"relu\"))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(40, ASSIGN = \"softmax\"))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model = Sequential()",
                "",
                "model.add(Conv2D(filters = 20, kernel_size = (5,5),padding = 'Same', ",
                "                 activation ='relu', input_shape = (64,64,1)))",
                "",
                "model.add(MaxPool2D(pool_size=(2,2)))",
                "model.add(Dropout(0.25))",
                "",
                "model.add(Conv2D(filters = 50, kernel_size = (6,6),padding = 'Same', ",
                "                 activation ='relu'))",
                "",
                "model.add(MaxPool2D(pool_size=(2,2)))",
                "model.add(Dropout(0.25))",
                "",
                "model.add(Conv2D(filters = 150, kernel_size = (5,5),padding = 'Same', ",
                "                 activation ='relu', input_shape = (64,64,1)))",
                "",
                "model.add(Flatten())",
                "model.add(Dense(256, activation = \"relu\"))",
                "model.add(Dropout(0.5))",
                "model.add(Dense(40, activation = \"softmax\"))",
                "",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = ReduceLROnPlateau(monitor='val_acc',",
                "ASSIGN=3,",
                "ASSIGN=1,",
                "ASSIGN=0.7,",
                "ASSIGN=0.00000000001)",
                "ASSIGN = EarlyStopping(patience=2)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', ",
                "                                            patience=3, ",
                "                                            verbose=1, ",
                "                                            factor=0.7, ",
                "                                            min_lr=0.00000000001)",
                "early_stopping_monitor = EarlyStopping(patience=2)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "model.summary()"
            ],
            "output_type": "stream",
            "content_old": [
                "model.summary()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "transfer_results"
            ],
            "content": [
                "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)",
                "model.compile(ASSIGN = ASSIGN , loss='sparse_categorical_crossentropy',",
                "ASSIGN=['sparse_categorical_accuracy'])",
                "ASSIGN = 37",
                "ASSIGN = 20",
                "ASSIGN = ImageDataGenerator(",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=5,",
                "ASSIGN = 0.05,",
                "ASSIGN=0,",
                "ASSIGN=0,",
                "ASSIGN=False,",
                "ASSIGN=False)",
                "ASSIGN.fit(X_train)",
                "ASSIGN = model.fit_generator(",
                "ASSIGN.flow(X_train,y_train, ASSIGN=ASSIGN),",
                "ASSIGN = epoch,",
                "ASSIGN = (X_test,y_test),",
                "ASSIGN = 2,",
                "ASSIGN=X_train.shape[0] path,",
                "ASSIGN=[learning_rate_reduction]",
                ")"
            ],
            "output_type": "stream",
            "content_old": [
                "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)",
                "model.compile(optimizer = optimizer , loss='sparse_categorical_crossentropy',",
                "            metrics=['sparse_categorical_accuracy'])",
                "epoch = 37",
                "batch_size = 20",
                "",
                "datagen = ImageDataGenerator(",
                "        featurewise_center=False,  # set input mean to 0 over the dataset",
                "        samplewise_center=False,  # set each sample mean to 0",
                "        featurewise_std_normalization=False,  # divide inputs by std of the dataset",
                "        samplewise_std_normalization=False,  # divide each input by its std",
                "        zca_whitening=False,  # apply ZCA whitening",
                "        rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)",
                "        zoom_range = 0.05, # Randomly zoom image ",
                "        width_shift_range=0,  # randomly shift images horizontally (fraction of total width)",
                "        height_shift_range=0,  # randomly shift images vertically (fraction of total height)",
                "        horizontal_flip=False,  # randomly flip images",
                "        vertical_flip=False)  # randomly flip images",
                "datagen.fit(X_train)",
                "",
                "history = model.fit_generator(",
                "                              datagen.flow(X_train,y_train, batch_size=batch_size),",
                "                              epochs = epoch, ",
                "                              validation_data = (X_test,y_test),",
                "                              verbose = 2, ",
                "                              steps_per_epoch=X_train.shape[0] // batch_size,",
                "                              callbacks=[learning_rate_reduction]",
                "                             )"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print(history.history.keys())",
                "plt.plot(history.history['sparse_categorical_accuracy'])",
                "plt.plot(history.history['val_sparse_categorical_accuracy'])",
                "plt.title('model accuracy')",
                "plt.ylabel('accuracy')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()",
                "plt.plot(history.history['loss'])",
                "plt.plot(history.history['val_loss'])",
                "plt.title('model loss')",
                "plt.ylabel('loss')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "print(history.history.keys())",
                "# summarize history for accuracy",
                "plt.plot(history.history['sparse_categorical_accuracy'])",
                "plt.plot(history.history['val_sparse_categorical_accuracy'])",
                "plt.title('model accuracy')",
                "plt.ylabel('accuracy')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()",
                "# summarize history for loss",
                "plt.plot(history.history['loss'])",
                "plt.plot(history.history['val_loss'])",
                "plt.title('model loss')",
                "plt.ylabel('loss')",
                "plt.xlabel('epoch')",
                "plt.legend(['train', 'test'], loc='upper left')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "print()",
                "ASSIGN = model.evaluate(X_test,y_test,batch_size=32)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"CNN_predic\")",
                "print(\"=============\")",
                "",
                "score = model.evaluate(X_test,y_test,batch_size=32)",
                "print(score)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN=model.predict_classes(X_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "CNN_predic=model.predict_classes(X_test)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.reshape(-1,64,64)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_test = X_test.reshape(-1,64,64)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "del y_predictions",
                "ASSIGN=np.array([(i,y_test[i],CNN_predic[i] ) for i in range(len(X_test))])",
                "ASSIGN=ASSIGN.T",
                "ASSIGN = {'x': distanceTable[0], 'actually':distanceTable[1],'CNN_predic':distanceTable[2] }",
                "ASSIGN= pd.DataFrame(data=d)",
                "ASSIGN.head()",
                "ASSIGN[\"CNN_True\"]=ASSIGN[\"CNN_predic\"]==ASSIGN[\"actually\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "del y_predictions",
                "",
                "distanceTable=np.array([(i,y_test[i],CNN_predic[i] )  for i in range(len(X_test))])",
                "distanceTable=distanceTable.T",
                "d = {'x': distanceTable[0], 'actually':distanceTable[1],'CNN_predic':distanceTable[2] }",
                "y_predictions= pd.DataFrame(data=d)",
                "y_predictions.head()",
                "y_predictions[\"CNN_True\"]=y_predictions[\"CNN_predic\"]==y_predictions[\"actually\"]",
                "",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "y_predictions.head(100)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_predictions.head(100)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.nonzero(y_predictions[\"CNN_True\"].values==1)[0]",
                "ASSIGN = np.nonzero(y_predictions[\"CNN_True\"].values==0)[0]",
                "print(len(ASSIGN),)",
                "print(len(ASSIGN),)"
            ],
            "output_type": "stream",
            "content_old": [
                "correct_predictions = np.nonzero(y_predictions[\"CNN_True\"].values==1)[0]",
                "incorrect_predictions = np.nonzero(y_predictions[\"CNN_True\"].values==0)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ShowPredictions(\"CNN\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "",
                "",
                "ShowPredictions(\"CNN\")",
                "",
                "",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import cv2 ",
                "import matplotlib.pyplot as plt",
                "from matplotlib.patches import Rectangle "
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = cv2.CascadeClassifier('..path')",
                "ASSIGN =cv2.imread( \"..path\",0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "face_cascade = cv2.CascadeClassifier('../input/opencv-haarcascade/data/haarcascades/haarcascade_frontalface_default.xml')",
                "gray =cv2.imread( \"../input/opencv-samples-images/data/lena.jpg\",0)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10, 10))",
                "ASSIGN = face_cascade.detectMultiScale(gray, 1.3, 5)",
                "for (x,y,w,h) in ASSIGN:",
                "ASSIGN = plt.gca()",
                "ASSIGN.add_patch( Rectangle((x,y),",
                "w, h,",
                "ASSIGN ='none',",
                "ASSIGN ='b',",
                "ASSIGN = 4) )",
                "plt.imshow(gray,cmap = 'gray')",
                "plt.title('template'), plt.xticks([]), plt.yticks([])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(10, 10))",
                "faces = face_cascade.detectMultiScale(gray, 1.3, 5)",
                "for (x,y,w,h) in faces:",
                "    ax = plt.gca()",
                "    ax.add_patch( Rectangle((x,y), ",
                "                       w,   h,",
                "                        fc ='none',  ",
                "                        ec ='b', ",
                "                        lw = 4) ) ",
                "",
                "plt.imshow(gray,cmap = 'gray')",
                "plt.title('template'), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import cv2 as cv",
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import matplotlib.image as mpimg",
                "from matplotlib.patches import Rectangle "
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "plt.figure(figsize=(20, 20))",
                "plt.title(\"Original\")",
                "plt.imshow(mpimg.imread('..path'))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(20, 20))",
                "plt.title(\"Original\")",
                "plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg'))",
                "plt.show()",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "ASSIGN = cv.imread('..path',0)",
                "ASSIGN =img[500:650, 500:600]",
                "plt.imshow(ASSIGN,cmap = 'gray')",
                "plt.title('ASSIGN'), plt.xticks([]), plt.yticks([])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "",
                "",
                "img = cv.imread('../input/opencv-samples-images/WaldoBeach.jpg',0)",
                "",
                "template =img[500:650, 500:600]",
                "# template =img[500:650, 200:300]",
                "plt.imshow(template,cmap = 'gray')",
                "plt.title('template'), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data",
                "ingest_data"
            ],
            "content": [
                "ASSIGN = img.copy()",
                "ASSIGN = template.shape[::-1]",
                "ASSIGN = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',",
                "'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']",
                "plt.figure(figsize=(20, 20))",
                "plt.imshow(mpimg.imread('..path'))",
                "plt.title('Detected Point')",
                "ASSIGN=[]",
                "for meth in ASSIGN:",
                "ASSIGN = img2.copy()",
                "ASSIGN = eval(meth)",
                "ASSIGN = cv.matchTemplate(img,template,method)",
                "ASSIGN = cv.minMaxLoc(res)",
                "if ASSIGN in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:",
                "ASSIGN = min_loc",
                "else:",
                "ASSIGN = max_loc",
                "ASSIGN+=[(meth,ASSIGN,ASSIGN)]",
                "ASSIGN = plt.gca()",
                "ASSIGN.add_patch( Rectangle(ASSIGN,",
                "ASSIGN,",
                "ASSIGN ='none',",
                "ASSIGN ='b',",
                "ASSIGN = 4) )",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "img2 = img.copy()",
                "w, h = template.shape[::-1]",
                "# All the 6 methods for comparison in a list",
                "methods = ['cv.TM_CCOEFF', 'cv.TM_CCOEFF_NORMED', 'cv.TM_CCORR',",
                "            'cv.TM_CCORR_NORMED', 'cv.TM_SQDIFF', 'cv.TM_SQDIFF_NORMED']",
                "plt.figure(figsize=(20, 20))",
                "plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg'))",
                "plt.title('Detected Point')",
                "result=[]",
                "",
                "for meth in methods:",
                "    img = img2.copy()",
                "    method = eval(meth)",
                "    # Apply template Matching",
                "    res = cv.matchTemplate(img,template,method)",
                "    min_val, max_val, min_loc, max_loc = cv.minMaxLoc(res)",
                "    # If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take minimum",
                "    if method in [cv.TM_SQDIFF, cv.TM_SQDIFF_NORMED]:",
                "        top_left = min_loc",
                "    else:",
                "        top_left = max_loc",
                "",
                "    result+=[(meth,top_left,w,   h)]",
                " ",
                "    ax = plt.gca()",
                "    ax.add_patch( Rectangle(top_left, ",
                "                       w,   h,",
                "                        fc ='none',  ",
                "                        ec ='b', ",
                "                        lw = 4) ) ",
                "",
                "    ",
                "plt.show()",
                "    "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=160",
                "plt.figure(figsize=(20, 20))",
                "for r in result:",
                "ASSIGN+=1",
                "plt.subplot(ASSIGN),plt.imshow(res,cmap = 'gray')",
                "ASSIGN =img[ r[1][1]:r[1][1]+r[3], r[1][0]: r[1][0]+r[2]]",
                "plt.imshow(ASSIGN,cmap = 'gray')",
                "plt.title(r[0]), plt.xticks([]), plt.yticks([])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "index=160",
                "plt.figure(figsize=(20, 20))",
                "for r in result:",
                "    index+=1",
                "    plt.subplot(index),plt.imshow(res,cmap = 'gray')",
                "    template =img[ r[1][1]:r[1][1]+r[3], r[1][0]: r[1][0]+r[2]]",
                "    plt.imshow(template,cmap = 'gray')",
                "    plt.title(r[0]), plt.xticks([]), plt.yticks([])",
                "",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!pip install pycaret==2.0"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import random",
                "import numpy as np",
                "import pandas as pd",
                "import matplotlib.pyplot as plt",
                "from pycaret.classification import *",
                "from sklearn.model_selection import train_test_split"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN=10000"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sampleNumber=10000"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {",
                "'x1': np.random.randint(0,3,sampleNumber) ,",
                "'x2': np.random.randint(0,3,sampleNumber) ,",
                "'x3': np.random.randint(0,2,sampleNumber) ,",
                "'x4': np.random.randint(0,3,sampleNumber) ,",
                "'x5': np.random.randint(0,2,sampleNumber) ,",
                "'x6': np.random.randint(0,2,sampleNumber) ,",
                "'y' : np.zeros(sampleNumber, dtype=bool)",
                "}",
                "ASSIGN = pd.DataFrame(ASSIGN=d)",
                "SLICE=(SLICE + SLICE + SLICE + SLICE + SLICE+ SLICE)>4",
                "X=ASSIGN[['x1','x2','x3','x4','x5','x6']]",
                "ASSIGN=data[[\"ASSIGN\"]]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "d = {",
                "    'x1': np.random.randint(0,3,sampleNumber) ,#Make sacrifices",
                "    'x2': np.random.randint(0,3,sampleNumber) ,#Punctuality",
                "    'x3': np.random.randint(0,2,sampleNumber) ,#Not feeling bored while you are together",
                "    'x4': np.random.randint(0,3,sampleNumber) ,#You evaluate gifts",
                "    'x5': np.random.randint(0,2,sampleNumber) ,#Take care of my problems",
                "    'x6': np.random.randint(0,2,sampleNumber) ,#Rai respect ",
                "    'y' : np.zeros(sampleNumber, dtype=bool)",
                "    }",
                "data  = pd.DataFrame(data=d)",
                "",
                "",
                " ",
                "data[\"y\"]=(data['x1'] +   data['x2'] +    data['x3'] +   data['x4'] +   data['x5']+    data['x6'])>4",
                "",
                "",
                "X=data[['x1','x2','x3','x4','x5','x6']]",
                "y=data[[\"y\"]]",
                "",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test = train_test_split(",
                "ASSIGN=.20, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train, X_test, y_train, y_test = train_test_split(    ",
                "    X, y, test_size=.20, random_state=42)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=X_train",
                "ASSIGN[\"y\"]=y_train",
                "del X_train",
                "del y_train"
            ],
            "output_type": "stream",
            "content_old": [
                "train_Data=X_train",
                "train_Data[\"y\"]=y_train",
                "del X_train",
                "del y_train"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= setup(data = train_Data, target = \"y\")"
            ],
            "output_type": "stream",
            "content_old": [
                "",
                "clf= setup(data = train_Data, target = \"y\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "compare_models()"
            ],
            "output_type": "display_data",
            "content_old": [
                "compare_models()"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = create_model('ASSIGN')"
            ],
            "output_type": "display_data",
            "content_old": [
                "lr = create_model('lr')"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_model(lr,\"confusion_matrix\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "plot_model(lr,\"confusion_matrix\")"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "evaluate_model(lr)"
            ],
            "output_type": "display_data",
            "content_old": [
                "evaluate_model(lr)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = predict_model(lr, data = X_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "lr_pred = predict_model(lr, data = X_test) #new_data is pd dataframe"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "lr_pred"
            ],
            "output_type": "execute_result",
            "content_old": [
                "lr_pred"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "y_test.reset_index(drop=True, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_test.reset_index(drop=True, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_test"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_test"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.astype(\"int\").values.T"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_test=y_test.astype(\"int\").values.T"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.nonzero(lr_pred[\"Label\"].values==y_test)[0]",
                "ASSIGN = np.nonzero(lr_pred[\"Label\"].values!=y_test)[0]",
                "print(len(ASSIGN),)",
                "print(len(ASSIGN),)"
            ],
            "output_type": "stream",
            "content_old": [
                "correct_predictions = np.nonzero(lr_pred[\"Label\"].values==y_test)[0]",
                "incorrect_predictions = np.nonzero(lr_pred[\"Label\"].values!=y_test)[0]",
                "print(len(correct_predictions),\" classified correctly\")",
                "print(len(incorrect_predictions),\" classified incorrectly\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import random",
                "import re",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def getpatterns(symbol):",
                "return \"$$$.{6}|.{3}$$$.{3}|.{6}$$$|$.{2}$.{2}$.{2}|.$.{2}$.{2}$.|.{2}$.{2}$.{2}$|$.{3}$.{3}$|.{2}$.$.$.{2}\".replace(\"$\",symbol)",
                "def checkPatterns(pattern,TicTecBoard):",
                "return len(re.findall(pattern,\"\".join(TicTecBoard)))",
                "def printTicTecBoard(TicTecBoard):",
                "print()",
                "ASSIGN=\"\"",
                "ASSIGN=\"\"",
                "for i in range(0,9):",
                "ASSIGN+= 3*\" \" + str(i+1) + \" \"*3+\"|\"",
                "ASSIGN+= 3*\" \" + TicTecBoard[i] + \" \"*3+\"|\"",
                "if (i+1)%3==0:",
                "ASSIGN=ASSIGN[0:-1]+\" \"*25+strTicTecBoard[0:-1] +\"\\n\"+\"_\"*25+\" \"*25+\"_\"*20+\"\\n\"",
                "ASSIGN=\"\"",
                "print(ASSIGN[0:-75],)",
                "def getValidPlace(TicTecBoard):",
                "return [str(i+1) for i,x in enumerate(TicTecBoard) if x==\"-\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def getpatterns(symbol):",
                "    return \"$$$.{6}|.{3}$$$.{3}|.{6}$$$|$.{2}$.{2}$.{2}|.$.{2}$.{2}$.|.{2}$.{2}$.{2}$|$.{3}$.{3}$|.{2}$.$.$.{2}\".replace(\"$\",symbol)",
                "",
                "def checkPatterns(pattern,TicTecBoard):",
                "    return len(re.findall(pattern,\"\".join(TicTecBoard)))",
                "",
                "def printTicTecBoard(TicTecBoard):",
                "    print(\"\\n\")",
                "    strTicTecBoard=\"\"",
                "    strTicTecBoardLearn=\"\"",
                "    for i in range(0,9):",
                "        strTicTecBoardLearn+= 3*\" \" +  str(i+1) + \" \"*3+\"|\"",
                "        strTicTecBoard+= 3*\" \" + TicTecBoard[i] + \" \"*3+\"|\"",
                "        if (i+1)%3==0:",
                "            strTicTecBoardLearn=strTicTecBoardLearn[0:-1]+\" \"*25+strTicTecBoard[0:-1] +\"\\n\"+\"_\"*25+\" \"*25+\"_\"*20+\"\\n\"",
                "            strTicTecBoard=\"\"",
                "    ",
                "    print(strTicTecBoardLearn[0:-75],\"\\n\")",
                "",
                "def getValidPlace(TicTecBoard):",
                "    return [str(i+1) for i,x in enumerate(TicTecBoard) if x==\"-\"]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=getpatterns(\"O\")",
                "ASSIGN=getpatterns(\"X\")",
                "TicTecBoard=[\"-\" for i in range(9)]",
                "ASSIGN=\"O\"",
                "try:",
                "while True:",
                "ValidIndexList=getValidPlace(TicTecBoard)",
                "ASSIGN==\"O\":",
                "while True:",
                "printTicTecBoard(TicTecBoard)",
                "ASSIGN=input(\" \\n Enter Cell Number from Valid Index List \"+str(ValidIndexList )+\" : \\n\")",
                "if ASSIGN in ValidIndexList:",
                "ASSIGN=int(ASSIGN)-1",
                "break",
                "else:",
                "print()",
                "else:",
                "ASSIGN=0",
                "for place in ValidIndexList:",
                "ASSIGN=list(TicTecBoard)",
                "ASSIGN=int(place)-1",
                "SLICE=ASSIGN",
                "if checkPatterns(ASSIGN,ASSIGN)>0:",
                "ASSIGN=testIndex",
                "ASSIGN=1",
                "ASSIGN==0:",
                "for place in ValidIndexList:",
                "ASSIGN=list(TicTecBoard)",
                "ASSIGN=int(place)-1",
                "SLICE=\"O\"",
                "if checkPatterns(ASSIGN,ASSIGN)>0:",
                "ASSIGN=testIndex",
                "ASSIGN=-1",
                "ASSIGN==0:",
                "ASSIGN=4 if \"5\" in ValidIndexList else int(random.choice(ValidIndexList))-1",
                "SLICE=ASSIGN",
                "if checkPatterns(getpatterns(ASSIGN),TicTecBoard)>0:",
                "printTicTecBoard(TicTecBoard)",
                "print('\\x1b[6;30;42m' +ASSIGN++ '\\x1b[0m')",
                "break",
                "elif checkPatterns(\"-\",TicTecBoard)==0:",
                "printTicTecBoard(TicTecBoard)",
                "print()",
                "break",
                "else:",
                "ASSIGN=\"X\" if ASSIGN==\"O\" else \"O\"",
                "except:",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "",
                "patternO=getpatterns(\"O\")",
                "patternX=getpatterns(\"X\")",
                "TicTecBoard=[\"-\" for i in range(9)]",
                "player=\"O\"",
                "try:",
                "",
                "",
                "",
                "    while True:    ",
                "        ValidIndexList=getValidPlace(TicTecBoard)",
                "        if player==\"O\":",
                "            while True:  ",
                "                printTicTecBoard(TicTecBoard)",
                "                index=input(\" \\n Enter Cell Number from Valid Index List  \"+str(ValidIndexList )+\" : \\n\")",
                "",
                "                if index in ValidIndexList:",
                "                    index=int(index)-1",
                "                    break",
                "                else:",
                "                    print(\"Plz Enter Valied Place\") ",
                "",
                "        else:",
                "            machineWin=0",
                "            for place in ValidIndexList:",
                "                testTicTecBoard=list(TicTecBoard)",
                "                testIndex=int(place)-1",
                "                testTicTecBoard[testIndex]=player",
                "                if checkPatterns(patternX,testTicTecBoard)>0: ",
                "                    index=testIndex",
                "                    machineWin=1",
                "            if machineWin==0:",
                "                for place in ValidIndexList:",
                "                    testTicTecBoard=list(TicTecBoard)",
                "                    testIndex=int(place)-1",
                "                    testTicTecBoard[testIndex]=\"O\"",
                "                    if checkPatterns(patternO,testTicTecBoard)>0: ",
                "                        index=testIndex",
                "                        machineWin=-1",
                "            if machineWin==0:",
                "                index=4 if \"5\" in ValidIndexList else int(random.choice(ValidIndexList))-1",
                "",
                "",
                "",
                "        TicTecBoard[index]=player",
                "",
                "",
                "        if checkPatterns(getpatterns(player),TicTecBoard)>0: ",
                "            printTicTecBoard(TicTecBoard)",
                "            print('\\x1b[6;30;42m'  +player+\" is Win \"+ '\\x1b[0m')  ",
                "            break",
                "        elif checkPatterns(\"-\",TicTecBoard)==0: ",
                "            printTicTecBoard(TicTecBoard)",
                "            print(\"\\033[93m Game is Draw  \\033[0m\")",
                "            break",
                "",
                "        else:",
                "            player=\"X\" if player==\"O\" else \"O\"",
                "",
                "",
                "",
                "except:",
                "    print(\"Plz Enter Valied Index\")",
                "",
                "",
                "",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from sklearn import preprocessing",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "X=pd.read_csv('path')",
                "ASSIGN=pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#read data files",
                "X=pd.read_csv('/kaggle/input/titanic/train.csv')",
                "test=pd.read_csv('/kaggle/input/titanic/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.head()",
                ""
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test.head()",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "SLICE=np.nan",
                "ASSIGN=pd.concat([X,test])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "test['Survived']=np.nan",
                "full=pd.concat([X,test])"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def data_inv(df):",
                "print('Number of Persons: ',df.shape[0])",
                "print('dataset variables: ',df.shape[1])",
                "print('-'*20)",
                "print('dateset columns: \\n')",
                "print(df.columns)",
                "print('-'*20)",
                "print('data-type of each column: \\n')",
                "print(df.dtypes)",
                "print('-'*20)",
                "print('missing rows in each column: \\n')",
                "ASSIGN=df.isnull().sum()",
                "print(ASSIGN[ASSIGN>0])",
                "print('-'*20)",
                "print('Missing vaules %age vise:\\n')",
                "print((100*(df.isnull().sum()path(df.index))))",
                "print('-'*20)",
                "print('Pictorial Representation:')",
                "plt.figure(figsize=(8,6))",
                "sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis')",
                "plt.show()",
                "data_inv(full)"
            ],
            "output_type": "stream",
            "content_old": [
                "",
                "def data_inv(df):",
                "    print('Number of Persons: ',df.shape[0])",
                "    print('dataset variables: ',df.shape[1])",
                "    print('-'*20)",
                "    print('dateset columns: \\n')",
                "    print(df.columns)",
                "    print('-'*20)",
                "    print('data-type of each column: \\n')",
                "    print(df.dtypes)",
                "    print('-'*20)",
                "    print('missing rows in each column: \\n')",
                "    c=df.isnull().sum()",
                "    print(c[c>0])",
                "    print('-'*20)",
                "    print('Missing vaules %age vise:\\n')",
                "    print((100*(df.isnull().sum()/len(df.index))))",
                "    print('-'*20)",
                "    print('Pictorial Representation:')",
                "    plt.figure(figsize=(8,6))",
                "    sns.heatmap(df.isnull(), yticklabels=False,cbar=False, cmap='viridis')",
                "    plt.show()   ",
                "data_inv(full)#function call"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.heatmap(full.corr(), annot = True)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.heatmap(full.corr(), annot = True)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "SLICE=SLICE.fillna(mode(SLICE))",
                "full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True)",
                "ASSIGN = full.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))",
                "full.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#fillna",
                "from statistics import mode",
                "full['Embarked']=full['Embarked'].fillna(mode(full['Embarked'])) ",
                "full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True)",
                "full['Age'] = full.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))",
                "full.isnull().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "SLICE=SLICE+SLICE"
            ],
            "output_type": "not_existent",
            "content_old": [
                "full['Fam']=full['Parch']+full['SibSp']"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=pd.get_dummies(data=ASSIGN,columns=['Sex','Embarked'],drop_first=True)",
                "ASSIGN.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "full=pd.get_dummies(data=full,columns=['Sex','Embarked'],drop_first=True)",
                "full.info()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.drop(['Cabin','Ticket','Name','Parch','SibSp'],axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "full=full.drop(['Cabin','Ticket','Name','Parch','SibSp'],axis=1)",
                ""
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "preprocessing.StandardScaler().fit(full).transform(full.astype(float))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Data Standardization ",
                "preprocessing.StandardScaler().fit(full).transform(full.astype(float))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = full[full['Survived'].isna()].drop(['Survived'], axis = 1)",
                "ASSIGN = full[full['Survived'].notna()]",
                "ASSIGN.info()"
            ],
            "output_type": "stream",
            "content_old": [
                "test = full[full['Survived'].isna()].drop(['Survived'], axis = 1)",
                "train = full[full['Survived'].notna()]",
                "train.info()",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]",
                "ASSIGN=train[['Survived']].astype(np.int8)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "",
                "",
                "X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]",
                "",
                "y=train[['Survived']].astype(np.int8)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN=[]",
                "ASSIGN=[]",
                "for cols in range(50,55):",
                "for rows in range(3,5):",
                "ASSIGN=(cols,rows)",
                "MLPClassifierModel = MLPClassifier(activation='logistic',",
                "ASSIGN='lbfgs',",
                "ASSIGN=0.1 ,hidden_layer_sizes=hidden_layer,random_state=33)",
                "MLPClassifierModel.fit(X_train, y_train)",
                "ASSIGN = MLPClassifierModel.predict(X_test)",
                "ASSIGN.append(MLPClassifierModel.score(X_test, y_test))",
                "ASSIGN.append(str(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "Scores=[]",
                "hidden_layer_sizes=[]",
                "",
                "",
                "for cols in range(50,55):",
                "    for rows in range(3,5):",
                "        hidden_layer=(cols,rows)",
                "",
                "        from sklearn.neural_network import MLPClassifier",
                "        MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu",
                "                                           solver='lbfgs',  # can be lbfgs also sgd , adam",
                "                                           alpha=0.1 ,hidden_layer_sizes=hidden_layer,random_state=33)",
                "        MLPClassifierModel.fit(X_train, y_train)",
                "",
                "        MLPClassifier_y_pred = MLPClassifierModel.predict(X_test)",
                "        Scores.append(MLPClassifierModel.score(X_test, y_test))",
                "        hidden_layer_sizes.append(str(hidden_layer))",
                "        ",
                "",
                "",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({",
                "'hidden_layer': hidden_layer_sizes,",
                "'Score': Scores})",
                "ASSIGN.sort_values(by='Score', ascending=False )"
            ],
            "output_type": "execute_result",
            "content_old": [
                "models = pd.DataFrame({",
                "    'hidden_layer': hidden_layer_sizes,",
                "    'Score': Scores})",
                "models.sort_values(by='Score', ascending=False )",
                "",
                "",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(hidden_layer_sizes,Scores)",
                "plt.ylabel('Accuracy ')",
                "plt.xlabel('hidden_layer_sizes ')",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                " ",
                "plt.plot(hidden_layer_sizes,Scores)",
                "plt.ylabel('Accuracy ')",
                "plt.xlabel('hidden_layer_sizes ')",
                "plt.tight_layout()",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "MLPClassifierModel = MLPClassifier(activation='logistic',",
                "ASSIGN='lbfgs',",
                "ASSIGN='adaptive',",
                "ASSIGN= False,",
                "ASSIGN=0.1 ,hidden_layer_sizes=(52, 3),random_state=33)",
                "MLPClassifierModel.fit(X_train, y_train)",
                "ASSIGN = MLPClassifierModel.predict(X_test)",
                "MLPClassifierModel.fit(X, y)",
                "ASSIGN= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.neural_network import MLPClassifier",
                "MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu",
                "                                   solver='lbfgs',  # can be lbfgs also sgd , adam",
                "                                   learning_rate='adaptive', # can be constant also invscaling , adaptive",
                "                                   early_stopping= False,",
                "                                   alpha=0.1 ,hidden_layer_sizes=(52, 3),random_state=33)",
                "MLPClassifierModel.fit(X_train, y_train)",
                "",
                "MLPClassifier_y_pred = MLPClassifierModel.predict(X_test)",
                "MLPClassifierModel.fit(X, y)",
                "MLPClassifier_y_pred= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])"
            ]
        },
        {
            "tags": [
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "ASSIGN=test['PassengerId']",
                "ASSIGN=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred})",
                "ASSIGN.to_csv('submission.csv',index=False)",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Id=test['PassengerId']",
                "sub_df=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred})",
                "sub_df.to_csv('submission.csv',index=False)",
                "sub_df.head()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os",
                "import json"
            ]
        },
        {
            "tags": [
                "process_data",
                "transfer_results"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = []",
                "ASSIGN['people'].append({",
                "'name': 'Scott',",
                "'website': 'stackabuse.com',",
                "'from': 'Nebraska'",
                "})",
                "ASSIGN['people'].append({",
                "'name': 'Larry',",
                "'website': 'google.com',",
                "'from': 'Michigan'",
                "})",
                "ASSIGN['people'].append({",
                "'name': 'Tim',",
                "'website': 'apple.com',",
                "'from': 'Alabama'",
                "})",
                "with open('ASSIGN.json', 'w') as outfile:",
                "json.dump(ASSIGN, outfile)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "",
                "data = {}",
                "data['people'] = []",
                "data['people'].append({",
                "    'name': 'Scott',",
                "    'website': 'stackabuse.com',",
                "    'from': 'Nebraska'",
                "})",
                "data['people'].append({",
                "    'name': 'Larry',",
                "    'website': 'google.com',",
                "    'from': 'Michigan'",
                "})",
                "data['people'].append({",
                "    'name': 'Tim',",
                "    'website': 'apple.com',",
                "    'from': 'Alabama'",
                "})",
                "",
                "with open('data.json', 'w') as outfile:",
                "    json.dump(data, outfile)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN=\"people\""
            ],
            "output_type": "not_existent",
            "content_old": [
                "dataset_name=\"people\"",
                "",
                "API={\"username\":\"tareksherif\",\"key\":\"f4cf963ba526c529b3a9b0ea5058e6f0\"}"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "os.environ['KAGGLE_USERNAME'] = API[\"username\"]",
                "os.environ['KAGGLE_KEY'] = API[\"key\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "",
                "os.environ['KAGGLE_USERNAME'] = API[\"username\"]",
                "os.environ['KAGGLE_KEY'] = API[\"key\"]"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = {",
                "\"title\": dataset_name,",
                "\"id\": os.environ['KAGGLE_USERNAME']+\"path\"+dataset_name,",
                "\"licenses\": [",
                "{",
                "\"name\": \"CC0-1.0\"",
                "}",
                "]",
                "}",
                "with open('dataset-metadata.json', 'w') as outfile:",
                "json.dump(ASSIGN, outfile)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "data = {",
                "  \"title\": dataset_name,",
                "  \"id\": os.environ['KAGGLE_USERNAME']+\"/\"+dataset_name,",
                "  \"licenses\": [",
                "    {",
                "      \"name\": \"CC0-1.0\"",
                "    }",
                "  ]",
                "}",
                " ",
                "with open('dataset-metadata.json', 'w') as outfile:",
                "    json.dump(data, outfile)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "!kaggle datasets create -p ."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "warnings.filterwarnings('ignore')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from scipy.stats import norm\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.mixture import GaussianMixture\n",
                "import scipy.stats as st\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "%matplotlib inline\n",
                "import missingno as msno"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(TRAIN_DATASET_PATH)",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "TRAIN_DATASET_PATH = '/kaggle/input/realestatepriceprediction/train.csv'\n",
                "\n",
                "df_train = pd.read_csv(TRAIN_DATASET_PATH)\n",
                "\n",
                "df_train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_train.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "msno.matrix(df_train.sample(250));"
            ],
            "output_type": "display_data",
            "content_old": [
                "msno.matrix(df_train.sample(250));"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = df_train.isnull().sum().sort_values(ascending=False)",
                "ASSIGN = (df_train.isnull().sum()path().count()).sort_values(ascending=False)",
                "ASSIGN = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent'])",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "total = df_train.isnull().sum().sort_values(ascending=False)\n",
                "percentage = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
                "missing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent'])\n",
                "missing_data.head()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train.corr()",
                "ASSIGN = plt.subplots(figsize=(12, 9))",
                "sns.heatmap(ASSIGN, vmax=.8, annot=True, fmt=' .2f', square=True);"
            ],
            "output_type": "display_data",
            "content_old": [
                "corrmat = df_train.corr()\n",
                "f, ax = plt.subplots(figsize=(12, 9))\n",
                "sns.heatmap(corrmat, vmax=.8, annot=True, fmt=' .2f', square=True);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = 10",
                "ASSIGN = corrmat.nlargest(k, 'Price')['Price'].index",
                "sns.set(font_scale=1.5)",
                "ASSIGN = sns.heatmap(df_train[cols].corr(), annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values);"
            ],
            "output_type": "display_data",
            "content_old": [
                "k = 10\n",
                "cols = corrmat.nlargest(k, 'Price')['Price'].index\n",
                "sns.set(font_scale=1.5)\n",
                "hm = sns.heatmap(df_train[cols].corr(), annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.set()",
                "ASSIGN = ['Price','DistrictId', 'Rooms', 'Square', 'Social_3', 'Floor', 'Helthcare_2', 'Shops_1', 'Healthcare_1']",
                "sns.pairplot(df_train[ASSIGN], size = 2.8)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.set()\n",
                "cols = ['Price','DistrictId', 'Rooms', 'Square', 'Social_3', 'Floor', 'Helthcare_2', 'Shops_1', 'Healthcare_1']\n",
                "sns.pairplot(df_train[cols], size = 2.8)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['Price','DistrictId', 'Rooms', 'Square', 'LifeSquare', 'Social_1', 'Shops_1']",
                "ASSIGN = pd.concat([df_train[cols], pd.Series(np.int8(df_train['Rooms'] == 0), name='flag')], axis=1)",
                "ASSIGN.loc[ASSIGN['Rooms'] == 0]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "cols = ['Price','DistrictId', 'Rooms', 'Square', 'LifeSquare', 'Social_1', 'Shops_1']\n",
                "df_train_temp = pd.concat([df_train[cols], pd.Series(np.int8(df_train['Rooms'] == 0), name='flag')], axis=1)\n",
                "df_train_temp.loc[df_train_temp['Rooms'] == 0]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.pairplot(df_train_temp, size = 2.5, hue='flag')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.pairplot(df_train_temp, size = 2.5, hue='flag')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize = (45, 10))",
                "sns.countplot(x = 'DistrictId', data = df_train)",
                "ASSIGN = plt.xticks(rotation=90)"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize = (45, 10))\n",
                "sns.countplot(x = 'DistrictId', data = df_train)\n",
                "xt = plt.xticks(rotation=90)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train['DistrictId'].value_counts()",
                "ASSIGN = district[district > 50]",
                "ASSIGN = district[district <= 50]",
                "ASSIGN = {'count_districts_gr_50': district_gr_50.count(), 'count_districts_ls_50': district_ls_50.count()}",
                "ASSIGN = {'pop_districts_gr_50': district_gr_50.sum(), 'pop_districts_ls_50': district_ls_50.sum()}",
                "ASSIGN = plt.subplots(1, 2)",
                "fig.set_size_inches(15, 8)",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)",
                "ax[0].set_title('Districts count')",
                "sns.barplot(list(ASSIGN.keys()), list(ASSIGN.values()), ax=ax[0])",
                "ax[1].set_title('Districts pop')",
                "sns.barplot(list(ASSIGN.keys()), list(ASSIGN.values()), ax=ax[1])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "district = df_train['DistrictId'].value_counts()\n",
                "district_gr_50 = district[district > 50]\n",
                "district_ls_50 = district[district <= 50]\n",
                "districts = {'count_districts_gr_50': district_gr_50.count(), 'count_districts_ls_50': district_ls_50.count()}\n",
                "districts_2 = {'pop_districts_gr_50': district_gr_50.sum(), 'pop_districts_ls_50': district_ls_50.sum()}\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(15, 8)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "#plt.figure(figsize=(12,8))    \n",
                "ax[0].set_title('Districts count')\n",
                "sns.barplot(list(districts.keys()), list(districts.values()), ax=ax[0])\n",
                "\n",
                "ax[1].set_title('Districts pop')\n",
                "sns.barplot(list(districts_2.keys()), list(districts_2.values()), ax=ax[1])\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(df_train['Square']);"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.distplot(df_train['Square']);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.distplot(df_train.loc[df_train['Square'] < 200,'Square'])",
                "plt.plot([25 for x in range(330)], [xpath(330)], ls='--', c='r')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.distplot(df_train.loc[df_train['Square'] < 200,'Square'])\n",
                "plt.plot([25 for x in range(330)], [x/10000 for x in range(330)], ls='--', c='r')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.loc[df_train['Square'] < 25, 'Square'].count()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.loc[df_train['Square'] < 25, 'Square'].count()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train.loc[df_train['Rooms'] > 5, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.loc[df_train['Rooms'] > 5, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train.loc[df_train['Rooms'] == 0, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.loc[df_train['Rooms'] == 0, ['DistrictId', 'Square', 'KitchenSquare', 'Rooms', 'Price']]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df_train['Square'], df_train['KitchenSquare'])",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')",
                "plt.plot([y for y in range(200)], [ 0 for x in range(200)], c = 'g')",
                "plt.xlabel('Square')",
                "plt.ylabel('KitchenSquare')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.scatter(df_train['Square'], df_train['KitchenSquare'])\n",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')\n",
                "plt.plot([y for y in range(200)], [ 0 for x in range(200)], c = 'g')\n",
                "plt.xlabel('Square')\n",
                "plt.ylabel('KitchenSquare')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train['KitchenSquare'] < df_train['Square'] + 0.5*df_train['Square'].std()",
                "plt.scatter(df_train.loc[ASSIGN, 'Square'], df_train.loc[ASSIGN, 'KitchenSquare'])",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')",
                "plt.plot([y for y in range(300)], [ 0 for x in range(300)], c = 'g')",
                "plt.xlabel('Square')",
                "plt.ylabel('KitchenSquare')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cond = df_train['KitchenSquare'] < df_train['Square'] + 0.5*df_train['Square'].std()\n",
                "plt.scatter(df_train.loc[cond, 'Square'], df_train.loc[cond, 'KitchenSquare'])\n",
                "plt.plot([x for x in range(120)], [y for y in range(120)], c = 'r')\n",
                "plt.plot([y for y in range(300)], [ 0 for x in range(300)], c = 'g')\n",
                "plt.xlabel('Square')\n",
                "plt.ylabel('KitchenSquare')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = (df_train['KitchenSquare'] >= 3) & (abs(df_train['Square'] - df_train['KitchenSquare']) > 10) &\\",
                "(df_train['KitchenSquare'] < 50) & (df_train['Square'] < 200)",
                "ASSIGN = df_train.loc[cond, ['Square', 'KitchenSquare']]",
                "ASSIGN = sns.jointplot(temp['Square'], temp['KitchenSquare'], kind='reg')",
                "plt.plot(np.arange(0, 40), np.arange(0, 40), color = 'red', linestyle='--')",
                "ASSIGN.fig.set_figwidth(8)",
                "ASSIGN.fig.set_figheight(8)"
            ],
            "output_type": "display_data",
            "content_old": [
                "cond = (df_train['KitchenSquare'] >= 3) & (abs(df_train['Square'] - df_train['KitchenSquare']) > 10) &\\\n",
                "                    (df_train['KitchenSquare'] < 50) &  (df_train['Square'] < 200)\n",
                "temp = df_train.loc[cond, ['Square', 'KitchenSquare']]\n",
                "grid = sns.jointplot(temp['Square'], temp['KitchenSquare'], kind='reg')\n",
                "plt.plot(np.arange(0, 40), np.arange(0, 40), color = 'red', linestyle='--')\n",
                "grid.fig.set_figwidth(8)\n",
                "grid.fig.set_figheight(8)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = ~df_train['LifeSquare'].isna()",
                "plt.plot([x for x in range(600)], [y for y in range(600)], c = 'r')",
                "sns.scatterplot(df_train.loc[ASSIGN, 'Square'], df_train.loc[ASSIGN, 'LifeSquare']);"
            ],
            "output_type": "display_data",
            "content_old": [
                "cond = ~df_train['LifeSquare'].isna()\n",
                "plt.plot([x for x in range(600)], [y for y in range(600)], c = 'r')\n",
                "sns.scatterplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare']);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = (~df_train['LifeSquare'].isna()) & (df_train['LifeSquare'] < df_train['LifeSquare'].quantile(q = 0.999)) & \\",
                "(df_train['Square'] < df_train['Square'].quantile(q = 0.999))",
                "ASSIGN = sns.jointplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare'], kind='reg')",
                "plt.plot(np.arange(0, 200), np.arange(0, 200), color = 'red', linestyle='--')",
                "ASSIGN.fig.set_figwidth(8)",
                "ASSIGN.fig.set_figheight(8)"
            ],
            "output_type": "display_data",
            "content_old": [
                "cond = (~df_train['LifeSquare'].isna()) & (df_train['LifeSquare'] < df_train['LifeSquare'].quantile(q = 0.999)) & \\\n",
                "                                        (df_train['Square'] < df_train['Square'].quantile(q = 0.999))\n",
                "grid = sns.jointplot(df_train.loc[cond, 'Square'], df_train.loc[cond, 'LifeSquare'], kind='reg')\n",
                "plt.plot(np.arange(0, 200), np.arange(0, 200), color = 'red', linestyle='--')\n",
                "grid.fig.set_figwidth(8)\n",
                "grid.fig.set_figheight(8)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train['HouseYear'].unique()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train['HouseYear'].unique()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['g' if i == 0.0 else 'b' for i in df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor']]",
                "ASSIGN.count('g')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "colors = ['g' if i == 0.0 else 'b' for i in df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor']]\n",
                "colors.count('g')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor'], df_train.loc[df_train['HouseFloor'] < 60, 'Floor'], c=colors)",
                "plt.plot([x for x in range(40)], [y for y in range(40)], c = 'r')",
                "plt.plot([x for x in range(40)], [y + 2 for y in range(40)], c = 'black')",
                "plt.xlabel('HouseFloor')",
                "plt.ylabel('Floor')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "\n",
                "plt.scatter(df_train.loc[df_train['HouseFloor'] < 60, 'HouseFloor'], df_train.loc[df_train['HouseFloor'] < 60, 'Floor'], c=colors)\n",
                "\n",
                "plt.plot([x for x in range(40)], [y for y in range(40)], c = 'r')\n",
                "plt.plot([x for x in range(40)], [y + 2 for y in range(40)], c = 'black')\n",
                "plt.xlabel('HouseFloor')\n",
                "plt.ylabel('Floor')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.scatterplot(df_train.loc[~df_train['Healthcare_1'].isna(), 'Healthcare_1'], df_train.loc[~df_train['Healthcare_1'].isna(), 'DistrictId']);"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.scatterplot(df_train.loc[~df_train['Healthcare_1'].isna(), 'Healthcare_1'], df_train.loc[~df_train['Healthcare_1'].isna(), 'DistrictId']);"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(35, 8))",
                "ASSIGN = ~df_train['Healthcare_1'].isna()",
                "ASSIGN = sns.boxplot(df_train.loc[cond, 'DistrictId'], df_train.loc[cond, 'Healthcare_1'])",
                "ASSIGN.set_xticklabels(ASSIGN.get_xticklabels(), rotation=90)",
                "plt.xlabel('DistrictId')",
                "plt.ylabel('Healthcare_1')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "plt.figure(figsize=(35, 8))\n",
                "cond = ~df_train['Healthcare_1'].isna()\n",
                "\n",
                "s = sns.boxplot(df_train.loc[cond, 'DistrictId'], df_train.loc[cond, 'Healthcare_1'])\n",
                "s.set_xticklabels(s.get_xticklabels(), rotation=90)\n",
                "\n",
                "plt.xlabel('DistrictId')\n",
                "plt.ylabel('Healthcare_1')\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df_train['Price'].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_train['Price'].describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print( % df_train['Price'].skew())",
                "print( % df_train['Price'].kurt())"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"Skewness: %f\" % df_train['Price'].skew())\n",
                "print(\"Kurtosis: %f\" % df_train['Price'].kurt())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train['Price']",
                "ASSIGN = plt.subplots(1, 2)",
                "fig.set_size_inches(14, 6)",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)",
                "sns.distplot(ASSIGN, kde=False, fit=st.norm, ax=ax[0])",
                "sns.distplot(pd.Series(np.log(ASSIGN), name ='LogPrice'), kde=False, fit=st.norm, ax=ax[1])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "y = df_train['Price']\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(14, 6)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "sns.distplot(y, kde=False, fit=st.norm, ax=ax[0])\n",
                "\n",
                "sns.distplot(pd.Series(np.log(y), name ='LogPrice'), kde=False, fit=st.norm, ax=ax[1])\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = df_train['Price']",
                "ASSIGN = plt.subplots(1, 2)",
                "fig.set_size_inches(14, 6)",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)",
                "sns.distplot(ASSIGN, kde=False, fit=st.johnsonsu, ax=ax[0])",
                "ASSIGN = st.johnsonsu.fit(y)",
                "ASSIGN = (y-params[2])path[3]",
                "ASSIGN = params[0] + params[1]*np.log(t + np.sqrt(np.power(t, 2) + 1))",
                "sns.distplot(pd.Series(ASSIGN, name ='TransformedToNormalPrice'), fit=st.norm, ax=ax[1])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "y = df_train['Price']\n",
                "\n",
                "fig, ax = plt.subplots(1, 2)\n",
                "\n",
                "fig.set_size_inches(14, 6)\n",
                "fig.subplots_adjust(wspace=0.3, hspace=0.3)\n",
                "\n",
                "sns.distplot(y, kde=False, fit=st.johnsonsu, ax=ax[0])\n",
                "\n",
                "#      \n",
                "params = st.johnsonsu.fit(y)\n",
                "t = (y-params[2])/params[3]\n",
                "y_norm = params[0] + params[1]*np.log(t + np.sqrt(np.power(t, 2) + 1))\n",
                "\n",
                "sns.distplot(pd.Series(y_norm, name ='TransformedToNormalPrice'), fit=st.norm, ax=ax[1])\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = params[2]*np.sinh((y_norm - params[0])path[1]) + params[3]",
                "sns.distplot(ASSIGN, fit=st.johnsonsu)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "y_back_to_Johnson = params[2]*np.sinh((y_norm - params[0])/params[1]) + params[3]\n",
                "\n",
                "sns.distplot(y_back_to_Johnson, fit=st.johnsonsu)\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "%matplotlib inline\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Image(filename='path')",
                "display(ASSIGN)"
            ],
            "output_type": "error",
            "content_old": [
                "from IPython.display import Image \n",
                "pil_img = Image(filename='/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/s/color_18_0100.png')\n",
                "\n",
                "display(pil_img)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "VALIDATION",
                "path"
            ],
            "output_type": "error",
            "content_old": [
                "/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/a/color_0_0002.png"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "SETUP",
                "ASSIGN = next(os.walk(\"path\"))",
                "ASSIGN = len(files)",
                "file_count"
            ],
            "output_type": "error",
            "content_old": [
                "#TO_DO\n",
                "#1 prepare the X_train , X_test \n",
                "\n",
                "import os\n",
                "\n",
                "path, dirs, files = next(os.walk(\"/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b\"))\n",
                "file_count = len(files)\n",
                "\n",
                "file_count"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for filename in glob.glob(os.path.join(directory_a, '*.png')):",
                "ASSIGN =cv2.imread(filename,0)",
                "print(ASSIGN.shape)"
            ],
            "output_type": "error",
            "content_old": [
                "for filename in glob.glob(os.path.join(directory_a, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    print(im1.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "def giveMeFeatures(image):",
                "ASSIGN = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')",
                "return res"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from skimage.feature import hog\n",
                "def giveMeFeatures(image):\n",
                "    res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')\n",
                "#     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)\n",
                "    return res\n",
                "\n",
                "\n",
                "    \n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = []",
                "ASSIGN = []",
                "for filename in glob.glob(os.path.join(ASSIGN, '*.png')):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN = giveMeFeatures(im1)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(0)",
                "for filename in glob.glob(os.path.join(ASSIGN, '*.png')):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN = giveMeFeatures(im1)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(1)",
                "ASSIGN = np.array(np.float32(ASSIGN))",
                "ASSIGN = np.array(np.float32(ASSIGN))",
                "ASSIGN = np.random.RandomState(321)",
                "ASSIGN = rand.permutation(len(X))",
                "ASSIGN = ASSIGN[shuffle]",
                "ASSIGN = ASSIGN[shuffle]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import glob\n",
                "import cv2\n",
                "directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'\n",
                "directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "\n",
                "for filename in glob.glob(os.path.join(directory_a, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features = giveMeFeatures(im1)\n",
                "    X.append(features)\n",
                "    y.append(0)\n",
                "\n",
                "for filename in glob.glob(os.path.join(directory_b, '*.png')):\n",
                "    im1 =cv2.imread(filename,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features = giveMeFeatures(im1)\n",
                "    X.append(features)\n",
                "    y.append(1)\n",
                "    \n",
                "X = np.array(np.float32(X))\n",
                "y = np.array(np.float32(y))\n",
                "\n",
                "\n",
                "rand = np.random.RandomState(321)\n",
                "shuffle = rand.permutation(len(X))\n",
                "X = X[shuffle]\n",
                "y = y[shuffle]\n",
                "    \n",
                "    \n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ],
            "output_type": "error",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN=svm.SVC()",
                "ASSIGN = {'C': [2,3,4,5,6,7,8,9,10,11,12],",
                "'kernel': ['rbf']}",
                "ASSIGN = GridSearchCV(model, param_grid=params, n_jobs=-1)",
                "ASSIGN.fit(X_train,y_train)",
                "print(,ASSIGN.best_params_)",
                "ASSIGN=model1.predict(X_test)"
            ],
            "output_type": "error",
            "content_old": [
                "#With Hyper Parameters Tuning\n",
                "#2-3,SVM\n",
                "#importing modules\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "from sklearn import svm\n",
                "#making the instance\n",
                "model=svm.SVC()\n",
                "#Hyper Parameters Set\n",
                "params = {'C': [2,3,4,5,6,7,8,9,10,11,12], \n",
                "          'kernel': ['rbf']}\n",
                "#Making models with hyper parameters sets\n",
                "model1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n",
                "#Learning\n",
                "model1.fit(X_train,y_train)\n",
                "#The best hyper parameters set\n",
                "print(\"Best Hyper Parameters:\\n\",model1.best_params_)\n",
                "#Prediction\n",
                "prediction=model1.predict(X_test)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "classification_report(y_test, prediction)"
            ],
            "output_type": "error",
            "content_old": [
                "from sklearn.metrics import classification_report,accuracy_score\n",
                "classification_report(y_test, prediction)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(+str(accuracy_score(y_test, prediction)))"
            ],
            "output_type": "error",
            "content_old": [
                "print(\"Accuracy: \"+str(accuracy_score(y_test, prediction)))\n",
                "# model1.score(X_test,y_test)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "def testModel(path):",
                "ASSIGN =cv2.imread(path,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN=[]",
                "ASSIGN.append(giveMeFeatures(ASSIGN))",
                "ASSIGN = np.array(np.float32(ASSIGN))",
                "ASSIGN =model1.predict(features)",
                "if(ASSIGN[0]==0):",
                "return 'fist'",
                "else:",
                "return 'palm'",
                "return"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def testModel(path):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    features=[]\n",
                "    features.append(giveMeFeatures(im1))\n",
                "    features = np.array(np.float32(features))    \n",
                "    res =model1.predict(features)\n",
                "    if(res[0]==0):\n",
                "        return 'fist'\n",
                "    else:\n",
                "        return 'palm'\n",
                "        \n",
                "    return \n",
                "    "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "testModel('path')"
            ],
            "output_type": "error",
            "content_old": [
                "testModel('/kaggle/input/testdata2/palm2.jpg')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "testModel('path')"
            ],
            "output_type": "error",
            "content_old": [
                "testModel('/kaggle/input/testdata2/palm1.jpg')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import os\n",
                "import matplotlib.pyplot as plt \n",
                "import cv2 as cv\n",
                "\n",
                "from keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\n",
                "from keras import models\n",
                "from keras.optimizers import Adam,RMSprop \n",
                "from keras.preprocessing.image import ImageDataGenerator\n",
                "from keras.callbacks import ReduceLROnPlateau\n",
                "\n",
                "import pickle\n",
                "\n",
                "%matplotlib inline"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = models.Sequential()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "model = models.Sequential()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))",
                "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))",
                "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))",
                "model.add(Flatten())",
                "model.add(Dense(units = 512 , activation = 'relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(units = 2 , activation = 'softmax'))",
                "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])",
                "model.summary()"
            ],
            "output_type": "stream",
            "content_old": [
                "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
                "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(units = 512 , activation = 'relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(units = 2 , activation = 'softmax'))\n",
                "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = 0.001",
                "ASSIGN = \"sparse_categorical_crossentropy\"",
                "model.compile(Adam(lr=ASSIGN), ASSIGN=ASSIGN ,metrics=['accuracy'])",
                "model.summary()"
            ],
            "output_type": "stream",
            "content_old": [
                "initial_lr = 0.001\n",
                "loss = \"sparse_categorical_crossentropy\"\n",
                "model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])\n",
                "model.summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = ['*.png', '*.jpg']",
                "ASSIGN=0",
                "ASSIGN=0",
                "for typ in ASSIGN:",
                "for directory in ASSIGN:",
                "for filename in glob.glob(os.path.join(directory, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(28,28))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append([1,0])",
                "ASSIGN+=1",
                "for directory in ASSIGN:",
                "for filename in glob.glob(os.path.join(directory, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(28,28))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append([0,1])",
                "ASSIGN+=1",
                "print('A: ', ASSIGN)",
                "print('B: ', ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "import glob\n",
                "import cv2\n",
                "directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'\n",
                "directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'\n",
                "directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_s = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/s'\n",
                "\n",
                "ADirectories = []\n",
                "BDirectories = []\n",
                "\n",
                "ADirectories.append(directory_a)\n",
                "ADirectories.append(directory_s)\n",
                "BDirectories.append(directory_b)\n",
                "BDirectories.append(directory_5)\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "countA=0\n",
                "countB=0\n",
                "for typ in types:\n",
                "    for directory in ADirectories:\n",
                "        for filename in glob.glob(os.path.join(directory, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(28,28))\n",
                "            X.append(im1)\n",
                "            y.append([1,0])\n",
                "            countA+=1\n",
                "    for directory in BDirectories:\n",
                "        for filename in glob.glob(os.path.join(directory, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(28,28))\n",
                "            X.append(im1)\n",
                "            y.append([0,1])\n",
                "            countB+=1\n",
                "print('A: ', countA)\n",
                "print('B: ', countB)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X=Xpath"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X=X/255"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.reshape(4527, 28, 28,1)"
            ],
            "output_type": "error",
            "content_old": [
                "# y = y.reshape(4527,1)\n",
                "X = X.reshape(4527, 28, 28,1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y[2].shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y[2].shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = LabelBinarizer()",
                "ASSIGN = label_binarizer.fit_transform(ASSIGN)",
                "ASSIGN = label_binarizer.fit_transform(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import LabelBinarizer\n",
                "label_binarizer = LabelBinarizer()\n",
                "y_train = label_binarizer.fit_transform(y_train)\n",
                "y_test = label_binarizer.fit_transform(y_test)"
            ]
        },
        {
            "tags": [
                "train_model",
                "process_data"
            ],
            "content": [
                "ASSIGN = ImageDataGenerator(",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=False,",
                "ASSIGN=10,",
                "ASSIGN = 0.1,",
                "ASSIGN=0.1,",
                "ASSIGN=0.1,",
                "ASSIGN=False,",
                "ASSIGN=False)",
                "ASSIGN.fit(X_train)"
            ],
            "output_type": "error",
            "content_old": [
                "# With data augmentation to prevent overfitting\n",
                "\n",
                "datagen = ImageDataGenerator(\n",
                "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
                "        samplewise_center=False,  # set each sample mean to 0\n",
                "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
                "        samplewise_std_normalization=False,  # divide each input by its std\n",
                "        zca_whitening=False,  # apply ZCA whitening\n",
                "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
                "        zoom_range = 0.1, # Randomly zoom image \n",
                "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
                "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
                "        horizontal_flip=False,  # randomly flip images\n",
                "        vertical_flip=False)  # randomly flip images\n",
                "\n",
                "\n",
                "datagen.fit(X_train)"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = 20",
                "ASSIGN = 256",
                "ASSIGN = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_test, y_test))"
            ],
            "output_type": "error",
            "content_old": [
                "epochs = 20\n",
                "batch_size = 256\n",
                "# history_1 = model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,validation_data=[X_test,y_test])\n",
                "history_1 = model.fit(datagen.flow(X_train,y_train, batch_size = 128) ,epochs = 20 , validation_data = (X_test, y_test))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.figure(figsize=(20,7))",
                "ASSIGN.add_subplot(121)",
                "plt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set",
                "plt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set",
                "plt.title(\"Accuracy Curve\",fontsize=18)",
                "plt.xlabel(\"Epochs\",fontsize=15)",
                "plt.ylabel(\"Accuracy\",fontsize=15)",
                "plt.grid(alpha=0.3)",
                "plt.legend()",
                "ASSIGN.add_subplot(122)",
                "plt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") # Loss curve for training set",
                "plt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\") # Loss curve for validation set",
                "plt.title(\"Loss Curve\",fontsize=18)",
                "plt.xlabel(\"Epochs\",fontsize=15)",
                "plt.ylabel(\"Loss\",fontsize=15)",
                "plt.grid(alpha=0.3)",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "error",
            "content_old": [
                "# Diffining Figure\n",
                "f = plt.figure(figsize=(20,7))\n",
                "\n",
                "#Adding Subplot 1 (For Accuracy)\n",
                "f.add_subplot(121)\n",
                "\n",
                "plt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\n",
                "plt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n",
                "\n",
                "plt.title(\"Accuracy Curve\",fontsize=18)\n",
                "plt.xlabel(\"Epochs\",fontsize=15)\n",
                "plt.ylabel(\"Accuracy\",fontsize=15)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.legend()\n",
                "\n",
                "#Adding Subplot 1 (For Loss)\n",
                "f.add_subplot(122)\n",
                "\n",
                "plt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") # Loss curve for training set\n",
                "plt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n",
                "\n",
                "plt.title(\"Loss Curve\",fontsize=18)\n",
                "plt.xlabel(\"Epochs\",fontsize=15)\n",
                "plt.ylabel(\"Loss\",fontsize=15)\n",
                "plt.grid(alpha=0.3)\n",
                "plt.legend()\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "def testCNNModel(path,model):",
                "ASSIGN =cv2.imread(path,0)",
                "ASSIGN = cv2.resize(ASSIGN,(28,28))",
                "ASSIGN = []",
                "ASSIGN.append(ASSIGN.reshape(28,28))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = ASSIGN.reshape(1,28,28,1)",
                "ASSIGN =model.predict(t)",
                "return res",
                "return"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(28,28))\n",
                "    t = []\n",
                "    t.append(im1.reshape(28,28))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,28,28,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('path',model)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "testCNNModel('/kaggle/input/testdata2/fist1.jpg',model)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "label_binarizer.transform(np.asarray([0]))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "label_binarizer.transform(np.asarray([0]))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = ['*.png', '*.jpg']",
                "for typ in ASSIGN:",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(0)",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(1)",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(2)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import glob\n",
                "import cv2\n",
                "directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_2 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/2'\n",
                "directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        for filename in glob.glob(os.path.join(directory_5, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "            \n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.reshape(-1,64,64,1)",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = Xpath",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(16, (2,2), input_shape=ASSIGN.shape[1:], activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))",
                "ASSIGN.add(Conv2D(32, (3,3), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))",
                "ASSIGN.add(Conv2D(64, (5,5), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(3, activation='softmax'))",
                "ASSIGN = TensorBoard(log_dir=\"path{}\".format(NAME))",
                "ASSIGN.compile(loss='sparse_categorical_crossentropy',",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(ASSIGN, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[ASSIGN])"
            ],
            "output_type": "stream",
            "content_old": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(3, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "def testCNNModel(path,model):",
                "ASSIGN =cv2.imread(path,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN = []",
                "ASSIGN.append(ASSIGN.reshape(64,64))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = ASSIGN.reshape(1,64,64,1)",
                "ASSIGN =model.predict(t)",
                "return res",
                "return"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('path',model)"
            ],
            "output_type": "error",
            "content_old": [
                "testCNNModel('/kaggle/input/testdata2/two14.jpg',model)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = ['*.png', '*.jpg']",
                "for typ in ASSIGN:",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(0)",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(1)",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(2)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import glob\n",
                "import cv2\n",
                "directory_1 = '/kaggle/input/3shapesdataset/resized/1'\n",
                "directory_2 = '/kaggle/input/3shapesdataset/resized/2'\n",
                "directory_3 = '/kaggle/input/3shapesdataset/resized/3'\n",
                "#directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_1, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        for filename in glob.glob(os.path.join(directory_3, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "#         for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "#             im1 =cv2.imread(filename,0)\n",
                "#             im1 = cv2.resize(im1,(64,64))\n",
                "#             X.append(im1)\n",
                "#             y.append(3)\n",
                "            \n",
                "            \n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.reshape(-1,64,64,1)",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = Xpath",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(16, (2,2), input_shape=ASSIGN.shape[1:], activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))",
                "ASSIGN.add(Conv2D(32, (3,3), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))",
                "ASSIGN.add(Conv2D(64, (5,5), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(3, activation='softmax'))",
                "ASSIGN = TensorBoard(log_dir=\"path{}\".format(NAME))",
                "ASSIGN.compile(loss='sparse_categorical_crossentropy',",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(ASSIGN, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[ASSIGN])"
            ],
            "output_type": "stream",
            "content_old": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(3, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "def testCNNModel(path,model):",
                "ASSIGN =cv2.imread(path,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN = []",
                "ASSIGN.append(ASSIGN.reshape(64,64))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = ASSIGN.reshape(1,64,64,1)",
                "ASSIGN =model.predict(t)",
                "return res",
                "return"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('path',model)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "testCNNModel('/kaggle/input/testdata2/fist5.jpg',model)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('asd')"
            ],
            "output_type": "stream",
            "content_old": [
                "print('asd')\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = 'path'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = ['*.png', '*.jpg']",
                "for typ in ASSIGN:",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(0)",
                "print('finished')",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(1)",
                "print('finished')",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(2)",
                "print('finished')",
                "for filename in glob.glob(os.path.join(ASSIGN, typ)):",
                "ASSIGN =cv2.imread(filename,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(3)",
                "print('finished')"
            ],
            "output_type": "stream",
            "content_old": [
                "import glob\n",
                "import cv2\n",
                "directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'\n",
                "directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'\n",
                "directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'\n",
                "# directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'\n",
                "# directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'\n",
                "directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'\n",
                "#directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "X = []\n",
                "y = []\n",
                "types = ['*.png', '*.jpg']\n",
                "for typ in types:\n",
                "        for filename in glob.glob(os.path.join(directory_1, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(0)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_2, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(1)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_3, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(2)\n",
                "        print('finished')\n",
                "        for filename in glob.glob(os.path.join(directory_unk, typ)):\n",
                "            im1 =cv2.imread(filename,0)\n",
                "            im1 = cv2.resize(im1,(64,64))\n",
                "            X.append(im1)\n",
                "            y.append(3)\n",
                "        print('finished')\n",
                "            \n",
                "            \n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(X)\n",
                "y = np.asarray(y)\n",
                "\n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = ASSIGN.reshape(-1,64,64,1)",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X = X.reshape(-1,64,64,1)\n",
                "X.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "SETUP",
                "ASSIGN = Xpath",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(16, (2,2), input_shape=ASSIGN.shape[1:], activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))",
                "ASSIGN.add(Conv2D(32, (3,3), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))",
                "ASSIGN.add(Conv2D(64, (5,5), activation='relu'))",
                "ASSIGN.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(4, activation='softmax'))",
                "ASSIGN = TensorBoard(log_dir=\"path{}\".format(NAME))",
                "ASSIGN.compile(loss='sparse_categorical_crossentropy',",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(ASSIGN, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[ASSIGN])"
            ],
            "output_type": "stream",
            "content_old": [
                "IMG_SIZE=64\n",
                "import tensorflow as tf\n",
                "from tensorflow.keras.datasets import cifar10\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.models import Sequential\n",
                "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
                "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
                "from tensorflow.keras.callbacks import TensorBoard\n",
                "import time\n",
                "\n",
                "import pickle\n",
                "\n",
                "NAME = \"Numbers-CNN-Model-{}\".format(str(time.ctime())) # Model Name\n",
                "\n",
                "\n",
                "X = X/255.0\n",
                "\n",
                "model = Sequential()\n",
                "\n",
                "model.add(Conv2D(16, (2,2), input_shape=X.shape[1:], activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
                "model.add(Conv2D(32, (3,3), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))\n",
                "model.add(Conv2D(64, (5,5), activation='relu'))\n",
                "model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dropout(0.2))\n",
                "model.add(Dense(4, activation='softmax')) # size must be equal to number of classes i.e. 11\n",
                "\n",
                "tensorboard = TensorBoard(log_dir=\"/kaggle/working/logs/{}\".format(NAME))\n",
                "\n",
                "model.compile(loss='sparse_categorical_crossentropy',\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "\n",
                "model.fit(X, y, batch_size=32, epochs=10, validation_split=0.2, callbacks=[tensorboard])"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data",
                "ingest_data"
            ],
            "content": [
                "VALIDATION",
                "def testCNNModel(path,model):",
                "ASSIGN =cv2.imread(path,0)",
                "ASSIGN = cv2.resize(ASSIGN,(64,64))",
                "ASSIGN = []",
                "ASSIGN.append(ASSIGN.reshape(64,64))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = ASSIGN.reshape(1,64,64,1)",
                "ASSIGN =model.predict(t)",
                "return res",
                "return"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def testCNNModel(path,model):\n",
                "    im1 =cv2.imread(path,0)\n",
                "    im1 = cv2.resize(im1,(64,64))\n",
                "    t = []\n",
                "    t.append(im1.reshape(64,64))\n",
                "    t = np.asarray(t)\n",
                "    t = t.reshape(1,64,64,1)\n",
                "    res =model.predict(t)\n",
                "    return res\n",
                "        \n",
                "    return "
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "testCNNModel('path',model)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "testCNNModel('/kaggle/input/testdata2/two4.jpg',model)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import numpy as np",
                "import matplotlib.pyplot as plt",
                "import random",
                "%matplotlib inline",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = range(1000)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = 2",
                "ASSIGN = 500",
                "def line_function(ASSIGN):",
                "ASSIGN = w1 * X + b",
                "return y",
                "ASSIGN = line_function(X)",
                "plt.plot(ASSIGN,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = range(1000)",
                "#changing our list to numpy array to benifit from numpy's broadcasting",
                "X = np.asarray(X)",
                "w1 = 2",
                "b  = 500",
                "def line_function(X):",
                "    y = w1 * X + b",
                "    return y",
                "y = line_function(X)",
                "",
                "plt.plot(X,y)",
                ""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = range(1000)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = 9path",
                "ASSIGN = 32",
                "ASSIGN = line_function(C)",
                "ASSIGN = plt.figure(figsize=(4,3))",
                "ASSIGN = fig.add_subplot(111)",
                "ASSIGN.set_title('change of ASSIGN with respect to ASSIGN')",
                "plt.plot(ASSIGN,ASSIGN)",
                "ASSIGN.set_xlabel('Celsius (ASSIGN)')",
                "ASSIGN.set_ylabel('Fahrenheit (ASSIGN)')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "C = range(1000)",
                "C = np.asarray(C)",
                "w1 = 9/2",
                "b  = 32",
                "F = line_function(C)",
                "",
                "fig = plt.figure(figsize=(4,3))",
                "ax = fig.add_subplot(111)",
                "ax.set_title('change of F with respect to C')",
                "# ax.scatter(x=data[:,0],y=data[:,1],label='Data')",
                "plt.plot(C,F)",
                "ax.set_xlabel('Celsius (C)')",
                "ax.set_ylabel('Fahrenheit (F)')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = range(1000)",
                "ASSIGN = range(1000)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = 8",
                "ASSIGN = 6",
                "ASSIGN = [w1,w2]",
                "ASSIGN = 500",
                "def hyperplane(Xs):",
                "ASSIGN=b",
                "for (w,x) in zip(ASSIGN,Xs):",
                "ASSIGN+=w*x",
                "return y",
                "ASSIGN = plt.axes(projection='3d')",
                "ASSIGN = np.meshgrid(X1, X2)",
                "ASSIGN = [xv, yv]",
                "ASSIGN = hyperplane(Xs)",
                "ASSIGN.plot_surface(ASSIGN,ASSIGN);"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X1 = range(1000)",
                "X2 = range(1000)",
                "X1 = np.asarray(X1)",
                "X2 = np.asarray(X2)",
                "w1 = 8",
                "w2 = 6",
                "Ws = [w1,w2]",
                "b  = 500",
                "def hyperplane(Xs):",
                "    y=b",
                "    for (w,x) in zip(Ws,Xs):",
                "        y+=w*x",
                "    return y",
                "ax = plt.axes(projection='3d')",
                "xv, yv = np.meshgrid(X1, X2)",
                "Xs = [xv, yv]",
                "y = hyperplane(Xs)",
                "ax.plot_surface(xv, yv,y);",
                "",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('..path')",
                "plt.scatter(ASSIGN.x,ASSIGN.y,s = 4)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "plt.scatter(df.x,df.y,s = 4) "
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('..path')",
                "plt.scatter(ASSIGN.x,ASSIGN.y,s = 4)",
                "X= range(100)",
                "Y= X",
                "plt.plot(X,Y,c='red')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "plt.scatter(df.x,df.y,s = 4) ",
                "X= range(100)",
                "Y= X",
                "plt.plot(X,Y,c='red')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(df.x,df.y,s = 4)",
                "X= np.asarray((range(100)))",
                "Y= -X",
                "plt.plot(X,Y,c='red')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(df.x,df.y,s = 4) ",
                "X= np.asarray((range(100)))",
                "Y= -X",
                "plt.plot(X,Y,c='red')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = ASSIGN.dropna()",
                "plt.scatter(ASSIGN.x,ASSIGN.y,s = 4)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#loading data from the data set ",
                "import pandas as pd",
                "df = pd.read_csv('../input/random-linear-regression/train.csv')",
                "df = df.dropna()",
                "plt.scatter(df.x,df.y,s = 4) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def getXYfromDF(df):",
                "ASSIGN = []",
                "for i in list(df.x):",
                "if(type(i)!=list):",
                "ASSIGN = [ASSIGN]",
                "ASSIGN.append(1)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(df.ASSIGN)",
                "return X,y",
                "def randomWeights(m):",
                "ASSIGN= []",
                "for ASSIGN in range(m):",
                "ASSIGN.append(random.randint(1,9))",
                "ASSIGN = np.asarray(ASSIGN)",
                "return w",
                "ASSIGN = getXYfromDF(df)",
                "ASSIGN = X.shape[0]",
                "ASSIGN = X.shape[1]",
                "ASSIGN = randomWeights(m)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in list(df.x):",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.y)",
                "    return X,y",
                "def randomWeights(m):",
                "    w= []",
                "    for i in range(m):",
                "        w.append(random.randint(1,9))",
                "    w = np.asarray(w)",
                "    return w",
                "",
                "",
                "X,y = getXYfromDF(df) ",
                "",
                "",
                "n = X.shape[0]",
                "m = X.shape[1]",
                "",
                "w = randomWeights(m)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y)",
                "w"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#finding the best wights analytically",
                "w = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y) ",
                "w"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotTheLineWithData(X,w):",
                "plt.scatter(df.x,df.y,s = 4)",
                "X=[]",
                "for i in range(100):",
                "X.append([i,1])",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.dot(X,w)",
                "plt.plot(ASSIGN[:,0],ASSIGN,c='red')",
                "plotTheLineWithData(ASSIGN,w)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def plotTheLineWithData(X,w):",
                "    plt.scatter(df.x,df.y,s = 4) ",
                "    #this X is to generate test samples",
                "    X=[]",
                "    for i in range(100):",
                "        X.append([i,1])",
                "    X = np.asarray(X)",
                "    predicted_y = np.dot(X,w) ",
                "    plt.plot(X[:,0],predicted_y,c='red')",
                "plotTheLineWithData(X,w)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = getXYfromDF(df)",
                "ASSIGN = randomWeights(m)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X,y = getXYfromDF(df) ",
                "w = randomWeights(m)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "def MSE(y,y_predicted):",
                "return ((y- y_predicted)**2).mean()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def MSE(y,y_predicted):",
                "    return ((y- y_predicted)**2).mean()",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001):",
                "ASSIGN = []",
                "ASSIGN = []",
                "for iteration in range(max_iteration):",
                "ASSIGN = np.dot(X,w)",
                "ASSIGN = MSE(y,predicted_y)",
                "ASSIGN = round(ASSIGN,9)",
                "ASSIGN.append(w)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = -(2path[0])* X.dot(loss).sum()",
                "ASSIGN = ASSIGN + lr * derivative",
                "return w_history,loss_hostory"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ",
                "    w_history  = []",
                "    loss_hostory = []",
                "    for iteration in range(max_iteration):",
                "        predicted_y = np.dot(X,w)",
                "        loss =  MSE(y,predicted_y)",
                "        loss = round(loss,9)",
                "        w_history.append(w)",
                "        loss_hostory.append(loss)",
                "        derivative = -(2/y.shape[0])* X.dot(loss).sum()",
                "        w = w + lr * derivative",
                "    return w_history,loss_hostory",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = gradient_descent(X,y,w,lr = 0.0000001)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "w_history,loss_hostory = gradient_descent(X,y,w,lr = 0.0000001)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = loss_hostory.index(min(loss_hostory))",
                "ASSIGN = w_history[perfect_i]",
                "ASSIGN= perfect_w"
            ],
            "output_type": "not_existent",
            "content_old": [
                "perfect_i = loss_hostory.index(min(loss_hostory)) ",
                "perfect_w = w_history[perfect_i]",
                "w= perfect_w"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plotTheLineWithData(X,w)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plotTheLineWithData(X,w)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "plt.scatter(ASSIGN.X1,ASSIGN.X2,c= ASSIGN.Y)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')",
                "plt.scatter(df.X1,df.X2,c= df.Y)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "plt.scatter(ASSIGN.X1,ASSIGN.X2,c= ASSIGN.Y)",
                "ASSIGN = range(1000)",
                "ASSIGN= X",
                "plt.plot(ASSIGN,ASSIGN,c='red')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')",
                "plt.scatter(df.X1,df.X2,c= df.Y)",
                "X = range(1000)",
                "y= X",
                "plt.plot(X,y,c='red')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.asarray(range(1000))",
                "ASSIGN = -(2path)*X",
                "plt.plot(ASSIGN,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.asarray(range(1000))",
                "ASSIGN = -(2path)*X",
                "plt.plot(ASSIGN,ASSIGN)",
                "plt.plot(800,-400,'+',c='green')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)",
                "plt.plot(800,-400,'+',c='green')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.asarray(range(1000))",
                "ASSIGN = -(2path)*X",
                "plt.plot(ASSIGN,ASSIGN)",
                "plt.plot(400,-500,'_',c='red')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = np.asarray(range(1000))",
                "y = -(2/3)*X ",
                "plt.plot(X,y)",
                "plt.plot(400,-500,'_',c='red')"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def getXYfromDF(df):",
                "ASSIGN = []",
                "for i in df[['X1','X2']].values.tolist():",
                "if(type(i)!=list):",
                "ASSIGN = [ASSIGN]",
                "ASSIGN.append(1)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(df.Y)",
                "return X,y",
                "def randomWeights(m):",
                "ASSIGN= []",
                "for ASSIGN in range(m):",
                "ASSIGN.append(random.randint(1,9)path)",
                "ASSIGN = np.asarray(ASSIGN)",
                "return w"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in df[['X1','X2']].values.tolist():",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.Y)",
                "    return X,y",
                "def randomWeights(m):",
                "    w= []",
                "    for i in range(m):",
                "        w.append(random.randint(1,9)/100)",
                "    w = np.asarray(w)",
                "    return w",
                ""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = getXYfromDF(df)",
                "ASSIGN= randomWeights(3)",
                "print(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X,y = getXYfromDF(df)",
                "w= randomWeights(3)",
                "print(w)",
                "",
                ""
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "ASSIGN = np.column_stack((range(1000),np.ones(1000)))",
                "print(ASSIGN.shape)",
                "print(w[1:2].shape)",
                "ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0])",
                "ASSIGN = [sub[0] for sub in X12]",
                "ASSIGN = res",
                "plt.plot(ASSIGN,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "train_model"
            ],
            "content": [
                "VALIDATION",
                "def equales(list1,list2):",
                "if(len(list1)!=len(list2)):",
                "return False",
                "else:",
                "for i in range(len(list1)):",
                "if(list1[i]!=list2[i]):",
                "return False",
                "return True",
                "def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):",
                "for iteration in range(max_iterations):",
                "ASSIGN = w",
                "for i in range(w.shape[0]):",
                "if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):",
                "ASSIGN=ASSIGN- learning_rate * X[i]",
                "elif(np.dot(np.dot(X[i],ASSIGN),y[i]) < 0 and y[i]>0):",
                "ASSIGN=ASSIGN+ learning_rate * X[i]",
                "if(equales(ASSIGN,ASSIGN)):",
                "print('ASSIGN == ASSIGN in ',iteration)",
                "break",
                "return w"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def equales(list1,list2):",
                "    if(len(list1)!=len(list2)):",
                "        return False",
                "    else: ",
                "        for i in range(len(list1)):",
                "            if(list1[i]!=list2[i]):",
                "                return False",
                "    return True",
                "def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):",
                "    for iteration in range(max_iterations):",
                "        prev_w = w",
                "        for i in range(w.shape[0]):",
                "            if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):",
                "                w=w- learning_rate * X[i]",
                "                ",
                "            elif(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]>0):",
                "                w=w+ learning_rate * X[i]",
                "        if(equales(prev_w,w)):",
                "            print('prev_w == w in ',iteration)",
                "            break",
                "        ",
                "        ",
                "    return w"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "new_w = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= new_w"
            ],
            "output_type": "not_existent",
            "content_old": [
                "w= new_w"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "ASSIGN = np.column_stack((range(1000),np.ones(1000)))",
                "print(ASSIGN.shape)",
                "print(w[1:2].shape)",
                "ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0])",
                "ASSIGN = [sub[0] for sub in X12]",
                "ASSIGN = res",
                "plt.plot(ASSIGN,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def softmax(z):",
                "ASSIGN = np.exp(z)",
                "return ASSIGN path()",
                "ASSIGN = (3,12,-5,0,10)",
                "np.round(softmax(ASSIGN),1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def softmax(z):",
                "    e_z = np.exp(z)",
                "    return e_z / e_z.sum()",
                "z = (3,12,-5,0,10) ",
                "np.round(softmax(z),1)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#load the data ",
                "df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def getOneHot(y):",
                "ASSIGN = []",
                "for i in range(y.shape[0]):",
                "if(y[i]==-1):",
                "ASSIGN.append([1,0])",
                "else:",
                "ASSIGN.append([0,1])",
                "return np.asarray(ASSIGN)",
                "def getXYfromDF(df):",
                "ASSIGN = []",
                "for i in df[['X1','X2']].values.tolist():",
                "if(type(i)!=list):",
                "ASSIGN = [ASSIGN]",
                "ASSIGN.append(1)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN = np.asarray(df.Y)",
                "return X,y",
                "def randomWeights(m,k):",
                "ASSIGN= []",
                "for ASSIGN in range(m):",
                "ASSIGN = []",
                "for j in range(k):",
                "ASSIGN.append(random.randint(1,9))",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = np.asarray(ASSIGN)",
                "return w",
                "ASSIGN = getXYfromDF(df)",
                "ASSIGN = getOneHot(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#this function changes the output from numerical values to one hot vectors",
                "#example:",
                "#[1,-1,1] becomes: ",
                "#[[0,1],[1,0],[0,1]",
                "#this function is not general it's just for the case if the data has -1 and 1 cases only,",
                "#the generalized version will be coded next.",
                "def getOneHot(y):",
                "    newY = []",
                "    for i in range(y.shape[0]):",
                "        if(y[i]==-1):",
                "            newY.append([1,0])",
                "        else:",
                "            newY.append([0,1])",
                "    return np.asarray(newY)",
                "#this function loads the data to X and y vectors",
                "def getXYfromDF(df):",
                "    X = []",
                "    for i in df[['X1','X2']].values.tolist():",
                "        if(type(i)!=list):",
                "            i = [i]",
                "        i.append(1)",
                "        X.append(i)",
                "    X = np.asarray(X)",
                "    y = np.asarray(df.Y)",
                "    return X,y",
                "#this function generates random weights to initailize the weights(+ biases ofcourse)",
                "def randomWeights(m,k):",
                "    w= []",
                "    for i in range(m):",
                "        temp = []",
                "        for j in range(k):",
                "            temp.append(random.randint(1,9))",
                "        w.append(temp)",
                "    w = np.asarray(w)",
                "    return w",
                "",
                "",
                "X,y = getXYfromDF(df) ",
                "y = getOneHot(y)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = X.shape[0]",
                "ASSIGN = X.shape[1]",
                "ASSIGN = 2",
                "ASSIGN = randomWeights(m,k)",
                "ASSIGN=np.asarray(ASSIGN,'float64')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "n = X.shape[0] #number of data samples",
                "m = X.shape[1] #number of features for each sample ",
                "k = 2 #number of classes",
                "w = randomWeights(m,k)",
                "w=np.asarray(w,'float64')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "w.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "w.shape"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "VALIDATION",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "ASSIGN = np.column_stack((range(1000),np.ones(1000)))",
                "ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0])",
                "ASSIGN = [sub[0] for sub in X12]",
                "ASSIGN = np.asarray(res)",
                "print(ASSIGN.shape)",
                "print(ASSIGN.shape)",
                "plt.plot(ASSIGN,ASSIGN[:,0],c='blue')",
                "plt.plot(ASSIGN,ASSIGN[:,1],c='red')",
                "plt.show"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#visualize the data that we are trying to fit, and plotting the current lines with the random weights",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = np.asarray(res)",
                "print(X1.shape) ",
                "print(y0.shape)",
                "#plot the first line that represents the first linear model",
                "plt.plot(X1,y0[:,0],c='blue')",
                "#plot the second line that represents the second linear model",
                "plt.plot(X1,y0[:,1],c='red')",
                "plt.show"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def softmax(x):",
                "ASSIGN = np.exp(x - np.max(x))",
                "return ASSIGN path(axis=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#the softmax that we use previously was right, but numerically it wasn't stable",
                "#this 'edited softmax' is more numerically stable ",
                "def softmax(x):",
                "    temp = np.exp(x - np.max(x))  # for numerical stability",
                "    return temp / temp.sum(axis=0)",
                ""
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "def cross_entropy(y, y_hat):",
                "ASSIGN = np.clip(ASSIGN, EPS, 1-EPS)",
                "return -np.sum(y * np.log(ASSIGN)path)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "EPS = 1e-9",
                "#same as in softmax,the first line in this function just gives numerical stability for cross entropy ",
                "def cross_entropy(y, y_hat):",
                "    y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability",
                "    return -np.sum(y * np.log(y_hat)/n)",
                ""
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN =1000",
                "ASSIGN = 0.1",
                "for _ in range(ASSIGN):",
                "ASSIGN = np.dot(X,w)",
                "ASSIGN = []",
                "for i in range(n):",
                "ASSIGN.append(softmax(ASSIGN[i]))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN.append(cross_entropy(y,ASSIGN))",
                "for j in range(k):",
                "ASSIGN=0",
                "for i in range(n):",
                "ASSIGN += np.dot(X.T,(y-ASSIGN))",
                "ASSIGN = - deltaTemppath",
                "ASSIGN = np.asarray(ASSIGN)",
                "w-=ASSIGN*ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "history = [] #loss history ",
                "numberOfRounds =1000 # max number of times the optimization algorithm will run",
                "learningRate = 0.1",
                "for _ in range(numberOfRounds):",
                "    z = np.dot(X,w)",
                "    y_hat = []",
                "    for i in range(n):",
                "        y_hat.append(softmax(z[i]))",
                "    y_hat = np.asarray(y_hat)",
                "    history.append(cross_entropy(y,y_hat))",
                "",
                "    for j in range(k):",
                "        deltaTemp=0 ",
                "        #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,",
                "        #and it works fine in case of offline training and smalle number of samples) ",
                "        for i in range(n):",
                "            deltaTemp += np.dot(X.T,(y-y_hat))",
                "        deltaTemp  = - deltaTemp/n",
                "        deltaTemp = np.asarray(deltaTemp)",
                "        w-=learningRate*deltaTemp"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "ASSIGN = np.column_stack((range(1000),np.ones(1000)))",
                "print(ASSIGN.shape)",
                "print(w[1:2].shape)",
                "ASSIGN = -np.divide(np.dot(X12,w[1:3]),w[0])",
                "ASSIGN = [sub[0] for sub in X12]",
                "ASSIGN = res",
                "plt.plot(ASSIGN,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(df.X1,df.X2,c=df.Y)",
                "X12 = np.column_stack((range(1000),np.ones(1000)))",
                "print(X12.shape)",
                "print(w[1:2].shape)",
                "y0 =  -np.divide(np.dot(X12,w[1:3]),w[0])",
                "res = [sub[0] for sub in X12] ",
                "X1 = res",
                "plt.plot(X1,y0)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = load_digits()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.datasets import load_digits",
                "digits = load_digits()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = digits.data",
                "ASSIGN = digits.target"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = digits.data",
                "y = digits.target"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = plt.subplots(2, 3, sharex='col', sharey='row')",
                "ASSIGN = 0",
                "for i in range(2):",
                "for j in range(3):",
                "ASSIGN = X[currNum,0:64]",
                "ASSIGN += 1",
                "ASSIGN = np.array(ASSIGN, dtype='float')",
                "ASSIGN = img.reshape((8, 8))",
                "ax[i, j].imshow(ASSIGN, cmap='gray')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt",
                "fig, ax = plt.subplots(2, 3, sharex='col', sharey='row')",
                "currNum = 0",
                "for i in range(2):",
                "    for j in range(3):",
                "        img = X[currNum,0:64]",
                "        currNum += 1",
                "        img = np.array(img, dtype='float')",
                "        pixels = img.reshape((8, 8))",
                "        ax[i, j].imshow(pixels, cmap='gray')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def add_bias(X):",
                "ASSIGN = []",
                "for i in range(X.shape[0]):",
                "ASSIGN.append(np.append(X[i],1))",
                "return np.asarray(ASSIGN)",
                "ASSIGN = add_bias(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#simple functions to add 1 to each sample's vector for the bias",
                "def add_bias(X):",
                "    newX = [] ",
                "    for i in range(X.shape[0]):",
                "        newX.append(np.append(X[i],1))",
                "    return np.asarray(newX)",
                "X = add_bias(X)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=[0,1,2,3,4,5,6,7,8,9]",
                "def oneHot(y,ASSIGN):",
                "ASSIGN = []",
                "for i in range(y.shape[0]):",
                "ASSIGN = []",
                "for j in ASSIGN:",
                "if(y[i]==ASSIGN[j]):",
                "ASSIGN.append(1)",
                "else:",
                "ASSIGN.append(0)",
                "ASSIGN.append(ASSIGN)",
                "return np.asarray(ASSIGN)",
                "ASSIGN = oneHot(ASSIGN,targets)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#change the form of the target values from a single digit to a onehot so we can apply out algorithm",
                "targets=[0,1,2,3,4,5,6,7,8,9]",
                "def oneHot(y,targets):",
                "    newY = []",
                "    for i in range(y.shape[0]): ",
                "        temp = [] ",
                "        for j in targets:",
                "            if(y[i]==targets[j]):",
                "                temp.append(1)",
                "            else:",
                "                temp.append(0)",
                "        newY.append(temp)",
                "    return np.asarray(newY)",
                "y = oneHot(y,targets)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = X.shape[0]",
                "ASSIGN = X.shape[1]",
                "ASSIGN = 10",
                "ASSIGN = randomWeights(m,k)",
                "ASSIGN=np.asarray(ASSIGN,'float64')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "n = X.shape[0] #number of data samples",
                "m = X.shape[1] #number of features for each sample ",
                "k = 10 #number of classes",
                "w = randomWeights(m,k)",
                "w=np.asarray(w,'float64')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "w.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "w.shape"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "ASSIGN = 20",
                "for iteration in range(ASSIGN):",
                "print('iteration: ',iteration)",
                "ASSIGN = np.dot(X,w)",
                "ASSIGN = []",
                "for i in range(n):",
                "ASSIGN.append(softmax(ASSIGN[i]))",
                "ASSIGN = np.asarray(ASSIGN)",
                "ASSIGN.append(cross_entropy(y,ASSIGN))",
                "for j in range(k):",
                "ASSIGN=0",
                "for i in range(n):",
                "ASSIGN += np.dot(X.T,(y-ASSIGN))",
                "ASSIGN = - deltaTemppath",
                "ASSIGN = np.asarray(ASSIGN)",
                "w-=0.1*ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "history = []",
                "maxNumOfIterations = 20",
                "for iteration in range(maxNumOfIterations): ",
                "    print('iteration: ',iteration)",
                "    z = np.dot(X,w)",
                "    y_hat = []",
                "    for i in range(n):",
                "        y_hat.append(softmax(z[i]))",
                "    y_hat = np.asarray(y_hat)",
                "    history.append(cross_entropy(y,y_hat))",
                "    for j in range(k):",
                "        deltaTemp=0",
                "        for i in range(n):",
                "            deltaTemp += np.dot(X.T,(y-y_hat))",
                "        deltaTemp  = - deltaTemp/n",
                "        deltaTemp = np.asarray(deltaTemp)",
                "        w-=0.1*deltaTemp"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.plot(history)",
                "plt.title('the change of loss with iterations')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def giveMeValueFromOneHot(y_hat):",
                "return np.where(y_hat == 1)[0][0]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def giveMeValueFromOneHot(y_hat):",
                "    return np.where(y_hat == 1)[0][0]"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "def predictDis(x):",
                "ASSIGN = np.array(x, dtype='float')",
                "ASSIGN = x[0:64].reshape((8, 8))",
                "plt.imshow(ASSIGN, cmap='gray')",
                "ASSIGN = np.dot(x,w)",
                "print (\"the class of this Image is: \",giveMeValueFromOneHot(softmax(ASSIGN)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def predictDis(x):",
                "    img = np.array(x, dtype='float')",
                "    pixels = x[0:64].reshape((8, 8))",
                "    plt.imshow(pixels, cmap='gray')",
                "    z = np.dot(x,w)",
                "    print (\"the class of this Image is: \",giveMeValueFromOneHot(softmax(z)))"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = X[0]",
                "predictDis(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x = X[0]",
                "predictDis(x)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = X[1]",
                "predictDis(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x = X[1]",
                "predictDis(x)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = X[3]",
                "predictDis(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x = X[3]",
                "predictDis(x)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(check_output([, ]).decode())"
            ],
            "output_type": "stream",
            "content_old": [
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "\"\"\"",
                "A Python Module to do automated Exploratory Data Analysis and some light weight data prep.",
                "https:path",
                "\"\"\"",
                "ASSIGN = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']",
                "def full_report(df, target_column=None):",
                "\"\"\"Tries to run every possible report on the provided dataframe\"\"\"",
                "display(HTML(\"<h1>Lazy EDA Report<path>\"))",
                "breakdown_date(df)",
                "show_dtypes(df)",
                "plot_nulls(df)",
                "plot_long_lat(df, target_column)",
                "if target_column is not None:",
                "plot_scatter_target(df, target_column)",
                "plot_hist_target(df, target_column)",
                "plot_correlations(df, target_column)",
                "def plot_correlations(df, target_column):",
                "display(HTML(\"<h2>Column Data Types<path>\"))",
                "display(HTML(\"<p>Below is a plot of the correlation coefficients of the dataframe's numeric columns and the target column<path>\"))",
                "ASSIGN = df.select_dtypes(include=numerics)",
                "del(ASSIGN[target_column])",
                "ASSIGN.corrwith(df[target_column]).sort_values(ascending=False).plot(",
                "ASSIGN='barh', figsize=(12,12), title=\"Correlation Coefficient with Target\")",
                "plt.show()",
                "def breakdown_date(df):",
                "\"\"\"",
                "Creates new columns in a dataframe representing the components of a date (year, month, day of year, & week day name)",
                "\"\"\"",
                "ASSIGN = df.dtypes[df.dtypes == 'datetime64[ns]'].index",
                "display(HTML(\"<h2>Breaking down date columns<path>\"))",
                "if len(ASSIGN) > 0:",
                "display(HTML(\"<p>The following columns will be broken down into year, month, day of year, and weekday columns<path> <ul>\"))",
                "for date_column in ASSIGN:",
                "display(HTML(\"<li>{}<path>\".format(date_column)))",
                "df['{}_year'.format(date_column)] = df[date_column].dt.year",
                "df['{}_month'.format(date_column)] = df[date_column].dt.month",
                "df['{}_dayofyear'.format(date_column)] = df[date_column].dt.dayofyear",
                "df['{}_weekday'.format(date_column)] = df[date_column].dt.weekday_name",
                "display(HTML(\"<path>\"))",
                "else:",
                "display(HTML(\"<p>No Date columns found to breakdown.<path>\"))",
                "return df",
                "def plot_nulls(df):",
                "\"\"\"",
                "Displays a horizontal bar chart representing the percentage of nulls in each column",
                "\"\"\"",
                "display(HTML(\"<h2>Plot Nulls<path>\"))",
                "ASSIGN = df.isnull().sum()path[0]*100",
                "ASSIGN = null_percentage[null_percentage > 0].sort_values()",
                "if len(ASSIGN) > 0:",
                "display(HTML(\"<p>The plot below shows the percentage of NaNs in each column in the dataframe<path>\"))",
                "ASSIGN.plot(ASSIGN='barh', figsize=(12,12), title=\"Plot Null Percentages\")",
                "plt.show()",
                "else:",
                "display(HTML(\"<p>The dataframe does not contain any missing data<path>\"))",
                "return null_percentage_filtered",
                "def show_dtypes(df):",
                "\"\"\"Shows the data types of all columns\"\"\"",
                "display(HTML(\"<h2>Column Data Types<path>\"))",
                "ASSIGN = pd.options.display.max_rows",
                "pd.options.display.max_rows = len(df.columns)",
                "ASSIGN = pd.DataFrame({\"Column Name\": df.dtypes.index,\"DType\": df.dtypes.values})",
                "display(ASSIGN)",
                "pd.options.display.max_rows = ASSIGN",
                "def plot_scatter_target(df, target_column):",
                "\"\"\"Plots a sorted scatter plot of the values in a numerical target column\"\"\"",
                "display(HTML(\"<h2>Plot Scatter Target<path>\"))",
                "display(HTML(\"<p>Below is a sorted scatter plot of the values in the target column<path>\"))",
                "plt.scatter(range(df[target_column].shape[0]), np.sort(df[target_column].values))",
                "plt.xlabel('index', fontsize=12)",
                "plt.ylabel(target_column, fontsize=12)",
                "plt.show()",
                "def plot_hist_target(df, target_column):",
                "display(HTML(\"<h2>Plot Histogram Target<path>\"))",
                "display(HTML(\"<p>Below is a histogram of the values in the target column<path>\"))",
                "ASSIGN = np.percentile(df.logerror.values, 99)",
                "ASSIGN = np.percentile(df.logerror.values, 1)",
                "ASSIGN = ASSIGN",
                "df['tempTarget'].ix[df['tempTarget']>ASSIGN] = ASSIGN",
                "df['tempTarget'].ix[df['tempTarget']<ASSIGN] = ASSIGN",
                "plt.figure(figsize=(12,8))",
                "sns.distplot(df['tempTarget'])",
                "plt.xlabel(target_column, fontsize=12)",
                "plt.show()",
                "del[df['tempTarget']]",
                "def plot_long_lat(df, target_column):",
                "if 'latitude' in df.columns.str.lower() and 'longitude' in df.columns.str.lower():",
                "display(HTML(\"<h2>Plot longitudepath<path>\"))",
                "display(HTML(\"<p>Below is a scatter plot of longpath<path>\"))",
                "plt.figure(figsize=(12,12))",
                "if target_column is None:",
                "sns.jointplot(x=df.latitude.values, y=df.longitude.values, size=10)",
                "else:",
                "ASSIGN = (ASSIGN - ASSIGN.min())path(ASSIGN.max() - ASSIGN.min())",
                "plt.scatter(x=df.latitude.values, y=df.longitude.values, c=df['tempTarget'].values)",
                "del(df['tempTarget'])",
                "plt.ylabel('Longitude', fontsize=12)",
                "plt.xlabel('Latitude', fontsize=12)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "\"\"\" \n",
                "A Python Module to do automated Exploratory Data Analysis and some light weight data prep.\n",
                "https://github.com/TareqAlKhatib/Lazy-EDA\n",
                "\"\"\"\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from IPython.display import display, HTML\n",
                "\n",
                "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
                "    \n",
                "def full_report(df, target_column=None):\n",
                "\t\"\"\"Tries to run every possible report on the provided dataframe\"\"\"\n",
                "\tdisplay(HTML(\"<h1>Lazy EDA Report</h1>\"))\n",
                "\t\n",
                "\tbreakdown_date(df)\n",
                "\tshow_dtypes(df)\n",
                "\tplot_nulls(df)\n",
                "\tplot_long_lat(df, target_column)\n",
                "\tif target_column is not None:\n",
                "\t\tplot_scatter_target(df, target_column)\n",
                "\t\tplot_hist_target(df, target_column)\n",
                "\t\tplot_correlations(df, target_column)            \n",
                "\n",
                "def plot_correlations(df, target_column):\n",
                "\tdisplay(HTML(\"<h2>Column Data Types</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a plot of the correlation coefficients of the dataframe's numeric columns and the target column</p>\"))\n",
                "\t\n",
                "\tnum_df = df.select_dtypes(include=numerics)\n",
                "\tdel(num_df[target_column])\n",
                "\tnum_df.corrwith(df[target_column]).sort_values(ascending=False).plot(\n",
                "\t\tkind='barh', figsize=(12,12), title=\"Correlation Coefficient with Target\")\n",
                "\tplt.show()\n",
                "\t\n",
                "def breakdown_date(df):\n",
                "\t\"\"\"\n",
                "\tCreates new columns in a dataframe representing the components of a date (year, month, day of year, & week day name)\n",
                "\t\"\"\"\n",
                "\tdate_cols = df.dtypes[df.dtypes == 'datetime64[ns]'].index\n",
                "\tdisplay(HTML(\"<h2>Breaking down date columns</h2>\"))\n",
                "\tif len(date_cols) > 0:\n",
                "\t\tdisplay(HTML(\"<p>The following columns will be broken down into year, month, day of year, and weekday columns</p> <ul>\"))\n",
                "\t\t\n",
                "\t\tfor date_column in date_cols:\n",
                "\t\t\tdisplay(HTML(\"<li>{}</li>\".format(date_column)))\n",
                "\t\t\tdf['{}_year'.format(date_column)] = df[date_column].dt.year\n",
                "\t\t\tdf['{}_month'.format(date_column)] = df[date_column].dt.month\n",
                "\t\t\tdf['{}_dayofyear'.format(date_column)] = df[date_column].dt.dayofyear\n",
                "\t\t\tdf['{}_weekday'.format(date_column)] = df[date_column].dt.weekday_name\n",
                "\t\t\n",
                "\t\tdisplay(HTML(\"</ul>\"))\n",
                "\telse:\n",
                "\t\tdisplay(HTML(\"<p>No Date columns found to breakdown.</p>\"))\n",
                "\t\t\n",
                "\treturn df\n",
                "\n",
                "def plot_nulls(df):\n",
                "\t\"\"\"\n",
                "\tDisplays a horizontal bar chart representing the percentage of nulls in each column\n",
                "\t\"\"\"\n",
                "\tdisplay(HTML(\"<h2>Plot Nulls</h2>\"))\n",
                "\t\n",
                "\tnull_percentage = df.isnull().sum()/df.shape[0]*100\n",
                "\tnull_percentage_filtered = null_percentage[null_percentage > 0].sort_values()\n",
                "\t\n",
                "\tif len(null_percentage_filtered) > 0:\n",
                "\t\tdisplay(HTML(\"<p>The plot below shows the percentage of NaNs in each column in the dataframe</p>\"))\n",
                "\t\tnull_percentage_filtered.plot(kind='barh', figsize=(12,12), title=\"Plot Null Percentages\")\n",
                "\t\tplt.show()\n",
                "\t\t\n",
                "\telse:\n",
                "\t\tdisplay(HTML(\"<p>The dataframe does not contain any missing data</p>\"))\n",
                "\treturn null_percentage_filtered\n",
                "\n",
                "def show_dtypes(df):\n",
                "\t\"\"\"Shows the data types of all columns\"\"\"\n",
                "\t\n",
                "\tdisplay(HTML(\"<h2>Column Data Types</h2>\"))\n",
                "\t\n",
                "\t# Saving the old display max\n",
                "\told_max = pd.options.display.max_rows\n",
                "\tpd.options.display.max_rows = len(df.columns)\n",
                "\t\n",
                "\t# Display DTypes\n",
                "\tdtype_df = pd.DataFrame({\"Column Name\": df.dtypes.index,\"DType\": df.dtypes.values})\n",
                "\tdisplay(dtype_df)\n",
                "\t\n",
                "\t# Restoring the old display max\n",
                "\tpd.options.display.max_rows = old_max\n",
                "\t\n",
                "def plot_scatter_target(df, target_column):\n",
                "\t\"\"\"Plots a sorted scatter plot of the values in a numerical target column\"\"\"\n",
                "\tdisplay(HTML(\"<h2>Plot Scatter Target</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a sorted scatter plot of the values in the target column</p>\"))\n",
                "\t\n",
                "\tplt.scatter(range(df[target_column].shape[0]), np.sort(df[target_column].values))\n",
                "\tplt.xlabel('index', fontsize=12)\n",
                "\tplt.ylabel(target_column, fontsize=12)\n",
                "\tplt.show()\n",
                "\n",
                "def plot_hist_target(df, target_column):\n",
                "\tdisplay(HTML(\"<h2>Plot Histogram Target</h2>\"))\n",
                "\tdisplay(HTML(\"<p>Below is a histogram of the values in the target column</p>\"))\n",
                "\t\n",
                "\t# Filter 1st and 99th percentiles\n",
                "\tulimit = np.percentile(df.logerror.values, 99)\n",
                "\tllimit = np.percentile(df.logerror.values, 1)\n",
                "\tdf['tempTarget'] = df[target_column]\n",
                "\tdf['tempTarget'].ix[df['tempTarget']>ulimit] = ulimit\n",
                "\tdf['tempTarget'].ix[df['tempTarget']<llimit] = llimit\n",
                "\t\n",
                "\t# Plot\n",
                "\tplt.figure(figsize=(12,8))\n",
                "\tsns.distplot(df['tempTarget'])\n",
                "\tplt.xlabel(target_column, fontsize=12)\n",
                "\tplt.show()\n",
                "\t\n",
                "\tdel[df['tempTarget']]\n",
                "\t\n",
                "def plot_long_lat(df, target_column):\n",
                "\tif 'latitude' in df.columns.str.lower() and 'longitude' in df.columns.str.lower():\n",
                "\t\tdisplay(HTML(\"<h2>Plot longitude/latitude</h2>\"))\n",
                "\t\tdisplay(HTML(\"<p>Below is a scatter plot of long/lat coordinate in the dataframe</p>\"))\n",
                "\t\t\n",
                "\t\tplt.figure(figsize=(12,12))\n",
                "\t\t\n",
                "\t\tif target_column is None:\n",
                "\t\t\tsns.jointplot(x=df.latitude.values, y=df.longitude.values, size=10)\n",
                "\t\telse:\n",
                "\t\t\tdf['tempTarget'] = (df['logerror'] - df['logerror'].min())/(df['logerror'].max() - df['logerror'].min())\n",
                "\t\t\tplt.scatter(x=df.latitude.values, y=df.longitude.values, c=df['tempTarget'].values)\n",
                "\t\t\tdel(df['tempTarget'])\n",
                "\t\tplt.ylabel('Longitude', fontsize=12)\n",
                "\t\tplt.xlabel('Latitude', fontsize=12)\n",
                "\t\tplt.show()\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\", parse_dates=[\"transactiondate\"])",
                "ASSIGN = pd.read_csv(\"..path\", dtype={",
                "'hashottuborspa': 'object',",
                "'propertycountylandusecode': 'object',",
                "'propertyzoningdesc': 'object',",
                "'fireplaceflag': 'object',",
                "'taxdelinquencyflag': 'object'",
                "})",
                "ASSIGN = pd.merge(ASSIGN, prop_df, on='parcelid', how='left')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_df = pd.read_csv(\"../input/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
                "prop_df = pd.read_csv(\"../input/properties_2016.csv\", dtype={\n",
                "    'hashottuborspa': 'object', \n",
                "    'propertycountylandusecode': 'object',\n",
                "    'propertyzoningdesc': 'object',\n",
                "    'fireplaceflag': 'object',\n",
                "    'taxdelinquencyflag': 'object'\n",
                "})\n",
                "\n",
                "train_df = pd.merge(train_df, prop_df, on='parcelid', how='left')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "full_report(train_df, target_column='logerror')"
            ],
            "output_type": "display_data",
            "content_old": [
                "full_report(train_df, target_column='logerror')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "py.init_notebook_mode(connected=True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "import plotly.offline as py\n",
                "py.init_notebook_mode(connected=True)\n",
                "import plotly.graph_objs as go\n",
                "import plotly.tools as tls\n",
                "import seaborn as sns\n",
                "import matplotlib.image as mpimg\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib\n",
                "%matplotlib inline\n",
                "\n",
                "# Import the 3 dimensionality reduction methods\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train = pd.read_csv('../input/train.csv')\n",
                "train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(train.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(train.shape)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train['label']",
                "ASSIGN = ASSIGN.drop(\"label\",axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# save the labels to a Pandas series target\n",
                "target = train['label']\n",
                "# Drop the label feature\n",
                "train = train.drop(\"label\",axis=1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = train.values",
                "ASSIGN = StandardScaler().fit_transform(X)",
                "ASSIGN = np.mean(X_std, axis=0)",
                "ASSIGN = np.cov(X_std.T)",
                "ASSIGN = np.linalg.eig(cov_mat)",
                "ASSIGN = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]",
                "ASSIGN.sort(key = lambda x: x[0], reverse= True)",
                "ASSIGN = sum(eig_vals)",
                "ASSIGN = [(ipath)*100 for i in sorted(eig_vals, reverse=True)]",
                "ASSIGN = np.cumsum(var_exp)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Standardize the data\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "X = train.values\n",
                "X_std = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Calculating Eigenvectors and eigenvalues of Cov matirx\n",
                "mean_vec = np.mean(X_std, axis=0)\n",
                "cov_mat = np.cov(X_std.T)\n",
                "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
                "# Create a list of (eigenvalue, eigenvector) tuples\n",
                "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
                "\n",
                "# Sort the eigenvalue, eigenvector pair from high to low\n",
                "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
                "\n",
                "# Calculation of Explained Variance from the eigenvalues\n",
                "tot = sum(eig_vals)\n",
                "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
                "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = go.Scatter(",
                "ASSIGN=list(range(784)),",
                "ASSIGN= cum_var_exp,",
                "ASSIGN='lines+markers',",
                "ASSIGN=\"'Cumulative Explained Variance'\",",
                "ASSIGN=dict(",
                "ASSIGN='spline',",
                "ASSIGN = 'goldenrod'",
                ")",
                ")",
                "ASSIGN = go.Scatter(",
                "ASSIGN=list(range(784)),",
                "ASSIGN= var_exp,",
                "ASSIGN='lines+markers',",
                "ASSIGN=\"'Individual Explained Variance'\",",
                "ASSIGN=dict(",
                "ASSIGN='linear',",
                "ASSIGN = 'black'",
                ")",
                ")",
                "ASSIGN = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],",
                "ASSIGN=True)",
                "ASSIGN.append_trace(ASSIGN, 1, 1)",
                "ASSIGN.append_trace(ASSIGN,1,1)",
                "ASSIGN.layout.title = 'Explained Variance plots - Full and Zoomed-in'",
                "ASSIGN.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')",
                "ASSIGN.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')"
            ],
            "output_type": "stream",
            "content_old": [
                "trace1 = go.Scatter(\n",
                "    x=list(range(784)),\n",
                "    y= cum_var_exp,\n",
                "    mode='lines+markers',\n",
                "    name=\"'Cumulative Explained Variance'\",\n",
                "#     hoverinfo= cum_var_exp,\n",
                "    line=dict(\n",
                "        shape='spline',\n",
                "        color = 'goldenrod'\n",
                "    )\n",
                ")\n",
                "trace2 = go.Scatter(\n",
                "    x=list(range(784)),\n",
                "    y= var_exp,\n",
                "    mode='lines+markers',\n",
                "    name=\"'Individual Explained Variance'\",\n",
                "#     hoverinfo= var_exp,\n",
                "    line=dict(\n",
                "        shape='linear',\n",
                "        color = 'black'\n",
                "    )\n",
                ")\n",
                "fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],\n",
                "                          print_grid=True)\n",
                "\n",
                "fig.append_trace(trace1, 1, 1)\n",
                "fig.append_trace(trace2,1,1)\n",
                "fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'\n",
                "fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')\n",
                "fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')\n",
                "# fig['data'] = []\n",
                "# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n",
                "# fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))\n",
                "\n",
                "# fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]\n",
                "# fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]\n",
                "\n",
                "# # fig['data'] = data\n",
                "# # fig['layout'] = layout\n",
                "# # fig['data'] += data2\n",
                "# # fig['layout'] += layout2\n",
                "# py.iplot(fig, filename='inset example')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = 30",
                "ASSIGN = PCA(n_components=n_components).fit(train.values)",
                "ASSIGN = pca.components_.reshape(n_components, 28, 28)",
                "ASSIGN = pca.components_"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Invoke SKlearn's PCA method\n",
                "n_components = 30\n",
                "pca = PCA(n_components=n_components).fit(train.values)\n",
                "\n",
                "eigenvalues = pca.components_.reshape(n_components, 28, 28)\n",
                "\n",
                "# Extracting the PCA components ( eignevalues )\n",
                "#eigenvalues = pca.components_.reshape(n_components, 28, 28)\n",
                "eigenvalues = pca.components_"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = 4",
                "ASSIGN = 7",
                "plt.figure(figsize=(13,12))",
                "for i in list(range(ASSIGN * ASSIGN)):",
                "ASSIGN =0",
                "plt.subplot(ASSIGN, ASSIGN, i + 1)",
                "plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet')",
                "ASSIGN = 'Eigenvalue ' + str(i + 1)",
                "plt.title(ASSIGN, size=6.5)",
                "plt.xticks(())",
                "plt.yticks(())",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "n_row = 4\n",
                "n_col = 7\n",
                "\n",
                "# Plot the first 8 eignenvalues\n",
                "plt.figure(figsize=(13,12))\n",
                "for i in list(range(n_row * n_col)):\n",
                "    offset =0\n",
                "    plt.subplot(n_row, n_col, i + 1)\n",
                "    plt.imshow(eigenvalues[i].reshape(28,28), cmap='jet')\n",
                "    title_text = 'Eigenvalue ' + str(i + 1)\n",
                "    plt.title(title_text, size=6.5)\n",
                "    plt.xticks(())\n",
                "    plt.yticks(())\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(14,12))",
                "for digit_num in range(0,70):",
                "plt.subplot(7,10,digit_num+1)",
                "ASSIGN = train.iloc[digit_num].as_matrix().reshape(28,28)",
                "plt.imshow(ASSIGN, interpolation = \"none\", cmap = \"afmhot\")",
                "plt.xticks([])",
                "plt.yticks([])",
                "plt.tight_layout()"
            ],
            "output_type": "stream",
            "content_old": [
                "# plot some of the numbers\n",
                "plt.figure(figsize=(14,12))\n",
                "for digit_num in range(0,70):\n",
                "    plt.subplot(7,10,digit_num+1)\n",
                "    grid_data = train.iloc[digit_num].as_matrix().reshape(28,28)  # reshape from 1d to 2d pixel array\n",
                "    plt.imshow(grid_data, interpolation = \"none\", cmap = \"afmhot\")\n",
                "    plt.xticks([])\n",
                "    plt.yticks([])\n",
                "plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "del X",
                "X= train[:6000].values",
                "del train",
                "ASSIGN = StandardScaler().fit_transform(X)",
                "ASSIGN = PCA(n_components=5)",
                "ASSIGN.fit(ASSIGN)",
                "ASSIGN = pca.transform(X_std)",
                "ASSIGN = target[:6000]"
            ],
            "output_type": "stream",
            "content_old": [
                "# Delete our earlier created X object\n",
                "del X\n",
                "# Taking only the first N rows to speed things up\n",
                "X= train[:6000].values\n",
                "del train\n",
                "# Standardising the values\n",
                "X_std = StandardScaler().fit_transform(X)\n",
                "\n",
                "# Call the PCA method with 5 components. \n",
                "pca = PCA(n_components=5)\n",
                "pca.fit(X_std)\n",
                "X_5d = pca.transform(X_std)\n",
                "\n",
                "# For cluster coloring in our Plotly plots, remember to also restrict the target values \n",
                "Target = target[:6000]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = go.Scatter(",
                "ASSIGN = X_5d[:,0],",
                "ASSIGN = X_5d[:,1],",
                "ASSIGN = 'markers',",
                "ASSIGN = Target,",
                "ASSIGN = False,",
                "ASSIGN = dict(",
                "ASSIGN = 8,",
                "ASSIGN = Target,",
                "ASSIGN ='Jet',",
                "ASSIGN = False,",
                "ASSIGN = dict(",
                "ASSIGN = 2,",
                "ASSIGN = 'rgb(255, 255, 255)'",
                "),",
                "ASSIGN = 0.8",
                ")",
                ")",
                "ASSIGN = [trace0]",
                "ASSIGN = go.Layout(",
                "ASSIGN= 'Principal Component Analysis (PCA)',",
                "ASSIGN= 'closest',",
                "ASSIGN= dict(",
                "ASSIGN= 'First Principal Component',",
                "ASSIGN= 5,",
                "ASSIGN= False,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN=dict(",
                "ASSIGN= 'Second Principal Component',",
                "ASSIGN= 5,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN= True",
                ")",
                "ASSIGN = dict(data=data, layout=layout)",
                "py.iplot(ASSIGN, filename='styled-scatter')"
            ],
            "output_type": "display_data",
            "content_old": [
                "trace0 = go.Scatter(\n",
                "    x = X_5d[:,0],\n",
                "    y = X_5d[:,1],\n",
                "#     name = Target,\n",
                "#     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = False,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [trace0]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'Principal Component Analysis (PCA)',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Principal Component',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Principal Component',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= True\n",
                ")\n",
                "\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = KMeans(n_clusters=9)",
                "ASSIGN = kmeans.fit_predict(X_5d)",
                "ASSIGN = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=\"markers\",",
                "ASSIGN=False,",
                "ASSIGN=dict(",
                "ASSIGN=8,",
                "ASSIGN = X_clustered,",
                "ASSIGN = 'Portland',",
                "ASSIGN=False,",
                "ASSIGN = dict(",
                "ASSIGN = 2,",
                "ASSIGN = 'rgb(255, 255, 255)'",
                ")",
                "))",
                "ASSIGN = go.Layout(",
                "ASSIGN= 'KMeans Clustering',",
                "ASSIGN= 'closest',",
                "ASSIGN= dict(",
                "ASSIGN= 'First Principal Component',",
                "ASSIGN= 5,",
                "ASSIGN= False,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN=dict(",
                "ASSIGN= 'Second Principal Component',",
                "ASSIGN= 5,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN= True",
                ")",
                "ASSIGN = [trace_Kmeans]",
                "ASSIGN = dict(data=data, layout= layout)",
                "py.iplot(ASSIGN, filename=\"svm\")"
            ],
            "output_type": "display_data",
            "content_old": [
                "from sklearn.cluster import KMeans # KMeans clustering \n",
                "# Set a KMeans clustering with 9 components ( 9 chosen sneakily ;) as hopefully we get back our 9 class labels)\n",
                "kmeans = KMeans(n_clusters=9)\n",
                "# Compute cluster centers and predict cluster indices\n",
                "X_clustered = kmeans.fit_predict(X_5d)\n",
                "\n",
                "trace_Kmeans = go.Scatter(x=X_5d[:, 0], y= X_5d[:, 1], mode=\"markers\",\n",
                "                    showlegend=False,\n",
                "                    marker=dict(\n",
                "                            size=8,\n",
                "                            color = X_clustered,\n",
                "                            colorscale = 'Portland',\n",
                "                            showscale=False, \n",
                "                            line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        )\n",
                "                   ))\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'KMeans Clustering',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Principal Component',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Principal Component',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= True\n",
                ")\n",
                "\n",
                "data = [trace_Kmeans]\n",
                "fig1 = dict(data=data, layout= layout)\n",
                "# fig1.append_trace(contour_list)\n",
                "py.iplot(fig1, filename=\"svm\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from IPython.display import display, Math, Latex"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = LDA(n_components=5)",
                "ASSIGN = lda.fit_transform(X_std, Target.values )"
            ],
            "output_type": "stream",
            "content_old": [
                "lda = LDA(n_components=5)\n",
                "# Taking in as second argument the Target as labels\n",
                "X_LDA_2D = lda.fit_transform(X_std, Target.values )"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "8",
                "ASSIGN = go.Scatter(",
                "ASSIGN = X_LDA_2D[:,0],",
                "ASSIGN = X_LDA_2D[:,1],",
                "ASSIGN = 'markers',",
                "ASSIGN = Target,",
                "ASSIGN = True,",
                "ASSIGN = dict(",
                "ASSIGN = 8,",
                "ASSIGN = Target,",
                "ASSIGN ='Jet',",
                "ASSIGN = False,",
                "ASSIGN = dict(",
                "ASSIGN = 2,",
                "ASSIGN = 'rgb(255, 255, 255)'",
                "),",
                "ASSIGN = 0.8",
                ")",
                ")",
                "ASSIGN = [traceLDA]",
                "ASSIGN = go.Layout(",
                "ASSIGN= 'Linear Discriminant Analysis (LDA)',",
                "ASSIGN= 'closest',",
                "ASSIGN= dict(",
                "ASSIGN= 'First Linear Discriminant',",
                "ASSIGN= 5,",
                "ASSIGN= False,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN=dict(",
                "ASSIGN= 'Second Linear Discriminant',",
                "ASSIGN= 5,",
                "ASSIGN= 2,",
                "),",
                "ASSIGN= False",
                ")",
                "ASSIGN = dict(data=data, layout=layout)",
                "py.iplot(ASSIGN, filename='styled-scatter')"
            ],
            "output_type": "display_data",
            "content_old": [
                "8# Using the Plotly library again\n",
                "traceLDA = go.Scatter(\n",
                "    x = X_LDA_2D[:,0],\n",
                "    y = X_LDA_2D[:,1],\n",
                "#     name = Target,\n",
                "#     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = True,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [traceLDA]\n",
                "\n",
                "layout = go.Layout(\n",
                "    title= 'Linear Discriminant Analysis (LDA)',\n",
                "    hovermode= 'closest',\n",
                "    xaxis= dict(\n",
                "         title= 'First Linear Discriminant',\n",
                "        ticklen= 5,\n",
                "        zeroline= False,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    yaxis=dict(\n",
                "        title= 'Second Linear Discriminant',\n",
                "        ticklen= 5,\n",
                "        gridwidth= 2,\n",
                "    ),\n",
                "    showlegend= False\n",
                ")\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = TSNE(n_components=2)",
                "ASSIGN = tsne.fit_transform(X_std)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Invoking the t-SNE method\n",
                "tsne = TSNE(n_components=2)\n",
                "tsne_results = tsne.fit_transform(X_std) "
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = go.Scatter(",
                "ASSIGN = tsne_results[:,0],",
                "ASSIGN = tsne_results[:,1],",
                "ASSIGN = Target,",
                "ASSIGN = Target,",
                "ASSIGN = 'markers',",
                "ASSIGN = Target,",
                "ASSIGN = True,",
                "ASSIGN = dict(",
                "ASSIGN = 8,",
                "ASSIGN = Target,",
                "ASSIGN ='Jet',",
                "ASSIGN = False,",
                "ASSIGN = dict(",
                "ASSIGN = 2,",
                "ASSIGN = 'rgb(255, 255, 255)'",
                "),",
                "ASSIGN = 0.8",
                ")",
                ")",
                "ASSIGN = [traceTSNE]",
                "ASSIGN = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',",
                "ASSIGN= 'closest',",
                "ASSIGN = dict(zeroline = False),",
                "ASSIGN = dict(zeroline = False),",
                "ASSIGN= False,",
                ")",
                "ASSIGN = dict(data=data, layout=layout)",
                "py.iplot(ASSIGN, filename='styled-scatter')"
            ],
            "output_type": "error",
            "content_old": [
                "traceTSNE = go.Scatter(\n",
                "    x = tsne_results[:,0],\n",
                "    y = tsne_results[:,1],\n",
                "    name = Target,\n",
                "     hoveron = Target,\n",
                "    mode = 'markers',\n",
                "    text = Target,\n",
                "    showlegend = True,\n",
                "    marker = dict(\n",
                "        size = 8,\n",
                "        color = Target,\n",
                "        colorscale ='Jet',\n",
                "        showscale = False,\n",
                "        line = dict(\n",
                "            width = 2,\n",
                "            color = 'rgb(255, 255, 255)'\n",
                "        ),\n",
                "        opacity = 0.8\n",
                "    )\n",
                ")\n",
                "data = [traceTSNE]\n",
                "\n",
                "layout = dict(title = 'TSNE (T-Distributed Stochastic Neighbour Embedding)',\n",
                "              hovermode= 'closest',\n",
                "              yaxis = dict(zeroline = False),\n",
                "              xaxis = dict(zeroline = False),\n",
                "              showlegend= False,\n",
                "\n",
                "             )\n",
                "\n",
                "fig = dict(data=data, layout=layout)\n",
                "py.iplot(fig, filename='styled-scatter')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = pd.read_csv(",
                "ASSIGN='..path',",
                "ASSIGN=None,",
                "ASSIGN=',')",
                "ASSIGN.columns=['A', 'B', 'C', 'D', 'class']",
                "ASSIGN.dropna(how=\"all\", inplace=True) # drops the empty line at file-end",
                "ASSIGN.tail()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import pandas as pd\n",
                "\n",
                "df = pd.read_csv(\n",
                "    filepath_or_buffer='../input/Seed_Data.csv',\n",
                "    header=None,\n",
                "    sep=',')\n",
                "\n",
                "df.columns=['A', 'B', 'C', 'D', 'class']\n",
                "df.dropna(how=\"all\", inplace=True) # drops the empty line at file-end\n",
                "\n",
                "df.tail()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df.ix[:,0:4].values",
                "ASSIGN = df.ix[:,4].values"
            ],
            "output_type": "stream",
            "content_old": [
                "# split data table into data X and class labels y\n",
                "\n",
                "X = df.ix[:,0:4].values\n",
                "y = df.ix[:,4].values\n",
                "\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = {1: 'type0',",
                "2: 'type1',",
                "3: 'type2'}",
                "ASSIGN = {0: 'A',",
                "1: 'B',",
                "2: 'C',",
                "3: 'D'}",
                "with plt.style.context('seaborn-whitegrid'):",
                "plt.figure(figsize=(8, 6))",
                "for cnt in range(4):",
                "plt.subplot(2, 2, cnt+1)",
                "for lab in ('type0', 'type1', 'type2'):",
                "plt.hist(X[y==lab, cnt],",
                "ASSIGN=lab,",
                "ASSIGN=10,",
                "ASSIGN=0.3,)",
                "plt.xlabel(ASSIGN[cnt])",
                "plt.legend(loc='upper right', fancybox=True, fontsize=8)",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "from matplotlib import pyplot as plt\n",
                "import numpy as np\n",
                "import math\n",
                "\n",
                "label_dict = {1: 'type0',\n",
                "              2: 'type1',\n",
                "              3: 'type2'}\n",
                "\n",
                "feature_dict = {0: 'A',\n",
                "                1: 'B',\n",
                "                2: 'C',\n",
                "                3: 'D'}\n",
                "\n",
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(8, 6))\n",
                "    for cnt in range(4):\n",
                "        plt.subplot(2, 2, cnt+1)\n",
                "        for lab in ('type0', 'type1', 'type2'):\n",
                "            plt.hist(X[y==lab, cnt],\n",
                "                     label=lab,\n",
                "                     bins=10,\n",
                "                     alpha=0.3,)\n",
                "        plt.xlabel(feature_dict[cnt])\n",
                "    plt.legend(loc='upper right', fancybox=True, fontsize=8)\n",
                "\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = StandardScaler().fit_transform(X)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import StandardScaler\n",
                "X_std = StandardScaler().fit_transform(X)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_std"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_std"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = np.mean(X_std, axis=0)",
                "ASSIGN = (X_std - mean_vec).T.dot((X_std - mean_vec)) path(X_std.shape[0]-1)",
                "print('Covariance matrix \\n%s' %ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "import numpy as np\n",
                "mean_vec = np.mean(X_std, axis=0)\n",
                "cov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) / (X_std.shape[0]-1)\n",
                "print('Covariance matrix \\n%s' %cov_mat)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.cov(X_std.T)",
                "ASSIGN = np.linalg.eig(cov_mat)",
                "print('Eigenvectors \\n%s' %eig_vecs)",
                "print('\\nEigenvalues \\n%s' %eig_vals)"
            ],
            "output_type": "stream",
            "content_old": [
                "cov_mat = np.cov(X_std.T)\n",
                "\n",
                "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
                "\n",
                "print('Eigenvectors \\n%s' %eig_vecs)\n",
                "print('\\nEigenvalues \\n%s' %eig_vals)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]",
                "ASSIGN.sort(key=lambda x: x[0], reverse=True)",
                "print('Eigenvalues in descending order:')",
                "for i in ASSIGN:",
                "print(i[0])"
            ],
            "output_type": "stream",
            "content_old": [
                "# Make a list of (eigenvalue, eigenvector) tuples\n",
                "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
                "\n",
                "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
                "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
                "\n",
                "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
                "print('Eigenvalues in descending order:')\n",
                "for i in eig_pairs:\n",
                "    print(i[0])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = sum(eig_vals)",
                "ASSIGN = [(i path)*100 for i in sorted(eig_vals, reverse=True)]",
                "ASSIGN = np.cumsum(var_exp)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "tot = sum(eig_vals)\n",
                "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
                "cum_var_exp = np.cumsum(var_exp)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "with plt.style.context('seaborn-whitegrid'):",
                "plt.figure(figsize=(6, 4))",
                "plt.bar(range(4), var_exp, alpha=0.5, align='center',",
                "ASSIGN='individual explained variance')",
                "plt.step(range(4), cum_var_exp, where='mid',",
                "ASSIGN='cumulative explained variance')",
                "plt.ylabel('Explained variance ratio')",
                "plt.xlabel('Principal components')",
                "plt.legend(loc='best')",
                "plt.tight_layout()"
            ],
            "output_type": "display_data",
            "content_old": [
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(6, 4))\n",
                "\n",
                "    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n",
                "            label='individual explained variance')\n",
                "    plt.step(range(4), cum_var_exp, where='mid',\n",
                "             label='cumulative explained variance')\n",
                "    plt.ylabel('Explained variance ratio')\n",
                "    plt.xlabel('Principal components')\n",
                "    plt.legend(loc='best')\n",
                "    plt.tight_layout()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = np.hstack((eig_pairs[0][1].reshape(4,1),",
                "eig_pairs[1][1].reshape(4,1)))",
                "print('Matrix W:\\n', ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n",
                "                      eig_pairs[1][1].reshape(4,1)))\n",
                "\n",
                "print('Matrix W:\\n', matrix_w)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = X_std.dot(matrix_w)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Y = X_std.dot(matrix_w)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "with plt.style.context('seaborn-whitegrid'):",
                "plt.figure(figsize=(6, 4))",
                "for lab, col in zip(('type0', 'type1', 'type2'),",
                "('blue', 'red', 'green')):",
                "plt.scatter(Y[y==lab, 0],",
                "Y[y==lab, 1],",
                "ASSIGN=lab,",
                "ASSIGN=col)",
                "plt.xlabel('Principal Component 1')",
                "plt.ylabel('Principal Component 2')",
                "plt.legend(loc='lower center')",
                "plt.tight_layout()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "with plt.style.context('seaborn-whitegrid'):\n",
                "    plt.figure(figsize=(6, 4))\n",
                "    for lab, col in zip(('type0', 'type1', 'type2'),\n",
                "                        ('blue', 'red', 'green')):\n",
                "        plt.scatter(Y[y==lab, 0],\n",
                "                    Y[y==lab, 1],\n",
                "                    label=lab,\n",
                "                    c=col)\n",
                "    plt.xlabel('Principal Component 1')\n",
                "    plt.ylabel('Principal Component 2')\n",
                "    plt.legend(loc='lower center')\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "import os\n",
                "import random\n",
                "import math\n",
                "print(os.listdir(\"../input\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []"
            ],
            "output_type": "not_existent",
            "content_old": [
                "chromosome = []\n",
                "fitval = []\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def generateCromosome():",
                "chromosome.clear()",
                "for i in range(4):",
                "ASSIGN = []",
                "for j in range(6):",
                "ASSIGN.append(random.randint(0,1))",
                "chromosome.append(ASSIGN)",
                "print (\"Generated chromosome = \",chromosome)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def generateCromosome():\n",
                "    chromosome.clear()\n",
                "    for i in range(4):\n",
                "        singleChromosome = []\n",
                "        for j in range(6):\n",
                "            singleChromosome.append(random.randint(0,1))\n",
                "        chromosome.append(singleChromosome)\n",
                "\n",
                "    print (\"Generated chromosome = \",chromosome)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def evaluateSolution():",
                "fitval.clear()",
                "for i in range(4):",
                "ASSIGN = 0",
                "for j in range(1,6):",
                "ASSIGN += math.pow(2,5-j)*chromosome[i][j]",
                "if chromosome[i][0] == 1:",
                "ASSIGN = - ASSIGN",
                "fitval.append(ASSIGN)",
                "print (\"Fitval = \",fitval)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def evaluateSolution():\n",
                "    fitval.clear()\n",
                "    for i in range(4):\n",
                "        val = 0\n",
                "        for j in range(1,6):\n",
                "            val += math.pow(2,5-j)*chromosome[i][j]\n",
                "        if chromosome[i][0] == 1:\n",
                "            val = - val\n",
                "        fitval.append(val)\n",
                "    print (\"Fitval = \",fitval)\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "def func(x):",
                "return -(x*x)+5"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def func(x):\n",
                "    return -(x*x)+5"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def selection():",
                "ASSIGN = [[0 for i in range(2)] for j in range(4)]",
                "print (ASSIGN)",
                "ASSIGN=-1",
                "ASSIGN=-1",
                "for i in range(4):",
                "ASSIGN[i][0]=func(fitval[i])",
                "ASSIGN[i][1]=i",
                "ASSIGN=sorted(ASSIGN,key=lambda l:l[0], reverse=True)",
                "ASSIGN=ftval2[0][1]",
                "ASSIGN=ftval2[1][1]",
                "ASSIGN = ftval2[0][0]",
                "print(,ASSIGN)",
                "return c1, c2, bstval"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def selection():\n",
                "    ftval2 = [[0 for i in range(2)] for j in range(4)]\n",
                "    print (ftval2)\n",
                "    c1=-1\n",
                "    c2=-1\n",
                "    for i in range(4):\n",
                "        ftval2[i][0]=func(fitval[i])\n",
                "        ftval2[i][1]=i\n",
                "        \n",
                "    ftval2=sorted(ftval2,key=lambda l:l[0], reverse=True)\n",
                "    \n",
                "    c1=ftval2[0][1]\n",
                "    c2=ftval2[1][1]\n",
                "    \n",
                "    bstval = ftval2[0][0]\n",
                "    print(\"Bestval = \",bstval)\n",
                "    return c1, c2, bstval\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def crossover(s1,s2):",
                "ASSIGN = random.randint(0,5)",
                "for i in range(ASSIGN, 6):",
                "chromosome[s1][i],chromosome[s2][i] = chromosome[s2][i],chromosome[s1][i]",
                "print(,ASSIGN,,c1,c2,,chromosome)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def crossover(s1,s2):\n",
                "    select = random.randint(0,5)\n",
                "    \n",
                "    for i in range(select, 6):\n",
                "        chromosome[s1][i],chromosome[s2][i] = chromosome[s2][i],chromosome[s1][i]\n",
                "    print(\"For crossover, Selected = \",select,\", Chromose no = \",c1,c2,\", After Crossover = \",chromosome)\n"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "def mutation():",
                "ASSIGN = random.randint(1,50)",
                "ASSIGN == 30:",
                "ASSIGN = random.randint(0,3)",
                "ASSIGN = random.randint(0,5)",
                "chromosome[ASSIGN][ASSIGN] = 1 - chromosome[ASSIGN][ASSIGN]",
                "print(,ASSIGN,,ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def mutation():\n",
                "    select = random.randint(1,50)\n",
                "    if select == 30:\n",
                "        select2 = random.randint(0,3)\n",
                "        select3 = random.randint(0,5)\n",
                "        chromosome[select2][select3] = 1 - chromosome[select2][select3]\n",
                "        print(\"Mutation occured at, Chromosome = \",select2,\", Position = \",select3)\n",
                "    "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "generateCromosome()",
                "for i in range(1000):",
                "print(,i+1)",
                "evaluateSolution()",
                "c1,c2,bstval=selection()",
                "ASSIGN==5.0:",
                "break",
                "crossover(c1,c2)",
                "mutation()"
            ],
            "output_type": "stream",
            "content_old": [
                "generateCromosome()\n",
                "for i in range(1000):\n",
                "    print(\"Iteration\",i+1)\n",
                "    evaluateSolution()\n",
                "    c1,c2,bstval=selection()\n",
                "    if bstval==5.0:\n",
                "        break\n",
                "    crossover(c1,c2)\n",
                "    mutation()\n",
                "\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv('path')",
                "ASSIGN=pd.read_csv('path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data=pd.read_csv('/kaggle/input/digit-recognizer/train.csv')\n",
                "test_data=pd.read_csv('/kaggle/input/digit-recognizer/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in train_data.columns if train_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "# No data for train_data is missing\n",
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in test_data.columns if test_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "# No data for test_data is missing\n",
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = train_data[\"label\"]",
                "ASSIGN = train_data.drop([\"label\"],axis = 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Y_train = train_data[\"label\"]\n",
                "\n",
                "# Drop 'label' column\n",
                "X_train = train_data.drop([\"label\"],axis = 1) "
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Y_train.value_counts()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Y_train.value_counts()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "sns.countplot(Y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import seaborn as sns\n",
                "%matplotlib inline\n",
                "sns.countplot(Y_train)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=28",
                "ASSIGN=28",
                "def data_prep_X(X):",
                "ASSIGN=len(X)",
                "ASSIGN=X.values.reshape(num_img,img_row,img_col,1)",
                "ASSIGN=x_as_arraypath",
                "return X_out"
            ],
            "output_type": "not_existent",
            "content_old": [
                "img_row=28\n",
                "img_col=28\n",
                "def data_prep_X(X):\n",
                "    num_img=len(X)\n",
                "    x_as_array=X.values.reshape(num_img,img_row,img_col,1)\n",
                "    X_out=x_as_array/255\n",
                "    return X_out"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN=10",
                "def data_prep_Y(Y):",
                "ASSIGN = to_categorical(Y, num_classes)",
                "return out_y"
            ],
            "output_type": "stream",
            "content_old": [
                "from keras.utils.np_utils import to_categorical\n",
                "num_classes=10\n",
                "def data_prep_Y(Y):\n",
                "    out_y = to_categorical(Y, num_classes)\n",
                "    return out_y"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = data_prep_X(ASSIGN)",
                "ASSIGN = data_prep_X(ASSIGN)",
                "ASSIGN = data_prep_Y(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train = data_prep_X(X_train)\n",
                "test_data = data_prep_X(test_data)\n",
                "Y_train = data_prep_Y(Y_train)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = plt.imshow(X_train[0][:,:,0])"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Some examples images\n",
                "import matplotlib.pyplot as plt\n",
                "g = plt.imshow(X_train[0][:,:,0])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.imshow(X_train[1][:,:,0])"
            ],
            "output_type": "display_data",
            "content_old": [
                "g = plt.imshow(X_train[1][:,:,0])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Conv2D(20, kernel_size=(3, 3),",
                "ASSIGN='relu',",
                "ASSIGN=(img_row, img_col, 1)))",
                "ASSIGN.add(Conv2D(20, kernel_size=(3, 3), ASSIGN='relu'))",
                "ASSIGN.add(Flatten())",
                "ASSIGN.add(Dense(128, ASSIGN='relu'))",
                "ASSIGN.add(Dense(num_classes, ASSIGN='softmax'))",
                "ASSIGN.compile(loss=keras.losses.categorical_crossentropy,",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy'])",
                "ASSIGN.fit(X_train, Y_train,",
                "ASSIGN=128,",
                "ASSIGN=30,",
                "ASSIGN = 0.2)"
            ],
            "output_type": "stream",
            "content_old": [
                "from tensorflow.python import keras\n",
                "from tensorflow.python.keras.models import Sequential\n",
                "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D\n",
                "\n",
                "model = Sequential()\n",
                "model.add(Conv2D(20, kernel_size=(3, 3),\n",
                "                 activation='relu',\n",
                "                 input_shape=(img_row, img_col, 1)))\n",
                "model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\n",
                "model.add(Flatten())\n",
                "model.add(Dense(128, activation='relu'))\n",
                "model.add(Dense(num_classes, activation='softmax'))\n",
                "\n",
                "model.compile(loss=keras.losses.categorical_crossentropy,\n",
                "              optimizer='adam',\n",
                "              metrics=['accuracy'])\n",
                "model.fit(X_train, Y_train,\n",
                "          batch_size=128,\n",
                "          epochs=30,\n",
                "          validation_split = 0.2)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = model.predict(test_data)",
                "ASSIGN = np.argmax(ASSIGN,axis = 1)",
                "ASSIGN = pd.Series(ASSIGN,name=\"Label\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# predict results\n",
                "results = model.predict(test_data)\n",
                "\n",
                "# select the indix with the maximum probability\n",
                "results = np.argmax(results,axis = 1)\n",
                "\n",
                "results = pd.Series(results,name=\"Label\")"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)",
                "ASSIGN.to_csv(\"mySubmission.csv\",index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
                "\n",
                "submission.to_csv(\"mySubmission.csv\",index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "pip install face_recognition"
            ],
            "output_type": "stream",
            "content_old": [
                "pip install face_recognition"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import os\n",
                "import cv2\n",
                "import face_recognition"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Image.open('path')",
                "display(ASSIGN)"
            ],
            "output_type": "display_data",
            "content_old": [
                "from PIL import Image, ImageDraw\n",
                "from IPython.display import display\n",
                "\n",
                "# Sample Image\n",
                "virat_img = Image.open('/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces/Kohli-Williamson.jpg')\n",
                "display(virat_img)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "KNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces'\n",
                "UNKNOWN_FACES_DIR = '/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces'\n",
                "TOLERANCE = 0.6"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print('Loading known faces...')",
                "ASSIGN = []",
                "ASSIGN = []"
            ],
            "output_type": "stream",
            "content_old": [
                "print('Loading known faces...')\n",
                "known_faces = []\n",
                "known_names = []"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "for name in os.listdir(KNOWN_FACES_DIR):",
                "for filename in os.listdir(f'{KNOWN_FACES_DIR}path{name}'):",
                "ASSIGN = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}path{name}path{filename}')",
                "ASSIGN = face_recognition.face_encodings(image)[0]",
                "known_faces.append(ASSIGN)",
                "known_names.append(name)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for name in os.listdir(KNOWN_FACES_DIR):\n",
                "\n",
                "    # Next we load every file of faces of known person\n",
                "    for filename in os.listdir(f'{KNOWN_FACES_DIR}/{name}'):\n",
                "\n",
                "        # Load an image\n",
                "        image = face_recognition.load_image_file(f'{KNOWN_FACES_DIR}/{name}/{filename}')\n",
                "\n",
                "        # Get 128-dimension face encoding\n",
                "        # Always returns a list of found faces, for this purpose we take first face only (assuming one face per image as you can't be twice on one image)\n",
                "        encoding = face_recognition.face_encodings(image)[0]\n",
                "\n",
                "        # Append encodings and name\n",
                "        known_faces.append(encoding)\n",
                "        known_names.append(name)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(known_names)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(known_names)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "for filename in os.listdir(UNKNOWN_FACES_DIR):",
                "print(f'Filename {filename}', end='')",
                "ASSIGN = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}path{filename}')",
                "ASSIGN = face_recognition.ASSIGN(unknown_image)",
                "ASSIGN = face_recognition.ASSIGN(unknown_image, face_locations)",
                "ASSIGN = Image.fromarray(unknown_image)",
                "ASSIGN = ImageDraw.Draw(pil_image)",
                "for (top, right, bottom, left), face_encoding in zip(ASSIGN, ASSIGN):",
                "ASSIGN = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)",
                "ASSIGN = \"Unknown\"",
                "ASSIGN = face_recognition.face_distance(known_faces, face_encoding)",
                "ASSIGN = np.argmin(face_distances)",
                "if ASSIGN[ASSIGN]:",
                "ASSIGN = known_names[best_match_index]",
                "ASSIGN.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))",
                "ASSIGN = draw.textsize(name)",
                "ASSIGN.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))",
                "ASSIGN.text((left + 6, bottom - text_height - 5), ASSIGN, fill=(255, 255, 255, 255))",
                "del draw",
                "display(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "for filename in os.listdir(UNKNOWN_FACES_DIR):\n",
                "\n",
                "    # Load image\n",
                "    print(f'Filename {filename}', end='')\n",
                "    unknown_image = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}/{filename}')\n",
                "    # Load an image with an unknown face\n",
                "    #unknown_image = face_recognition.load_image_file(\"/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces/Virat_Kohli/gettyimages-463104486-2048x2048.jpg\")\n",
                "\n",
                "    # Find all the faces and face encodings in the unknown image\n",
                "    face_locations = face_recognition.face_locations(unknown_image)\n",
                "    face_encodings = face_recognition.face_encodings(unknown_image, face_locations)\n",
                "\n",
                "    # Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\n",
                "    # See http://pillow.readthedocs.io/ for more about PIL/Pillow\n",
                "    pil_image = Image.fromarray(unknown_image)\n",
                "    # Create a Pillow ImageDraw Draw instance to draw with\n",
                "    draw = ImageDraw.Draw(pil_image)\n",
                "\n",
                "    # Loop through each face found in the unknown image\n",
                "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
                "        # See if the face is a match for the known face(s)\n",
                "        matches = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)\n",
                "\n",
                "        name = \"Unknown\"\n",
                "\n",
                "        # Or instead, use the known face with the smallest distance to the new face\n",
                "        face_distances = face_recognition.face_distance(known_faces, face_encoding)\n",
                "        best_match_index = np.argmin(face_distances)\n",
                "        if matches[best_match_index]:\n",
                "            name = known_names[best_match_index]\n",
                "\n",
                "        # Draw a box around the face using the Pillow module\n",
                "        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n",
                "\n",
                "        # Draw a label with a name below the face\n",
                "        text_width, text_height = draw.textsize(name)\n",
                "        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n",
                "        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n",
                "\n",
                "    # Remove the drawing library from memory as per the Pillow docs\n",
                "    del draw\n",
                "\n",
                "    # Display the resulting image\n",
                "    display(pil_image)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN=test_data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/train.csv\")\n",
                "test_data = pd.read_csv(\"/kaggle/input/home-data-for-ml-course/test.csv\")\n",
                "test=test_data"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in train_data.columns if train_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in test_data.columns if test_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN=[]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "drop_column=[]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in train_data.columns:",
                "if train_data[col].isnull().any():",
                "ASSIGN=train_data[col].isnull().sum()",
                "if ASSIGN>50:",
                "print(col++str(ASSIGN))",
                "drop_column.append(col)"
            ],
            "output_type": "stream",
            "content_old": [
                "for col in train_data.columns:\n",
                "    if train_data[col].isnull().any():\n",
                "        x=train_data[col].isnull().sum()\n",
                "        if x>50:\n",
                "            print(col+\"\\t\"+str(x))\n",
                "            drop_column.append(col)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in test_data.columns:",
                "if test_data[col].isnull().any():",
                "ASSIGN=test_data[col].isnull().sum()",
                "if ASSIGN>50:",
                "print(col++str(ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "for col in test_data.columns:\n",
                "    if test_data[col].isnull().any():\n",
                "        x=test_data[col].isnull().sum()\n",
                "        if x>50:\n",
                "            print(col+\"\\t\"+str(x))"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(drop_column)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(drop_column)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.drop(drop_column,axis=1)",
                "ASSIGN=ASSIGN.drop(drop_column,axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data=train_data.drop(drop_column,axis=1)\n",
                "test_data=test_data.drop(drop_column,axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=train_data.drop([\"SalePrice\"],axis=1)",
                "ASSIGN=train_data[\"SalePrice\"]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train=train_data.drop([\"SalePrice\"],axis=1)\n",
                "Y_train=train_data[\"SalePrice\"]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X_train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_train.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Y_train.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Y_train.head()"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN=[]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cat_column=[]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for col in X_train.columns:",
                "if X_train[col].dtype=='object':",
                "cat_column.append(col)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for col in X_train.columns:\n",
                "    if X_train[col].dtype=='object':\n",
                "        cat_column.append(col)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(cat_column)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(cat_column)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in cat_column:",
                "print(col)",
                "print(X_train[col].value_counts())",
                "print(*50)"
            ],
            "output_type": "stream",
            "content_old": [
                "for col in cat_column:\n",
                "    print(col)\n",
                "    print(X_train[col].value_counts())\n",
                "    print(\"-\"*50)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "def score_dataset(X_train, X_valid, y_train, y_valid):",
                "ASSIGN = XGBRegressor(n_estimators=1000, learning_rate=0.01)",
                "ASSIGN.fit(X_train, y_train, early_stopping_rounds=50,",
                "ASSIGN=[(X_valid, y_valid)], verbose=False)",
                "ASSIGN.fit(X_train, y_train)",
                "ASSIGN = my_model.predict(X_valid)",
                "return mean_absolute_error(y_valid, ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from xgboost import XGBRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "\n",
                "# function for comparing different approaches\n",
                "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
                "    my_model = XGBRegressor(n_estimators=1000, learning_rate=0.01)\n",
                "    my_model.fit(X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(X_valid, y_valid)], verbose=False)\n",
                "    my_model.fit(X_train, y_train)\n",
                "    preds = my_model.predict(X_valid)\n",
                "    return mean_absolute_error(y_valid, preds)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = train_test_split(X_train, Y_train,",
                "ASSIGN=0.8, test_size=0.2,",
                "ASSIGN=0)",
                "ASSIGN=test_data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train,\n",
                "                                                      train_size=0.8, test_size=0.2,\n",
                "                                                      random_state=0)\n",
                "x_test=test_data"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=SimpleImputer(strategy=\"most_frequent\")",
                "ASSIGN= pd.DataFrame(my_imputer.fit_transform(x_train))",
                "ASSIGN=pd.DataFrame(my_imputer.transform(x_test))",
                "ASSIGN=pd.DataFrame(my_imputer.transform(x_valid))",
                "ASSIGN.index = x_train.index",
                "ASSIGN.index = x_valid.index",
                "ASSIGN.index = x_test.index",
                "ASSIGN.columns=x_train.columns",
                "ASSIGN.columns=x_valid.columns",
                "ASSIGN.columns=x_test.columns"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.impute import SimpleImputer\n",
                "my_imputer=SimpleImputer(strategy=\"most_frequent\")\n",
                "imputed_X_train= pd.DataFrame(my_imputer.fit_transform(x_train))\n",
                "imputed_X_test=pd.DataFrame(my_imputer.transform(x_test))\n",
                "imputed_X_valid=pd.DataFrame(my_imputer.transform(x_valid))\n",
                "imputed_X_train.index = x_train.index\n",
                "imputed_X_valid.index = x_valid.index\n",
                "imputed_X_test.index = x_test.index\n",
                "imputed_X_train.columns=x_train.columns\n",
                "imputed_X_valid.columns=x_valid.columns\n",
                "imputed_X_test.columns=x_test.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Col_with_missing_2 = [col for col in imputed_X_valid.columns if imputed_X_valid[col].isnull().any()]",
                "print(Col_with_missing_2)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing_2 = [col for col in imputed_X_valid.columns if imputed_X_valid[col].isnull().any()]\n",
                "print(Col_with_missing_2)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN=[]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Num_col=[]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in x_train.columns:",
                "if(x_train[col].dtype!=\"object\"):",
                "print(col++str(x_train[col].dtype))",
                "if col!=\"Id\":",
                "Num_col.append(col)"
            ],
            "output_type": "stream",
            "content_old": [
                "for col in x_train.columns:\n",
                "    if(x_train[col].dtype!=\"object\"):\n",
                "        print(col+\"\\t\"+str(x_train[col].dtype))\n",
                "        if col!=\"Id\":\n",
                "            Num_col.append(col)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(Num_col)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(Num_col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "\"\"\"",
                "ASSIGN=[]",
                "for col in Num_col:",
                "ASSIGN.append(col)",
                "\"\"\"",
                "ASSIGN=['TotalBsmtSF', 'WoodDeckSF', 'BsmtUnfSF', 'YearRemodAdd', '3SsnPorch', 'KitchenAbvGr', '2ndFlrSF', 'ScreenPorch', 'PoolArea', 'TotRmsAbvGrd', 'MoSold', 'BedroomAbvGr', 'MiscVal', 'BsmtHalfBath', '1stFlrSF', 'GarageCars', 'OverallQual', 'YrSold', 'HalfBath', 'OpenPorchSF', 'BsmtFullBath', 'LowQualFinSF', 'LotArea', 'OverallCond', 'YearBuilt', 'EnclosedPorch', 'FullBath', 'Fireplaces', 'BsmtFinSF2', 'BsmtFinSF1', 'MSSubClass', 'GrLivArea', 'GarageArea']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#feature=[\"LotArea\",\"OverallQual\",\"OverallCond\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"1stFlrSF\",\"GrLivArea\",'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageCars',\"GarageArea\"]\n",
                "\"\"\"\n",
                "feature=[]\n",
                "for col in Num_col:\n",
                "    feature.append(col)\n",
                "\"\"\"\n",
                "feature=['TotalBsmtSF', 'WoodDeckSF', 'BsmtUnfSF', 'YearRemodAdd', '3SsnPorch', 'KitchenAbvGr', '2ndFlrSF', 'ScreenPorch', 'PoolArea', 'TotRmsAbvGrd', 'MoSold', 'BedroomAbvGr', 'MiscVal', 'BsmtHalfBath', '1stFlrSF', 'GarageCars', 'OverallQual', 'YrSold', 'HalfBath', 'OpenPorchSF', 'BsmtFullBath', 'LowQualFinSF', 'LotArea', 'OverallCond', 'YearBuilt', 'EnclosedPorch', 'FullBath', 'Fireplaces', 'BsmtFinSF2', 'BsmtFinSF1', 'MSSubClass', 'GrLivArea', 'GarageArea']"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(feature)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(feature)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "for col in cat_column:",
                "print(col)",
                "print(X_train[col].value_counts())",
                "print(*50)"
            ],
            "output_type": "stream",
            "content_old": [
                "for col in cat_column:\n",
                "    print(col)\n",
                "    print(X_train[col].value_counts())\n",
                "    print(\"-\"*50)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#cat_enc_col=[\"Street\",\"LotShape\",\"LotConfig\",\"BldgType\",\"HouseStyle\",\"MasVnrType\",\"ExterQual\",\"Foundation\",\"BsmtQual\",\"BsmtExposure\",\"BsmtFinType1\",\"KitchenQual\"]\n",
                "cat_enc_col=['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'ExterQual', 'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional', 'PavedDrive', 'SaleType', 'SaleCondition']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for col in cat_enc_col:",
                "feature.append(col)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for col in cat_enc_col:\n",
                "    feature.append(col)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN[feature]",
                "ASSIGN=ASSIGN[feature]",
                "ASSIGN=ASSIGN[feature]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "imputed_X_train=imputed_X_train[feature]\n",
                "imputed_X_valid=imputed_X_valid[feature]\n",
                "imputed_X_test=imputed_X_test[feature]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "imputed_X_train.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "imputed_X_train.head(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=cat_enc_col"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Cat_cols=cat_enc_col"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = OneHotEncoder(handle_unknown='ignore', sparse=False)",
                "ASSIGN = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))",
                "ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))",
                "ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))",
                "ASSIGN.index = imputed_X_train.index",
                "ASSIGN.index = imputed_X_valid.index",
                "ASSIGN.index = imputed_X_test.index",
                "ASSIGN = imputed_X_train.drop(Cat_cols, axis =1)",
                "ASSIGN = imputed_X_valid.drop(Cat_cols, axis =1)",
                "ASSIGN = imputed_X_test.drop(Cat_cols, axis =1)",
                "ASSIGN = pd.concat([num_X_train, OH_cols_train], axis=1)",
                "ASSIGN = pd.concat([num_X_valid, OH_cols_valid], axis=1)",
                "ASSIGN = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))\n",
                "OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))\n",
                "OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))\n",
                "OH_cols_train.index = imputed_X_train.index\n",
                "OH_cols_valid.index = imputed_X_valid.index\n",
                "OH_cols_test.index = imputed_X_test.index\n",
                "num_X_train = imputed_X_train.drop(Cat_cols, axis =1)\n",
                "num_X_valid = imputed_X_valid.drop(Cat_cols, axis =1)\n",
                "num_X_test = imputed_X_test.drop(Cat_cols, axis =1)\n",
                "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
                "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.apply(pd.to_numeric)",
                "ASSIGN = ASSIGN.apply(pd.to_numeric)",
                "ASSIGN = ASSIGN.apply(pd.to_numeric)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "OH_X_train = OH_X_train.apply(pd.to_numeric)\n",
                "OH_X_valid = OH_X_valid.apply(pd.to_numeric)\n",
                "OH_X_test = OH_X_test.apply(pd.to_numeric)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "OH_X_test.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "OH_X_test.head(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,end=)",
                "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"MAE:\",end=\" \")\n",
                "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=1000,max_depth=4,alpha=0.08)",
                "ASSIGN.fit(OH_X_train, y_train)",
                "ASSIGN = my_model.predict(OH_X_valid)",
                "print(,mean_absolute_error(y_valid, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "my_model = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=1000,max_depth=4,alpha=0.08)\n",
                "my_model.fit(OH_X_train, y_train)\n",
                "preds = my_model.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = XGBRegressor(n_estimators=2000, learning_rate=0.008)",
                "ASSIGN.fit(OH_X_train, y_train, early_stopping_rounds=50,",
                "ASSIGN=[(OH_X_valid, y_valid)], verbose=False)",
                "ASSIGN = my_model_1.predict(OH_X_valid)",
                "print(,mean_absolute_error(y_valid, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "from xgboost import XGBRegressor\n",
                "from sklearn.metrics import mean_absolute_error\n",
                "my_model_1 = XGBRegressor(n_estimators=2000, learning_rate=0.008)\n",
                "my_model_1.fit(OH_X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(OH_X_valid, y_valid)], verbose=False)\n",
                "preds_1 = my_model_1.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_1))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=2000,max_depth=4,alpha=0.08)",
                "ASSIGN.fit(OH_X_train, y_train)",
                "ASSIGN = my_model_2.predict(OH_X_valid)",
                "print(,mean_absolute_error(y_valid, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.ensemble import GradientBoostingRegressor\n",
                "my_model_2 = GradientBoostingRegressor(loss=\"ls\",learning_rate=0.01,n_estimators=2000,max_depth=4,alpha=0.08)\n",
                "my_model_2.fit(OH_X_train, y_train)\n",
                "preds_2 = my_model_2.predict(OH_X_valid)\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_2))"
            ]
        },
        {
            "tags": [
                "validate_data",
                "process_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=(preds_1+preds_2)path",
                "print(,mean_absolute_error(y_valid, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "preds_3=(preds_1+preds_2)/2\n",
                "print(\"MAE:\",mean_absolute_error(y_valid, preds_3))"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = my_model_1.predict(OH_X_test)",
                "ASSIGN = my_model_2.predict(OH_X_test)",
                "ASSIGN=(predictions1+predictions2)path"
            ],
            "output_type": "not_existent",
            "content_old": [
                "predictions1 = my_model_1.predict(OH_X_test)\n",
                "predictions2 = my_model_2.predict(OH_X_test)\n",
                "Preds_last=(predictions1+predictions2)/2"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Preds_last"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Preds_last"
            ]
        },
        {
            "tags": [
                "transfer_results"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({'Id': test.Id,'SalePrice': Preds_last})",
                "ASSIGN.to_csv('submission1.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "output = pd.DataFrame({'Id': test.Id,'SalePrice': Preds_last})\n",
                "output.to_csv('submission1.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\n",
                "test_data = pd.read_csv(\"/kaggle/input/titanic/test.csv\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in train_data.columns if train_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing = [col for col in train_data.columns if train_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = [col for col in test_data.columns if test_data[col].isnull().any()]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing = [col for col in test_data.columns if test_data[col].isnull().any()]\n",
                "print(Col_with_missing)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (train_data.dtypes == 'object')",
                "ASSIGN = list(s[s].index)",
                "print()",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Get list of categorical variables\n",
                "s = (train_data.dtypes == 'object')\n",
                "object_cols = list(s[s].index)\n",
                "\n",
                "print(\"Categorical variables:\")\n",
                "print(object_cols)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']",
                "X=train_data[ASSIGN]",
                "ASSIGN=train_data[\"Survived\"]",
                "ASSIGN=test_data[feature_name]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "feature_name=['Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']\n",
                "X=train_data[feature_name]\n",
                "y=train_data[\"Survived\"]\n",
                "X_test=test_data[feature_name]"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=SimpleImputer(strategy=\"most_frequent\")",
                "ASSIGN= pd.DataFrame(my_imputer.fit_transform(X_train))",
                "ASSIGN=pd.DataFrame(my_imputer.transform(X_test))",
                "ASSIGN=pd.DataFrame(my_imputer.transform(X_valid))",
                "ASSIGN.index = X_train.index",
                "ASSIGN.index = X_valid.index",
                "ASSIGN.index = X_test.index",
                "ASSIGN.columns=X_train.columns",
                "ASSIGN.columns=X_valid.columns",
                "ASSIGN.columns=X_test.columns"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.impute import SimpleImputer\n",
                "my_imputer=SimpleImputer(strategy=\"most_frequent\")\n",
                "imputed_X_train= pd.DataFrame(my_imputer.fit_transform(X_train))\n",
                "imputed_X_test=pd.DataFrame(my_imputer.transform(X_test))\n",
                "imputed_X_valid=pd.DataFrame(my_imputer.transform(X_valid))\n",
                "imputed_X_train.index = X_train.index\n",
                "imputed_X_valid.index = X_valid.index\n",
                "imputed_X_test.index = X_test.index\n",
                "imputed_X_train.columns=X_train.columns\n",
                "imputed_X_valid.columns=X_valid.columns\n",
                "imputed_X_test.columns=X_test.columns"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Col_with_missing_2 = [col for col in imputed_X_test.columns if imputed_X_test[col].isnull().any()]",
                "print(Col_with_missing_2)"
            ],
            "output_type": "stream",
            "content_old": [
                "Col_with_missing_2 = [col for col in imputed_X_test.columns if imputed_X_test[col].isnull().any()]\n",
                "print(Col_with_missing_2)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = imputed_X_train['Sex'] + \"_\" + imputed_X_train['Embarked']",
                "ASSIGN = imputed_X_valid['Sex'] + \"_\" + imputed_X_valid['Embarked']",
                "ASSIGN = imputed_X_test['Sex'] + \"_\" + imputed_X_test['Embarked']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Feature Generation\n",
                "New_feature_train = imputed_X_train['Sex'] + \"_\" + imputed_X_train['Embarked']\n",
                "New_feature_valid = imputed_X_valid['Sex'] + \"_\" + imputed_X_valid['Embarked']\n",
                "New_feature_test = imputed_X_test['Sex'] + \"_\" + imputed_X_test['Embarked']"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "imputed_X_train[\"Sex_Embarked\"]=New_feature_train",
                "imputed_X_valid[\"Sex_Embarked\"]=New_feature_valid",
                "imputed_X_test[\"Sex_Embarked\"]=New_feature_test"
            ],
            "output_type": "not_existent",
            "content_old": [
                "imputed_X_train[\"Sex_Embarked\"]=New_feature_train\n",
                "imputed_X_valid[\"Sex_Embarked\"]=New_feature_valid\n",
                "imputed_X_test[\"Sex_Embarked\"]=New_feature_test"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "imputed_X_test.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "imputed_X_test.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=['Sex','Embarked','Sex_Embarked']"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Cat_cols=['Sex','Embarked','Sex_Embarked']"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = OneHotEncoder(handle_unknown='ignore', sparse=False)",
                "ASSIGN = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))",
                "ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))",
                "ASSIGN = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))",
                "ASSIGN.index = imputed_X_train.index",
                "ASSIGN.index = imputed_X_valid.index",
                "ASSIGN.index = imputed_X_test.index",
                "ASSIGN = imputed_X_train.drop(Cat_cols, axis =1)",
                "ASSIGN = imputed_X_valid.drop(Cat_cols, axis =1)",
                "ASSIGN = imputed_X_test.drop(Cat_cols, axis =1)",
                "ASSIGN = pd.concat([num_X_train, OH_cols_train], axis=1)",
                "ASSIGN = pd.concat([num_X_valid, OH_cols_valid], axis=1)",
                "ASSIGN = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.preprocessing import OneHotEncoder\n",
                "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
                "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(imputed_X_train[Cat_cols]))\n",
                "OH_cols_valid = pd.DataFrame(OH_encoder.transform(imputed_X_valid[Cat_cols]))\n",
                "OH_cols_test = pd.DataFrame(OH_encoder.transform(imputed_X_test[Cat_cols]))\n",
                "OH_cols_train.index = imputed_X_train.index\n",
                "OH_cols_valid.index = imputed_X_valid.index\n",
                "OH_cols_test.index = imputed_X_test.index\n",
                "num_X_train = imputed_X_train.drop(Cat_cols, axis =1)\n",
                "num_X_valid = imputed_X_valid.drop(Cat_cols, axis =1)\n",
                "num_X_test = imputed_X_test.drop(Cat_cols, axis =1)\n",
                "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
                "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
                "OH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.apply(pd.to_numeric)",
                "ASSIGN = ASSIGN.apply(pd.to_numeric)",
                "ASSIGN = ASSIGN.apply(pd.to_numeric)",
                "ASSIGN=ASSIGN.rename(columns={0:\"Sex1\", 1:\"Sex2\"})",
                "ASSIGN=ASSIGN.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})",
                "ASSIGN=ASSIGN.rename(columns={0:\"Sex1\", 1:\"Sex2\"})",
                "ASSIGN=ASSIGN.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})",
                "ASSIGN=ASSIGN.rename(columns={0:\"Sex1\", 1:\"Sex2\"})",
                "ASSIGN=ASSIGN.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "OH_X_train = OH_X_train.apply(pd.to_numeric)\n",
                "OH_X_valid = OH_X_valid.apply(pd.to_numeric)\n",
                "OH_X_test = OH_X_test.apply(pd.to_numeric)\n",
                "OH_X_train=OH_X_train.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_train=OH_X_train.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})\n",
                "OH_X_valid=OH_X_valid.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_valid=OH_X_valid.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})\n",
                "OH_X_test=OH_X_test.rename(columns={0:\"Sex1\", 1:\"Sex2\"})\n",
                "OH_X_test=OH_X_test.rename(columns={2:\"C\", 3:\"Q\",4:\"S\"})"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "OH_X_test.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "OH_X_test.head(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = XGBClassifier(n_estimators=1000, learning_rate=0.001)",
                "ASSIGN.fit(OH_X_train, y_train, early_stopping_rounds=50,",
                "ASSIGN=[(OH_X_valid, y_valid)], verbose=False)",
                "ASSIGN.fit(OH_X_train, y_train)",
                "ASSIGN = my_model.predict(OH_X_valid)",
                "print(,metrics.accuracy_score(y_valid, ASSIGN))"
            ],
            "output_type": "stream",
            "content_old": [
                "from xgboost import XGBClassifier\n",
                "from sklearn import metrics\n",
                "my_model = XGBClassifier(n_estimators=1000, learning_rate=0.001)\n",
                "my_model.fit(OH_X_train, y_train, early_stopping_rounds=50, \n",
                "             eval_set=[(OH_X_valid, y_valid)], verbose=False)\n",
                "my_model.fit(OH_X_train, y_train)\n",
                "y_pred5 = my_model.predict(OH_X_valid)\n",
                "print(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred5))"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = my_model.predict(OH_X_test)",
                "ASSIGN = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})",
                "ASSIGN.to_csv('my_submission_02_06.csv', index=False)",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "predictions2 = my_model.predict(OH_X_test)\n",
                "\n",
                "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions2})\n",
                "output.to_csv('my_submission_02_06.csv', index=False)\n",
                "print(\"Your submission was successfully saved!\")"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import os\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. wpd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "from skopt import gp_minimize\n"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = pd.read_csv('path')",
                "print(f'Classifier output data shape: {ASSIGN.shape}')",
                "ASSIGN.head(10)"
            ],
            "output_type": "stream",
            "content_old": [
                "classifier_output = pd.read_csv('/kaggle/input/tractable_ds_excercise_data/classifier_output.csv')\n",
                "print(f'Classifier output data shape:  {classifier_output.shape}')\n",
                "classifier_output.head(10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "classifier_output.dropna(inplace=True)",
                "print(f'Classifier output data shape without nans:  {classifier_output.shape}')"
            ],
            "output_type": "stream",
            "content_old": [
                "# Rows with no urr_score aren't needed for this analysis.  We're missing a lot for some reason\n",
                "classifier_output.dropna(inplace=True)\n",
                "print(f'Classifier output data shape without nans:  {classifier_output.shape}')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "ASSIGN.append(os.path.join(dirname, filename))",
                "ASSIGN = pd.concat([pd.read_csv(filepath) for filepath in metadata_files])",
                "print(f'Line data shape: {ASSIGN.shape}')",
                "ASSIGN.head(10)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Get line data\n",
                "\n",
                "metadata_files = []\n",
                "for dirname, _, filenames in os.walk('/kaggle/input/tractable_ds_excercise_data/metadata'):\n",
                "    for filename in filenames:\n",
                "        metadata_files.append(os.path.join(dirname, filename))\n",
                "\n",
                "line_data = pd.concat([pd.read_csv(filepath) for filepath in metadata_files])\n",
                "print(f'Line data shape: {line_data.shape}')\n",
                "line_data.head(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(f'Unique claims: {len(line_data[].unique())}')"
            ],
            "output_type": "stream",
            "content_old": [
                "# We seem to be missing about 5000 of the promised claims - perhaps ones where no repairs or replacements were made\n",
                "\n",
                "print(f'Unique claims: {len(line_data[\"claim_id\"].unique())}')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = classifier_output.merge(line_data[['claim_id', 'make', 'model', 'year','poi']].drop_duplicates(subset=['claim_id'], keep='first'),",
                "ASSIGN='left', on='claim_id')",
                "print(f'Classifier outputs not associated with a claim: {ASSIGN[].isna().sum()}')",
                "ASSIGN.dropna(subset=['make'], inplace=True)",
                "ASSIGN = pd.merge(claim_merged, line_data[['claim_id', 'line_num', 'part', 'operation', 'part_price', 'labour_amt']],",
                "ASSIGN='left', on=['claim_id', 'part'])",
                "ASSIGN['operation'].fillna('undamaged', inplace=True)",
                "print(f'Merge ASSIGN shape: {ASSIGN.shape}')",
                "ASSIGN.head(10)"
            ],
            "output_type": "stream",
            "content_old": [
                "# Merge the claim-level data first, and then the line-level data\n",
                "claim_merged = classifier_output.merge(line_data[['claim_id', 'make', 'model', 'year','poi']].drop_duplicates(subset=['claim_id'], keep='first'),\n",
                "                                       how='left', on='claim_id')\n",
                "\n",
                "print(f'Classifier outputs not associated with a claim: {claim_merged[\"make\"].isna().sum()}')\n",
                "# Remove any classifier outputs that can't be associated with a claim\n",
                "claim_merged.dropna(subset=['make'], inplace=True)\n",
                "\n",
                "data = pd.merge(claim_merged, line_data[['claim_id', 'line_num', 'part', 'operation', 'part_price', 'labour_amt']],\n",
                "                how='left', on=['claim_id', 'part'])\n",
                "\n",
                "data['operation'].fillna('undamaged', inplace=True)\n",
                "print(f'Merge data shape: {data.shape}')\n",
                "data.head(10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "data['rounded_urr_score'] = data['urr_score'].apply(lambda x: round(x, 2))",
                "ASSIGN = (data[(data['set']==2)][['rounded_urr_score', 'operation', 'urr_score']]",
                ".groupby(['rounded_urr_score', 'operation'])",
                ".count()",
                ".reset_index()",
                ".rename(columns={'urr_score': 'count'})",
                ".set_index('rounded_urr_score')",
                ".pivot(columns='operation', values='count')",
                ".fillna(0)",
                ")",
                "ASSIGN = ASSIGN[['undamaged', 'repair', 'replace']]",
                "ASSIGN.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Visualise the effectiveness of the classifier on the test set\n",
                "\n",
                "data['rounded_urr_score'] = data['urr_score'].apply(lambda x: round(x, 2))\n",
                "\n",
                "bucket_counts = (data[(data['set']==2)][['rounded_urr_score', 'operation', 'urr_score']]\n",
                "                 .groupby(['rounded_urr_score', 'operation'])\n",
                "                 .count()\n",
                "                 .reset_index()\n",
                "                 .rename(columns={'urr_score': 'count'})\n",
                "                 .set_index('rounded_urr_score')\n",
                "                 .pivot(columns='operation', values='count')\n",
                "                 .fillna(0)\n",
                "                )\n",
                "\n",
                "bucket_counts = bucket_counts[['undamaged', 'repair', 'replace']]\n",
                "\n",
                "bucket_counts.head(10)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "bucket_counts.sum(axis=1).plot.bar()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "bucket_counts.sum(axis=1).plot.bar()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = bucket_counts.divide(bucket_counts.sum(axis=1), axis=0)",
                "ASSIGN.plot.area()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "bucket_counts_divided = bucket_counts.divide(bucket_counts.sum(axis=1), axis=0)\n",
                "\n",
                "bucket_counts_divided.plot.area()"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = {'undamaged': 0,",
                "'repair': 1,",
                "'replace': 2}",
                "data['operation_rank'] = data['operation'].apply(lambda x: ASSIGN[x])",
                "def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):",
                "ASSIGN = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)",
                "return abs(ASSIGN - operation_rank)",
                "assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)",
                "assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)",
                "assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "operation_ranks = {'undamaged': 0,\n",
                "                   'repair': 1,\n",
                "                   'replace': 2}\n",
                "\n",
                "data['operation_rank'] = data['operation'].apply(lambda x: operation_ranks[x])\n",
                "\n",
                "def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):\n",
                "    classified_outcome_rank = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)\n",
                "\n",
                "    return abs(classified_outcome_rank - operation_rank)\n",
                "\n",
                "assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)\n",
                "assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)\n",
                "assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)\n",
                "    "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def mae_dataset(data, repair_threshold, replace_threshold):",
                "ASSIGN =[]",
                "for i in range(2):",
                "ASSIGN = data[(data['operation_rank']==i)]",
                "ASSIGN = sum(class_data",
                ".apply(lambda row: mae_single_point(row['urr_score'], row['operation_rank'], repair_threshold, replace_threshold), axis=1))path(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = sum(class_maes)path",
                "return total_mae"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def mae_dataset(data, repair_threshold, replace_threshold):\n",
                "    class_maes =[]\n",
                "    for i in range(2):\n",
                "        class_data = data[(data['operation_rank']==i)]\n",
                "        class_mae = sum(class_data\n",
                "                        .apply(lambda row: mae_single_point(row['urr_score'], row['operation_rank'], repair_threshold, replace_threshold), axis=1))/len(class_data)\n",
                "        class_maes.append(class_mae)\n",
                "    total_mae = sum(class_maes)/3\n",
                "    return total_mae"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = data[(data['set']==2)][['urr_score', 'operation_rank']]",
                "def mae(thresholds):",
                "return mae_dataset(ASSIGN, thresholds[0], thresholds[1])",
                "ASSIGN = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)",
                "print(f'Best thresholds: {ASSIGN.x}')",
                "print(f'Best average mse: {ASSIGN.fun}')"
            ],
            "output_type": "stream",
            "content_old": [
                "# Use only the test set to evaluate the best thresholds\n",
                "\n",
                "test_set = data[(data['set']==2)][['urr_score', 'operation_rank']]\n",
                "\n",
                "def mae(thresholds):\n",
                "    return mae_dataset(test_set, thresholds[0], thresholds[1])\n",
                "\n",
                "# Calculating mse is somewhat expensive at a couple of seconds a time, so use an optimizer and small number of iterations\n",
                "# Takes about 2.5 minutes\n",
                "opt = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)\n",
                "\n",
                "print(f'Best thresholds: {opt.x}')\n",
                "print(f'Best average mse: {opt.fun}')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = 'https:path'",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)",
                "ASSIGN = 'https:path'",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)",
                "ASSIGN = 'https:path'",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)",
                "ASSIGN = 'https:path'",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)",
                "ASSIGN = 'https:path'",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)",
                "ASSIGN=datetime.strftime(datetime.today() - timedelta(1), 'https:path%m-%d-%Y.csv')",
                "ASSIGN = requests.get(url, allow_redirects=True)",
                "open('.path', 'wb').write(ASSIGN.content)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import requests\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_confirmed_US.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_confirmed_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_US.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_deaths_US.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_deaths_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_recovered_global.csv'\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./time_series_covid19_recovered_global.csv', 'wb').write(r.content)\n",
                "#\n",
                "from datetime import datetime, timedelta\n",
                "url=datetime.strftime(datetime.today() - timedelta(1), 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/%m-%d-%Y.csv')\n",
                "r = requests.get(url, allow_redirects=True)\n",
                "open('./csse_covid_19_daily_reports.csv', 'wb').write(r.content)\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = r'..path'",
                "ASSIGN = r'.path'",
                "shutil.copyfile(ASSIGN, ASSIGN)",
                "ASSIGN = r'..path'",
                "ASSIGN = r'.path'",
                "shutil.copyfile(ASSIGN, ASSIGN)",
                "ASSIGN = r'..path'",
                "ASSIGN = r'.path'",
                "shutil.copyfile(ASSIGN, ASSIGN)",
                "ASSIGN = r'..path'",
                "ASSIGN = r'.path'",
                "shutil.copyfile(ASSIGN, ASSIGN)",
                "ASSIGN = r'..path'",
                "ASSIGN = r'.path'",
                "shutil.copyfile(ASSIGN, ASSIGN)",
                "print(os.listdir())"
            ],
            "output_type": "stream",
            "content_old": [
                "import shutil\n",
                "\n",
                "original = r'../input/input1/covid19_by_country.csv'\n",
                "target = r'./covid19_by_country.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/GlobalLandTemperaturesByCountry.csv'\n",
                "target = r'./GlobalLandTemperaturesByCountry.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/GlobalLandTemperaturesByMajorCity.csv'\n",
                "target = r'./GlobalLandTemperaturesByMajorCity.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\n",
                "target = r'./API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'\n",
                "shutil.copyfile(original, target)\n",
                "original = r'../input/input1/shift.JPG'\n",
                "target = r'./shift.JPG'\n",
                "shutil.copyfile(original, target)\n",
                "\n",
                "print(os.listdir(\"./\"))\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "sns.set(style=\"white\", color_codes=True)",
                "warnings.filterwarnings(\"ignore\")",
                "print(os.listdir())",
                "pio.renderers.default = \"browser\""
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# Loading datasets required for analysis\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import seaborn as sns\n",
                "import matplotlib.pyplot as plt\n",
                "sns.set(style=\"white\", color_codes=True)\n",
                "import warnings # current version of seaborn generates a bunch of warnings that we'll ignore\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "import os\n",
                "print(os.listdir(\"./\"))\n",
                "#print(os.listdir(\"../input/input\"))\n",
                "%load_ext autoreload\n",
                "%autoreload 2\n",
                "import plotly.io as pio\n",
                "pio.renderers.default = \"browser\""
            ]
        },
        {
            "tags": [
                "ingest_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv('.path')",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country=pd.read_csv('./covid19_by_country.csv')\n",
                "codiv_country.sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country.describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Population 2020']=codiv_country['Population 2020']*1000",
                "codiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0path['Population 2020']",
                "codiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp'])",
                "codiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']path['Population 2020']))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#nomalize some datas, also change the population in real size\n",
                "codiv_country['Population 2020']=codiv_country['Population 2020']*1000\n",
                "codiv_country['Tests_per_10kp']=codiv_country['Tests']*10000.0/codiv_country['Population 2020']\n",
                "codiv_country['Tests_per_10kp_log1p']=np.log1p(codiv_country['Tests_per_10kp'])\n",
                "codiv_country['GDP 2018 per_1p_log1p']=np.log1p((codiv_country['GDP 2018']/codiv_country['Population 2020']))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.drop([",
                "'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'",
                "], axis=1)",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country=codiv_country.drop([\n",
                "    'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'\n",
                "], axis=1)\n",
                "codiv_country.sample(10)"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv(os.path.join('.path', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'))",
                "ASSIGN=ASSIGN.set_index('Country Name')",
                "ASSIGN.sample(3)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Health_expenditure=pd.read_csv(os.path.join('./', 'API_SH.XPD.CHEX.GD.ZS_DS2_en_csv_v2_989101.csv'))\n",
                "Health_expenditure=Health_expenditure.set_index('Country Name')\n",
                "Health_expenditure.sample(3)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Health expenditure Ratio'] = 0.0",
                "for countryindex in Health_expenditure.index:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country['Health expenditure Ratio'] = 0.0\n",
                "for countryindex in Health_expenditure.index:\n",
                "    # If the country exists in the other table\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Health expenditure Ratio']=Health_expenditure[Health_expenditure.index==countryindex]['2017'][0]\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country.hist(figsize=(12, 12))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "codiv_country.hist(figsize=(12, 12))\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "codiv_country.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.isna().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country.isna().sum()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country[codiv_country['Health expenditure Ratio'].isnull()]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country[codiv_country['Health expenditure Ratio'].isnull()]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country['Health expenditure Ratio'] = codiv_country['Health expenditure Ratio'].fillna(codiv_country['Health expenditure Ratio'].mean())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country['Tests_per_10kp_log1p'] = codiv_country['Tests_per_10kp_log1p'].fillna(0) "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country.Country[codiv_country.Country == \"United States\"] = \"US\""
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country.Country[codiv_country.Country == \"United States\"] = \"US\""
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) path['Females 2018'].std()",
                "ASSIGN.plot()",
                "plt.hlines(-1,0,100,colors=\"red\")",
                "plt.hlines(-2,0,100,colors=\"yellow\")",
                "plt.hlines(-3,0,100,colors=\"green\")",
                "plt.hlines(1,0,100,colors=\"red\")",
                "plt.hlines(2,0,100,colors=\"yellow\")",
                "plt.hlines(3,0,100,colors=\"green\")",
                "plt.ylabel(\"ASSIGN\")",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "z_scores_Females= (codiv_country['Females 2018']- codiv_country['Females 2018'].mean()) /codiv_country['Females 2018'].std()\n",
                "z_scores_Females.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Females\")\n",
                "plt.xlabel(\"country index\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country['Females 2018'].plot()",
                "plt.ylabel(\"Females %\")",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country['Females 2018'].plot()\n",
                "plt.ylabel(\"Females %\")\n",
                "plt.xlabel(\"country index\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (np.abs(z_scores_Females) > 2)",
                "codiv_country['Females 2018'][ASSIGN]=codiv_country['Females 2018'].mean()",
                "print('z-scores_Females:', z_scores_Females.shape)",
                "codiv_country['Females 2018'].plot()"
            ],
            "output_type": "stream",
            "content_old": [
                "idx_Females = (np.abs(z_scores_Females) > 2)\n",
                "codiv_country['Females 2018'][idx_Females]=codiv_country['Females 2018'].mean()\n",
                "print('z-scores_Females:', z_scores_Females.shape)\n",
                "codiv_country['Females 2018'].plot()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) path['Female Lung'].std()",
                "ASSIGN.plot()",
                "plt.hlines(-1,0,100,colors=\"red\")",
                "plt.hlines(-2,0,100,colors=\"yellow\")",
                "plt.hlines(-3,0,100,colors=\"green\")",
                "plt.hlines(1,0,100,colors=\"red\")",
                "plt.hlines(2,0,100,colors=\"yellow\")",
                "plt.hlines(3,0,100,colors=\"green\")",
                "plt.ylabel(\"ASSIGN\")",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "z_scores_Female_Lung= (codiv_country['Female Lung']- codiv_country['Female Lung'].mean()) /codiv_country['Female Lung'].std()\n",
                "z_scores_Female_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Female_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (np.abs(z_scores_Female_Lung) > 2)",
                "codiv_country['Female Lung'][ASSIGN]=codiv_country['Female Lung'].mean()",
                "print('z-scores_Female_Lung:', z_scores_Female_Lung.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "idx_Female_Lung = (np.abs(z_scores_Female_Lung) > 2)\n",
                "codiv_country['Female Lung'][idx_Female_Lung]=codiv_country['Female Lung'].mean()\n",
                "print('z-scores_Female_Lung:', z_scores_Female_Lung.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) path['Male Lung'].std()",
                "ASSIGN.plot()",
                "plt.hlines(-1,0,100,colors=\"red\")",
                "plt.hlines(-2,0,100,colors=\"yellow\")",
                "plt.hlines(-3,0,100,colors=\"green\")",
                "plt.hlines(1,0,100,colors=\"red\")",
                "plt.hlines(2,0,100,colors=\"yellow\")",
                "plt.hlines(3,0,100,colors=\"green\")",
                "plt.ylabel(\"ASSIGN\")",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "z_scores_Male_Lung= (codiv_country['Male Lung']- codiv_country['Male Lung'].mean()) /codiv_country['Male Lung'].std()\n",
                "z_scores_Male_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\")\n",
                "plt.hlines(-3,0,100,colors=\"green\")\n",
                "plt.hlines(1,0,100,colors=\"red\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\")\n",
                "plt.hlines(3,0,100,colors=\"green\")\n",
                "plt.ylabel(\"z_scores_Male_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (np.abs(z_scores_Male_Lung) > 2)",
                "codiv_country['Male Lung'][ASSIGN]=codiv_country['Male Lung'].mean()",
                "print('z-scores_Male_Lung:', z_scores_Male_Lung.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "idx_Male_Lung = (np.abs(z_scores_Male_Lung) > 2)\n",
                "codiv_country['Male Lung'][idx_Male_Lung]=codiv_country['Male Lung'].mean()\n",
                "print('z-scores_Male_Lung:', z_scores_Male_Lung.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN= (codiv_country['lung']- codiv_country['lung'].mean()) path['lung'].std()",
                "ASSIGN.plot()",
                "plt.hlines(-1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")",
                "plt.hlines(-2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")",
                "plt.hlines(-3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")",
                "plt.hlines(1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")",
                "plt.hlines(2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")",
                "plt.hlines(3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")",
                "plt.ylabel(\"ASSIGN\")",
                "plt.xlabel(\"country index\")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "z_scores_Lung= (codiv_country['lung']- codiv_country['lung'].mean()) /codiv_country['lung'].std()\n",
                "z_scores_Lung.plot()\n",
                "plt.hlines(-1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\n",
                "plt.hlines(-2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\n",
                "plt.hlines(-3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\n",
                "plt.hlines(1,0,100,colors=\"red\",label=\"np.abs(z_scores_Females) > 1)\")\n",
                "plt.hlines(2,0,100,colors=\"yellow\",label=\"np.abs(z_scores_Females) > 2)\")\n",
                "plt.hlines(3,0,100,colors=\"green\",label=\"np.abs(z_scores_Females) > 3)\")\n",
                "plt.ylabel(\"z_scores_Lung\")\n",
                "plt.xlabel(\"country index\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = (np.abs(z_scores_Lung) > 2)",
                "codiv_country['lung'][ASSIGN]=codiv_country['lung'].mean()",
                "print('z-scores_Lung:', z_scores_Lung.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "idx_Lung = (np.abs(z_scores_Lung) > 2)\n",
                "codiv_country['lung'][idx_Lung]=codiv_country['lung'].mean()\n",
                "print('z-scores_Lung:', z_scores_Lung.shape)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv(os.path.join('.path', 'csse_covid_19_daily_reports.csv'))",
                "ASSIGN=ASSIGN.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse=pd.read_csv(os.path.join('./', 'csse_covid_19_daily_reports.csv'))\n",
                "codiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)\n",
                "codiv_csse.sample(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.rename(columns={'Confirmed': 'Infected'})"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_csse = codiv_csse.rename(columns={'Confirmed': 'Infected'})"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.groupby('Country_Region').sum().reset_index()",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse=codiv_csse.groupby('Country_Region').sum().reset_index()\n",
                "codiv_csse.sample(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_csse[\"Deaths Ratio\"]=codiv_csse[\"Deaths\"]*1000path[\"Infected\"]",
                "codiv_csse.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse[\"Deaths Ratio\"]=codiv_csse[\"Deaths\"]*1000/codiv_csse[\"Infected\"]\n",
                "codiv_csse.sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique())"
            ],
            "output_type": "execute_result",
            "content_old": [
                "set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique())"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Korea, South\"] = \"South Korea\"",
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Czechia\"] = \"Czech Republic\""
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Korea, South\"] = \"South Korea\"\n",
                "codiv_csse.Country_Region[codiv_csse.Country_Region == \"Czechia\"] = \"Czech Republic\""
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=ASSIGN.set_index('Country_Region')",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse=codiv_csse.set_index('Country_Region')\n",
                "codiv_csse.sample(10)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_csse.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "codiv_csse.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_csse.isna().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_csse.isna().sum()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Total Infected'] = 0.0",
                "codiv_country['Total Deaths'] = 0.0",
                "codiv_country['Total Recovered'] = 0.0",
                "codiv_country['Total Active'] = 0.0",
                "codiv_country['Deaths Ratio'] = 0.0",
                "for countryindex in codiv_csse.index:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0])",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0])",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]",
                "codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0]",
                "if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0:",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]",
                "else:",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0])",
                "codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country['Total Infected'] = 0.0\n",
                "codiv_country['Total Deaths'] = 0.0\n",
                "codiv_country['Total Recovered'] = 0.0\n",
                "codiv_country['Total Active'] = 0.0\n",
                "codiv_country['Deaths Ratio'] = 0.0\n",
                "for countryindex in codiv_csse.index:\n",
                "    # If the country exists in the other table\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Infected']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Deaths'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Deaths']=codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Total Recovered']=codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Deaths Ratio']=codiv_csse[codiv_csse.index==countryindex]['Deaths Ratio'][0]\n",
                "        if codiv_csse[codiv_csse.index==countryindex]['Active'][0] ==0.0:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0])\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Infected'][0]-codiv_csse[codiv_csse.index==countryindex]['Deaths'][0]-codiv_csse[codiv_csse.index==countryindex]['Recovered'][0]\n",
                "        else:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active Log10']=np.log1p(codiv_csse[codiv_csse.index==countryindex]['Active'][0])\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Total Active']=codiv_csse[codiv_csse.index==countryindex]['Active'][0]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country.isnull()[['Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active', 'Deaths Ratio']].sum()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=codiv_country[codiv_country['Quarantine'].notnull()]",
                "ASSIGN=codiv_country[codiv_country['Restrictions'].notnull()]",
                "ASSIGN=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()]",
                "print( , ASSIGN.shape)",
                "print( , ASSIGN.shape)",
                "print( , ASSIGN.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "codiv_country_qurantine=codiv_country[codiv_country['Quarantine'].notnull()]\n",
                "codiv_country_Restrictions=codiv_country[codiv_country['Restrictions'].notnull()]\n",
                "codiv_country_without_Restrictions_qurantine=codiv_country[codiv_country['Restrictions'].isnull() & codiv_country['Quarantine'].isnull()]\n",
                "\n",
                "print(\"codiv_country_qurantine shape\" , codiv_country_qurantine.shape)\n",
                "print(\"codiv_country_Restrictionse shape\" , codiv_country_Restrictions.shape)\n",
                "print(\"codiv_country_without_Restrictions_quarantine\" , codiv_country_without_Restrictions_qurantine.shape)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_confirmed_global.csv'))",
                "ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1)",
                "ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_deaths_global.csv'))",
                "ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1)",
                "ASSIGN=pd.read_csv(os.path.join('.path', 'time_series_covid19_recovered_global.csv'))",
                "ASSIGN=ASSIGN.drop(['Lat','Long'], axis=1)",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_time_confirmed=pd.read_csv(os.path.join('./', 'time_series_covid19_confirmed_global.csv'))\n",
                "codiv_time_confirmed=codiv_time_confirmed.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_deaths=pd.read_csv(os.path.join('./', 'time_series_covid19_deaths_global.csv'))\n",
                "codiv_time_deaths=codiv_time_deaths.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_recovered=pd.read_csv(os.path.join('./', 'time_series_covid19_recovered_global.csv'))\n",
                "codiv_time_recovered=codiv_time_recovered.drop(['Lat','Long'], axis=1)\n",
                "\n",
                "codiv_time_confirmed.sample(10)"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=pd.read_csv('.path')",
                "ASSIGN=ASSIGN.drop(['AverageTemperatureUncertainty'], axis=1)",
                "ASSIGN=ASSIGN[((ASSIGN['dt'] > '2013-01-01') & (ASSIGN['dt'] < '2013-04-01')) |",
                "((ASSIGN['dt'] > '2012-01-01') & (ASSIGN['dt'] < '2012-04-01')) |",
                "((ASSIGN['dt'] > '2011-01-01') & (ASSIGN['dt'] < '2011-04-01'))]",
                "ASSIGN = ASSIGN.groupby(['Country'])['AverageTemperature'].mean()",
                "print(ASSIGN.sample(10))",
                "print(,ASSIGN.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "codiv_country_temp=pd.read_csv('.//GlobalLandTemperaturesByCountry.csv')\n",
                "#codiv_country_temp.loc['2020-01-01':'2020-04-01']\n",
                "codiv_country_temp=codiv_country_temp.drop(['AverageTemperatureUncertainty'], axis=1)\n",
                "codiv_country_temp=codiv_country_temp[((codiv_country_temp['dt'] > '2013-01-01') & (codiv_country_temp['dt'] < '2013-04-01')) | \n",
                "                                      ((codiv_country_temp['dt'] > '2012-01-01') & (codiv_country_temp['dt'] < '2012-04-01')) | \n",
                "                                      ((codiv_country_temp['dt'] > '2011-01-01') & (codiv_country_temp['dt'] < '2011-04-01'))]\n",
                "codiv_country_temp = codiv_country_temp.groupby(['Country'])['AverageTemperature'].mean()\n",
                "print(codiv_country_temp.sample(10))\n",
                "print(\"codiv_country_temp\",codiv_country_temp.shape)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(codiv_country_temp.describe())"
            ],
            "output_type": "stream",
            "content_old": [
                "print(codiv_country_temp.describe())"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=pd.read_csv('.path')",
                "ASSIGN=ASSIGN.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1)",
                "ASSIGN=ASSIGN[((ASSIGN['dt'] > '2013-01-01') & (ASSIGN['dt'] < '2013-04-01')) |",
                "((ASSIGN['dt'] > '2012-01-01') & (ASSIGN['dt'] < '2012-04-01')) |",
                "((ASSIGN['dt'] > '2011-01-01') & (ASSIGN['dt'] < '2011-04-01'))]",
                "ASSIGN= ASSIGN.groupby(['Country'])['AverageTemperature'].mean()",
                "print(ASSIGN.sample(10))",
                "print(,ASSIGN.shape)",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "codiv_country_tempMajorCity=pd.read_csv('.//GlobalLandTemperaturesByMajorCity.csv')\n",
                "\n",
                "codiv_country_tempMajorCity=codiv_country_tempMajorCity.drop(['AverageTemperatureUncertainty','Latitude','Longitude'], axis=1)\n",
                "codiv_country_tempMajorCity=codiv_country_tempMajorCity[((codiv_country_tempMajorCity['dt'] > '2013-01-01') & (codiv_country_tempMajorCity['dt'] < '2013-04-01')) | \n",
                "                                      ((codiv_country_tempMajorCity['dt'] > '2012-01-01') & (codiv_country_tempMajorCity['dt'] < '2012-04-01')) | \n",
                "                                      ((codiv_country_tempMajorCity['dt'] > '2011-01-01') & (codiv_country_tempMajorCity['dt'] < '2011-04-01'))]\n",
                "codiv_country_tempMajorCity= codiv_country_tempMajorCity.groupby(['Country'])['AverageTemperature'].mean()\n",
                "print(codiv_country_tempMajorCity.sample(10))\n",
                "print(\"codiv_country_tempMajorCity\",codiv_country_tempMajorCity.shape)\n",
                "print(\"243-49=194 countries dont have temperature for big city\")"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN=(codiv_country_tempMajorCity-codiv_country_temp).dropna()",
                "ASSIGN=ASSIGN.sort_values( ascending=False)",
                "ASSIGN=Temperature_difference.index",
                "ASSIGN=Temperature_difference",
                "ASSIGN = plt.subplots(figsize=(20,10))",
                "ax.scatter(ASSIGN, ASSIGN, alpha=0.5)",
                "plt.xticks(rotation=45)",
                "plt.ylabel(\"Delta Temp AvrMaincity-AvrCountry\")",
                "plt.grid(True)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "Temperature_difference=(codiv_country_tempMajorCity-codiv_country_temp).dropna()\n",
                "Temperature_difference=Temperature_difference.sort_values( ascending=False)\n",
                "x=Temperature_difference.index\n",
                "y=Temperature_difference\n",
                "fig, ax = plt.subplots(figsize=(20,10))\n",
                "ax.scatter(x, y, alpha=0.5)\n",
                "plt.xticks(rotation=45)\n",
                "plt.ylabel(\"Delta Temp AvrMaincity-AvrCountry\")\n",
                "plt.grid(True)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country['Temp_mean_jan_apr'] = 0.0",
                "for countryindex in codiv_country_tempMajorCity.index:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex]",
                "codiv_country['Temp_mean_jan_apr'].sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country['Temp_mean_jan_apr'] = 0.0\n",
                "for countryindex in codiv_country_tempMajorCity.index:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_tempMajorCity.loc[countryindex]\n",
                "codiv_country['Temp_mean_jan_apr'].sample(10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "for countryindex in codiv_country_temp.index:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty:",
                "if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0:",
                "codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex]",
                "codiv_country.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "for countryindex in codiv_country_temp.index:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty:\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Temp_mean_jan_apr'].values[0] == 0:\n",
                "            codiv_country.at[codiv_country['Country']==countryindex,'Temp_mean_jan_apr']=codiv_country_temp.loc[countryindex]\n",
                "codiv_country.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)]"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country.sort_values(by='Total Infected', ascending=False)",
                "ASSIGN = ASSIGN.reset_index(drop=True)",
                "ASSIGN.head(11).style.background_gradient(cmap='Blues')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "transit=codiv_country.sort_values(by='Total Infected', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Blues')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country.sort_values(by='Total Active', ascending=False)",
                "ASSIGN = ASSIGN.reset_index(drop=True)",
                "ASSIGN.head(11).style.background_gradient(cmap='Greens')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "transit=codiv_country.sort_values(by='Total Active', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Greens')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country.sort_values(by='Total Deaths', ascending=False)",
                "ASSIGN = ASSIGN.reset_index(drop=True)",
                "ASSIGN.head(11).style.background_gradient(cmap='Reds')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "transit=codiv_country.sort_values(by='Total Deaths', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Reds')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country.sort_values(by='Deaths Ratio', ascending=False)",
                "ASSIGN = ASSIGN.reset_index(drop=True)",
                "ASSIGN.head(11).style.background_gradient(cmap='Oranges')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "transit=codiv_country.sort_values(by='Deaths Ratio', ascending=False)\n",
                "transit = transit.reset_index(drop=True)\n",
                "transit.head(11).style.background_gradient(cmap='Oranges')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True)",
                "codiv_country_short"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_short=codiv_country.sort_values(by='Total Infected', ascending=False).head(n=60).reset_index(drop=True)\n",
                "codiv_country_short"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country_short.describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_short.describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\",
                ".map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Infected Log10\") \\",
                ".add_legend()",
                "plt.grid(True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "\n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Infected Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)",
                "plt.xlabel('Total Infected Log1p')",
                "plt.ylabel('Nr Country density')",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p == 0\n",
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Infected Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Infected Log1p')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Infected Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)",
                "plt.xlabel('Total Infected Log1p')",
                "plt.ylabel('Nr Country density')",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#histogram of \"total infected\" for countries that have Tests_per_10kp_log1p != 0\n",
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Infected Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Infected Log1p')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\",
                ".map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Active Log10\") \\",
                ".add_legend()",
                "plt.grid(True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Tests_per_10kp_log1p\", \"Total Active Log10\") \\\n",
                "   .add_legend()\n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Active Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)",
                "plt.xlabel('Total Active Log10')",
                "plt.ylabel('Nr Country density')",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] == 0][\"Total Active Log10\"].hist(figsize=(6, 6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Active Log10')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p==0')"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Active Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)",
                "plt.xlabel('Total Active Log10')",
                "plt.ylabel('Nr Country density')",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_short[codiv_country_short[\"Tests_per_10kp_log1p\"] != 0][\"Total Active Log10\"].hist(figsize=(6,6), alpha=0.5, density=True)\n",
                "plt.xlabel('Total Active Log10')\n",
                "plt.ylabel('Nr Country density')\n",
                "plt.title('Histogram Tests_per_10kp_log1p not 0')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_short.drop([   'Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active'], axis=1)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1)",
                "for col in ASSIGN.loc[:, ASSIGN.dtypes == np.number].keys():",
                "sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=\"blue\")",
                "ASSIGN = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col])",
                "print( %(col, corr))",
                "sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=\"red\")",
                "ASSIGN = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col])",
                "print( %(col, corr))",
                "sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=\"green\")",
                "ASSIGN = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col])",
                "print( %(col, corr))",
                "sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=\"orange\")",
                "ASSIGN = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col])",
                "print( %(col, corr))",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "import seaborn as sns\n",
                "import numpy as np\n",
                "from scipy.stats import pearsonr\n",
                "#I use a temp table to hide some columns not needed for the sns.\n",
                "codiv_country_short_tmp=codiv_country_short.drop([ 'Deaths Ratio','Total Infected', 'Total Deaths', 'Total Recovered', 'Total Active','Total Recovered Log10','Total Infected Log10','Total Deaths Log10','Total Active Log10'], axis=1)\n",
                "for col in codiv_country_short_tmp.loc[:, codiv_country_short_tmp.dtypes == np.number].keys():\n",
                "    sns.jointplot(codiv_country_short['Total Infected Log10'], col, data=codiv_country_short, height=6, s=1, color=\"blue\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Infected Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Infected Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Total Deaths Log10'], col, data=codiv_country_short, height=6, s=1, color=\"red\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Deaths Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Deaths Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Total Active Log10'], col, data=codiv_country_short, height=6, s=1, color=\"green\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Total Active Log10'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Total Active Log10 <-> %s : %.3f\" %(col, corr))\n",
                "    sns.jointplot(codiv_country_short['Deaths Ratio'], col, data=codiv_country_short, height=6, s=1, color=\"orange\")\n",
                "    corr, _ = pearsonr(codiv_country_short['Deaths Ratio'], codiv_country_short[col])\n",
                "    print(\"Pearsons correlation Deaths Ratio <-> %s : %.3f\" %(col, corr))\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.polyfit(codiv_country_short[\"Temp_mean_jan_apr\"], codiv_country_short[\"Total Infected Log10\"], deg=8)",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\",
                ".map(plt.scatter, \"Temp_mean_jan_apr\", \"Total Infected Log10\") \\",
                ".add_legend()",
                "for indexq in range(-80,300,1):",
                "plt.plot(indexqpath,np.polyval(ASSIGN, indexqpath), '.', color='black')",
                "plt.vlines(7,7,14,colors=\"red\",linestyles='dashed')",
                "plt.vlines(24,7,14,colors=\"red\",linestyles='dashed')",
                "plt.grid(True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "coefs_poly3 = np.polyfit(codiv_country_short[\"Temp_mean_jan_apr\"], codiv_country_short[\"Total Infected Log10\"], deg=8) \n",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Temp_mean_jan_apr\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "for indexq in range(-80,300,1):\n",
                "    plt.plot(indexq/10,np.polyval(coefs_poly3, indexq/10), '.', color='black')\n",
                "plt.vlines(7,7,14,colors=\"red\",linestyles='dashed')\n",
                "plt.vlines(24,7,14,colors=\"red\",linestyles='dashed')\n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = np.polyfit(codiv_country_short[\"Health expenditure Ratio\"], codiv_country_short[\"Total Infected Log10\"], 1)",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\",
                ".map(plt.scatter, \"Health expenditure Ratio\", \"Total Infected Log10\") \\",
                ".add_legend()",
                "plt.plot(codiv_country_short[\"Health expenditure Ratio\"], m*codiv_country_short[\"Health expenditure Ratio\"]+b, '--k')",
                "plt.grid(True)"
            ],
            "output_type": "display_data",
            "content_old": [
                "m,b = np.polyfit(codiv_country_short[\"Health expenditure Ratio\"], codiv_country_short[\"Total Infected Log10\"], 1) \n",
                "\n",
                "sns.FacetGrid(codiv_country_short, hue=\"Country\", size=6.5) \\\n",
                "   .map(plt.scatter, \"Health expenditure Ratio\", \"Total Infected Log10\") \\\n",
                "   .add_legend()\n",
                "plt.plot(codiv_country_short[\"Health expenditure Ratio\"], m*codiv_country_short[\"Health expenditure Ratio\"]+b, '--k') \n",
                "plt.grid(True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = codiv_country.copy()",
                "ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int)",
                "ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int)",
                "ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int)",
                "ASSIGN=ASSIGN.drop([",
                "'Quarantine','Schools','Restrictions',",
                "'Country',",
                "'Total Deaths','Total Infected','Total Active','Total Recovered',",
                "\"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Deaths Ratio\"",
                "], axis=1)",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Working on a copy\n",
                "codiv_country_analyze = codiv_country.copy()\n",
                "\n",
                "# Creating categorical variables\n",
                "codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\n",
                "codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\n",
                "codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n",
                "\n",
                "# \n",
                "codiv_country_analyze=codiv_country_analyze.drop([\n",
                "    'Quarantine','Schools','Restrictions', # now categorical\n",
                "    'Country', # not helpful\n",
                "    \n",
                "    # Only keep \"Total Infected Log10\"\n",
                "    'Total Deaths','Total Infected','Total Active','Total Recovered', \n",
                "    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Deaths Ratio\"\n",
                "], axis=1)\n",
                "\n",
                "codiv_country_analyze.sample(10)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_analyze[codiv_country_analyze.isnull().any(axis=1)]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN[ASSIGN.notnull().all(axis=1)]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country_analyze = codiv_country_analyze[codiv_country_analyze.notnull().all(axis=1)]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['method','Cross-validation']",
                "ASSIGN=range(8)",
                "ASSIGN = pd.DataFrame(index=index, columns=columns)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "columns = ['method','Cross-validation']\n",
                "index=range(8)\n",
                "df_resultat = pd.DataFrame(index=index, columns=columns)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True)",
                "print(bin_edges_balanced)"
            ],
            "output_type": "stream",
            "content_old": [
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6,labels=False, retbins=True)\n",
                "print(bin_edges_balanced)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#explanation of these values\n",
                "pd.qcut(codiv_country_analyze['Total Infected Log10'], q=6)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\n",
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]",
                "Level1_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level2_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level3_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level4_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level5_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level6_balanced='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "print(,Level1_balanced)",
                "print(,Level2_balanced)",
                "print(,Level3_balanced)",
                "print(,Level4_balanced)",
                "print(,Level5_balanced)",
                "print(,Level6_balanced)"
            ],
            "output_type": "stream",
            "content_old": [
                "ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\n",
                "ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\n",
                "ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\n",
                "ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\n",
                "ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\n",
                "ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\n",
                "ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\n",
                "Level1_balanced='%.3f<Infected<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\n",
                "Level2_balanced='%.3f<Infected<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\n",
                "Level3_balanced='%.3f<Infected<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\n",
                "Level4_balanced='%.3f<Infected<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\n",
                "Level5_balanced='%.3f<Infected<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\n",
                "Level6_balanced='%.3f<Infected<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\n",
                "print(\"Level1_balanced= \",Level1_balanced)\n",
                "print(\"Level2_balanced= \",Level2_balanced)\n",
                "print(\"Level3_balanced= \",Level3_balanced)\n",
                "print(\"Level4_balanced= \",Level4_balanced)\n",
                "print(\"Level5_balanced= \",Level5_balanced)\n",
                "print(\"Level6_balanced= \",Level6_balanced)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "list(codiv_country_analyze['category_balanced'].value_counts())"
            ],
            "output_type": "execute_result",
            "content_old": [
                "list(codiv_country_analyze['category_balanced'].value_counts())"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=codiv_country_analyze['Total Infected Log10'].values.max()",
                "ASSIGN=ASSIGN+ASSIGN*0.001",
                "print(,ASSIGN)",
                "ASSIGN=(codiv_country_analyze['Total Infected Log10'].values.min())",
                "ASSIGN=ASSIGN-ASSIGN*0.001",
                "print(,ASSIGN)",
                "ASSIGN=(ymax-ymin)path",
                "ASSIGN = ymin",
                "ASSIGN = ymin+yinterval*1",
                "ASSIGN = ymin+yinterval*2",
                "ASSIGN = ymin+yinterval*3",
                "ASSIGN = ymin+yinterval*4",
                "ASSIGN = ymin+yinterval*5",
                "ASSIGN = ymin+yinterval*6",
                "Level1_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level2_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level3_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level4_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level5_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "Level6_equidistant='%.3f<Infected<%.3f'%(ASSIGN,ASSIGN,)",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values,",
                "ASSIGN=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])",
                "print(,Level1_equidistant)",
                "print(,Level2_equidistant)",
                "print(,Level3_equidistant)",
                "print(,Level4_equidistant)",
                "print(,Level5_equidistant)",
                "print(,Level6_equidistant)"
            ],
            "output_type": "stream",
            "content_old": [
                "ymax=codiv_country_analyze['Total Infected Log10'].values.max()\n",
                "ymax=ymax+ymax*0.001\n",
                "print(\"ymax = \",ymax)\n",
                "ymin=(codiv_country_analyze['Total Infected Log10'].values.min())\n",
                "ymin=ymin-ymin*0.001\n",
                "print(\"ymin = \",ymin)\n",
                "yinterval=(ymax-ymin)/6\n",
                "# \n",
                "ylevel0_equidistant = ymin\n",
                "ylevel1_equidistant = ymin+yinterval*1\n",
                "ylevel2_equidistant = ymin+yinterval*2\n",
                "ylevel3_equidistant = ymin+yinterval*3\n",
                "ylevel4_equidistant = ymin+yinterval*4\n",
                "ylevel5_equidistant = ymin+yinterval*5\n",
                "ylevel6_equidistant = ymin+yinterval*6\n",
                "\n",
                "#\n",
                "Level1_equidistant='%.3f<Infected<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\n",
                "Level2_equidistant='%.3f<Infected<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\n",
                "Level3_equidistant='%.3f<Infected<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\n",
                "Level4_equidistant='%.3f<Infected<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\n",
                "Level5_equidistant='%.3f<Infected<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\n",
                "Level6_equidistant='%.3f<Infected<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\n",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Total Infected Log10'].values,\n",
                "                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\n",
                "print(\"Level1_equidistant= \",Level1_equidistant)\n",
                "print(\"Level2_equidistant= \",Level2_equidistant)\n",
                "print(\"Level3_equidistant= \",Level3_equidistant)\n",
                "print(\"Level4_equidistant= \",Level4_equidistant)\n",
                "print(\"Level5_equidistant= \",Level5_equidistant)\n",
                "print(\"Level6_equidistant= \",Level6_equidistant)"
            ]
        },
        {
            "tags": [
                "process_data",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = codiv_country_analyze['category_equidistant']",
                "ASSIGN = codiv_country_analyze['category_balanced']",
                "ASSIGN = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1)",
                "sns.countplot(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_equidistant = codiv_country_analyze['category_equidistant']\n",
                "y_balanced = codiv_country_analyze['category_balanced']\n",
                "X = codiv_country_analyze.drop(['Total Infected Log10','category_balanced','category_equidistant'], axis=1) \n",
                "sns.countplot(y_balanced)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_equidistant)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_equidistant)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "X.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.sample(10)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\n",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\n",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_equid)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_te_equid)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_balan)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_te_balan)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = KFold(n_splits=5, shuffle=False, random_state=None)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import KFold\n",
                "cv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X, y_equidistant)",
                "ASSIGN.predict(X)",
                "print(, ASSIGN.score(X, y_equidistant))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.dummy import DummyClassifier\n",
                "from sklearn.model_selection import cross_validate\n",
                "from sklearn.model_selection import cross_val_score\n",
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_equidistant)\n",
                "dummy_clf.predict(X)\n",
                "print(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"",
                "ASSIGN = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(, np.mean(ASSIGN['test_score']) )"
            ],
            "output_type": "stream",
            "content_old": [
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN= DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X_tr_equid, y_tr_equid)",
                "ASSIGN=dummy_clf.predict(X_te_equid)",
                "ASSIGN = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "dummy_clf= DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_equid, y_tr_equid)\n",
                "y_pred_equid=dummy_clf.predict(X_te_equid)\n",
                "matrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\n",
                "print(matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"",
                "ASSIGN = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(, np.mean(ASSIGN['test_score']) )"
            ],
            "output_type": "stream",
            "content_old": [
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X_tr_balan, y_tr_balan)",
                "ASSIGN=dummy_clf.predict(X_te_balan)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_balan, y_tr_balan)\n",
                "y_pred_balan=dummy_clf.predict(X_te_balan)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\n",
                "print(matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.tree import DecisionTreeClassifier"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(1,15)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for depth in ASSIGN:",
                "DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth)",
                "DecisionTree.fit(X, y_equidistant)",
                "ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(DecisionTree.score(X, y_equidistant))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"decision tree accurency\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Tree depth', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(depths)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "DecisionTree = DecisionTreeClassifier(",
                "ASSIGN='gini', random_state=0, max_depth=OptDepth)",
                "DecisionTree.fit(X, y_equidistant)",
                "ASSIGN = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(,DecisionTree.score(X, y_equidistant))",
                "print(,np.mean(ASSIGN['test_score']))"
            ],
            "output_type": "stream",
            "content_old": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_equidistant)\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\n",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "                                                              \n",
                "print(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\n",
                "print(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "os.environ[\"PATH\"] += os.pathsep + 'D:path(x86)path'",
                "ASSIGN = export_graphviz(",
                "DecisionTree, out_file=None,",
                "ASSIGN=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],",
                "ASSIGN=True, rounded=True, proportion=True",
                ")",
                "graphviz.Source(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(1,15)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for depth in ASSIGN:",
                "DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth)",
                "DecisionTree.fit(X, y_balanced)",
                "ASSIGN = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(DecisionTree.score(X, y_balanced))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"decision tree accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Tree depth', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(depths)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "DecisionTree = DecisionTreeClassifier(",
                "ASSIGN='gini', random_state=0, max_depth=OptDepth)",
                "DecisionTree.fit(X, y_balanced)",
                "ASSIGN = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(,DecisionTree.score(X, y_balanced))",
                "print(,np.mean(ASSIGN['test_score']))"
            ],
            "output_type": "stream",
            "content_old": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_balanced)\n",
                "# Get score\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\n",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "print(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\n",
                "print(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "os.environ[\"PATH\"] += os.pathsep + 'D:path(x86)path'",
                "ASSIGN = export_graphviz(",
                "DecisionTree, out_file=None,",
                "ASSIGN=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],",
                "ASSIGN=True, rounded=True, proportion=True",
                ")",
                "graphviz.Source(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN=range(100,2000,100)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for Estimator in ASSIGN:",
                "ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0)",
                "ASSIGN.fit(X, y_equidistant)",
                "ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.ensemble import RandomForestClassifier\n",
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"Forest accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Forest Estimator', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(Estimators)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X, y_equidistant)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Build a forest and compute the feature importances\n",
                "forest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest.fit(X, y_equidistant)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = cross_validate(forest, X, y_equidistant, cv=cv_strategy)",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Mean test score of a 250x decision tree \n",
                "Forest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\n",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\n",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\n",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X_tr_equid, y_tr_equid)",
                "confusion_matrix(y_te_equid, ASSIGN.predict(X_te_equid))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = forest.feature_importances_",
                "ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_],axis=0)",
                "ASSIGN = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "for f in range(X.shape[1]):",
                "print( % (f + 1, X.columns[indices[f]], importances[indices[f]]))",
                "plt.figure(figsize=(20,10))",
                "plt.title(\"Feature importances\")",
                "plt.bar(range(X.shape[1]), importances[indices],",
                "ASSIGN=\"r\", yerr=std[indices], align=\"center\")",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)",
                "plt.xlim([-1, X.shape[1]])",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by equidistand interval:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(100,2000,100)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for Estimator in ASSIGN:",
                "ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0)",
                "ASSIGN.fit(X, y_balanced)",
                "ASSIGN = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(ASSIGN.fit(X, y_balanced).score(X, y_balanced))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"Forest accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Forest Estimator', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(Estimators)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = cross_validate(forest, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import cross_validate\n",
                "Forest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\n",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\n",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X_tr_balan, y_tr_balan)",
                "confusion_matrix(y_te_balan, ASSIGN.predict(X_te_balan))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = forest.feature_importances_",
                "ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_],",
                "ASSIGN=0)",
                "ASSIGN = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "for f in range(X.shape[1]):",
                "print( % (f + 1, X.columns[indices[f]], importances[indices[f]]))",
                "plt.figure(figsize=(20,10))",
                "plt.title(\"Feature importances\")",
                "plt.bar(range(X.shape[1]), importances[indices],",
                "ASSIGN=\"r\", yerr=std[indices], align=\"center\")",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)",
                "plt.xlim([-1, X.shape[1]])",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by balanced bins:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = PCA(n_components=2)",
                "ASSIGN.fit(X, y=None);",
                "ASSIGN= pca.transform(X)",
                "ASSIGN=pd.DataFrame(feature_2)",
                "ASSIGN=feature_2_df.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.decomposition import PCA\n",
                "pca = PCA(n_components=2)\n",
                "# Apply PCA\n",
                "pca.fit(X, y=None); # Unsupervised learning, no y variable\n",
                "feature_2= pca.transform(X)\n",
                "feature_2_df=pd.DataFrame(feature_2)\n",
                "len_cluster, col=feature_2_df.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\n",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\n",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "ASSIGN = Pipeline([",
                "('scaler', None),",
                "('knn', KNeighborsClassifier(",
                "ASSIGN=-1",
                "))",
                "])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "# Create k-NN classifier\n",
                "pipe = Pipeline([\n",
                "    #('scaler', StandardScaler()), # With standardization\n",
                "    ('scaler', None), # Better performance without standardization!\n",
                "    ('knn', KNeighborsClassifier(\n",
                "        n_jobs=-1 # As many parallel jobs as possible\n",
                "    ))\n",
                "])\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = list(range(1,35))",
                "ASSIGN = []",
                "for n_neighborss in ASSIGN:",
                "ASSIGN='distance'",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN)) ])",
                "ASSIGN.fit(feature_2, y_equidistant)",
                "ASSIGN = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )",
                "ASSIGN.append(ASSIGN.mean())",
                "Missclassification_Error = [1-x for x in ASSIGN]",
                "print(,format(Missclassification_Error.index(min(Missclassification_Error))))",
                "ASSIGN=Missclassification_Error.index(min(Missclassification_Error))"
            ],
            "output_type": "stream",
            "content_old": [
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn import neighbors, datasets\n",
                "from sklearn import metrics\n",
                "from sklearn.model_selection import cross_val_score\n",
                "neighbors = list(range(1,35))\n",
                "CV_scores = []\n",
                "\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights)) ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_equidistant)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "plt.plot(Missclassification_Error)",
                "plt.xlabel(\"number of neighbors K\")",
                "plt.ylabel(\"Missclassification Error\")",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot(Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = ListedColormap(['",
                "ASSIGN = ListedColormap(['",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN='distance'",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal))",
                "])",
                "ASSIGN.fit(feature_2, y_equidistant)",
                "feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))",
                "ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])",
                "ASSIGN = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from matplotlib.colors import ListedColormap\n",
                "from sklearn import neighbors, datasets\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal))\n",
                "    #, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_equidistant)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(18, 10))",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)",
                "Label1=pd.DataFrame([1,2,3,4,5])",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)",
                "ASSIGN = plt.colorbar()",
                "ASSIGN.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])",
                "plt.xlim(xx.min(), xx.max())",
                "plt.ylim(yy.min(), yy.max())",
                "plt.xlabel(\"feature 1\")",
                "plt.ylabel(\"feature 2\")",
                "plt.title(\"6-Class classification,  equidistant bins - total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\n",
                "#color bar with label\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import cross_validate\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\n",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))",
                "])",
                "ASSIGN=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)",
                "ASSIGN = knn_pipe.predict(X_fea_2_te_equid)",
                "confusion_matrix(y_fea_2_te_equid, ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_equid)\n",
                "confusion_matrix(y_fea_2_te_equid, knn_predict)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.metrics import classification_report\n",
                "print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = list(range(1,35))",
                "ASSIGN = []",
                "for n_neighborss in ASSIGN:",
                "ASSIGN='distance'",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_balanced)",
                "ASSIGN = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )",
                "ASSIGN.append(ASSIGN.mean())",
                "Missclassification_Error = [1-x for x in ASSIGN]",
                "print(,format(Missclassification_Error.index(min(Missclassification_Error))))",
                "ASSIGN=Missclassification_Error.index(min(Missclassification_Error))",
                "plt.plot( Missclassification_Error)",
                "plt.xlabel(\"number of ASSIGN K\")",
                "plt.ylabel(\"Missclassification Error\")",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "neighbors = list(range(1,35))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_balanced)\n",
                "    #\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = ListedColormap(['",
                "ASSIGN = ListedColormap(['",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN='distance'",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_balanced)",
                "feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))",
                "ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])",
                "ASSIGN = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "#\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "#\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_balanced)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(18, 10))",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)",
                "Label1=pd.DataFrame([1,2,3,4,5])",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)",
                "ASSIGN = plt.colorbar()",
                "ASSIGN.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])",
                "plt.xlim(xx.min(), xx.max())",
                "plt.ylim(yy.min(), yy.max())",
                "plt.xlabel(\"feature 1\")",
                "plt.ylabel(\"feature 2\")",
                "plt.title(\"6-Class classification,  balanced bins - total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))",
                "print('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import cross_validate\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\n",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\n",
                "print('knn by equidistant bins of element - mean test_score {:.3f}'.format(np.mean(knn_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))",
                "])",
                "ASSIGN=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)",
                "ASSIGN = knn_pipe.predict(X_fea_2_te_balan)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_balan)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_resultat"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = codiv_country.copy()",
                "ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int)",
                "ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int)",
                "ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int)",
                "ASSIGN=ASSIGN.drop([",
                "'Quarantine','Schools','Restrictions',",
                "'Country',",
                "'Total Deaths','Total Infected','Total Active','Total Recovered',",
                "\"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Total Infected Log10\"",
                "], axis=1)",
                "ASSIGN.sample(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Working on a copy\n",
                "codiv_country_analyze = codiv_country.copy()\n",
                "\n",
                "# Creating categorical variables\n",
                "codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)\n",
                "codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)\n",
                "codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)\n",
                "\n",
                "# \n",
                "codiv_country_analyze=codiv_country_analyze.drop([\n",
                "    'Quarantine','Schools','Restrictions', # now categorical\n",
                "    'Country', # not helpful\n",
                "    'Total Deaths','Total Infected','Total Active','Total Recovered', \n",
                "    \"Total Deaths Log10\",\"Total Recovered Log10\",\"Total Active Log10\",\"Total Infected Log10\"\n",
                "], axis=1)\n",
                "\n",
                "codiv_country_analyze.sample(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ['method','Cross-validation']",
                "ASSIGN=range(8)",
                "ASSIGN = pd.DataFrame(index=index, columns=columns)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "columns = ['method','Cross-validation']\n",
                "index=range(8)\n",
                "df_resultat = pd.DataFrame(index=index, columns=columns)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "codiv_country_analyze['Deaths Ratio'].describe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_country_analyze['Deaths Ratio'].describe()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True)",
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country_analyze['category_balanced'], bin_edges_balanced=pd.qcut(codiv_country_analyze['Deaths Ratio'], q=6,labels=False, retbins=True)\n",
                "#balanced bins deliver category for 0-5, euqidistant from 1-6, so i will add 1 to the category numbers\n",
                "codiv_country_analyze['category_balanced'] = codiv_country_analyze['category_balanced'] +1"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]",
                "ASSIGN=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]",
                "Level1_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level2_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level3_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level4_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level5_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level6_balanced='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "print(,Level1_balanced)",
                "print(,Level2_balanced)",
                "print(,Level3_balanced)",
                "print(,Level4_balanced)",
                "print(,Level5_balanced)",
                "print(,Level6_balanced)"
            ],
            "output_type": "stream",
            "content_old": [
                "ylevel0_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][0]\n",
                "ylevel1_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][1]\n",
                "ylevel2_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][2]\n",
                "ylevel3_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][3]\n",
                "ylevel4_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][4]\n",
                "ylevel5_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][5]\n",
                "ylevel6_balanced=pd.DataFrame(zip(bin_edges_balanced),columns=['Threshold'])['Threshold'][6]\n",
                "Level1_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel0_balanced,ylevel1_balanced,)\n",
                "Level2_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel1_balanced,ylevel2_balanced,)\n",
                "Level3_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel2_balanced,ylevel3_balanced,)\n",
                "Level4_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel3_balanced,ylevel4_balanced,)\n",
                "Level5_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel4_balanced,ylevel5_balanced,)\n",
                "Level6_balanced='%.3f<Deaths Ratio<%.3f'%(ylevel5_balanced,ylevel6_balanced,)\n",
                "print(\"Level1_balanced= \",Level1_balanced)\n",
                "print(\"Level2_balanced= \",Level2_balanced)\n",
                "print(\"Level3_balanced= \",Level3_balanced)\n",
                "print(\"Level4_balanced= \",Level4_balanced)\n",
                "print(\"Level5_balanced= \",Level5_balanced)\n",
                "print(\"Level6_balanced= \",Level6_balanced)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=codiv_country_analyze['Deaths Ratio'].values.max()",
                "print(ASSIGN)",
                "ASSIGN=ASSIGN+ASSIGN*0.001",
                "print(,ASSIGN)",
                "ASSIGN=(codiv_country_analyze['Deaths Ratio'].values.min())",
                "ASSIGN=ASSIGN-ASSIGN*0.001",
                "print(,ASSIGN)",
                "ASSIGN=(ymax-ymin)path",
                "print(ASSIGN,ASSIGN)",
                "ASSIGN = ymin",
                "ASSIGN = ymin+yinterval*1",
                "ASSIGN = ymin+yinterval*2",
                "ASSIGN = ymin+yinterval*3",
                "ASSIGN = ymin+yinterval*4",
                "ASSIGN = ymin+yinterval*5",
                "ASSIGN = ymin+yinterval*6",
                "Level1_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level2_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level3_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level4_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level5_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "Level6_equidistant='%.3f<Deaths Ratio<%.3f'%(ASSIGN,ASSIGN,)",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values,",
                "ASSIGN=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])",
                "print(,Level1_equidistant)",
                "print(,Level2_equidistant)",
                "print(,Level3_equidistant)",
                "print(,Level4_equidistant)",
                "print(,Level5_equidistant)",
                "print(,Level6_equidistant)"
            ],
            "output_type": "stream",
            "content_old": [
                "ymax=codiv_country_analyze['Deaths Ratio'].values.max()\n",
                "print(ymax)\n",
                "ymax=ymax+ymax*0.001\n",
                "print(\"ymax = \",ymax)\n",
                "ymin=(codiv_country_analyze['Deaths Ratio'].values.min())\n",
                "ymin=ymin-ymin*0.001\n",
                "print(\"ymin = \",ymin)\n",
                "yinterval=(ymax-ymin)/6\n",
                "print(ymin,ymax)\n",
                "# \n",
                "ylevel0_equidistant = ymin\n",
                "ylevel1_equidistant = ymin+yinterval*1\n",
                "ylevel2_equidistant = ymin+yinterval*2\n",
                "ylevel3_equidistant = ymin+yinterval*3\n",
                "ylevel4_equidistant = ymin+yinterval*4\n",
                "ylevel5_equidistant = ymin+yinterval*5\n",
                "ylevel6_equidistant = ymin+yinterval*6\n",
                "\n",
                "#\n",
                "Level1_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel0_equidistant,ylevel1_equidistant,)\n",
                "Level2_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel1_equidistant,ylevel2_equidistant,)\n",
                "Level3_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel2_equidistant,ylevel3_equidistant,)\n",
                "Level4_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel3_equidistant,ylevel4_equidistant,)\n",
                "Level5_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel4_equidistant,ylevel5_equidistant,)\n",
                "Level6_equidistant='%.3f<Deaths Ratio<%.3f'%(ylevel5_equidistant,ylevel6_equidistant,)\n",
                "codiv_country_analyze['category_equidistant']=np.digitize(codiv_country_analyze['Deaths Ratio'].values,\n",
                "                          bins=[ylevel0_equidistant,ylevel1_equidistant,ylevel2_equidistant,ylevel3_equidistant,ylevel4_equidistant,ylevel5_equidistant,ylevel6_equidistant])\n",
                "print(\"Level1_equidistant= \",Level1_equidistant)\n",
                "print(\"Level2_equidistant= \",Level2_equidistant)\n",
                "print(\"Level3_equidistant= \",Level3_equidistant)\n",
                "print(\"Level4_equidistant= \",Level4_equidistant)\n",
                "print(\"Level5_equidistant= \",Level5_equidistant)\n",
                "print(\"Level6_equidistant= \",Level6_equidistant)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = codiv_country_analyze['category_equidistant']",
                "ASSIGN = codiv_country_analyze['category_balanced']",
                "ASSIGN = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1)",
                "sns.countplot(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_equidistant = codiv_country_analyze['category_equidistant']\n",
                "y_balanced = codiv_country_analyze['category_balanced']\n",
                "X = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) \n",
                "sns.countplot(y_balanced)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_equidistant)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_equidistant)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_tr_equid, X_te_equid, y_tr_equid, y_te_equid = train_test_split(X, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_tr_equid.shape, y_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_te_equid.shape, y_te_equid.shape)\n",
                "X_tr_balan, X_te_balan, y_tr_balan, y_te_balan = train_test_split(X, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_tr_balan.shape, y_tr_balan.shape)\n",
                "print('Testn set balanced:', X_te_balan.shape, y_te_balan.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_equid)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_te_equid)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(y_te_balan)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(y_te_balan)"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X, y_equidistant)",
                "ASSIGN.predict(X)",
                "print(, ASSIGN.score(X, y_equidistant))"
            ],
            "output_type": "stream",
            "content_old": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_equidistant)\n",
                "dummy_clf.predict(X)\n",
                "print(\"scores baseline mostfrequent classifier equidistant = \", dummy_clf.score(X, y_equidistant))"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"",
                "ASSIGN = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(, np.mean(ASSIGN['test_score']) )"
            ],
            "output_type": "stream",
            "content_old": [
                "df_resultat.at[df_resultat.index==0,\"method\"]=\"baseline_equidistant\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==0,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier equidistant = \", np.mean(Baseline_training_scores['test_score']) )"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN= DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X_tr_equid, y_tr_equid)",
                "ASSIGN=dummy_clf.predict(X_te_equid)",
                "ASSIGN = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "dummy_clf= DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_equid, y_tr_equid)\n",
                "y_pred_equid=dummy_clf.predict(X_te_equid)\n",
                "matrix = confusion_matrix(y_true=y_te_equid, y_pred=y_pred_equid)\n",
                "print(matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_equid, y_te_equid, cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X, y_balanced)",
                "ASSIGN.predict(X)",
                "print(, ASSIGN.score(X, y_balanced))"
            ],
            "output_type": "stream",
            "content_old": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X, y_balanced)\n",
                "dummy_clf.predict(X)\n",
                "print(\"training_scores baseline mostfrequent classifier balanced = \", dummy_clf.score(X, y_balanced))"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"",
                "ASSIGN = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(, np.mean(ASSIGN['test_score']) )"
            ],
            "output_type": "stream",
            "content_old": [
                "df_resultat.at[df_resultat.index==1,\"method\"]=\"baseline balanced bins\"\n",
                "Baseline_training_scores = cross_validate(dummy_clf, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==1,\"Cross-validation\"]=np.mean(Baseline_training_scores['test_score'])\n",
                "print(\"crossvalidation baseline mostfrequent classifier balanced = \", np.mean(Baseline_training_scores['test_score']) )"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = DummyClassifier(strategy=\"most_frequent\")",
                "ASSIGN.fit(X_tr_balan, y_tr_balan)",
                "ASSIGN=dummy_clf.predict(X_te_balan)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
                "dummy_clf.fit(X_tr_balan, y_tr_balan)\n",
                "y_pred_balan=dummy_clf.predict(X_te_balan)"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)\n",
                "print(matrix)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(dummy_clf, X_te_balan, y_te_balan, cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(1,15)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for depth in ASSIGN:",
                "DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth)",
                "DecisionTree.fit(X, y_equidistant)",
                "ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(DecisionTree.score(X, y_equidistant))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f std %f score %f\"%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f std %f score %f\"%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by depth = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"decision tree accurency\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Tree depth', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(depths)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "DecisionTree = DecisionTreeClassifier(",
                "ASSIGN='gini', random_state=0, max_depth=OptDepth)",
                "DecisionTree.fit(X, y_equidistant)",
                "ASSIGN = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(,DecisionTree.score(X, y_equidistant))",
                "print(,np.mean(ASSIGN['test_score']))"
            ],
            "output_type": "stream",
            "content_old": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_equidistant)\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==2,\"method\"]=\"decision-tree equidistant\"\n",
                "df_resultat.at[df_resultat.index==2,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "                                                              \n",
                "print(\"score DecistionTree equidistant =\",DecisionTree.score(X, y_equidistant))\n",
                "print(\"cross validation DecisionTree equidistant =\",np.mean(Tree_scores['test_score']))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, DecisionTree_confusion.predict(X_te_equid))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_equid, y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "transfer_results"
            ],
            "content": [
                "SETUP",
                "os.environ[\"PATH\"] += os.pathsep + 'D:path(x86)path'",
                "ASSIGN = export_graphviz(",
                "DecisionTree, out_file=None,",
                "ASSIGN=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],",
                "ASSIGN=True, rounded=True, proportion=True",
                ")",
                "graphviz.Source(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_equidistant,Level2_equidistant, Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(1,15)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for depth in ASSIGN:",
                "DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth)",
                "DecisionTree.fit(X, y_balanced)",
                "ASSIGN = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(DecisionTree.score(X, y_balanced))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "depths=range(1,15)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for depth in depths:\n",
                "    DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)\n",
                "    DecisionTree.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(DecisionTree.score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by depth_balanced = \",np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)\n",
                "OptDepth=np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"decision tree accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Tree depth', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(depths)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(depths, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(depths, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(depths, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"decision tree accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Tree depth', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(depths)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "DecisionTree = DecisionTreeClassifier(",
                "ASSIGN='gini', random_state=0, max_depth=OptDepth)",
                "DecisionTree.fit(X, y_balanced)",
                "ASSIGN = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print(,DecisionTree.score(X, y_balanced))",
                "print(,np.mean(ASSIGN['test_score']))"
            ],
            "output_type": "stream",
            "content_old": [
                "DecisionTree = DecisionTreeClassifier(\n",
                "    criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "# Fit decision tree\n",
                "DecisionTree.fit(X, y_balanced)\n",
                "# Get score\n",
                "Tree_scores = cross_validate(DecisionTree, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==3,\"method\"]=\"decision-tree _balanced bins\"\n",
                "df_resultat.at[df_resultat.index==3,\"Cross-validation\"]=np.mean(Tree_scores['test_score'])\n",
                "print(\"score DecisionTree _balanced bins =\",DecisionTree.score(X, y_balanced))\n",
                "print(\"cross validation DecisionTree balanced bins=\",np.mean(Tree_scores['test_score']))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import confusion_matrix\n",
                "DecisionTree_confusion = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=OptDepth)\n",
                "DecisionTree_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, DecisionTree_confusion.predict(X_te_balan))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(DecisionTree_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "os.environ[\"PATH\"] += os.pathsep + 'D:path(x86)path'",
                "ASSIGN = export_graphviz(",
                "DecisionTree, out_file=None,",
                "ASSIGN=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],",
                "ASSIGN=True, rounded=True, proportion=True",
                ")",
                "graphviz.Source(ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.tree import export_graphviz\n",
                "import os\n",
                "os.environ[\"PATH\"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'\n",
                "# Export decision tree\n",
                "dot_data = export_graphviz(\n",
                "    DecisionTree, out_file=None,\n",
                "    feature_names=X.columns, class_names=[Level1_balanced,Level2_balanced, Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced],\n",
                "    filled=True, rounded=True, proportion=True   \n",
                ")\n",
                "import graphviz\n",
                "\n",
                "# Display decision tree\n",
                "graphviz.Source(dot_data)"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(100,2000,100)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for Estimator in ASSIGN:",
                "ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0)",
                "ASSIGN.fit(X, y_equidistant)",
                "ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_equidistant)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"Forest accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Forest Estimator', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(Estimators)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X, y_equidistant)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Build a forest and compute the feature importances\n",
                "forest = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest.fit(X, y_equidistant)"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = cross_validate(forest, X, y_equidistant, cv=cv_strategy)",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "# Mean test score of a 250x decision tree \n",
                "Forest_scores = cross_validate(forest, X, y_equidistant, cv=cv_strategy)\n",
                "print('Forest by equidistant interval score {:.3f}'.format(forest.score(X, y_equidistant)))\n",
                "df_resultat.at[df_resultat.index==4,\"method\"]=\"Forest equidistant\"\n",
                "df_resultat.at[df_resultat.index==4,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by equidistant interval cross validation {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X_tr_equid, y_tr_equid)",
                "confusion_matrix(y_te_equid, ASSIGN.predict(X_te_equid))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_equid, y_tr_equid)\n",
                "confusion_matrix(y_te_equid, forest_confusion.predict(X_te_equid))"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(forest_confusion, X_te_equid,y_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = forest.feature_importances_",
                "ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_],",
                "ASSIGN=0)",
                "ASSIGN = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "for f in range(X.shape[1]):",
                "print( % (f + 1, X.columns[indices[f]], importances[indices[f]]))",
                "plt.figure(figsize=(20,10))",
                "plt.title(\"Feature importances\")",
                "plt.bar(range(X.shape[1]), importances[indices],",
                "ASSIGN=\"r\", yerr=std[indices], align=\"center\")",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)",
                "plt.xlim([-1, X.shape[1]])",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by equidistand interval:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=range(100,2000,100)",
                "ASSIGN='accuracy'",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for Estimator in ASSIGN:",
                "ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0)",
                "ASSIGN.fit(X, y_balanced)",
                "ASSIGN = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN.mean())",
                "ASSIGN.append(ASSIGN.std())",
                "ASSIGN.append(ASSIGN.fit(X, y_balanced).score(X, y_balanced))",
                "ASSIGN=\"depth %d, cv_val_scores_mean %f score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))",
                "print(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = np.array(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Estimators=range(100,2000,100)\n",
                "scoring='accuracy'\n",
                "cv_val_scores_list = []\n",
                "cv_val_scores_std = []\n",
                "cv_val_scores_mean = []\n",
                "accuracy_scores = []\n",
                "for Estimator in Estimators:\n",
                "    forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)\n",
                "    forest.fit(X, y_balanced)\n",
                "    cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)\n",
                "    cv_val_scores_list.append(cv_val_scores)\n",
                "    cv_val_scores_mean.append(cv_val_scores.mean())\n",
                "    cv_val_scores_std.append(cv_val_scores.std())\n",
                "    accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))\n",
                "    Text=\"depth %d, cv_val_scores_mean %f  score %f\"%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))\n",
                "    print(Text)\n",
                "cv_val_scores_mean = np.array(cv_val_scores_mean)\n",
                "cv_val_scores_std = np.array(cv_val_scores_std)\n",
                "accuracy_scores = np.array(accuracy_scores)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(,(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ],
            "output_type": "stream",
            "content_old": [
                "print(\"The maximum is by Estimator_balanced = \",(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100)\n",
                "OptEstimator=(np.where(cv_val_scores_mean==cv_val_scores_mean.max())[0][0]+1)*100"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(1,1, figsize=(15,5))",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)",
                "ASSIGN = plt.ASSIGN()",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)",
                "ASSIGN=\"Forest accurency - _balanced bins\"",
                "ax.set_title(ASSIGN, fontsize=16)",
                "ax.set_xlabel('Forest Estimator', fontsize=14)",
                "ax.set_ylabel('accuracy', fontsize=14)",
                "ax.set_xticks(Estimators)",
                "ax.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "fig, ax = plt.subplots(1,1, figsize=(15,5))\n",
                "ax.plot(Estimators, cv_val_scores_mean, '-o', label='mean cross-validation accuracy', alpha=0.9)\n",
                "ax.fill_between(Estimators, cv_val_scores_mean-2*cv_val_scores_std, cv_val_scores_mean+2*cv_val_scores_std, alpha=0.2)\n",
                "ylim = plt.ylim()\n",
                "ax.plot(Estimators, accuracy_scores, '-*', label='training_scores', alpha=0.9)\n",
                "title=\"Forest accurency - _balanced bins\"\n",
                "ax.set_title(title, fontsize=16)\n",
                "ax.set_xlabel('Forest Estimator', fontsize=14)\n",
                "ax.set_ylabel('accuracy', fontsize=14)\n",
                "ax.set_xticks(Estimators)\n",
                "ax.legend()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = cross_validate(forest, X, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.model_selection import cross_validate\n",
                "Forest_scores = cross_validate(forest, X, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==5,\"method\"]=\"Forest balanced bins\"\n",
                "df_resultat.at[df_resultat.index==5,\"Cross-validation\"]=np.mean(Forest_scores['test_score'])\n",
                "print('Forest by balanced bins of element score {:.3f}'.format(forest.score(X, y_balanced)))\n",
                "print('Forest by balanced bins of element - mean test {:.3f}'.format(np.mean(Forest_scores['test_score'])))"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)",
                "ASSIGN.fit(X_tr_balan, y_tr_balan)",
                "confusion_matrix(y_te_balan, ASSIGN.predict(X_te_balan))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "forest_confusion = RandomForestClassifier(n_estimators=OptEstimator,random_state=0)\n",
                "forest_confusion.fit(X_tr_balan, y_tr_balan)\n",
                "confusion_matrix(y_te_balan, forest_confusion.predict(X_te_balan))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.metrics import plot_confusion_matrix\n",
                "plot_confusion_matrix(forest_confusion, X_te_balan, y_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = forest.feature_importances_",
                "ASSIGN = np.ASSIGN([tree.feature_importances_ for tree in forest.estimators_],",
                "ASSIGN=0)",
                "ASSIGN = np.argsort(importances)[::-1]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "importances = forest.feature_importances_\n",
                "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
                "             axis=0)\n",
                "indices = np.argsort(importances)[::-1]"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "print()",
                "for f in range(X.shape[1]):",
                "print( % (f + 1, X.columns[indices[f]], importances[indices[f]]))",
                "plt.figure(figsize=(20,10))",
                "plt.title(\"Feature importances\")",
                "plt.bar(range(X.shape[1]), importances[indices],",
                "ASSIGN=\"r\", yerr=std[indices], align=\"center\")",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)",
                "plt.xlim([-1, X.shape[1]])",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Print the feature ranking\n",
                "print(\"Feature ranking by balanced bins:\")\n",
                "\n",
                "for f in range(X.shape[1]):\n",
                "    print(\"%d. feature %s (%f)\" % (f + 1, X.columns[indices[f]], importances[indices[f]]))\n",
                "\n",
                "# Plot the feature importances of the forest\n",
                "plt.figure(figsize=(20,10))\n",
                "plt.title(\"Feature importances\")\n",
                "plt.bar(range(X.shape[1]), importances[indices],\n",
                "       color=\"r\", yerr=std[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation = 65)\n",
                "plt.xlim([-1, X.shape[1]])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = PCA(n_components=2)",
                "ASSIGN.fit(X, y=None);",
                "ASSIGN= pca.transform(X)",
                "ASSIGN=pd.DataFrame(feature_2)",
                "ASSIGN=feature_2_df.shape"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pca = PCA(n_components=2)\n",
                "# Apply PCA\n",
                "pca.fit(X, y=None); # Unsupervised learning, no y variable\n",
                "feature_2= pca.transform(X)\n",
                "feature_2_df=pd.DataFrame(feature_2)\n",
                "len_cluster, col=feature_2_df.shape"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "X_fea_2_tr_equid, X_fea_2_te_equid, y_fea_2_tr_equid, y_fea_2_te_equid = train_test_split(feature_2, y_equidistant, test_size=0.15, random_state=1)\n",
                "print('Train set equidistant:', X_fea_2_tr_equid.shape, y_fea_2_tr_equid.shape)\n",
                "print('Testn set equidistant:', X_fea_2_te_equid.shape, y_fea_2_te_equid.shape)\n",
                "X_fea_2_tr_balan, X_fea_2_te_balan, y_fea_2_tr_balan, y_fea_2_te_balan = train_test_split(feature_2, y_balanced, test_size=0.15, random_state=1)\n",
                "print('Train set balanced:', X_fea_2_tr_balan.shape, y_fea_2_tr_balan.shape)\n",
                "print('Testn set balanced:', X_fea_2_te_balan.shape, y_fea_2_te_balan.shape)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = list(range(1,25))",
                "ASSIGN = []",
                "for n_neighborss in ASSIGN:",
                "ASSIGN='distance'",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_equidistant)",
                "ASSIGN = knn_pipe.predict(feature_2)",
                "ASSIGN = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )",
                "ASSIGN.append(ASSIGN.mean())",
                "Missclassification_Error = [1-x for x in ASSIGN]",
                "print(,format(Missclassification_Error.index(min(Missclassification_Error))))",
                "ASSIGN=Missclassification_Error.index(min(Missclassification_Error))",
                "plt.plot( Missclassification_Error)",
                "plt.xlabel(\"number of ASSIGN K\")",
                "plt.ylabel(\"Missclassification Error\")",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "neighbors = list(range(1,25))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_equidistant)\n",
                "    #\n",
                "    Z = knn_pipe.predict(feature_2)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_equidistant,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = ListedColormap(['",
                "ASSIGN = ListedColormap(['",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN='distance'",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_equidistant)",
                "feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))",
                "ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])",
                "ASSIGN = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_equidistant)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(18, 10))",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)",
                "Label1=pd.DataFrame([1,2,3,4,5])",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)",
                "ASSIGN = plt.colorbar()",
                "ASSIGN.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])",
                "plt.xlim(xx.min(), xx.max())",
                "plt.ylim(yy.min(), yy.max())",
                "plt.xlabel(\"feature 1\")",
                "plt.ylabel(\"feature 2\")",
                "plt.title(\"6-Class classification,  equidistant bins - total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_equidistant, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_equidistant,Level2_equidistant,Level3_equidistant,Level4_equidistant,Level5_equidistant,Level6_equidistant])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   equidistant bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==6,\"method\"]=\"knn equidistant bins\"\n",
                "df_resultat.at[df_resultat.index==6,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))",
                "])",
                "ASSIGN=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)",
                "ASSIGN = knn_pipe.predict(X_fea_2_te_equid)",
                "confusion_matrix(y_fea_2_te_equid, ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_equid, y_fea_2_tr_equid)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_equid)\n",
                "confusion_matrix(y_fea_2_te_equid, knn_predict)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_equid, y_fea_2_te_equid,cmap=plt.cm.Blues,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = list(range(1,25))",
                "ASSIGN = []",
                "for n_neighborss in ASSIGN:",
                "ASSIGN='distance'",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=n_neighborss, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_balanced)",
                "ASSIGN = knn_pipe.predict(feature_2)",
                "ASSIGN = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )",
                "ASSIGN.append(ASSIGN.mean())",
                "Missclassification_Error = [1-x for x in ASSIGN]",
                "print(,format(Missclassification_Error.index(min(Missclassification_Error))))",
                "ASSIGN=Missclassification_Error.index(min(Missclassification_Error))",
                "plt.plot( Missclassification_Error)",
                "plt.xlabel(\"number of ASSIGN K\")",
                "plt.ylabel(\"Missclassification Error\")",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "neighbors = list(range(1,25))\n",
                "CV_scores = []\n",
                "\n",
                "for n_neighborss in neighbors:\n",
                "    weights='distance'\n",
                "    feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "    feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "    h = .02  # step size in the mesh\n",
                "    # we create an instance of Neighbours Classifier and fit the data.\n",
                "    # Create a k-NN pipeline\n",
                "    knn_pipe = Pipeline([\n",
                "        ('scaler', StandardScaler()),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=n_neighborss, weights=weights))\n",
                "    ])\n",
                "    # Fit estimator\n",
                "    knn_pipe.fit(feature_2, y_balanced)\n",
                "    #\n",
                "    Z = knn_pipe.predict(feature_2)\n",
                "    scores = cross_val_score(knn_pipe,feature_2,y_balanced,cv=cv_strategy )\n",
                "    CV_scores.append(scores.mean())\n",
                "    \n",
                "Missclassification_Error = [1-x for x in CV_scores]\n",
                "print(\"the optimal k is \",format(Missclassification_Error.index(min(Missclassification_Error))))\n",
                "k_Optimal=Missclassification_Error.index(min(Missclassification_Error))\n",
                "    \n",
                "from matplotlib.legend_handler import HandlerLine2D\n",
                "plt.plot( Missclassification_Error)\n",
                "plt.xlabel(\"number of neighbors K\")\n",
                "plt.ylabel(\"Missclassification Error\")\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = ListedColormap(['",
                "ASSIGN = ListedColormap(['",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][0]",
                "ASSIGN=pd.DataFrame(feature_2).loc[:][1]",
                "ASSIGN='distance'",
                "ASSIGN = .02",
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN))",
                "])",
                "ASSIGN.fit(feature_2, y_balanced)",
                "feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1",
                "ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))",
                "ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])",
                "ASSIGN = Z.reshape(xx.shape)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Create color maps\n",
                "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])\n",
                "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])\n",
                "\n",
                "feature1=pd.DataFrame(feature_2).loc[:][0]\n",
                "feature2=pd.DataFrame(feature_2).loc[:][1]\n",
                "\n",
                "weights='distance'\n",
                "h = .02  # step size in the mesh\n",
                "# we create an instance of Neighbours Classifier and fit the data.\n",
                "# Create a k-NN pipeline\n",
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_pipe.fit(feature_2, y_balanced)\n",
                "# Plot the decision boundary. For that, we will assign a color to each\n",
                "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
                "feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1\n",
                "feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1\n",
                "xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))\n",
                "Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "Zo = Z.reshape(xx.shape)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(18, 10))",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)",
                "Label1=pd.DataFrame([1,2,3,4,5])",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)",
                "ASSIGN = plt.colorbar()",
                "ASSIGN.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])",
                "plt.xlim(xx.min(), xx.max())",
                "plt.ylim(yy.min(), yy.max())",
                "plt.xlabel(\"feature 1\")",
                "plt.ylabel(\"feature 2\")",
                "plt.title(\"6-Class classification,  balanced bins - total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "# Put the result into a color plot\n",
                "plt.figure(figsize=(18, 10))\n",
                "plt.pcolormesh(xx, yy, Zo, cmap=cmap_light)\n",
                "#Label1=pd.DataFrame(list(class_index.items()))\n",
                "Label1=pd.DataFrame([1,2,3,4,5])\n",
                "# Plot also the training points\n",
                "plt.scatter(feature1, feature2, c=y_balanced, cmap=cmap_bold)\n",
                "cbar = plt.colorbar() \n",
                "cbar.ax.set_yticklabels([Level1_balanced,Level2_balanced,Level3_balanced,Level4_balanced,Level5_balanced,Level6_balanced])\n",
                "plt.xlim(xx.min(), xx.max())\n",
                "plt.ylim(yy.min(), yy.max())\n",
                "plt.xlabel(\"feature 1\")\n",
                "plt.ylabel(\"feature 2\")\n",
                "plt.title(\"6-Class classification,   balanced bins -  total infected log10 (k = %i, weights = '%s')\" % (k_Optimal, weights))\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(ASSIGN['test_score'])",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))"
            ],
            "output_type": "stream",
            "content_old": [
                "\n",
                "knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)\n",
                "df_resultat.at[df_resultat.index==7,\"method\"]=\"knn balanced bins\"\n",
                "df_resultat.at[df_resultat.index==7,\"Cross-validation\"]=np.mean(knn_scores['test_score'])\n",
                "print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))\n",
                "print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score'])))\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN = Pipeline([",
                "('scaler', StandardScaler()),",
                "('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))",
                "])",
                "ASSIGN=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)",
                "ASSIGN = knn_pipe.predict(X_fea_2_te_balan)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "knn_pipe = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))\n",
                "])\n",
                "# Fit estimator\n",
                "knn_confusion=knn_pipe.fit(X_fea_2_tr_balan, y_fea_2_tr_balan)\n",
                "knn_predict = knn_pipe.predict(X_fea_2_te_balan)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "confusion_matrix(y_fea_2_te_balan, knn_predict)"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "evaluate_model"
            ],
            "content": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plot_confusion_matrix(knn_confusion, X_fea_2_te_balan, y_fea_2_te_balan,cmap=plt.cm.Greens,display_labels=[1,2,3,4,5,6])"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df_resultat"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df_resultat"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "sns.barplot(x=df_resultat[\"Cross-validation\"],y=df_resultat['method'])\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "codiv_time_confirmed"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_time_confirmed"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=codiv_time_confirmed.groupby('Countrypath').sum().reset_index()",
                "ASSIGN=codiv_time_confirmed_grouped.agg(['sum'])",
                "SLICE=\"total_confirmed\"",
                "ASSIGN=codiv_time_confirmed_grouped.append(dfsum)",
                "ASSIGN=codiv_time_recovered.groupby('Countrypath').sum().reset_index()",
                "ASSIGN=codiv_time_recovered_grouped.agg(['sum'])",
                "SLICE=\"total_recovered\"",
                "ASSIGN=codiv_time_recovered_grouped.append(dfsum)",
                "ASSIGN=codiv_time_deaths.groupby('Countrypath').sum().reset_index()",
                "ASSIGN=codiv_time_deaths_grouped.agg(['sum'])",
                "SLICE=\"total_deaths\"",
                "ASSIGN=codiv_time_deaths_grouped.append(dfsum)",
                "codiv_time_deaths_grouped_sum"
            ],
            "output_type": "execute_result",
            "content_old": [
                "codiv_time_confirmed_grouped=codiv_time_confirmed.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_confirmed_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_confirmed\"\n",
                "codiv_time_confirmed_grouped_sum=codiv_time_confirmed_grouped.append(dfsum)\n",
                "#\n",
                "codiv_time_recovered_grouped=codiv_time_recovered.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_recovered_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_recovered\"\n",
                "codiv_time_recovered_grouped_sum=codiv_time_recovered_grouped.append(dfsum)\n",
                "#\n",
                "codiv_time_deaths_grouped=codiv_time_deaths.groupby('Country/Region').sum().reset_index()\n",
                "dfsum=codiv_time_deaths_grouped.agg(['sum'])\n",
                "dfsum['Country/Region']=\"total_deaths\"\n",
                "codiv_time_deaths_grouped_sum=codiv_time_deaths_grouped.append(dfsum)\n",
                "codiv_time_deaths_grouped_sum"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "ASSIGN=1",
                "for countryInd in codiv_country_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_Restrictions['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_Restrictions['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_Restrictions['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd])",
                "ASSIGN=1",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:",
                "if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:",
                "ASSIGN==1:",
                "ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]",
                "ASSIGN=0",
                "else:",
                "ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#Recovered\n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]) \n",
                "#deaths   \n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]) \n",
                "#confirmed\n",
                "\n",
                "Start=1\n",
                "for countryInd in codiv_country_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])\n",
                "\n",
                "Start=1\n",
                "for countryInd in codiv_country_Restrictions['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])\n",
                "\n",
                "#codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country/Region')\n",
                "#\n",
                "Start=1\n",
                "for countryInd in codiv_country_without_Restrictions_qurantine['Country']:\n",
                "    if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:\n",
                "        if Start==1:\n",
                "            codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]\n",
                "            Start=0\n",
                "        else:\n",
                "            codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]) \n",
                "\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Countrypath')",
                "ASSIGN= codiv_country_qurantine_confirmed.set_index('Countrypath')-codiv_country_qurantine_deaths.set_index('Countrypath')-codiv_country_qurantine_recovered.set_index('Countrypath')",
                "ASSIGN= codiv_country_Restrictions_confirmed.set_index('Countrypath')- codiv_country_Restrictions_deaths.set_index('Countrypath')-codiv_country_Restrictions_recovered.set_index('Countrypath')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "codiv_country_without_Restrictions_qurantine_active= codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region')-codiv_country_without_Restrictions_qurantine_recovered.set_index('Country/Region')\n",
                "codiv_country_qurantine_active= codiv_country_qurantine_confirmed.set_index('Country/Region')-codiv_country_qurantine_deaths.set_index('Country/Region')-codiv_country_qurantine_recovered.set_index('Country/Region')\n",
                "codiv_country_Restrictions_active= codiv_country_Restrictions_confirmed.set_index('Country/Region')- codiv_country_Restrictions_deaths.set_index('Country/Region')-codiv_country_Restrictions_recovered.set_index('Country/Region')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "AutoMinorLocator)",
                "ASSIGN= codiv_country_qurantine_confirmed.shape",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))",
                "ASSIGN = pd.Series(",
                "np.random.randn(col-1),",
                "ASSIGN=pd.date_range('20path', periods=col-1))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_qurantine_confirmed.set_index('Countrypath').index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_Restrictions_confirmed.set_index('Countrypath').index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Countrypath').index))",
                "ax0.plot(ASSIGN.ASSIGN,ASSIGN)",
                "ax1.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax2.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax0.set_title(\"quarantine_confirmed\")",
                "ax1.set_title(\"Restrictions_confirmed\")",
                "ax2.set_title(\"without_Restrictions_quarantine_confirmed\")",
                "ax0.legend(codiv_country_qurantine_confirmed['Countrypath'].tolist(),loc='upper left')",
                "ax1.legend(codiv_country_Restrictions_confirmed['Countrypath'].tolist(),loc='upper left')",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Countrypath'].tolist(),loc='upper left')",
                "ax0.set_ylabel(\"quarantine_confirmed\")",
                "ax1.set_ylabel(\"Restrictions_confirmed\")",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_confirmed\")",
                "ax0.grid(True)",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax1.grid(True)",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax2.grid(True)",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax0.tick_params(axis=\"x\", rotation=45)",
                "ax1.tick_params(axis=\"x\", rotation=45)",
                "ax2.tick_params(axis=\"x\", rotation=45)",
                "for countryindex in codiv_country_qurantine_confirmed['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":",
                "QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]",
                "QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]",
                "ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')",
                "for countryindex in codiv_country_Restrictions_confirmed['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":",
                "RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]",
                "RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]",
                "ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')"
            ],
            "output_type": "display_data",
            "content_old": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_confirmed.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col-1),\n",
                "    index=pd.date_range('20/1/2020', periods=col-1))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_confirmed.set_index('Country/Region').index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_confirmed.set_index('Country/Region').index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_confirmed.set_index('Country/Region').index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_confirmed\")\n",
                "ax1.set_title(\"Restrictions_confirmed\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_confirmed\")\n",
                "ax0.legend(codiv_country_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_confirmed['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_confirmed\")\n",
                "ax1.set_ylabel(\"Restrictions_confirmed\")\n",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_confirmed\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_confirmed['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_confirmed['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "AutoMinorLocator)",
                "ASSIGN= codiv_country_qurantine_deaths.shape",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))",
                "ASSIGN = pd.Series(",
                "np.random.randn(col-1),",
                "ASSIGN=pd.date_range('20path', periods=col-1))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_qurantine_deaths.set_index('Countrypath').index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_Restrictions_deaths.set_index('Countrypath').index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath').T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Countrypath').index))",
                "ax0.plot(ASSIGN.ASSIGN,ASSIGN)",
                "ax1.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax2.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax0.set_title(\"quarantine_deaths\")",
                "ax1.set_title(\"Restrictions_deaths\")",
                "ax2.set_title(\"without_Restrictions_quarantine_deaths\")",
                "ax0.legend(codiv_country_qurantine_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax1.legend(codiv_country_Restrictions_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax0.set_ylabel(\"quarantine_deaths\")",
                "ax1.set_ylabel(\"Restrictions_deaths\")",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_deaths\")",
                "ax0.grid(True)",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax1.grid(True)",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax2.grid(True)",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax0.tick_params(axis=\"x\", rotation=45)",
                "ax1.tick_params(axis=\"x\", rotation=45)",
                "ax2.tick_params(axis=\"x\", rotation=45)",
                "for countryindex in codiv_country_qurantine_deaths['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":",
                "QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]",
                "QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]",
                "ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')",
                "for countryindex in codiv_country_Restrictions_deaths['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":",
                "RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]",
                "RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]",
                "ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')"
            ],
            "output_type": "display_data",
            "content_old": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_deaths.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col-1),\n",
                "    index=pd.date_range('20/1/2020', periods=col-1))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_deaths.set_index('Country/Region').index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_deaths.set_index('Country/Region').index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_deaths.set_index('Country/Region').index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_deaths\")\n",
                "ax1.set_title(\"Restrictions_deaths\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_deaths\")\n",
                "ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_deaths\")\n",
                "ax1.set_ylabel(\"Restrictions_deaths\")\n",
                "ax2.set_ylabel(\"without_Restrictions_quarantine_deaths\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "visualize_data"
            ],
            "content": [
                "SETUP",
                "AutoMinorLocator)",
                "ASSIGN= codiv_country_qurantine_active.shape",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))",
                "ASSIGN = pd.Series(",
                "np.random.randn(col),",
                "ASSIGN=pd.date_range('20path', periods=col))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_qurantine_active.index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_Restrictions_active.index))",
                "ASSIGN = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()),",
                "ASSIGN=ts.ASSIGN,",
                "ASSIGN=list(codiv_country_without_Restrictions_qurantine_active.index))",
                "ax0.plot(ASSIGN.ASSIGN,ASSIGN)",
                "ax1.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax2.plot(ASSIGN.ASSIGN, ASSIGN)",
                "ax0.set_title(\"quarantine_active\")",
                "ax1.set_title(\"Restrictions_active\")",
                "ax2.set_title(\"without_Restrictions_quarantine_active\")",
                "ax0.legend(codiv_country_qurantine_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax1.legend(codiv_country_Restrictions_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Countrypath'].tolist(),loc='upper left')",
                "ax0.set_ylabel(\"quarantine_active\")",
                "ax1.set_ylabel(\"Restrictions_active\")",
                "ax2.set_ylabel(\"without_Restrictions_qurantine_active\")",
                "ax0.grid(True)",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax1.grid(True)",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax2.grid(True)",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())",
                "ax0.tick_params(axis=\"x\", rotation=45)",
                "ax1.tick_params(axis=\"x\", rotation=45)",
                "ax2.tick_params(axis=\"x\", rotation=45)",
                "for countryindex in codiv_country_qurantine_deaths['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":",
                "QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]",
                "QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]",
                "ax0.annotate('Quarantine', (QuarantineTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')",
                "for countryindex in codiv_country_Restrictions_deaths['Countrypath']:",
                "if not codiv_country[codiv_country['Country']==countryindex].empty :",
                "if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":",
                "RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]",
                "RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%mpath%dpath%Y\")",
                "ASSIGN=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]",
                "ax1.annotate('Restrictions', (RestrictionsTimeD, ASSIGN),",
                "ASSIGN=(0.2, 0.95), textcoords='axes fraction',",
                "ASSIGN=dict(facecolor='red', shrink=0.001),",
                "ASSIGN=16,",
                "ASSIGN='right', verticalalignment='top')"
            ],
            "output_type": "display_data",
            "content_old": [
                "import datetime\n",
                "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
                "                               AutoMinorLocator)\n",
                "row, col= codiv_country_qurantine_active.shape\n",
                "fig, (ax0,ax1,ax2) = plt.subplots(3, sharey=True,figsize=(25,40))\n",
                "ts = pd.Series(\n",
                "    np.random.randn(col),\n",
                "    index=pd.date_range('20/1/2020', periods=col))\n",
                "df_qua_conf = pd.DataFrame((np.log1p(codiv_country_qurantine_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_qurantine_active.index))\n",
                "df_res_conf = pd.DataFrame((np.log1p(codiv_country_Restrictions_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_Restrictions_active.index))\n",
                "df_whi_conf = pd.DataFrame((np.log1p(codiv_country_without_Restrictions_qurantine_active.T).to_numpy()),\n",
                "    index=ts.index,\n",
                "    columns=list(codiv_country_without_Restrictions_qurantine_active.index))\n",
                "\n",
                "ax0.plot(df_qua_conf.index,df_qua_conf)\n",
                "ax1.plot(df_res_conf.index, df_res_conf)\n",
                "ax2.plot(df_whi_conf.index, df_whi_conf)\n",
                "ax0.set_title(\"quarantine_active\")\n",
                "ax1.set_title(\"Restrictions_active\")\n",
                "ax2.set_title(\"without_Restrictions_quarantine_active\")\n",
                "ax0.legend(codiv_country_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax1.legend(codiv_country_Restrictions_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax2.legend(codiv_country_without_Restrictions_qurantine_deaths['Country/Region'].tolist(),loc='upper left')\n",
                "ax0.set_ylabel(\"quarantine_active\")\n",
                "ax1.set_ylabel(\"Restrictions_active\")\n",
                "ax2.set_ylabel(\"without_Restrictions_qurantine_active\")\n",
                "\n",
                "ax0.grid(True)\n",
                "ax0.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax1.grid(True)\n",
                "ax1.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax2.grid(True)\n",
                "ax2.xaxis.set_minor_locator(AutoMinorLocator())\n",
                "ax0.tick_params(axis=\"x\", rotation=45)\n",
                "ax1.tick_params(axis=\"x\", rotation=45)\n",
                "ax2.tick_params(axis=\"x\", rotation=45)\n",
                "####\n",
                "for countryindex in codiv_country_qurantine_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]!=\"2000-01-01\" and countryindex!=\"New Zealand\":\n",
                "            QuarantineTime=codiv_country[codiv_country['Country']==countryindex]['Quarantine'].values[0]\n",
                "            QuarantineTimeD = datetime.datetime.strptime(str(QuarantineTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_qua_conf[df_qua_conf.index==QuarantineTimeD][countryindex].values[0]\n",
                "            ax0.annotate('Quarantine', (QuarantineTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')\n",
                "####\n",
                "for countryindex in codiv_country_Restrictions_deaths['Country/Region']:\n",
                "    if not codiv_country[codiv_country['Country']==countryindex].empty :\n",
                "        if codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]!=\"2000-01-01\":\n",
                "            RestrictionsTime=codiv_country[codiv_country['Country']==countryindex]['Restrictions'].values[0]\n",
                "            RestrictionsTimeD = datetime.datetime.strptime(str(RestrictionsTime), \"%m/%d/%Y\")\n",
                "            Centervalue=df_res_conf[df_res_conf.index==RestrictionsTimeD][countryindex].values[0]\n",
                "            ax1.annotate('Restrictions', (RestrictionsTimeD, Centervalue),\n",
                "                xytext=(0.2, 0.95), textcoords='axes fraction',\n",
                "                arrowprops=dict(facecolor='red', shrink=0.001),\n",
                "                fontsize=16,\n",
                "                horizontalalignment='right', verticalalignment='top')"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "Image(filename=os.path.join('.path', 'shift.JPG'))"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from IPython.display import Image\n",
                "Image(filename=os.path.join('.//', 'shift.JPG'))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "StartDate=codiv_country_Restrictions_active.T.index[0]",
                "print(,StartDate)",
                "ASSIGN = [\"date\"]",
                "ASSIGN = pd.DataFrame(columns = column_names)",
                "for Dateindex in range(20):",
                "ASSIGN = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])",
                "ASSIGN=ASSIGN.append(df_date1, ignore_index = True)"
            ],
            "output_type": "stream",
            "content_old": [
                "# evalue the stadt date of the stats\n",
                "StartDate=codiv_country_Restrictions_active.T.index[0]\n",
                "print(\" Epidemy StartDate = \",StartDate)\n",
                "column_names = [\"date\"]\n",
                "dfdate = pd.DataFrame(columns = column_names)\n",
                "for Dateindex in range(20):\n",
                "    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n",
                "    dfdate=dfdate.append(df_date1, ignore_index = True)\n"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "def rmse(y, y_pred):",
                "return np.sqrt(np.mean(np.square(y - y_pred)))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Root mean squared error (RMSE)\n",
                "def rmse(y, y_pred):\n",
                "    return np.sqrt(np.mean(np.square(y - y_pred)))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "def Polifq(Country,Version=\"restrictions\",shift=0,trigger=40):",
                "ASSIGN==\"restrictions\":",
                "ASSIGN=codiv_country_Restrictions_active.T[Country]",
                "else:",
                "ASSIGN==\"quarantine\":",
                "ASSIGN=codiv_country_qurantine_active.T[Country]",
                "else:",
                "ASSIGN=codiv_country_without_Restrictions_qurantine_active.T[Country]",
                "StartCountryIndex=0",
                "ShiftIndex=0",
                "ASSIGN=0",
                "ASSIGN=0",
                "ASSIGN=1.0",
                "LargeFactor=1.0",
                "ASSIGN = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())",
                "ASSIGN = df_codiv.loc[filter0]",
                "ASSIGN = ASSIGN.drop(outliers.index, axis=0)",
                "ASSIGN=max(df_codiv)",
                "ASSIGN = pd.DataFrame(columns = [Country,\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[Country]",
                "ASSIGN = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10)",
                "ASSIGN=0",
                "ASSIGN==1:",
                "ASSIGN=1",
                "ASSIGN=max(codiv_country_qurantine_active.T[\"China\"])",
                "for Index in range(0,len(ASSIGN)):",
                "if ASSIGN[Index]>trigger*(ASSIGN*1.0path) and ASSIGN==1 and shift==1:",
                "if ASSIGN>ASSIGN:",
                "ASSIGN=ASSIGN*(maxCountry*1.5path)",
                "StartCountryIndex=Index",
                "ASSIGN=0",
                "if ASSIGN[Index]==ASSIGN:",
                "ASSIGN=Index",
                "LargeCountry=ASSIGN-StartCountryIndex",
                "ASSIGN=codiv_country_qurantine_active.T[\"China\"]",
                "ASSIGN = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())",
                "ASSIGN = df_codiv_China_ref.loc[filter0]",
                "ASSIGN = df_codiv_China_ref.drop(outliers.index, axis=0)",
                "ASSIGN=max(df_codiv_China_ref)",
                "ASSIGN=1",
                "for Index in range(0,len(ASSIGN)):",
                "if ASSIGN[Index]>60 and ASSIGN==1 and shift==1:",
                "StartChinaIndex=Index",
                "ASSIGN=0",
                "if ASSIGN[Index]==ASSIGN:",
                "ASSIGN=Index",
                "ASSIGN=maxCountry*1.0path",
                "LargeChina=ASSIGN-StartChinaIndex",
                "LargeFactor=LargeCountry*1.0path",
                "ShiftIndex=StartCountryIndex-StartChinaIndex",
                "ASSIGN=0.02",
                "ASSIGN = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[\"China\"]",
                "ASSIGN = rmse(y_tr1_China_model,y_tr1_Country)",
                "ASSIGN=0",
                "for runNr in range(1,60,1):",
                "if ASSIGN<ASSIGN:",
                "LargeFactor=LargeFactor+ASSIGN",
                "ASSIGN=error1",
                "ASSIGN = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[\"China\"]",
                "ASSIGN = rmse(y_tr1_China_model,y_tr1_Country)",
                "if ASSIGN<ASSIGN:",
                "LargeFactor=LargeFactor+ASSIGN",
                "ASSIGN=error1",
                "ASSIGN = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[\"China\"]",
                "ASSIGN = rmse(y_tr1_China_model,y_tr1_Country)",
                "if ASSIGN<ASSIGN:",
                "ASSIGN=ASSIGN+step",
                "ASSIGN=error1",
                "ASSIGN = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[\"China\"]",
                "ASSIGN = rmse(y_tr1_China_model,y_tr1_Country)",
                "if ASSIGN<ASSIGN:",
                "ASSIGN=ASSIGN+step",
                "ASSIGN=error1",
                "ASSIGN = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')",
                "for Index in range(0,len(ASSIGN)):",
                "ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])",
                "ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True)",
                "ASSIGN=df_country[\"Value\"]",
                "ASSIGN=df_country[\"China\"]",
                "ASSIGN = rmse(y_tr1_China_model,y_tr1_Country)",
                "ASSIGN = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10)",
                "return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import scipy.stats as stats\n",
                "import datetime\n",
                "def Polifq(Country,Version=\"restrictions\",shift=0,trigger=40):\n",
                "    # Version= works for restrictions, quarantin and without_Restrictions_qurantine.\n",
                "    if Version==\"restrictions\":\n",
                "        df_codiv=codiv_country_Restrictions_active.T[Country]\n",
                "    else:\n",
                "        if Version==\"quarantine\":\n",
                "            df_codiv=codiv_country_qurantine_active.T[Country]\n",
                "        else:\n",
                "            df_codiv=codiv_country_without_Restrictions_qurantine_active.T[Country]\n",
                "    #######################################\n",
                "    ######################## parameter setup\n",
                "    StartCountryIndex=0\n",
                "    ShiftIndex=0\n",
                "    x_tr1_China_model=0\n",
                "    y_tr1_China_model=0\n",
                "    highfactor=1.0\n",
                "    LargeFactor=1.0\n",
                "    ##############################################################################\n",
                "    ####################################### fitting the Country ##################\n",
                "    ######################## filter outlier for the Country\n",
                "    filter0 = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())\n",
                "    outliers = df_codiv.loc[filter0]\n",
                "    df_codiv = df_codiv.drop(outliers.index, axis=0)\n",
                "    ###########\n",
                "    # max actual value of the country\n",
                "    maxCountry=max(df_codiv)\n",
                "    #######################################\n",
                "    ########### Build the dataframe for the country\n",
                "    df_country = pd.DataFrame(columns = [Country,\"Value\",\"time\"], dtype='int')\n",
                "    for Index in range(0,len(df_codiv)):\n",
                "        df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,\"Value\",\"time\"])\n",
                "        df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "    #######################################\n",
                "    #################### polyfit 10gr for the country value\n",
                "    x_tr1_Country=df_country[\"Value\"]\n",
                "    y_tr1_Country=df_country[Country]\n",
                "    #\n",
                "    # * Polyfit with degree 10\n",
                "    coefs_Country_poly10 = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) # Fit to train data\n",
                "    ##############################################################################\n",
                "    ##############################################################################\n",
                "    coefs_predict_poly10=0\n",
                "    #######################################\n",
                "    ############### Now shifting the china model to fitt to the country\n",
                "    if shift==1:\n",
                "        Start=1\n",
                "        ####### for the country\n",
                "        maxChina=max(codiv_country_qurantine_active.T[\"China\"])\n",
                "        for Index in range(0,len(df_codiv)):\n",
                "            #look for the start position of the country epidemy, it should be higher then the trigger value, it will be setup only by start of procedure ( STart=1)\n",
                "            if df_codiv[Index]>trigger*(maxCountry*1.0/maxChina) and Start==1 and shift==1:\n",
                "                \n",
                "                if maxCountry>maxChina:\n",
                "                    trigger=trigger*(maxCountry*1.5/maxChina)\n",
                "                StartCountryIndex=Index\n",
                "                Start=0\n",
                "            # register the index of the coutry max position\n",
                "            if df_codiv[Index]==maxCountry:\n",
                "                maxCountryIndex=Index\n",
                "        # deliver the actual distance between start and maximum for the country\n",
                "        LargeCountry=maxCountryIndex-StartCountryIndex\n",
                "        #################################\n",
                "        #######################################\n",
                "        ####### for china\n",
                "        df_codiv_China_ref=codiv_country_qurantine_active.T[\"China\"]\n",
                "        #######################################\n",
                "        ###### outiler\n",
                "        filter0 = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())\n",
                "        outliers = df_codiv_China_ref.loc[filter0]\n",
                "        df_codiv = df_codiv_China_ref.drop(outliers.index, axis=0)\n",
                "        #######################################\n",
                "        ###### check for the start point by 40 infected\n",
                "        maxChina=max(df_codiv_China_ref)\n",
                "        Start=1\n",
                "        for Index in range(0,len(df_codiv_China_ref)):\n",
                "            if df_codiv_China_ref[Index]>60 and Start==1 and shift==1:\n",
                "                StartChinaIndex=Index\n",
                "                Start=0\n",
                "            if df_codiv_China_ref[Index]==maxChina:\n",
                "                maxChinaIndex=Index\n",
                "        #######################################\n",
                "        ###### start value for the fitting\n",
                "        highfactor=maxCountry*1.0/maxChina\n",
                "        LargeChina=maxChinaIndex-StartChinaIndex\n",
                "        LargeFactor=LargeCountry*1.0/LargeChina\n",
                "        ShiftIndex=StartCountryIndex-StartChinaIndex\n",
                "        ##########OPTIMISATION \n",
                "        step=0.02\n",
                "        ############# the target\n",
                "        # Predictions with the current a,b values\n",
                "        ################# generate the function country is china\n",
                "        df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "        for Index in range(0,len(df_codiv)):\n",
                "            df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "            df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "        x_tr1_China_model=df_country[\"Value\"]\n",
                "        y_tr1_China_model=df_country[\"China\"]      \n",
                "        error0 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "        error1=0\n",
                "        for runNr in range(1,60,1):\n",
                "            if error1<error0:\n",
                "                LargeFactor=LargeFactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            if error1<error0:\n",
                "                LargeFactor=LargeFactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            ##############\n",
                "            if error1<error0:\n",
                "                highfactor=highfactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "            #################\n",
                "            if error1<error0:\n",
                "                highfactor=highfactor+step\n",
                "                error0=error1\n",
                "                ################# generate the function\n",
                "                df_country = pd.DataFrame(columns = [\"China\",\"Value\",\"time\"], dtype='int')\n",
                "                for Index in range(0,len(df_codiv)):\n",
                "                    df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [\"China\",\"Value\",\"time\"])\n",
                "                    df_country=df_country.append(df_codiv1, ignore_index = True)\n",
                "                x_tr1_China_model=df_country[\"Value\"]\n",
                "                y_tr1_China_model=df_country[\"China\"] \n",
                "                error1 = rmse(y_tr1_China_model,y_tr1_Country)\n",
                "        #after having this fitting, we can fit the modified china data  on a 10grade polynome\n",
                "        #\n",
                "        # * Polyfit with degree 10\n",
                "        coefs_predict_poly10 = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) # Fit to train data\n",
                "    return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=codiv_country_Restrictions_active.shape",
                "ASSIGN = np.linspace(0, lendata+40, num=lendata+30)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# how much data are available for a country\n",
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)",
                "ASSIGN=max(x_tr_China)-7",
                "ASSIGN = np.polyval(coefs_China, x_values)",
                "ASSIGN=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#compute the ymax china\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "StartDate=codiv_country_Restrictions_active.T.index[0]",
                "ASSIGN = [\"date\"]",
                "ASSIGN = pd.DataFrame(columns = column_names)",
                "for Dateindex in range(0,lendata*20,10):",
                "ASSIGN = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])",
                "ASSIGN=ASSIGN.append(df_date1, ignore_index = True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "StartDate=codiv_country_Restrictions_active.T.index[0]\n",
                "column_names = [\"date\"]\n",
                "dfdate = pd.DataFrame(columns = column_names)\n",
                "for Dateindex in range(0,lendata*20,10):\n",
                "    df_date1 = pd.DataFrame ([[str(pd.Timestamp(StartDate,unit=\"D\")+ pd.Timedelta(days=Dateindex))]], columns = [\"date\"])\n",
                "    dfdate=dfdate.append(df_date1, ignore_index = True)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_qurantine_active.T:",
                "ASSIGN==\"China\":",
                "coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=\"quarantine\",trigger=70)",
                "ASSIGN = np.polyval(coefs_Country, x_values)",
                "ASSIGN=0",
                "else:",
                "coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"quarantine\",trigger=170)",
                "ASSIGN = np.polyval(coefs_Country, x_values)",
                "ASSIGN = np.polyval(coefs_Country_prev, x_values)",
                "ASSIGN=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)",
                "if ymaxchina>ASSIGN:",
                "ASSIGN=ymaxchina",
                "ASSIGN = plt.figure(figsize= (18, 10))",
                "plt.scatter(x_tr_China, y_tr_China, s=10)",
                "plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")",
                "ASSIGN!=\"China\":",
                "plt.scatter(x_tr_Country, y_tr_Country, s=10)",
                "plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=\"magenta\")",
                "plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")",
                "PredictionName='%s-prediction'%(countryindex,)",
                "plt.plot(x_values, ASSIGN, label=PredictionName,c=\"brown\",ls='dashed')",
                "plt.ylim(-5,ASSIGN)",
                "plt.ylabel(\"Number\")",
                "plt.grid(True)",
                "ASSIGN = range(0,len(dfdate),10)",
                "plt.xticks(ticks = ASSIGN ,labels = dfdate[\"date\"], rotation = 75)",
                "ASSIGN!=\"China\":",
                "ASSIGN='%s-prediction - Quarantine'%(countryindex,)",
                "plt.title(ASSIGN)",
                "else:",
                "plt.title(\"China\")",
                "ASSIGN=\"\"",
                "ASSIGN==\"Belgium\":",
                "ASSIGN=\"Belgium only success to slow the infection. The model wait for the peak to fit better\"",
                "ASSIGN==\"France\":",
                "ASSIGN=\"France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear\"",
                "ASSIGN==\"Germany\" or countryindex==\"Austria\":",
                "ASSIGN=\"Germany was relaxing the quarantine. but the model fit mostly good\"",
                "ASSIGN==\"India\" or countryindex==\"Argentina\" or countryindex==\"Peru\" or countryindex==\"Colombia\":",
                "ASSIGN=\" The model wait for the peak to fit better\"",
                "ASSIGN==\"Spain\":",
                "ASSIGN=\" Spain had no clear data, the model need the next days data to fit better\"",
                "ASSIGN==\"China\":",
                "ASSIGN=\" China is the reference country, but we see they have to face to some epidemic restart ( imported case).\"",
                "plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15)",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "for countryindex in codiv_country_qurantine_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    if countryindex==\"China\":\n",
                "        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=0,Version=\"quarantine\",trigger=70)\n",
                "        y_Country = np.polyval(coefs_Country, x_values)\n",
                "        ymax=0\n",
                "    else:\n",
                "        coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"quarantine\",trigger=170)\n",
                "        y_Country = np.polyval(coefs_Country, x_values)\n",
                "        y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "        #adjust the ymax of the graphic\n",
                "        ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n",
                "    if ymaxchina>ymax:\n",
                "        ymax=ymaxchina\n",
                "    #\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    #china reference country\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    #plot the second country\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - Quarantine'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Belgium\":\n",
                "        Texte=\"Belgium only success to slow the infection. The model wait for the peak to fit better\"\n",
                "    if countryindex==\"France\":\n",
                "        Texte=\"France was relaxing the quarantine? the model need the next days data to fit better. the datas are not clear\"      \n",
                "    if countryindex==\"Germany\" or countryindex==\"Austria\":\n",
                "        Texte=\"Germany was relaxing the quarantine. but the model fit mostly good\"      \n",
                "    if countryindex==\"India\" or countryindex==\"Argentina\" or countryindex==\"Peru\" or countryindex==\"Colombia\":\n",
                "        Texte=\" The model wait for the peak to fit better\"       \n",
                "    if countryindex==\"Spain\":\n",
                "        Texte=\" Spain had no clear data, the model need the next days data to fit better\"  \n",
                "    if countryindex==\"China\":\n",
                "        Texte=\" China is the reference country, but we see they have to face to some epidemic restart ( imported case).\"  \n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country_Restrictions_active.shape",
                "ASSIGN = np.linspace(0, lendata+40, num=lendata+30)",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)",
                "ASSIGN=max(x_tr_China)-7",
                "ASSIGN = np.polyval(coefs_China, x_values)",
                "ASSIGN=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_Restrictions_active.T:",
                "ASSIGN==\"Japan\":",
                "ASSIGN=4500",
                "else:",
                "ASSIGN=250",
                "coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"restrictions\",ASSIGN=ASSIGN)",
                "ASSIGN = np.polyval(coefs_Country, x_values)",
                "ASSIGN = np.polyval(coefs_Country_prev, x_values)",
                "ASSIGN=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)",
                "if ymaxchina>ASSIGN:",
                "ASSIGN=ymaxchina",
                "ASSIGN = plt.figure(figsize= (18, 10))",
                "plt.scatter(x_tr_China, y_tr_China, s=10)",
                "plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")",
                "ASSIGN!=\"China\":",
                "plt.scatter(x_tr_Country, y_tr_Country, s=10)",
                "plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=\"magenta\")",
                "plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")",
                "PredictionName='%s-prediction'%(countryindex,)",
                "plt.plot(x_values, ASSIGN, label=PredictionName,c=\"brown\",ls='dashed')",
                "plt.ylim(-5,ASSIGN)",
                "plt.ylabel(\"Number\")",
                "plt.grid(True)",
                "ASSIGN = range(0,len(dfdate),10)",
                "plt.xticks(ticks = ASSIGN ,labels = dfdate[\"date\"], rotation = 75)",
                "ASSIGN!=\"China\":",
                "ASSIGN='%s-prediction - Restrictions'%(countryindex,)",
                "plt.title(ASSIGN)",
                "else:",
                "plt.title(\"China\")",
                "ASSIGN=\"\"",
                "ASSIGN==\"Ireland\":",
                "ASSIGN=\"Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?\"",
                "ASSIGN==\"Japan\":",
                "ASSIGN=\"Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher\"",
                "ASSIGN==\"Canada\" or countryindex==\"Netherlands\" or countryindex==\"Peru\" or countryindex==\"Colombia\"or countryindex==\"Portugal\"or countryindex==\"Sweden\"or countryindex==\"United Kingdom\":",
                "ASSIGN=\" The model wait for the peak to fit better\"",
                "ASSIGN==\"Norway\" or countryindex==\"Poland\":",
                "ASSIGN=\" The model wait for the peak to fit better,they take long time to reach it\"",
                "plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15)",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "for countryindex in codiv_country_Restrictions_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    # China is only in quarantine group\n",
                "    if countryindex==\"Japan\":\n",
                "        trigger=4500\n",
                "    else:\n",
                "        trigger=250\n",
                "    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"restrictions\",trigger=trigger)\n",
                "    y_Country = np.polyval(coefs_Country, x_values)\n",
                "    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "    ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)\n",
                "    #adjust the ymax limit\n",
                "    if ymaxchina>ymax:\n",
                "        ymax=ymaxchina\n",
                "    #\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - Restrictions'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    ##\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Ireland\":\n",
                "        Texte=\"Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?\"\n",
                "    if countryindex==\"Japan\":\n",
                "        Texte=\"Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher\" \n",
                "    if countryindex==\"Canada\" or countryindex==\"Netherlands\" or countryindex==\"Peru\" or countryindex==\"Colombia\"or countryindex==\"Portugal\"or countryindex==\"Sweden\"or countryindex==\"United Kingdom\":\n",
                "        Texte=\" The model wait for the peak to fit better\"     \n",
                "    if countryindex==\"Norway\" or countryindex==\"Poland\":\n",
                "        Texte=\" The model wait for the peak to fit better,they take long time to reach it\" \n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    ##\n",
                "    plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN=codiv_country_Restrictions_active.shape",
                "ASSIGN = np.linspace(0, lendata+40, num=lendata+30)",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)",
                "ASSIGN=max(x_tr_China)-7",
                "ASSIGN = np.polyval(coefs_China, x_values)",
                "ASSIGN=int(max(y_tr_China)*1.5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "number,lendata=codiv_country_Restrictions_active.shape\n",
                "x_values = np.linspace(0, lendata+40, num=lendata+30)\n",
                "coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(\"China\",shift=0,Version=\"quarantine\",trigger=70)\n",
                "lendataChina=max(x_tr_China)-7\n",
                "y_China = np.polyval(coefs_China, x_values)\n",
                "ymaxchina=int(max(y_tr_China)*1.5)"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "for countryindex in codiv_country_without_Restrictions_qurantine_active.T:",
                "ASSIGN==\"Singapore\" or countryindex==\"Qatar\" or countryindex==\"Kuwait\":",
                "ASSIGN=2000",
                "else:",
                "ASSIGN=300",
                "coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"NoQuaNoRES\",ASSIGN=ASSIGN)",
                "ASSIGN = np.polyval(coefs_Country, x_values)",
                "ASSIGN = np.polyval(coefs_Country_prev, x_values)",
                "ASSIGN==\"US\":",
                "ASSIGN=1500000",
                "else:",
                "ASSIGN==\"Brazil\":",
                "ASSIGN=180000",
                "else:",
                "ASSIGN=60000",
                "ASSIGN = plt.figure(figsize= (18, 10))",
                "plt.scatter(x_tr_China, y_tr_China, s=10)",
                "plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")",
                "ASSIGN!=\"China\":",
                "plt.scatter(x_tr_Country, y_tr_Country, s=10)",
                "plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=\"magenta\")",
                "plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")",
                "PredictionName='%s-prediction'%(countryindex,)",
                "plt.plot(x_values, ASSIGN, label=PredictionName,c=\"brown\",ls='dashed')",
                "plt.ylim(-5,ASSIGN)",
                "plt.ylabel(\"Number\")",
                "plt.grid(True)",
                "ASSIGN = range(0,len(dfdate),10)",
                "plt.xticks(ticks = ASSIGN ,labels = dfdate[\"date\"], rotation = 75)",
                "ASSIGN!=\"China\":",
                "ASSIGN='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)",
                "plt.title(ASSIGN)",
                "else:",
                "plt.title(\"China\")",
                "ASSIGN=\"\"",
                "ASSIGN==\"Armenia\" or countryindex==\"Bahrain\" or countryindex==\"Bangladesh\" or countryindex==\"Belarus\"or countryindex==\"Brazil\"or countryindex==\"Chile\"or countryindex==\"United Kingdom\"or countryindex==\"US\":",
                "ASSIGN=\" The model wait for the peak to fit better\"",
                "ASSIGN==\"Ecuador\" or countryindex==\"Indonesia\" or countryindex==\"Kuwait\" or countryindex==\"Mexico\" or countryindex==\"Oman\" or countryindex==\"Pakistan\" or countryindex==\"Quatar\" or countryindex==\"South Africa\":",
                "ASSIGN=\" The model wait for the peak to fit better\"",
                "ASSIGN==\"Dominican Republic\" or countryindex==\"Ghana\":",
                "ASSIGN=\" The model wait for more data to fit better\"",
                "plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15)",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "for countryindex in codiv_country_without_Restrictions_qurantine_active.T:\n",
                "#codiv_country_quarantine_active.T:\n",
                "    # china is only in quarntine group\n",
                "    if countryindex==\"Singapore\" or countryindex==\"Qatar\" or countryindex==\"Kuwait\":\n",
                "        trigger=2000\n",
                "    else:\n",
                "        trigger=300\n",
                "    coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=\"NoQuaNoRES\",trigger=trigger)\n",
                "    y_Country = np.polyval(coefs_Country, x_values)\n",
                "    y_Country_prev = np.polyval(coefs_Country_prev, x_values)\n",
                "    # adjust the ymax graph limit\n",
                "    if countryindex==\"US\":\n",
                "        ymax=1500000\n",
                "    else:\n",
                "        if countryindex==\"Brazil\":\n",
                "            ymax=180000\n",
                "        else:\n",
                "            ymax=60000\n",
                "    fig = plt.figure(figsize= (18, 10))\n",
                "    plt.scatter(x_tr_China, y_tr_China, s=10)\n",
                "    plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=\"blue\")\n",
                "    if countryindex!=\"China\":\n",
                "        plt.scatter(x_tr_Country, y_tr_Country, s=10)\n",
                "        plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=\"magenta\")\n",
                "        plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=\"black\")\n",
                "        PredictionName='%s-prediction'%(countryindex,)\n",
                "        plt.plot(x_values, y_Country_prev, label=PredictionName,c=\"brown\",ls='dashed')\n",
                "    plt.ylim(-5,ymax)\n",
                "    plt.ylabel(\"Number\")\n",
                "    plt.grid(True)\n",
                "    tickvalues = range(0,len(dfdate),10)\n",
                "    plt.xticks(ticks = tickvalues ,labels = dfdate[\"date\"], rotation = 75)\n",
                "    if countryindex!=\"China\":\n",
                "        Title='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)\n",
                "        plt.title(Title)\n",
                "    else:\n",
                "        plt.title(\"China\")\n",
                "    ##\n",
                "    Texte=\"\"\n",
                "    if countryindex==\"Armenia\" or countryindex==\"Bahrain\" or countryindex==\"Bangladesh\" or countryindex==\"Belarus\"or countryindex==\"Brazil\"or countryindex==\"Chile\"or countryindex==\"United Kingdom\"or countryindex==\"US\":\n",
                "        Texte=\" The model wait for the peak to fit better\"  \n",
                "    if countryindex==\"Ecuador\" or countryindex==\"Indonesia\" or countryindex==\"Kuwait\" or countryindex==\"Mexico\" or countryindex==\"Oman\" or countryindex==\"Pakistan\" or countryindex==\"Quatar\" or countryindex==\"South Africa\":\n",
                "        Texte=\" The model wait for the peak to fit better\"\n",
                "    if countryindex==\"Dominican Republic\" or countryindex==\"Ghana\":\n",
                "        Texte=\" The model wait for more data to fit better\"\n",
                "    plt.text(1, ymax-10000, Texte, fontsize=15)\n",
                "    plt.legend()\n",
                "    plt.show()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from mpl_toolkits.mplot3d import Axes3D\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "import matplotlib.pyplot as plt # plotting\n",
                "import numpy as np # linear algebra\n",
                "import os # accessing directory structure\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(os.listdir('..path'))"
            ],
            "output_type": "stream",
            "content_old": [
                "print(os.listdir('../input'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):",
                "ASSIGN = df.ASSIGN()",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if nunique[col] > 1 and nunique[col] < 50]]",
                "nRow, nCol = ASSIGN.shape",
                "ASSIGN = list(df)",
                "ASSIGN = (nCol + nGraphPerRow - 1) path",
                "plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * ASSIGN), dpi = 80, facecolor = 'w', edgecolor = 'k')",
                "for i in range(min(nCol, nGraphShown)):",
                "plt.subplot(ASSIGN, nGraphPerRow, i + 1)",
                "ASSIGN = df.iloc[:, i]",
                "if (not np.issubdtype(type(ASSIGN.iloc[0]), np.number)):",
                "ASSIGN = columnDf.value_counts()",
                "ASSIGN.plot.bar()",
                "else:",
                "ASSIGN.hist()",
                "plt.ylabel('counts')",
                "plt.xticks(rotation = 90)",
                "plt.title(f'{ASSIGN[i]} (column {i})')",
                "plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Distribution graphs (histogram/bar graph) of column data\n",
                "def plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n",
                "    nunique = df.nunique()\n",
                "    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n",
                "    nRow, nCol = df.shape\n",
                "    columnNames = list(df)\n",
                "    nGraphRow = (nCol + nGraphPerRow - 1) / nGraphPerRow\n",
                "    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
                "    for i in range(min(nCol, nGraphShown)):\n",
                "        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n",
                "        columnDf = df.iloc[:, i]\n",
                "        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n",
                "            valueCounts = columnDf.value_counts()\n",
                "            valueCounts.plot.bar()\n",
                "        else:\n",
                "            columnDf.hist()\n",
                "        plt.ylabel('counts')\n",
                "        plt.xticks(rotation = 90)\n",
                "        plt.title(f'{columnNames[i]} (column {i})')\n",
                "    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "def plotCorrelationMatrix(df, graphWidth):",
                "ASSIGN = df.dataframeName",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "if ASSIGN.shape[1] < 2:",
                "print(f'No correlation plots shown: The number of non-NaN or constant columns ({ASSIGN.shape[1]}) is less than 2')",
                "return",
                "ASSIGN = df.ASSIGN()",
                "plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')",
                "ASSIGN = plt.matshow(corr, fignum = 1)",
                "plt.xticks(range(len(ASSIGN.columns)), ASSIGN.columns, rotation=90)",
                "plt.yticks(range(len(ASSIGN.columns)), ASSIGN.columns)",
                "plt.gca().xaxis.tick_bottom()",
                "plt.colorbar(ASSIGN)",
                "plt.title(f'Correlation Matrix for {ASSIGN}', fontsize=15)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Correlation matrix\n",
                "def plotCorrelationMatrix(df, graphWidth):\n",
                "    filename = df.dataframeName\n",
                "    df = df.dropna('columns') # drop columns with NaN\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    if df.shape[1] < 2:\n",
                "        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
                "        return\n",
                "    corr = df.corr()\n",
                "    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
                "    corrMat = plt.matshow(corr, fignum = 1)\n",
                "    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
                "    plt.yticks(range(len(corr.columns)), corr.columns)\n",
                "    plt.gca().xaxis.tick_bottom()\n",
                "    plt.colorbar(corrMat)\n",
                "    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "def plotScatterMatrix(df, plotSize, textSize):",
                "ASSIGN = ASSIGN.select_dtypes(include =[np.number])",
                "ASSIGN = ASSIGN.dropna('columns')",
                "ASSIGN = ASSIGN[[col for col in ASSIGN if ASSIGN[col].nunique() > 1]]",
                "ASSIGN = list(df)",
                "if len(ASSIGN) > 10:",
                "ASSIGN = ASSIGN[:10]",
                "ASSIGN = ASSIGN[columnNames]",
                "ASSIGN = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')",
                "ASSIGN = df.corr().values",
                "for i, j in zip(*plt.np.triu_indices_from(ASSIGN, k = 1)):",
                "ASSIGN[i, j].annotate('Corr. coef = %.3f' % ASSIGN[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)",
                "plt.suptitle('Scatter and Density Plot')",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Scatter and density plots\n",
                "def plotScatterMatrix(df, plotSize, textSize):\n",
                "    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n",
                "    # Remove rows and columns that would lead to df being singular\n",
                "    df = df.dropna('columns')\n",
                "    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
                "    columnNames = list(df)\n",
                "    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n",
                "        columnNames = columnNames[:10]\n",
                "    df = df[columnNames]\n",
                "    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n",
                "    corrs = df.corr().values\n",
                "    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n",
                "        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n",
                "    plt.suptitle('Scatter and Density Plot')\n",
                "    plt.show()\n"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(os.listdir())"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import matplotlib.pyplot as plt",
                "import seaborn as sns  # visualization tool",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "forest_data = pd.read_csv(\"../input/forest-area-of-land-area/forest_area.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "forest_data.head(20)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "forest_data.head(20)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "forest_data.info()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "forest_data.info()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data = pd.read_csv(\"../input/world-happiness/2015.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "happiness_data.info()",
                "happiness_data.head(5)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data.info()",
                "happiness_data.head(5)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = pd.concat([forest_data.iloc[:,:1],forest_data.iloc[:,-1]],axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "new_forest_data = pd.concat([forest_data.iloc[:,:1],forest_data.iloc[:,-1]],axis=1) # getting country name and 2015 forest rate."
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = happiness_data.merge(new_forest_data, left_on='Country',right_on='CountryName')",
                "ASSIGN.rename(columns={'2015': 'ForestRatio'}, inplace=True)",
                "ASSIGN.drop([\"CountryName\"], axis= 1, inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#merging happiness data and forest data, and then rename 2015 column to ForestRatio and Drop CountryName column which is dublicate because of merging.",
                "result = happiness_data.merge(new_forest_data, left_on='Country',right_on='CountryName') ",
                "result.rename(columns={'2015': 'ForestRatio'}, inplace=True)",
                "result.drop([\"CountryName\"], axis= 1, inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "result.head(20)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "result.head(20)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = result[pd.notnull(result.ForestRatio) & result.ForestRatio > 0]",
                "ASSIGN.columns = [each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in ASSIGN.columns]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "main_data = result[pd.notnull(result.ForestRatio) & result.ForestRatio > 0] #ForestRatio should be not null and greater than 0",
                "main_data.columns = [each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in main_data.columns]"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "main_data.corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "main_data.corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(x = main_data.HappinessScore, y = main_data.ForestRatio)",
                "plt.xlabel(\"Happiness Score\")",
                "plt.ylabel(\"Forest Ratio\")",
                "plt.title(\"Happiness Score vs Forest Ratio\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(x = main_data.HappinessScore, y = main_data.ForestRatio)",
                "plt.xlabel(\"Happiness Score\")",
                "plt.ylabel(\"Forest Ratio\")",
                "plt.title(\"Happiness Score vs Forest Ratio\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = main_data.groupby([\"Region\"]).mean()",
                "ASSIGN.corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#ok lets start to change our perspective",
                "new_data = main_data.groupby([\"Region\"]).mean()",
                "new_data.corr()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "new_data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "new_data"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(x = new_data.ForestRatio, y = new_data.Freedom)",
                "plt.xlabel(\"Forest Ratio\")",
                "plt.ylabel(\"Freedom\")",
                "plt.title(\"Forest Ratio vs Freedom\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "plt.scatter(x = new_data.ForestRatio, y = new_data.Freedom)",
                "plt.xlabel(\"Forest Ratio\")",
                "plt.ylabel(\"Freedom\")",
                "plt.title(\"Forest Ratio vs Freedom\")"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "happiness_data.columns = [ each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in happiness_data.columns ]",
                "happiness_data.columns"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data.columns = [ each.replace(\" \",\"\").replace(\"(\",\"_\").replace(\")\",\"\") for each in happiness_data.columns ]",
                "happiness_data.columns"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "happiness_data.corr()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data.corr()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(figsize=(15, 15))",
                "sns.heatmap(happiness_data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#correlation map",
                "f,ax = plt.subplots(figsize=(15, 15))",
                "sns.heatmap(happiness_data.corr(), annot=True, linewidths=.5, fmt= '.2f',ax=ax)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "happiness_data.Family.plot(color = 'r',label = 'Family',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Economy_GDPperCapita.plot(color = 'orange',label = 'Economy_GDPperCapita',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Health_LifeExpectancy.plot(color = 'yellow',label = 'Health (Life Expectancy)',linewidth=1, alpha = 1.0,grid = True)",
                "happiness_data.Freedom.plot(color = 'g',label = 'Freedom',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Trust_GovernmentCorruption.plot(color = 'black',label = 'Trust (Government Corruption)',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Generosity.plot(color = 'b',label = 'Generosity',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.DystopiaResidual.plot(color = 'gray',label = 'Dystopia Residual',linewidth=1, alpha = 0.9,grid = True)",
                "plt.legend()",
                "plt.xlabel('Happiness Rank')",
                "plt.ylabel('Score')",
                "plt.title('Happiness Factors Line Plot Graph')",
                "ASSIGN = plt.gcf()",
                "ASSIGN.set_size_inches(18.5, 10.5, forward=True)",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data.Family.plot(color = 'r',label = 'Family',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Economy_GDPperCapita.plot(color = 'orange',label = 'Economy_GDPperCapita',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Health_LifeExpectancy.plot(color = 'yellow',label = 'Health (Life Expectancy)',linewidth=1, alpha = 1.0,grid = True)",
                "happiness_data.Freedom.plot(color = 'g',label = 'Freedom',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Trust_GovernmentCorruption.plot(color = 'black',label = 'Trust (Government Corruption)',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.Generosity.plot(color = 'b',label = 'Generosity',linewidth=1, alpha = 0.9,grid = True)",
                "happiness_data.DystopiaResidual.plot(color = 'gray',label = 'Dystopia Residual',linewidth=1, alpha = 0.9,grid = True)",
                "",
                "plt.legend() ",
                "plt.xlabel('Happiness Rank')",
                "plt.ylabel('Score')",
                "plt.title('Happiness Factors Line Plot Graph')",
                "fig = plt.gcf()",
                "fig.set_size_inches(18.5, 10.5, forward=True)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.plot(kind = \"scatter\", x= \"Family\", y = \"HappinessScore\", ax = axes[0][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Economy_GDPperCapita\", y = \"HappinessScore\", ax = axes[0][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"Health_LifeExpectancy\", y = \"HappinessScore\", ax = axes[0][2])",
                "happiness_data.plot(kind = \"scatter\", x= \"Freedom\", y = \"HappinessScore\", ax = axes[0][3])",
                "happiness_data.plot(kind = \"scatter\", x= \"Trust_GovernmentCorruption\", y = \"HappinessScore\", ax = axes[1][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Generosity\", y = \"HappinessScore\", ax = axes[1][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"DystopiaResidual\", y = \"HappinessScore\", ax = axes[1][2])",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.plot(kind = \"scatter\", x= \"Family\", y = \"HappinessScore\", ax = axes[0][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Economy_GDPperCapita\", y = \"HappinessScore\", ax = axes[0][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"Health_LifeExpectancy\", y = \"HappinessScore\", ax = axes[0][2])",
                "happiness_data.plot(kind = \"scatter\", x= \"Freedom\", y = \"HappinessScore\", ax = axes[0][3])",
                "happiness_data.plot(kind = \"scatter\", x= \"Trust_GovernmentCorruption\", y = \"HappinessScore\", ax = axes[1][0])",
                "happiness_data.plot(kind = \"scatter\", x= \"Generosity\", y = \"HappinessScore\", ax = axes[1][1])",
                "happiness_data.plot(kind = \"scatter\", x= \"DystopiaResidual\", y = \"HappinessScore\", ax = axes[1][2])",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.Family.plot(kind = 'hist',bins = 50, ax = axes[0][0])",
                "happiness_data.Economy_GDPperCapita.plot(kind = \"hist\", ax = axes[0][1])",
                "happiness_data.Health_LifeExpectancy.plot(kind = \"hist\", ax = axes[0][2])",
                "happiness_data.Freedom.plot(kind = \"hist\", ax = axes[0][3])",
                "happiness_data.Trust_GovernmentCorruption.plot(kind = \"hist\", ax = axes[1][0])",
                "happiness_data.Generosity.plot(kind = \"hist\", ax = axes[1][1])",
                "happiness_data.DystopiaResidual.plot(kind = \"hist\", ax = axes[1][2])",
                "plt.show()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "fig, axes = plt.subplots(nrows = 2, ncols = 4, figsize=(30, 15))",
                "happiness_data.Family.plot(kind = 'hist',bins = 50, ax = axes[0][0])",
                "happiness_data.Economy_GDPperCapita.plot(kind = \"hist\", ax = axes[0][1])",
                "happiness_data.Health_LifeExpectancy.plot(kind = \"hist\", ax = axes[0][2])",
                "happiness_data.Freedom.plot(kind = \"hist\", ax = axes[0][3])",
                "happiness_data.Trust_GovernmentCorruption.plot(kind = \"hist\", ax = axes[1][0])",
                "happiness_data.Generosity.plot(kind = \"hist\", ax = axes[1][1])",
                "happiness_data.DystopiaResidual.plot(kind = \"hist\",  ax = axes[1][2])",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "happiness_data.boxplot(column='HappinessScore',by = 'Region', figsize=(30, 15))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data.boxplot(column='HappinessScore',by = 'Region', figsize=(30, 15))"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "happiness_data[\"HappinessDegree\"] = ['Happy' if each > 6 else 'Normal' if each > 5 else 'Unhappy' for each in happiness_data.HappinessScore ]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "happiness_data[\"HappinessDegree\"] = ['Happy' if each > 6 else 'Normal' if each > 5 else 'Unhappy' for each in happiness_data.HappinessScore ]"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = happiness_data.pivot_table( index=['Region'], columns = \"HappinessDegree\", values = \"HappinessRank\",aggfunc='count')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pivot_data = happiness_data.pivot_table( index=['Region'], columns = \"HappinessDegree\", values = \"HappinessRank\",aggfunc='count')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "pivot_data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "pivot_data"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "class SimpleLinearRegression:",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "ASSIGN = 0",
                "def fit(self, x_train, y_train):",
                "ASSIGN = sum(x_train)",
                "ASSIGN = sum(y_train)",
                "ASSIGN = np.sum(np.square(x_train))",
                "ASSIGN = np.sum(np.square(y_train))",
                "ASSIGN = np.dot(x_train,y_train)",
                "ASSIGN = len(x_train)",
                "ASSIGN = sum_of_x2 - sum_of_x * sum_of_xpath",
                "ASSIGN = sum_of_y2 - sum_of_y * sum_of_ypath",
                "ASSIGN = length * dotproduct - sum_of_x * sum_of_y",
                "ASSIGN = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y))",
                "ASSIGN = dotproduct - sum_of_x * sum_of_y path",
                "self.ASSIGN = np.square(ASSIGN path(ASSIGN))",
                "self.ASSIGN = ASSIGN path((ASSIGN path) * sum_of_xpath)",
                "self.ASSIGN = ASSIGN path",
                "def predict(self,x_test):",
                "return x_test * self.coef + self.intercept"
            ],
            "output_type": "not_existent",
            "content_old": [
                "class SimpleLinearRegression:",
                "    coef = 0",
                "    intercept = 0",
                "    rsquared = 0",
                "    def fit(self, x_train, y_train):",
                "        sum_of_x = sum(x_train)",
                "        sum_of_y = sum(y_train)",
                "        sum_of_x2 = np.sum(np.square(x_train))",
                "        sum_of_y2 = np.sum(np.square(y_train))",
                "        dotproduct = np.dot(x_train,y_train)",
                "        length = len(x_train)",
                "        dif_x = sum_of_x2 - sum_of_x * sum_of_x/length",
                "        dif_y = sum_of_y2 - sum_of_y * sum_of_y/length",
                "        numerator = length * dotproduct - sum_of_x * sum_of_y",
                "        denom = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y))",
                "        co = dotproduct - sum_of_x * sum_of_y / length",
                "        self.rsquared = np.square(numerator / np.sqrt(denom))",
                "        self.intercept = sum_of_y / length - ((co / dif_x) * sum_of_x/length)",
                "        self.coef = co / dif_x",
                "    def predict(self,x_test):",
                "        return x_test * self.coef + self.intercept",
                "        "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = np.array([ 1, 2, 3, 4])",
                "ASSIGN = np.array([ 2, 3, 4, 4])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "x_train = np.array([ 1, 2, 3, 4])",
                "y_train = np.array([ 2, 3, 4, 4])"
            ]
        },
        {
            "tags": [
                "train_model"
            ],
            "content": [
                "ASSIGN = SimpleLinearRegression()",
                "ASSIGN.fit(x_train,y_train)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "slr = SimpleLinearRegression()",
                "slr.fit(x_train,y_train)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(, slr.coef)",
                "print('Y-Intercept:',slr.intercept)",
                "print('R-Squared:',slr.rsquared)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "print(\"Coefficient:\", slr.coef)",
                "print('Y-Intercept:',slr.intercept)",
                "print('R-Squared:',slr.rsquared)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(x_train.reshape(-1,1), y_train.reshape(-1,1))",
                "print(ASSIGN.coef_)",
                "print(ASSIGN.intercept_)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.linear_model import LinearRegression",
                "lr = LinearRegression()",
                "lr.fit(x_train.reshape(-1,1), y_train.reshape(-1,1))",
                "print(lr.coef_)",
                "print(lr.intercept_)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn import metrics \n",
                "from sklearn.metrics import r2_score\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sales_data = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sales_data.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sales_data.head()"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "itemcategories_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n",
                "items_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n",
                "shops_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n",
                "test_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "itemcategories_data.info()",
                "itemcategories_data.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "itemcategories_data.info()\n",
                "itemcategories_data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "items_data.info()",
                "items_data.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "items_data.info()\n",
                "items_data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "shops_data.info()",
                "shops_data.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "shops_data.info()\n",
                "shops_data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "test_data.info()",
                "test_data.head()"
            ],
            "output_type": "stream",
            "content_old": [
                "test_data.info()\n",
                "test_data.head()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "shops_data.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "shops_data.isnull().sum()\n"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "sales_data.isnull().sum()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sales_data.isnull().sum()\n"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,4))",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,4))\n",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN[ASSIGN.item_price<45000]",
                "ASSIGN = ASSIGN[ASSIGN.item_cnt_day<600]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sales_data = sales_data[sales_data.item_price<45000]\n",
                "sales_data = sales_data[sales_data.item_cnt_day<600]"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(10,4))",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(10,4))\n",
                "sns.scatterplot(x=sales_data.item_cnt_day, y=sales_data.item_price, data=sales_data)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "ASSIGN = sales_data",
                "ASSIGN['month'] = pd.DatetimeIndex(ASSIGN['date']).month",
                "ASSIGN['year'] = pd.DatetimeIndex(ASSIGN['date']).year",
                "ASSIGN.head(10)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sales_train_sub = sales_data\n",
                "sales_train_sub['month'] = pd.DatetimeIndex(sales_train_sub['date']).month\n",
                "sales_train_sub['year'] = pd.DatetimeIndex(sales_train_sub['date']).year\n",
                "sales_train_sub.head(10)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "sats_grup = sales_train_sub.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg('sum').reset_index()",
                "ASSIGN=sats_grup.iloc[:,:-1]",
                "ASSIGN=sats_grup.iloc[:,-1:]",
                "ASSIGN = train_test_split(x,y,test_size=0.25, random_state=0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "sats_grup = sales_train_sub.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"item_cnt_day\"].agg('sum').reset_index()\n",
                "\n",
                "x=sats_grup.iloc[:,:-1]\n",
                "y=sats_grup.iloc[:,-1:]\n",
                "x_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "x"
            ],
            "output_type": "execute_result",
            "content_old": [
                "x"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "ASSIGN = ExtraTreesRegressor(n_estimators=25,random_state=16)",
                "ASSIGN.fit(x_train,y_train.values.ravel())",
                "ASSIGN = etr.predict(x_test)",
                "print(,r2_score(y_test,ASSIGN))",
                "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ASSIGN))",
                "print('Root Mean Squared Error:', metrics.mean_squared_error(y_test, ASSIGN, squared=False))"
            ],
            "output_type": "stream",
            "content_old": [
                "from sklearn.ensemble import ExtraTreesRegressor\n",
                "etr = ExtraTreesRegressor(n_estimators=25,random_state=16)\n",
                "etr.fit(x_train,y_train.values.ravel())\n",
                "y_pred = etr.predict(x_test)\n",
                "\n",
                "\n",
                "print(\"R2 Score:\",r2_score(y_test,y_pred))\n",
                "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
                "print('Root Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred, squared=False))"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "print(check_output([, ]).decode())"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
                "\n",
                "from subprocess import check_output\n",
                "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "binder.bind(globals())",
                "print()"
            ],
            "output_type": "stream",
            "content_old": [
                "# Set up feedack system\n",
                "from learntools.core import binder\n",
                "binder.bind(globals())\n",
                "from learntools.sql.ex1 import *\n",
                "print(\"Setup Complete\")"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = bigquery.Client()",
                "ASSIGN = client.dataset(\"chicago_crime\", project=\"bigquery-public-data\")",
                "ASSIGN = client.get_dataset(dataset_ref)"
            ],
            "output_type": "stream",
            "content_old": [
                "from google.cloud import bigquery\n",
                "\n",
                "# Create a \"Client\" object\n",
                "client = bigquery.Client()\n",
                "\n",
                "# Construct a reference to the \"chicago_crime\" dataset\n",
                "dataset_ref = client.dataset(\"chicago_crime\", project=\"bigquery-public-data\")\n",
                "\n",
                "# API request - fetch the dataset\n",
                "dataset = client.get_dataset(dataset_ref)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = list(client.list_tables(dataset))"
            ],
            "output_type": "not_existent",
            "content_old": [
                "# Write the code you need here to figure out the answer\n",
                "tables = list(client.list_tables(dataset))"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=client.get_table(dataset_ref.ASSIGN(\"crime\"))",
                "table.schema"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Write the code to figure out the answer\n",
                "table=client.get_table(dataset_ref.table(\"crime\"))\n",
                "table.schema"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "client.list_rows(table, max_results=5).to_dataframe()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# Write the code here to explore the data so you can find the answer\n",
                "client.list_rows(table, max_results=5).to_dataframe()"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load in \n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the \"../input/\" directory.\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# Any results you write to the current directory are saved as output."
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN=pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import matplotlib.pyplot as plt\n",
                "df=pd.read_csv('../input/boston-housing-dataset/HousingData.csv')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(df)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(df)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                " df.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                " df.dtypes"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.astype('float64')",
                "ASSIGN = ASSIGN.astype('float64')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df['RAD'] = df['RAD'].astype('float64')\n",
                "df['TAX'] = df['TAX'].astype('float64')"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                " df.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().any()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.isnull().any()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "df.dropna(inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df.dropna(inplace=True)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df.isnull().any()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.isnull().any()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "df['CHAS'].value_counts(dropna=False)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df['CHAS'].value_counts(dropna=False)"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = df[['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','CHAS']]",
                "print(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "df1 = df[['CRIM','ZN','INDUS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV','CHAS']]\n",
                "print(df1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = ASSIGN.dropna()"
            ],
            "output_type": "not_existent",
            "content_old": [
                "df1 = df1.dropna()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df.shape"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = df1.iloc[:, :-1].values",
                "ASSIGN = df1.iloc[:,13].values"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X = df1.iloc[:, :-1].values\n",
                "y = df1.iloc[:,13].values"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "df1.dtypes"
            ],
            "output_type": "execute_result",
            "content_old": [
                "df1.dtypes"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y.shape"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.model_selection import train_test_split\n",
                "X_train,X_test,y_train,y_test =train_test_split(X,y,test_size =0.2,random_state =0)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "X_train"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_train"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "y_train"
            ],
            "output_type": "execute_result",
            "content_old": [
                "y_train"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model"
            ],
            "content": [
                "SETUP",
                "ASSIGN = LinearRegression()",
                "ASSIGN.fit(X_train,y_train)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.linear_model import LinearRegression\n",
                "regressor = LinearRegression()\n",
                "regressor.fit(X_train,y_train)"
            ]
        },
        {
            "tags": [
                "evaluate_model"
            ],
            "content": [
                "ASSIGN =regressor.predict(X_test)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y_pred =regressor.predict(X_test)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = np.append(arr=np.ones((394,1)).astype(int),values=ASSIGN,axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import statsmodels.formula.api as sm\n",
                "X = np.append(arr=np.ones((394,1)).astype(int),values=X,axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(X)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(X)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import statsmodels.api as sm"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN = X[:,:13]",
                "ASSIGN=sm.OLS(endog=y,exog=X_opt).fit()",
                "ASSIGN.summary()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_opt = X[:,:13] \n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN =X[:,[1,3,5,7,8,9,10,11,12]]",
                "ASSIGN=sm.OLS(endog=y,exog=X_opt).fit()",
                "ASSIGN.summary()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_opt =X[:,[1,3,5,7,8,9,10,11,12]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN =X[:,[1,3,5,7,8,9,10,11]]",
                "ASSIGN=sm.OLS(endog=y,exog=X_opt).fit()",
                "ASSIGN.summary()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_opt =X[:,[1,3,5,7,8,9,10,11]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN =X[:,[3,5,8,9,10,11]]",
                "ASSIGN=sm.OLS(endog=y,exog=X_opt).fit()",
                "ASSIGN.summary()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X_opt =X[:,[3,5,8,9,10,11]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ]
        },
        {
            "tags": [
                "train_model",
                "validate_data"
            ],
            "content": [
                "ASSIGN =X[:,[0,1,4,10,14]]",
                "ASSIGN=sm.OLS(endog=y,exog=X_opt).fit()",
                "ASSIGN.summary()"
            ],
            "output_type": "error",
            "content_old": [
                "X_opt =X[:,[0,1,4,10,14]]\n",
                "regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()\n",
                "regressor_OLS.summary()"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "from sklearn.model_selection import KFold\n"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "train_data = pd.read_csv('../input/train.csv')\n",
                "test_data = pd.read_csv('../input/test.csv')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = train_data.columns[2:]",
                "ASSIGN = []",
                "ASSIGN = []",
                "for dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]):",
                "ASSIGN == object:",
                "ASSIGN.append(feature)",
                "else:",
                "ASSIGN.append(feature)",
                "categorical_features"
            ],
            "output_type": "execute_result",
            "content_old": [
                "features = train_data.columns[2:]\n",
                "\n",
                "numeric_features = []\n",
                "categorical_features = []\n",
                "\n",
                "for dtype, feature in zip(train_data.dtypes[2:], train_data.columns[2:]):\n",
                "    if dtype == object:\n",
                "        #print(column)\n",
                "        #print(train_data[column].describe())\n",
                "        categorical_features.append(feature)\n",
                "    else:\n",
                "        numeric_features.append(feature)\n",
                "categorical_features"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "np.random.seed(13)",
                "def impact_coding(data, feature, target='y'):",
                "'''",
                "In this implementation we get the values and the dictionary as two different steps.",
                "This is just because initially we were ignoring the dictionary as a result variable.",
                "In this implementation the KFolds use shuffling. If you want reproducibility the cv",
                "could be moved to a parameter.",
                "'''",
                "ASSIGN = 20",
                "ASSIGN = 10",
                "ASSIGN = pd.Series()",
                "ASSIGN = data[target].mean()",
                "ASSIGN = KFold(n_splits=n_folds, shuffle=True)",
                "ASSIGN = pd.DataFrame()",
                "ASSIGN = 0",
                "for infold, oof in ASSIGN.ASSIGN(data[feature]):",
                "ASSIGN = pd.Series()",
                "ASSIGN = KFold(n_splits=n_inner_folds, shuffle=True)",
                "ASSIGN = 0",
                "ASSIGN = pd.DataFrame()",
                "ASSIGN = data.iloc[infold][target].mean()",
                "for infold_inner, oof_inner in ASSIGN.ASSIGN(data.iloc[infold]):",
                "ASSIGN = data.iloc[infold_inner].groupby(by=feature)[target].mean()",
                "ASSIGN = ASSIGN.append(data.iloc[infold].apply(",
                "lambda x: ASSIGN[x[feature]]",
                "if x[feature] in oof_mean.index",
                "else oof_default_inner_mean",
                ", axis=1))",
                "ASSIGN = ASSIGN.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')",
                "ASSIGN.fillna(value=ASSIGN, inplace=True)",
                "ASSIGN += 1",
                "ASSIGN = ASSIGN.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')",
                "ASSIGN.fillna(value=ASSIGN, inplace=True)",
                "ASSIGN += 1",
                "ASSIGN = ASSIGN.append(data.iloc[oof].apply(",
                "lambda x: ASSIGN.loc[x[feature]].mean()",
                "if x[feature] in inner_oof_mean_cv.index",
                "else oof_default_mean",
                ", axis=1))",
                "return ASSIGN, ASSIGN.mean(axis=1), ASSIGN",
                "ASSIGN = {}",
                "for f in categorical_features:",
                "print(.format(f))",
                "train_data[\"impact_encoded_{}\".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)",
                "ASSIGN[f] = (impact_coding_mapping, default_coding)",
                "ASSIGN = impact_coding_map[f]",
                "test_data[\"impact_encoded_{}\".format(f)] = test_data.apply(lambda x: mapping[x[f]]",
                "if x[f] in mapping",
                "else default_mean",
                ", axis=1)"
            ],
            "output_type": "stream",
            "content_old": [
                "# This way we have randomness and are able to reproduce the behaviour within this cell.\n",
                "np.random.seed(13)\n",
                "\n",
                "def impact_coding(data, feature, target='y'):\n",
                "    '''\n",
                "    In this implementation we get the values and the dictionary as two different steps.\n",
                "    This is just because initially we were ignoring the dictionary as a result variable.\n",
                "    \n",
                "    In this implementation the KFolds use shuffling. If you want reproducibility the cv \n",
                "    could be moved to a parameter.\n",
                "    '''\n",
                "    n_folds = 20\n",
                "    n_inner_folds = 10\n",
                "    impact_coded = pd.Series()\n",
                "    \n",
                "    oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)\n",
                "    kf = KFold(n_splits=n_folds, shuffle=True)\n",
                "    oof_mean_cv = pd.DataFrame()\n",
                "    split = 0\n",
                "    for infold, oof in kf.split(data[feature]):\n",
                "            impact_coded_cv = pd.Series()\n",
                "            kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)\n",
                "            inner_split = 0\n",
                "            inner_oof_mean_cv = pd.DataFrame()\n",
                "            oof_default_inner_mean = data.iloc[infold][target].mean()\n",
                "            for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):\n",
                "                # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)\n",
                "                oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()\n",
                "                impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(\n",
                "                            lambda x: oof_mean[x[feature]]\n",
                "                                      if x[feature] in oof_mean.index\n",
                "                                      else oof_default_inner_mean\n",
                "                            , axis=1))\n",
                "\n",
                "                # Also populate mapping (this has all group -> mean for all inner CV folds)\n",
                "                inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')\n",
                "                inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)\n",
                "                inner_split += 1\n",
                "\n",
                "            # Also populate mapping\n",
                "            oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')\n",
                "            oof_mean_cv.fillna(value=oof_default_mean, inplace=True)\n",
                "            split += 1\n",
                "            \n",
                "            impact_coded = impact_coded.append(data.iloc[oof].apply(\n",
                "                            lambda x: inner_oof_mean_cv.loc[x[feature]].mean()\n",
                "                                      if x[feature] in inner_oof_mean_cv.index\n",
                "                                      else oof_default_mean\n",
                "                            , axis=1))\n",
                "\n",
                "    return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean\n",
                "\n",
                "# Apply the encoding to training and test data, and preserve the mapping\n",
                "impact_coding_map = {}\n",
                "for f in categorical_features:\n",
                "    print(\"Impact coding for {}\".format(f))\n",
                "    train_data[\"impact_encoded_{}\".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)\n",
                "    impact_coding_map[f] = (impact_coding_mapping, default_coding)\n",
                "    mapping, default_mean = impact_coding_map[f]\n",
                "    test_data[\"impact_encoded_{}\".format(f)] = test_data.apply(lambda x: mapping[x[f]]\n",
                "                                                                         if x[f] in mapping\n",
                "                                                                         else default_mean\n",
                "                                                               , axis=1)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "train_data[['y', 'X0'] + list(train_data.columns[-8:])]"
            ],
            "output_type": "execute_result",
            "content_old": [
                "train_data[['y', 'X0'] + list(train_data.columns[-8:])]"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "InteractiveShell.ast_node_interactivity = \"all\"",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "#STEP 1: Get right arrows in quiver\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import plotly.express as px\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler,normalize\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.mixture import GaussianMixture as GMM\n",
                "from sklearn.manifold import TSNE\n",
                "\n",
                "import warnings #To hide warnings\n",
                "\n",
                "#1.1: Set the stage\n",
                "from IPython.core.interactiveshell import InteractiveShell\n",
                "from IPython.core.display import display, HTML\n",
                "\n",
                "InteractiveShell.ast_node_interactivity = \"all\"\n",
                "warnings.filterwarnings(\"ignore\")"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data",
                "transfer_results",
                "validate_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.info()",
                "cc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T",
                "ASSIGN.sample(5)",
                "ASSIGN.describe()"
            ],
            "output_type": "stream",
            "content_old": [
                "cc = pd.read_csv('/kaggle/input/ccdata/CC GENERAL.csv')\n",
                "cc.info()\n",
                "cc[cc.columns[cc.isna().any()]].isna().sum().to_frame().T\n",
                "cc.sample(5)\n",
                "cc.describe()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "cc.quantile([0.75,0.8,.85,.9,.95,1])"
            ],
            "output_type": "execute_result",
            "content_old": [
                "cc.quantile([0.75,0.8,.85,.9,.95,1])"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))",
                "+' customers in the list who have more balance than the credit limit assigned. '",
                "+'It may be due to more payament than usage andpath<path>'))"
            ],
            "output_type": "display_data",
            "content_old": [
                "display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))\n",
                "             +' customers in the list who have more balance than the credit limit assigned. '\n",
                "             +'It may be due to more payament than usage and/or continuous pre-payment.</h4>'))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True)",
                "sns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "cc.rename(columns = {col:col.lower() for col in cc.columns.values},inplace=True)\n",
                "sns.jointplot(cc.credit_limit,cc.minimum_payments,kind = 'kde', dropna=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "cc.fillna(cc.median(),inplace=True)",
                "ASSIGN = cc.cust_id",
                "cc.drop(columns = ['cust_id'],inplace=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "cc.fillna(cc.median(),inplace=True) #More outliers thus median in both cases\n",
                "cust = cc.cust_id\n",
                "cc.drop(columns = ['cust_id'],inplace=True)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = StandardScaler()",
                "X= normalize(ASSIGN.fit_transform(cc.copy()))",
                "ASSIGN = pd.DataFrame(ASSIGN,columns=cc.columns.values)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "ss = StandardScaler()\n",
                "X= normalize(ss.fit_transform(cc.copy()))\n",
                "X = pd.DataFrame(X,columns=cc.columns.values)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = plt.subplots(6,3, figsize=(20, 20))",
                "for i in range(17):",
                "ASSIGN = sns.distplot(cc[cc.columns[i]], ax=axs[ipath,i%3],kde_kws = {'bw':2})",
                "ASSIGN = sns.despine()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "fig, axs = plt.subplots(6,3, figsize=(20, 20))\n",
                "for i in range(17):\n",
                "        p = sns.distplot(cc[cc.columns[i]], ax=axs[i//3,i%3],kde_kws = {'bw':2})\n",
                "        p = sns.despine()\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.figure(figsize=(16,12))",
                "ASSIGN = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(\"Correlation of credit card data\\'s features\",fontsize=20)",
                "plt.show()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.figure(figsize=(16,12))\n",
                "p = sns.heatmap(cc.corr(),annot=True,cmap='jet').set_title(\"Correlation of credit card data\\'s features\",fontsize=20)\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "train_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = [GMM(n,random_state=0).fit(X) for n in range(1,12)]",
                "ASSIGN = pd.DataFrame({'BIC Score':[m.bic(X) for m in models],",
                "'AIC Score': [m.aic(X) for m in ASSIGN]},index=np.arange(1,12))",
                "ASSIGN.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "#Selecting correct number of components for GMM\n",
                "models = [GMM(n,random_state=0).fit(X) for n in range(1,12)]\n",
                "d = pd.DataFrame({'BIC Score':[m.bic(X) for m in models],\n",
                "                  'AIC Score': [m.aic(X) for m in models]},index=np.arange(1,12))\n",
                "d.plot(use_index=True,title='AIC and BIC Scores for GMM wrt n_Compnents',figsize = (10,5),fontsize=12)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "SETUP",
                "class GMClusters(GMM, ClusterMixin):",
                "def __init__(self, n_clusters=1, **kwargs):",
                "kwargs[\"n_components\"] = n_clusters",
                "kwargs['covariance_type'] = 'full'",
                "super(GMClusters, self).__init__(**kwargs)",
                "def fit(self, X):",
                "super(GMClusters, self).fit(X)",
                "self.labels_ = self.predict(X)",
                "return self",
                "ASSIGN = KElbow(GMClusters(), k=(2,12), force_model=True)",
                "ASSIGN.fit(X)",
                "ASSIGN.show()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "from sklearn.base import ClusterMixin\n",
                "from yellowbrick.cluster import KElbow\n",
                "\n",
                "class GMClusters(GMM, ClusterMixin):\n",
                "\n",
                "    def __init__(self, n_clusters=1, **kwargs):\n",
                "        kwargs[\"n_components\"] = n_clusters\n",
                "        kwargs['covariance_type'] = 'full'\n",
                "        super(GMClusters, self).__init__(**kwargs)\n",
                "\n",
                "    def fit(self, X):\n",
                "        super(GMClusters, self).fit(X)\n",
                "        self.labels_ = self.predict(X)\n",
                "        return self \n",
                "\n",
                "oz = KElbow(GMClusters(), k=(2,12), force_model=True)\n",
                "oz.fit(X)\n",
                "oz.show()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN= models[6]",
                "ASSIGN.n_init = 10",
                "model"
            ],
            "output_type": "execute_result",
            "content_old": [
                "model= models[6]\n",
                "model.n_init = 10\n",
                "model"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "visualize_data"
            ],
            "content": [
                "ASSIGN = model.fit_predict(X)",
                "display(HTML('<b>The model has converged :<path>'+str(model.converged_)))",
                "display(HTML('<b>The model has taken iterations :<path>'+str(model.n_iter_)))"
            ],
            "output_type": "display_data",
            "content_old": [
                "clusters = model.fit_predict(X)\n",
                "display(HTML('<b>The model has converged :</b>'+str(model.converged_)))\n",
                "display(HTML('<b>The model has taken iterations :</b>'+str(model.n_iter_)))"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "sns.countplot(clusters).set_title('Cluster sizes',fontsize=20)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = cc.copy()",
                "ASSIGN['cluster']=clusters",
                "for c in ASSIGN:",
                "if c != 'cluster':",
                "ASSIGN= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False)",
                "ASSIGN = grid.map(sns.distplot, c,kde_kws = {'bw':2})",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "cc1 = cc.copy()\n",
                "cc1['cluster']=clusters\n",
                "for c in cc1:\n",
                "    if c != 'cluster':\n",
                "        grid= sns.FacetGrid(cc1, col='cluster',sharex=False,sharey=False)\n",
                "        p = grid.map(sns.distplot, c,kde_kws = {'bw':2})\n",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "validate_data"
            ],
            "content": [
                "for i in range(7):",
                "display(HTML('<h2>Cluster'+str(i)+'<path>'))",
                "cc1[cc1.cluster == i].describe()"
            ],
            "output_type": "display_data",
            "content_old": [
                "#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T\n",
                "for i in range(7):\n",
                "    display(HTML('<h2>Cluster'+str(i)+'</h2>'))\n",
                "    cc1[cc1.cluster == i].describe()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = TSNE(n_components = 2)",
                "ASSIGN = tsne.fit_transform(X.copy())",
                "plt.scatter(ASSIGN[:, 0], ASSIGN[:, 1],",
                "ASSIGN=10,",
                "ASSIGN=10,",
                "ASSIGN=5,",
                "ASSIGN=clusters",
                ")"
            ],
            "output_type": "execute_result",
            "content_old": [
                "tsne = TSNE(n_components = 2)\n",
                "tsne_out = tsne.fit_transform(X.copy())\n",
                "\n",
                "plt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n",
                "            marker=10,\n",
                "            s=10,              # marker size\n",
                "            linewidths=5,      # linewidth of marker edges\n",
                "            c=clusters   # Colour as per gmm\n",
                "            )"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = model.score_samples(X)",
                "ASSIGN = np.percentile(density,4)",
                "cc1['cluster']=clusters",
                "cc1['Anamoly'] = ASSIGN<ASSIGN",
                "cc1"
            ],
            "output_type": "execute_result",
            "content_old": [
                "density = model.score_samples(X)\n",
                "density_threshold = np.percentile(density,4)\n",
                "cc1['cluster']=clusters\n",
                "cc1['Anamoly'] = density<density_threshold\n",
                "cc1"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = cc1.melt(['Anamoly'], var_name='cols', value_name='vals')",
                "ASSIGN = sns.FacetGrid(df, row='cols', hue=\"Anamoly\", palette=\"Set1\",sharey=False,sharex=False,aspect=3)",
                "ASSIGN = (ASSIGN.map(sns.distplot, \"vals\", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())"
            ],
            "output_type": "display_data",
            "content_old": [
                "df = cc1.melt(['Anamoly'], var_name='cols',  value_name='vals')\n",
                "\n",
                "g = sns.FacetGrid(df, row='cols', hue=\"Anamoly\", palette=\"Set1\",sharey=False,sharex=False,aspect=3)\n",
                "g = (g.map(sns.distplot, \"vals\", hist=True, rug=True,kde_kws = {'bw':2}).add_legend())"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = X[density>=density_threshold]",
                "ASSIGN = clusters[density>=density_threshold]",
                "ASSIGN = TSNE(n_components = 2)",
                "ASSIGN = tsne.fit_transform(unanomaly)",
                "plt.figure(figsize=(15,10))",
                "plt.scatter(ASSIGN[:, 0], ASSIGN[:, 1],marker='x',s=10, linewidths=5, ASSIGN=ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "unanomaly = X[density>=density_threshold]\n",
                "c = clusters[density>=density_threshold]\n",
                "tsne = TSNE(n_components = 2)\n",
                "tsne_out = tsne.fit_transform(unanomaly)\n",
                "plt.figure(figsize=(15,10))\n",
                "plt.scatter(tsne_out[:, 0], tsne_out[:, 1],marker='x',s=10, linewidths=5, c=c)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "output_type": "stream",
            "content_old": [
                "# This Python 3 environment comes with many helpful analytics libraries installed\n",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
                "# For example, here's several helpful packages to load\n",
                "\n",
                "import numpy as np # linear algebra\n",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
                "\n",
                "# Input data files are available in the read-only \"../input/\" directory\n",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
                "\n",
                "import os\n",
                "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
                "    for filename in filenames:\n",
                "        print(os.path.join(dirname, filename))\n",
                "\n",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "filterwarnings('ignore')",
                "pd.set_option('display.max_columns', None)"
            ],
            "output_type": "stream",
            "content_old": [
                "!pip install ycimpute\n",
                "\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.preprocessing import LabelEncoder\n",
                "from sklearn import datasets, metrics, model_selection, svm\n",
                "import missingno as msno\n",
                "from ycimpute.imputer import iterforest,EM\n",
                "from fancyimpute import KNN\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "import numpy as np\n",
                "import pandas as pd \n",
                "import statsmodels.api as sm\n",
                "import statsmodels.formula.api as smf\n",
                "import seaborn as sns\n",
                "from sklearn.preprocessing import scale \n",
                "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
                "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
                "from sklearn.metrics import roc_auc_score,roc_curve\n",
                "import statsmodels.formula.api as smf\n",
                "import matplotlib.pyplot as plt\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from xgboost import XGBClassifier\n",
                "from lightgbm import LGBMClassifier\n",
                "from catboost import CatBoostClassifier\n",
                "\n",
                "from warnings import filterwarnings\n",
                "filterwarnings('ignore')\n",
                "\n",
                "pd.set_option('display.max_columns', None)\n",
                "import gc"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "VALIDATION",
                "def reduce_mem_usage2(df):",
                "\"\"\" iterate through all the columns of a dataframe and modify the data type",
                "to reduce memory usage.",
                "\"\"\"",
                "ASSIGN = df.memory_usage().sum() path**2",
                "print('Memory usage of dataframe is {:.2f} MB'.format(ASSIGN))",
                "for col in df.columns:",
                "ASSIGN = df[col].dtype",
                "if ASSIGN != object:",
                "ASSIGN = df[col].min()",
                "ASSIGN = df[col].max()",
                "if str(ASSIGN)[:3] == 'int':",
                "if ASSIGN > np.iinfo(np.int8).min and ASSIGN < np.iinfo(np.int8).max:",
                "ASSIGN = ASSIGN.astype(np.int8)",
                "elif ASSIGN > np.iinfo(np.int16).min and ASSIGN < np.iinfo(np.int16).max:",
                "ASSIGN = ASSIGN.astype(np.int16)",
                "elif ASSIGN > np.iinfo(np.int32).min and ASSIGN < np.iinfo(np.int32).max:",
                "ASSIGN = ASSIGN.astype(np.int32)",
                "elif ASSIGN > np.iinfo(np.int64).min and ASSIGN < np.iinfo(np.int64).max:",
                "ASSIGN = ASSIGN.astype(np.int64)",
                "else:",
                "if ASSIGN > np.finfo(np.float16).min and ASSIGN < np.finfo(np.float16).max:",
                "ASSIGN = ASSIGN.astype(np.float16)",
                "elif ASSIGN > np.finfo(np.float32).min and ASSIGN < np.finfo(np.float32).max:",
                "ASSIGN = ASSIGN.astype(np.float32)",
                "else:",
                "ASSIGN = ASSIGN.astype(np.float64)",
                "else:",
                "ASSIGN = ASSIGN.astype('category')",
                "ASSIGN = df.memory_usage().sum() path**2",
                "print('Memory usage after optimization is: {:.2f} MB'.format(ASSIGN))",
                "print('Decreased by {:.1f}%'.format(100 * (ASSIGN - ASSIGN) path))",
                "return df"
            ],
            "output_type": "stream",
            "content_old": [
                "%%time\n",
                "# From kernel https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
                "# WARNING! THIS CAN DAMAGE THE DATA \n",
                "def reduce_mem_usage2(df):\n",
                "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
                "        to reduce memory usage.        \n",
                "    \"\"\"\n",
                "    start_mem = df.memory_usage().sum() / 1024**2\n",
                "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
                "    \n",
                "    for col in df.columns:\n",
                "        col_type = df[col].dtype\n",
                "        \n",
                "        if col_type != object:\n",
                "            c_min = df[col].min()\n",
                "            c_max = df[col].max()\n",
                "            if str(col_type)[:3] == 'int':\n",
                "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
                "                    df[col] = df[col].astype(np.int8)\n",
                "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
                "                    df[col] = df[col].astype(np.int16)\n",
                "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
                "                    df[col] = df[col].astype(np.int32)\n",
                "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
                "                    df[col] = df[col].astype(np.int64)  \n",
                "            else:\n",
                "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
                "                    df[col] = df[col].astype(np.float16)\n",
                "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
                "                    df[col] = df[col].astype(np.float32)\n",
                "                else:\n",
                "                    df[col] = df[col].astype(np.float64)\n",
                "        else:\n",
                "            df[col] = df[col].astype('category')\n",
                "\n",
                "    end_mem = df.memory_usage().sum() / 1024**2\n",
                "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
                "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model"
            ],
            "content": [
                "ASSIGN=OrdinalEncoder()",
                "ASSIGN=KNN()",
                "def encode(data):",
                "'''function to encode non-null data and replace it in the original data'''",
                "ASSIGN = np.array(data.dropna())",
                "ASSIGN = nonulls.reshape(-1,1)",
                "ASSIGN = encoder.fit_transform(impute_reshape)",
                "data.loc[data.notnull()] = np.squeeze(ASSIGN)",
                "return data"
            ],
            "output_type": "not_existent",
            "content_old": [
                "encoder=OrdinalEncoder()\n",
                "imputer=KNN()\n",
                "\n",
                "def encode(data):\n",
                "    '''function to encode non-null data and replace it in the original data'''\n",
                "    #retains only non-null values\n",
                "    nonulls = np.array(data.dropna())\n",
                "    #reshapes the data for encoding\n",
                "    impute_reshape = nonulls.reshape(-1,1)\n",
                "    #encode date\n",
                "    impute_ordinal = encoder.fit_transform(impute_reshape)\n",
                "    #Assign back encoded values to non-null values\n",
                "    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n",
                "    return data"
            ]
        },
        {
            "tags": [
                "ingest_data"
            ],
            "content": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")",
                "ASSIGN = pd.read_csv(\"path\")"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ktest_identity = pd.read_csv('/kaggle/input/ieee-fraud-detection/test_identity.csv')\n",
                "Ktest_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/test_transaction.csv\")\n",
                "Ktrain_identity = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_identity.csv\")\n",
                "Ktrain_transaction = pd.read_csv(\"/kaggle/input/ieee-fraud-detection/train_transaction.csv\")"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= pd.merge(Ktrain_transaction, Ktrain_identity, on='TransactionID', how='left', left_index=True, right_index=True)",
                "ASSIGN= pd.merge(Ktest_transaction, Ktest_identity, on='TransactionID', how='left', left_index=True, right_index=True)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ktrain= pd.merge(Ktrain_transaction, Ktrain_identity, on='TransactionID', how='left', left_index=True, right_index=True)\n",
                "Ktest= pd.merge(Ktest_transaction, Ktest_identity, on='TransactionID', how='left', left_index=True, right_index=True)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=Ktrain.select_dtypes(include='object')",
                "ASSIGN =Ktest.select_dtypes(include='object')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ktrain_cat=Ktrain.select_dtypes(include='object')\n",
                "Ktest_cat =Ktest.select_dtypes(include='object')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain_cat1=Ktrain_cat.drop(['P_emaildomain','R_emaildomain','id_30','id_31','id_33','DeviceInfo'], axis=1)",
                "Ktest_cat1=Ktest_cat.drop(['P_emaildomain','R_emaildomain','id-30','id-31','id-33','DeviceInfo'], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ktrain_cat1=Ktrain_cat.drop(['P_emaildomain','R_emaildomain','id_30','id_31','id_33','DeviceInfo'], axis=1)\n",
                "Ktest_cat1=Ktest_cat.drop(['P_emaildomain','R_emaildomain','id-30','id-31','id-33','DeviceInfo'], axis=1)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in Ktrain_cat1:",
                "encode(Ktrain[i])",
                "for i in Ktest_cat1:",
                "encode(Ktest[i])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for i in Ktrain_cat1:\n",
                "    encode(Ktrain[i])\n",
                "for i in Ktest_cat1:\n",
                "    encode(Ktest[i])"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "Ktrain_cat2=pd.concat([Ktrain['P_emaildomain'],Ktrain['R_emaildomain'],Ktrain['id_30'],Ktrain['id_31'],Ktrain['id_33'],Ktrain['DeviceInfo']], axis=1)",
                "Ktest_cat2=pd.concat([Ktest['P_emaildomain'],Ktest['R_emaildomain'],Ktest['id-30'],Ktest['id-31'],Ktest['id-33'],Ktest['DeviceInfo']], axis=1)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "Ktrain_cat2=pd.concat([Ktrain['P_emaildomain'],Ktrain['R_emaildomain'],Ktrain['id_30'],Ktrain['id_31'],Ktrain['id_33'],Ktrain['DeviceInfo']], axis=1)\n",
                "Ktest_cat2=pd.concat([Ktest['P_emaildomain'],Ktest['R_emaildomain'],Ktest['id-30'],Ktest['id-31'],Ktest['id-33'],Ktest['DeviceInfo']], axis=1)\n"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for i in Ktrain_cat2:",
                "encode(Ktrain[i])",
                "for i in Ktest_cat2:",
                "encode(Ktest[i])"
            ],
            "output_type": "not_existent",
            "content_old": [
                "for i in Ktrain_cat2:\n",
                "    encode(Ktrain[i])\n",
                "for i in Ktest_cat2:\n",
                "    encode(Ktest[i])"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP",
                "del Ktest_identity",
                "del Ktest_transaction",
                "del Ktrain_identity",
                "del Ktrain_transaction",
                "del Ktrain_cat1",
                "del Ktest_cat1",
                "del Ktrain_cat2",
                "del Ktest_cat2",
                "gc.collect()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "import gc\n",
                "del Ktest_identity\n",
                "del Ktest_transaction\n",
                "del Ktrain_identity\n",
                "del Ktrain_transaction\n",
                "del Ktrain_cat1\n",
                "del Ktest_cat1\n",
                "del Ktrain_cat2\n",
                "del Ktest_cat2\n",
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = reduce_mem_usage2(ASSIGN)",
                "ASSIGN = reduce_mem_usage2(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "Ktest = reduce_mem_usage2(Ktest)\n",
                "Ktrain = reduce_mem_usage2(Ktrain)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Ktrain.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Ktrain.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "Ktest.shape"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Ktest.shape"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "Ktest.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "Ktest.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN= Ktest.loc[:,'id-01':'id-38'].columns.str.replace('-','_')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "z= Ktest.loc[:,'id-01':'id-38'].columns.str.replace('-','_')"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN=list(ASSIGN)",
                "z"
            ],
            "output_type": "execute_result",
            "content_old": [
                "z=list(z)\n",
                "z"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "for x,y in zip(Ktest.loc[:,'id-01':'id-38'].columns, z):",
                "SLICE=SLICE",
                "del Ktest[x]",
                "gc.collect()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "for x,y in zip(Ktest.loc[:,'id-01':'id-38'].columns, z):\n",
                "    Ktest[y]=Ktest[x]\n",
                "    del Ktest[x]\n",
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN=Ktrain[\"isFraud\"]",
                "X=Ktrain.drop([\"isFraud\", \"TransactionID\"], axis=1).astype('float64')",
                "X= X.fillna(-999)",
                "ASSIGN = Ktest['TransactionID']",
                "ASSIGN = Ktest.drop(['TransactionID'], axis=1).astype('float64')",
                "ASSIGN = ASSIGN.fillna(-999)",
                "ASSIGN = ASSIGN[X.columns]"
            ],
            "output_type": "not_existent",
            "content_old": [
                "y=Ktrain[\"isFraud\"]\n",
                "X=Ktrain.drop([\"isFraud\", \"TransactionID\"], axis=1).astype('float64')\n",
                "X= X.fillna(-999)\n",
                "\n",
                "Ktest_id = Ktest['TransactionID']\n",
                "X_Ktest = Ktest.drop(['TransactionID'], axis=1).astype('float64')\n",
                "X_Ktest = X_Ktest.fillna(-999)\n",
                "\n",
                "X_Ktest = X_Ktest[X.columns]"
            ]
        },
        {
            "tags": [
                "visualize_data",
                "process_data"
            ],
            "content": [
                "'''",
                "ASSIGN=StandardScaler().fit_transform(X)",
                "ASSIGN=PCA().fit(X_fit)",
                "plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_))",
                "plt.title('All columns included', color='gray')",
                "plt.xlabel(\"Number of Component\", color='green')",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)",
                "plt.show()",
                "ASSIGN=StandardScaler().fit_transform(X_Ktest)",
                "ASSIGN=PCA().fit(X_Ktest_fit)",
                "plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_))",
                "plt.title('All columns included', color='gray')",
                "plt.xlabel(\"Number of Component\", color='green')",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)",
                "plt.show()",
                "'''"
            ],
            "output_type": "execute_result",
            "content_old": [
                "'''\n",
                "X_fit=StandardScaler().fit_transform(X)\n",
                "X_pca=PCA().fit(X_fit)\n",
                "plt.plot(np.cumsum(X_pca.explained_variance_ratio_))\n",
                "plt.title('All columns included', color='gray')\n",
                "plt.xlabel(\"Number of Component\", color='green')\n",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')\n",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
                "plt.show()\n",
                "\n",
                "X_Ktest_fit=StandardScaler().fit_transform(X_Ktest)\n",
                "X_Ktest_pca=PCA().fit(X_Ktest_fit)\n",
                "plt.plot(np.cumsum(X_Ktest_pca.explained_variance_ratio_))\n",
                "plt.title('All columns included', color='gray')\n",
                "plt.xlabel(\"Number of Component\", color='green')\n",
                "plt.ylabel(\"Cumulative Variance Ratio\", color='green')\n",
                "plt.grid(color='gray', linestyle='-', linewidth=0.3)\n",
                "plt.show()\n",
                "'''"
            ]
        },
        {
            "tags": [
                "process_data",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "\"\"\"",
                "ASSIGN = PCA(n_components=100).fit(X_fit)",
                "ASSIGN = X_pca.fit_transform(ASSIGN)",
                "ASSIGN.explained_variance_ratio_",
                "ASSIGN = PCA(n_components=100).fit(X_Ktest_fit)",
                "ASSIGN = X_Ktest_pca.fit_transform(ASSIGN)",
                "print(ASSIGN.explained_variance_ratio_.sum())",
                "print(ASSIGN.explained_variance_ratio_.sum())",
                "\"\"\""
            ],
            "output_type": "execute_result",
            "content_old": [
                "# PCA Analysis is misleading here because we filled missing data. \n",
                "\n",
                "\"\"\"\n",
                "#Final Model for Ktrain\n",
                "X_pca = PCA(n_components=100).fit(X_fit)\n",
                "X_fit = X_pca.fit_transform(X_fit)\n",
                "X_pca.explained_variance_ratio_\n",
                "\n",
                "#Final Model for Ktest\n",
                "\n",
                "X_Ktest_pca = PCA(n_components=100).fit(X_Ktest_fit)\n",
                "X_Ktest_fit = X_Ktest_pca.fit_transform(X_Ktest_fit)\n",
                "print(X_pca.explained_variance_ratio_.sum())\n",
                "print(X_Ktest_pca.explained_variance_ratio_.sum())\n",
                "\"\"\""
            ]
        },
        {
            "tags": [
                "process_data",
                "train_model",
                "validate_data"
            ],
            "content": [
                "'''",
                "ASSIGN=[]",
                "for i in range(1,101):",
                "ASSIGN.append(\"a\"+str(i))",
                "ASSIGN= pd.DataFrame(data=X_fit, columns= xlist)",
                "ASSIGN= pd.DataFrame(data=X_Ktest_fit, columns= xlist)",
                "ASSIGN.head()",
                "'''"
            ],
            "output_type": "execute_result",
            "content_old": [
                "'''\n",
                "xlist=[]\n",
                "for i in range(1,101):\n",
                "    xlist.append(\"a\"+str(i))\n",
                "\n",
                "X_new= pd.DataFrame(data=X_fit, columns= xlist)\n",
                "X_Ktest_new= pd.DataFrame(data=X_Ktest_fit, columns= xlist)\n",
                "X_Ktest_new.head()\n",
                "'''"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=42)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "X_train, X_test, y_train, y_test= train_test_split(X,y, test_size=0.25, random_state=42)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = reduce_mem_usage2(ASSIGN)",
                "ASSIGN = reduce_mem_usage2(ASSIGN)"
            ],
            "output_type": "stream",
            "content_old": [
                "X = reduce_mem_usage2(X)\n",
                "X_Ktest = reduce_mem_usage2(X_Ktest)"
            ]
        },
        {
            "tags": [
                "None"
            ],
            "content": [
                "gc.collect()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "gc.collect()"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "print(Ktrain.shape)",
                "print(Ktest.shape)",
                "print(X.shape)",
                "print(X_Ktest.shape)"
            ],
            "output_type": "stream",
            "content_old": [
                "print(Ktrain.shape)\n",
                "print(Ktest.shape)\n",
                "print(X.shape)\n",
                "print(X_Ktest.shape)\n"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model",
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "'''models = [LogisticRegression,",
                "KNeighborsClassifier,",
                "GaussianNB,",
                "SVC,",
                "DecisionTreeClassifier,",
                "RandomForestClassifier,",
                "GradientBoostingClassifier,",
                "LGBMClassifier,",
                "XGBClassifier",
                "]",
                "def compML (df, y, algorithm):",
                "ASSIGN=algorithm().fit(X_train, y_train)",
                "ASSIGN=model.predict(X_test)",
                "ASSIGN= accuracy_score(y_test, y_pred)",
                "ASSIGN= algorithm.__name__",
                "print(ASSIGN,,ASSIGN)",
                "for i in models:",
                "compML(X,\"isFraud\",i)",
                "'''"
            ],
            "output_type": "execute_result",
            "content_old": [
                "# It gets so much time to run with each models, so passed for now.\n",
                "\n",
                "'''models = [LogisticRegression,\n",
                "          KNeighborsClassifier,\n",
                "          GaussianNB,\n",
                "          SVC,\n",
                "          DecisionTreeClassifier,\n",
                "          RandomForestClassifier,\n",
                "          GradientBoostingClassifier,\n",
                "          LGBMClassifier,\n",
                "          XGBClassifier\n",
                "          #CatBoostClassifier\n",
                "         ]\n",
                "\n",
                "def compML (df, y, algorithm):\n",
                "    \n",
                "    #y=df[y]\n",
                "    #X=df.drop([\"PassengerId\",\"Survived\"], axis=1).astype('float64')\n",
                "    #X_train, X_test,y_train,y_test=train_test_split(X,y, test_size=0.25, random_state=42)\n",
                "    \n",
                "    model=algorithm().fit(X_train, y_train)\n",
                "    y_pred=model.predict(X_test)\n",
                "    accuracy= accuracy_score(y_test, y_pred)\n",
                "    #return accuracy\n",
                "    model_name= algorithm.__name__\n",
                "    print(model_name,\": \",accuracy)\n",
                "    \n",
                "    \n",
                "for i in models:\n",
                "    compML(X,\"isFraud\",i)\n",
                "    \n",
                "'''"
            ]
        },
        {
            "tags": [
                "train_model",
                "evaluate_model"
            ],
            "content": [
                "ASSIGN= LGBMClassifier().fit(X_train, y_train)",
                "ASSIGN=model.predict(X_test)",
                "accuracy_score(y_test,ASSIGN)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "model= LGBMClassifier().fit(X_train, y_train)\n",
                "y_pred=model.predict(X_test)\n",
                "accuracy_score(y_test,y_pred)"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "model"
            ],
            "output_type": "execute_result",
            "content_old": [
                "model"
            ]
        },
        {
            "tags": [
                "evaluate_model",
                "transfer_results"
            ],
            "content": [
                "ASSIGN=model.predict(X_Ktest)",
                "ASSIGN=pd.DataFrame({\"TransactionID\":Ktest_id, \"isFraud\":predictions})",
                "ASSIGN.to_csv('submission_model.csv', index=False)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "predictions=model.predict(X_Ktest)\n",
                "output=pd.DataFrame({\"TransactionID\":Ktest_id, \"isFraud\":predictions})\n",
                "output.to_csv('submission_model.csv', index=False)"
            ]
        },
        {
            "tags": [
                "setup_notebook"
            ],
            "content": [
                "SETUP"
            ],
            "output_type": "not_existent",
            "content_old": [
                "import random as rd",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "class Student:",
                "def __init__(self, id, shyness, attitude, cap, usageRate, year):",
                "self.id = id",
                "self.shyness = shyness",
                "'''",
                "attitude towards drinking",
                "(normal distribution, mean is MeanAttitude and s.d 1)",
                "determines usagerate, whether student may drink alone and addiction",
                "'''",
                "self.attitude = attitude",
                "'''",
                "updated at the end of the school year",
                "if below a threshold, student can't continue next year (inSchool = 0)",
                "'''",
                "self.cap = cap",
                "self.usageRate = usageRate",
                "self.inSchool = True",
                "self.year = year",
                "self.friends = []",
                "self.host = []",
                "self.guestList = []",
                "self.whenAttend = []",
                "def host_party(self, student_list):",
                "ASSIGN = self.whenAttend",
                "ASSIGN = (0.2 * self.shyness**2) - (len(attending))path",
                "ASSIGN = rd.random()",
                "if ASSIGN <= ASSIGN:",
                "ASSIGN = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']",
                "for a in ASSIGN:",
                "if a in ASSIGN:",
                "ASSIGN.remove(a)",
                "ASSIGN = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35])",
                "self.host.append(ASSIGN)",
                "for others in student_list:",
                "if others.id in self.friends:",
                "others.willAttend(ASSIGN)",
                "if ASSIGN in others.whenAttend:",
                "self.guestList.append(others.id)",
                "def willAttend(self, ASSIGN):",
                "if ASSIGN not in self.whenAttend:",
                "ASSIGN = rd.random()",
                "ASSIGN = ['Sun','Sat']",
                "if ASSIGN in ASSIGN:",
                "if ASSIGN <= 0.8:",
                "self.whenAttend.append(ASSIGN)",
                "else:",
                "if ASSIGN <= 0.4:",
                "self.whenAttend.append(ASSIGN)",
                "def partyDrink(self, peer_pressure):",
                "ASSIGN = (self.attitudepath) + (2*peer_pressure) - 0.1*(self.usageRatepath)",
                "ASSIGN = rd.random()",
                "if ASSIGN <= ASSIGN:",
                "self.usageRate += 1",
                "self.experience()",
                "return 1",
                "return 0",
                "def drinkAlone(self):",
                "if self.attitude > 3:",
                "ASSIGN = self.attitudepath",
                "ASSIGN = rd.random()",
                "if ASSIGN <= ASSIGN:",
                "self.usageRate += 1",
                "def experience(self):",
                "ASSIGN = rd.noramlvariate(meanExperience, stDevExperience)",
                "if ASSIGN < -6:",
                "ASSIGN = -6",
                "elif ASSIGN > 3:",
                "ASSIGN = 3",
                "ASSIGN = rd.random()",
                "if ASSIGN < probabilityofBust:",
                "ASSIGN -= 3",
                "self.attitude += ASSIGN*0.1",
                "def gradeExperience(self):",
                "if self.usageRate > usageRateGradeDrop:",
                "ASSIGN = -1*(self.usageRatepath)",
                "self.attitude += (ASSIGN * 0.2)",
                "def gradeUpdate(self):",
                "ASSIGN = 0.2path",
                "ASSIGN = rd.normalvariate(0.1,std_dev)",
                "if self.usageRate > usageRateGradeDrop:",
                "ASSIGN = self.usageRatepath(30*self.year)",
                "ASSIGN = 0.5path",
                "ASSIGN = rd.normalvariate(mean_drop,s_dev)",
                "ASSIGN += ASSIGN",
                "student.cap -= ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### create Student Class",
                "class Student:",
                "",
                "  def __init__(self, id, shyness, attitude, cap, usageRate, year):",
                "    self.id = id",
                "",
                "    ##characteristics (integers)",
                "    #low score means more shy (normal distribution, mean 0 and s.d 1)",
                "    self.shyness = shyness",
                "    '''",
                "    attitude towards drinking ",
                "    (normal distribution, mean is MeanAttitude and s.d 1)",
                "    determines usagerate, whether student may drink alone and addiction ",
                "    '''",
                "    self.attitude = attitude",
                "    '''",
                "    updated at the end of the school year",
                "    if below a threshold, student can't continue next year (inSchool = 0)",
                "    '''",
                "    self.cap = cap",
                "    #number of times drinking (in the past 30 days?)>should update every 30 or just keep it for the whole sem?",
                "    self.usageRate = usageRate",
                "",
                "    self.inSchool = True    ",
                "    self.year = year",
                "",
                "    ##social life (lists)",
                "    self.friends = []",
                "    #list of days they are hosting a party in the next week, and a list of attendees",
                "    self.host = []",
                "    self.guestList = []",
                "    #list of days student will attend someone else's party",
                "    self.whenAttend = []",
                "",
                "",
                "  def host_party(self, student_list):   ",
                "    #determine if hosting that week,",
                "    attending = self.whenAttend     ",
                "    p_host = (0.2 * self.shyness**2) - (len(attending))/5",
                "    num = rd.random()",
                "    if num <= p_host:",
                "",
                "      #choose which day to host",
                "      week = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']",
                "      for a in attending:",
                "        if a in week:",
                "          week.remove(a)",
                "      #",
                "      day = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35])",
                "      self.host.append(day)   ",
                "",
                "      #send out invites",
                "      ",
                "      for others in student_list:",
                "        #is there a better way to select the respective students based on their id?",
                "        #two nested for loops - might get hard over large student populations",
                "        if others.id in self.friends:",
                "          others.willAttend(day) ",
                "          if day in others.whenAttend:",
                "            self.guestList.append(others.id)",
                "  ",
                "  #when student gets invited to a party",
                "  def willAttend(self, day):",
                "    if day not in self.whenAttend:",
                "      num = rd.random()",
                "      weekend = ['Sun','Sat']",
                "      if day in weekend:",
                "        if num <= 0.8:",
                "          self.whenAttend.append(day)",
                "      else:",
                "        if num <= 0.4:",
                "          self.whenAttend.append(day)",
                "       ",
                "  def partyDrink(self, peer_pressure):",
                "    # arbitrary coefficients ",
                "    # limits - self.attitude = [2, 3]",
                "    #        - attitudeUse = [1,1]",
                "    #        - peer_pressure = [0, 1]",
                "    #        - usageRate = []",
                "    #        - usageRateUse = []",
                "    # take note of the limits",
                "    p_drink = (self.attitude/ attitudeUse) + (2*peer_pressure) - 0.1*(self.usageRate/usageRateUse)",
                "    num = rd.random()",
                "    if num <= p_drink:",
                "      self.usageRate += 1",
                "      self.experience()",
                "      return 1",
                "    return 0",
                "     ",
                "      ",
                "",
                "  def drinkAlone(self):",
                "    if self.attitude > 3:",
                "      p_alone = self.attitude/15",
                "      num = rd.random()",
                "      if num <= p_alone:",
                "        self.usageRate += 1",
                "",
                "  #after a drink at the party  ",
                "  def experience(self):",
                "    num = rd.noramlvariate(meanExperience, stDevExperience)",
                "    if num < -6:",
                "      num = -6",
                "    elif num > 3:",
                "      num = 3",
                "",
                "    #if party gets busted",
                "    bust = rd.random()",
                "    if bust < probabilityofBust:",
                "      num -= 3",
                "",
                "    #update attitude towards drinking",
                "    # attitude update is arbitrary at 0.1",
                "    self.attitude += num*0.1",
                "#    self.attitude += num*0.5",
                "",
                "    ",
                "  #every 3 weeks",
                "  def gradeExperience(self):",
                "    if self.usageRate > usageRateGradeDrop:",
                "      poor_exp = -1*(self.usageRate/15)",
                "      self.attitude += (poor_exp * 0.2)",
                "",
                "  #at the end of the semester",
                "  def gradeUpdate(self):",
                "    std_dev = 0.2/self.year",
                "    change = rd.normalvariate(0.1,std_dev)",
                "",
                "    #add on to grade drop due to drinking",
                "    if self.usageRate > usageRateGradeDrop:",
                "      mean_drop = self.usageRate/(30*self.year)",
                "      s_dev = 0.5/self.year",
                "      grade_drop = rd.normalvariate(mean_drop,s_dev)",
                "      change += grade_drop",
                "    ",
                "    student.cap -= change    "
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "def make_friends(student1, student2):",
                "if student2.id not in student1.friends:",
                "shy1, shy2 = student1.shyness, student2.shyness",
                "fr1,fr2 = len(student1.friends), len(student2.friends)",
                "att1, att2 = student1.attitude, student2.attitude",
                "ASSIGN = ((wShy*(shy1 + shy2 + 4)) - (wFr*((fr1path(3+shy1)) + (fr2path(3+shy2)))) - (wAtt*(att1 - att2))) path(8*wShy)",
                "ASSIGN = rd.random()",
                "if ASSIGN <= ASSIGN:",
                "student1.friends.append(student2.id)",
                "student2.friends.append(student1.id)",
                "def party_friends(attendees):",
                "for guest in attendees:",
                "for others in attendees:",
                "if guest != others:",
                "make_friends(guest, others)",
                "def party_time(host, student_list, day):",
                "ASSIGN = host.guestList",
                "ASSIGN = [host]",
                "ASSIGN = 0",
                "for others in student_list:",
                "if others.id in ASSIGN:",
                "ASSIGN.append(others)",
                "party_friends(ASSIGN)",
                "for j in range(3):",
                "for member in ASSIGN:",
                "ASSIGN = member.partyDrink(peer_pressure)",
                "ASSIGN += (xpath(ASSIGN))",
                "host.guestList.remove(day)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### students interacting",
                "",
                "def make_friends(student1, student2):",
                "  if student2.id not in student1.friends:",
                "    shy1, shy2 = student1.shyness, student2.shyness",
                "    fr1,fr2 = len(student1.friends), len(student2.friends)",
                "    att1, att2 = student1.attitude, student2.attitude",
                "    p = ((wShy*(shy1 + shy2 + 4)) - (wFr*((fr1/(3+shy1)) + (fr2/(3+shy2)))) - (wAtt*(att1 - att2))) / (8*wShy)",
                "    num = rd.random()",
                "    if num <= p:",
                "      student1.friends.append(student2.id) ",
                "      student2.friends.append(student1.id)",
                "",
                "#for all students at the party",
                "def party_friends(attendees):",
                "  ",
                "  for guest in attendees:",
                "    for others in attendees: ",
                "      if guest != others:",
                "        make_friends(guest, others)",
                "  #will it be running the function twice for some of them? How to avoid?",
                "",
                "#when there's a party host",
                "def party_time(host, student_list, day):",
                "  invited = host.guestList",
                "  attendees = [host]",
                "  peer_pressure = 0",
                "  for others in student_list:",
                "    if others.id in invited:",
                "      attendees.append(others)",
                "",
                "  party_friends(attendees)",
                "",
                "  #3 rounds of drinking or not",
                "  for j in range(3):",
                "    for member in attendees:",
                "      x = member.partyDrink(peer_pressure)",
                "      peer_pressure += (x/len(attendees))",
                "",
                "  host.guestList.remove(day)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "MeanAttitude = 3",
                "NumberOfAgents = 1000",
                "ASSIGN = 0.8",
                "ASSIGN = 0.8",
                "ASSIGN = 0.5",
                "ASSIGN = 6",
                "ASSIGN = 1",
                "ASSIGN = 1.6",
                "ASSIGN = 1.6",
                "ASSIGN = 0.001",
                "ASSIGN = 3",
                "ASSIGN = 13"
            ],
            "output_type": "not_existent",
            "content_old": [
                "## Input variables",
                "",
                "MeanAttitude = 3",
                "NumberOfAgents = 1000",
                "wShy = 0.8 ",
                "wFr = 0.8 ",
                "wAtt = 0.5 ",
                "usageRateUse = 6 ",
                "attitudeUse = 1 ",
                "meanExperience = 1.6 ",
                "stdDevExperience = 1.6 ",
                "probabilityOfBust = 0.001",
                "maxGoodExperience = 3 ",
                "usageRateGradeDrop = 13"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for student in range(NumberOfAgents):",
                "ASSIGN = rd.normalvariate(0,1)",
                "ASSIGN = rd.normalvariate(MeanAttitude,0.5)",
                "ASSIGN = rd.normalvariate(3.5, 0.5)",
                "if ASSIGN < 2:",
                "ASSIGN = 2",
                "elif ASSIGN > 5:",
                "ASSIGN = 5",
                "if ASSIGN < 0:",
                "ASSIGN = 0",
                "elif ASSIGN <= 5:",
                "ASSIGN = round(attitude_)",
                "else:",
                "ASSIGN = round(2*attitude_)",
                "ASSIGN = rd.choice([1,2,3,4])",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "##making lists for the variables",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "process_data",
                "validate_data"
            ],
            "content": [
                "SETUP",
                "ASSIGN = rd.sample(range(1, 100000), NumberOfAgents)",
                "ASSIGN = pd.DataFrame({\"id\": id_list,",
                "\"shyness\": shyness_list,",
                "\"attitude\": attitude_list,",
                "\"cap\": cap_list,",
                "\"usage\": usagerate_list,",
                "\"year\": year_list,",
                "})",
                "ASSIGN.head()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "## making a dataframe that represents student attributes",
                "",
                "import pandas as pd",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "df.head()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "for index, row in df.iterrows():",
                "ASSIGN = Student(row['id'],",
                "row['shyness'],",
                "row['attitude'],",
                "row['cap'],",
                "row['usage'],",
                "row['year'])",
                "ASSIGN.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN.append(student.cap)",
                "ASSIGN += student.cap",
                "ASSIGN = total_start_cappath(student_list)",
                "print(ASSIGN)",
                "plt.hist(ASSIGN, color = 'purple')",
                "plt.title('Starting CAP')",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "start_cap = []",
                "total_start_cap = 0",
                "for student in student_list:",
                "  start_cap.append(student.cap)",
                "  total_start_cap += student.cap",
                "",
                "avg_start_cap = total_start_cap/len(student_list)",
                "print(avg_start_cap)",
                "",
                "plt.hist(start_cap, color = 'purple')",
                "plt.title('Starting CAP')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "for student in student_list:",
                "ASSIGN.append(student.attitude)",
                "plt.hist(ASSIGN)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "now_attitude = []",
                "for student in student_list:",
                "  now_attitude.append(student.attitude)",
                "",
                "plt.hist(now_attitude)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "for student in student_list:",
                "ASSIGN.append(student.usageRate)",
                "plt.hist(ASSIGN)",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "start_usage = []",
                "for student in student_list:",
                "  start_usage.append(student.usageRate)",
                "",
                "plt.hist(start_usage)",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = {}",
                "for week in range(13):",
                "for student in student_list:",
                "student.host_party(student_list)",
                "ASSIGN == 0:",
                "for student in student_list:",
                "if student.year == 1:",
                "for i in range(20):",
                "if student != student_list[i]:",
                "make_friends(student, student_list[i])",
                "else:",
                "for j in range(30):",
                "if student != student_list[j]:",
                "make_friends(student, student_list[j])",
                "elif week %3 == 0 and week != 12:",
                "for student in student_list:",
                "student.gradeExperience()",
                "ASSIGN == 12:",
                "for student in student_list:",
                "student.gradeUpdate()",
                "if student.cap < 2:",
                "student.inSchool = False",
                "student_list.remove(student)",
                "ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "for day in ASSIGN:",
                "for student in student_list:",
                "if day in student.host:",
                "party_time(student, student_list, day)",
                "else:",
                "for loners in student_list:",
                "loners.drinkAlone()",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.usageRate",
                "ASSIGN = total_usepath(student_list)",
                "ASSIGN[week] = ASSIGN",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.attitude",
                "ASSIGN = total_attpath(student_list)",
                "ASSIGN[week] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### The simulation",
                "usage_list = {}",
                "attitude_list = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list[week] = avg_att"
            ]
        },
        {
            "tags": [
                "validate_data"
            ],
            "content": [
                "VALIDATION",
                "attitude_list"
            ],
            "output_type": "execute_result",
            "content_old": [
                "attitude_list"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in attitude_list.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "plt.plot(ASSIGN, ASSIGN)",
                "plt.title('Average attitude over the semester')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "weeks = []",
                "att = []",
                "",
                "for key, value in attitude_list.items():",
                "  weeks.append(key)",
                "  att.append(value)",
                "",
                "plt.plot(weeks, att)",
                "plt.title('Average attitude over the semester')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "validate_data",
                "visualize_data"
            ],
            "content": [
                "VALIDATION",
                "ASSIGN = []",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN.append(student.cap)",
                "ASSIGN += student.cap",
                "ASSIGN = total_cappath(student_list)",
                "print(ASSIGN)",
                "print(max(start_cap))",
                "print(max(ASSIGN))",
                "plt.hist(start_cap, color = 'orange')",
                "plt.hist(ASSIGN, color = 'indigo')",
                "plt.legend(['Start', 'End'])",
                "plt.title('Average CAP Score')",
                "plt.show()"
            ],
            "output_type": "stream",
            "content_old": [
                "end_cap = []",
                "total_cap = 0",
                "for student in student_list:",
                "  end_cap.append(student.cap)",
                "  total_cap += student.cap",
                "",
                "avg_cap = total_cap/len(student_list)",
                "print(avg_cap)",
                "print(max(start_cap))",
                "print(max(end_cap))",
                "",
                "plt.hist(start_cap, color = 'orange')",
                "plt.hist(end_cap, color = 'indigo')",
                "plt.legend(['Start', 'End'])",
                "plt.title('Average CAP Score')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "for student in student_list:",
                "ASSIGN.append(student.usageRate)",
                "plt.hist(start_usage, color = 'blue')",
                "plt.hist(ASSIGN, color = 'red')",
                "plt.title('Number of drinks taken over the semester')",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "end_usage = []",
                "for student in student_list:",
                "  end_usage.append(student.usageRate)",
                "",
                "plt.hist(start_usage, color = 'blue')",
                "plt.hist(end_usage, color = 'red')",
                "plt.title('Number of drinks taken over the semester')",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = pd.DataFrame({'usage': end_usage, 'cap':end_cap})",
                "ASSIGN = end_test[end_test['usage'] < 8]",
                "ASSIGN = end_test[end_test['usage'] > 8]",
                "plt.hist(ASSIGN['cap'], color = 'blue')",
                "plt.hist(ASSIGN['cap'], color = 'orange')"
            ],
            "output_type": "execute_result",
            "content_old": [
                "end_test = pd.DataFrame({'usage': end_usage, 'cap':end_cap})",
                "",
                "g1 = end_test[end_test['usage'] < 8]",
                "g2 = end_test[end_test['usage'] > 8]",
                "plt.hist(g1['cap'], color = 'blue')",
                "plt.hist(g2['cap'], color = 'orange')"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for student in range(NumberOfAgents):",
                "ASSIGN = rd.normalvariate(0,1)",
                "ASSIGN = rd.normalvariate(MeanAttitude,0.5)",
                "ASSIGN = rd.normalvariate(3.5, 0.5)",
                "if ASSIGN < 2:",
                "ASSIGN = 2",
                "elif ASSIGN > 5:",
                "ASSIGN = 5",
                "if ASSIGN < 0:",
                "ASSIGN = 0",
                "elif ASSIGN <= 5:",
                "ASSIGN = round(attitude_)",
                "else:",
                "ASSIGN = round(2*attitude_)",
                "ASSIGN = rd.choice([1,2,3,4])",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = rd.sample(range(1, 100000), NumberOfAgents)",
                "ASSIGN = pd.DataFrame({\"id\": id_list,",
                "\"shyness\": ASSIGN,",
                "\"attitude\": ASSIGN,",
                "\"cap\": ASSIGN,",
                "\"usage\": ASSIGN,",
                "\"year\": ASSIGN,",
                "})",
                "ASSIGN = []",
                "for index, row in ASSIGN.iterrows():",
                "ASSIGN = Student(row['id'],",
                "row['shyness'],",
                "row['attitude'],",
                "row['cap'],",
                "row['usage'],",
                "row['year'])",
                "ASSIGN.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "##making lists for the variables - second run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = {}",
                "for week in range(13):",
                "for student in student_list:",
                "student.host_party(student_list)",
                "ASSIGN == 0:",
                "for student in student_list:",
                "if student.year == 1:",
                "for i in range(20):",
                "if student != student_list[i]:",
                "make_friends(student, student_list[i])",
                "else:",
                "for j in range(30):",
                "if student != student_list[j]:",
                "make_friends(student, student_list[j])",
                "elif week %3 == 0 and week != 12:",
                "for student in student_list:",
                "student.gradeExperience()",
                "ASSIGN == 12:",
                "for student in student_list:",
                "student.gradeUpdate()",
                "if student.cap < 2:",
                "student.inSchool = False",
                "student_list.remove(student)",
                "ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "for day in ASSIGN:",
                "for student in student_list:",
                "if day in student.host:",
                "party_time(student, student_list, day)",
                "else:",
                "for loners in student_list:",
                "loners.drinkAlone()",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.usageRate",
                "ASSIGN = total_usepath(student_list)",
                "ASSIGN[week] = ASSIGN",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.attitude",
                "ASSIGN = total_attpath(student_list)",
                "ASSIGN[week] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### The simulation - 2nd run",
                "usage_list_2 = {}",
                "attitude_list_2 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_2[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_2[week] = avg_att"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for student in range(NumberOfAgents):",
                "ASSIGN = rd.normalvariate(0,1)",
                "ASSIGN = rd.normalvariate(MeanAttitude,0.5)",
                "ASSIGN = rd.normalvariate(3.5, 0.5)",
                "if ASSIGN < 2:",
                "ASSIGN = 2",
                "elif ASSIGN > 5:",
                "ASSIGN = 5",
                "if ASSIGN < 0:",
                "ASSIGN = 0",
                "elif ASSIGN <= 5:",
                "ASSIGN = round(attitude_)",
                "else:",
                "ASSIGN = round(2*attitude_)",
                "ASSIGN = rd.choice([1,2,3,4])",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = rd.sample(range(1, 100000), NumberOfAgents)",
                "ASSIGN = pd.DataFrame({\"id\": id_list,",
                "\"shyness\": ASSIGN,",
                "\"attitude\": ASSIGN,",
                "\"cap\": ASSIGN,",
                "\"usage\": ASSIGN,",
                "\"year\": ASSIGN,",
                "})",
                "ASSIGN = []",
                "for index, row in ASSIGN.iterrows():",
                "ASSIGN = Student(row['id'],",
                "row['shyness'],",
                "row['attitude'],",
                "row['cap'],",
                "row['usage'],",
                "row['year'])",
                "ASSIGN.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "##making lists for the variables - third run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = {}",
                "for week in range(13):",
                "for student in student_list:",
                "student.host_party(student_list)",
                "ASSIGN == 0:",
                "for student in student_list:",
                "if student.year == 1:",
                "for i in range(20):",
                "if student != student_list[i]:",
                "make_friends(student, student_list[i])",
                "else:",
                "for j in range(30):",
                "if student != student_list[j]:",
                "make_friends(student, student_list[j])",
                "elif week %3 == 0 and week != 12:",
                "for student in student_list:",
                "student.gradeExperience()",
                "ASSIGN == 12:",
                "for student in student_list:",
                "student.gradeUpdate()",
                "if student.cap < 2:",
                "student.inSchool = False",
                "student_list.remove(student)",
                "ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "for day in ASSIGN:",
                "for student in student_list:",
                "if day in student.host:",
                "party_time(student, student_list, day)",
                "else:",
                "for loners in student_list:",
                "loners.drinkAlone()",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.usageRate",
                "ASSIGN = total_usepath(student_list)",
                "ASSIGN[week] = ASSIGN",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.attitude",
                "ASSIGN = total_attpath(student_list)",
                "ASSIGN[week] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### The simulation - run 3",
                "usage_list_3 = {}",
                "attitude_list_3 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_3[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_3[week] = avg_att"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "ASSIGN = []",
                "for student in range(NumberOfAgents):",
                "ASSIGN = rd.normalvariate(0,1)",
                "ASSIGN = rd.normalvariate(MeanAttitude,0.5)",
                "ASSIGN = rd.normalvariate(3.5, 0.5)",
                "if ASSIGN < 2:",
                "ASSIGN = 2",
                "elif ASSIGN > 5:",
                "ASSIGN = 5",
                "if ASSIGN < 0:",
                "ASSIGN = 0",
                "elif ASSIGN <= 5:",
                "ASSIGN = round(attitude_)",
                "else:",
                "ASSIGN = round(2*attitude_)",
                "ASSIGN = rd.choice([1,2,3,4])",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = rd.sample(range(1, 100000), NumberOfAgents)",
                "ASSIGN = pd.DataFrame({\"id\": id_list,",
                "\"shyness\": ASSIGN,",
                "\"attitude\": ASSIGN,",
                "\"cap\": ASSIGN,",
                "\"usage\": ASSIGN,",
                "\"year\": ASSIGN,",
                "})",
                "ASSIGN = []",
                "for index, row in ASSIGN.iterrows():",
                "ASSIGN = Student(row['id'],",
                "row['shyness'],",
                "row['attitude'],",
                "row['cap'],",
                "row['usage'],",
                "row['year'])",
                "ASSIGN.append(ASSIGN)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "##making lists for the variables - fourth run",
                "",
                "attitude_list = []",
                "shyness_list = []",
                "cap_list = []",
                "usagerate_list = []",
                "year_list = []",
                "",
                "",
                "for student in range(NumberOfAgents):",
                "  shyness_ = rd.normalvariate(0,1)",
                "  attitude_ = rd.normalvariate(MeanAttitude,0.5)",
                "  cap_ = rd.normalvariate(3.5, 0.5)",
                "  if cap_ < 2:",
                "    cap_ = 2",
                "  elif cap_ > 5:",
                "    cap_ = 5",
                "",
                "  if attitude_ < 0:",
                "    usagerate_ = 0",
                "  elif attitude_ <= 5:",
                "    usagerate_ = round(attitude_)",
                "  else:",
                "    usagerate_ = round(2*attitude_)",
                "",
                "  year_ = rd.choice([1,2,3,4])",
                "  ",
                "  shyness_list.append(shyness_)",
                "  attitude_list.append(attitude_)",
                "  cap_list.append(cap_)",
                "  year_list.append(year_)",
                "  usagerate_list.append(usagerate_)",
                "",
                "  #id, shyness, attitude, cap, usageRate, year",
                "",
                "",
                "id_list = rd.sample(range(1, 100000), NumberOfAgents)",
                "",
                "df = pd.DataFrame({\"id\": id_list,",
                "                   \"shyness\": shyness_list,",
                "                   \"attitude\": attitude_list,",
                "                   \"cap\": cap_list,",
                "                   \"usage\": usagerate_list,",
                "                   \"year\": year_list,",
                "                   })",
                "",
                "### making a list of student objects",
                "",
                "student_list = []",
                "",
                "for index, row in df.iterrows():",
                "  #print(index)",
                "  #print(row)",
                "  student = Student(row['id'], ",
                "                    row['shyness'],",
                "                    row['attitude'],",
                "                    row['cap'],",
                "                    row['usage'],",
                "                    row['year'])",
                "  student_list.append(student)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = {}",
                "ASSIGN = {}",
                "for week in range(13):",
                "for student in student_list:",
                "student.host_party(student_list)",
                "ASSIGN == 0:",
                "for student in student_list:",
                "if student.year == 1:",
                "for i in range(20):",
                "if student != student_list[i]:",
                "make_friends(student, student_list[i])",
                "else:",
                "for j in range(30):",
                "if student != student_list[j]:",
                "make_friends(student, student_list[j])",
                "elif week %3 == 0 and week != 12:",
                "for student in student_list:",
                "student.gradeExperience()",
                "ASSIGN == 12:",
                "for student in student_list:",
                "student.gradeUpdate()",
                "if student.cap < 2:",
                "student.inSchool = False",
                "student_list.remove(student)",
                "ASSIGN = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "for day in ASSIGN:",
                "for student in student_list:",
                "if day in student.host:",
                "party_time(student, student_list, day)",
                "else:",
                "for loners in student_list:",
                "loners.drinkAlone()",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.usageRate",
                "ASSIGN = total_usepath(student_list)",
                "ASSIGN[week] = ASSIGN",
                "ASSIGN = 0",
                "for student in student_list:",
                "ASSIGN += student.attitude",
                "ASSIGN = total_attpath(student_list)",
                "ASSIGN[week] = ASSIGN"
            ],
            "output_type": "not_existent",
            "content_old": [
                "### The simulation - run 4",
                "usage_list_4 = {}",
                "attitude_list_4 = {}",
                "",
                "for week in range(13):",
                "  for student in student_list:",
                "    student.host_party(student_list)",
                "  if week == 0:",
                "    #make some friends",
                "    for student in student_list:            ",
                "      if student.year == 1:             ",
                "        for i in range(20):",
                "          if student != student_list[i]:",
                "            make_friends(student, student_list[i])",
                "      else:             ",
                "        for j in range(30):",
                "          if student != student_list[j]:",
                "            make_friends(student, student_list[j])",
                "",
                "  elif week %3 == 0 and week != 12:",
                "    #grade experience",
                "    for student in student_list:",
                "      student.gradeExperience()",
                "",
                "  elif week == 12:",
                "    #grade update",
                "    for student in student_list:",
                "      student.gradeUpdate()",
                "      if student.cap < 2:",
                "        student.inSchool = False",
                "        student_list.remove(student)",
                "",
                "  days = ['Mon','Tue','Wed','Thu','Fri','Sat','Sun']",
                "  for day in days:",
                "    for student in student_list:",
                "      if day in student.host:",
                "        #party starts",
                "        party_time(student, student_list, day)",
                "                  ",
                "    #if there's no one hosting",
                "    else:",
                "      for loners in student_list:",
                "        loners.drinkAlone()",
                "",
                "  total_use = 0",
                "  for student in student_list:",
                "    total_use += student.usageRate",
                "  avg_use = total_use/len(student_list)  ",
                "  usage_list_4[week] = avg_use        ",
                "",
                "  total_att = 0",
                "  for student in student_list:",
                "    total_att += student.attitude",
                "  avg_att = total_att/len(student_list)",
                "  attitude_list_4[week] = avg_att"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in usage_list_2.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in usage_list_3.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in usage_list_4.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 4')",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 3')",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 2')",
                "plt.title('Average usage over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average usage rate')",
                "plt.legend()"
            ],
            "output_type": "execute_result",
            "content_old": [
                "week2 = []",
                "usage2 = []",
                "",
                "for key, value in usage_list_2.items():",
                "  week2.append(key)",
                "  usage2.append(value)",
                "",
                "week3 = []",
                "usage3 = []",
                "",
                "for key, value in usage_list_3.items():",
                "  week3.append(key)",
                "  usage3.append(value)",
                "",
                "week4 = []",
                "usage4 = []",
                "",
                "for key, value in usage_list_4.items():",
                "  week4.append(key)",
                "  usage4.append(value)",
                "",
                "plt.plot(week4, usage4, label = 'sem 4')",
                "plt.plot(week3, usage3, label = 'sem 3')",
                "plt.plot(week2, usage2, label = 'sem 2')",
                "#plt.plot(weeks, usages, label = 'sem 1')",
                "plt.title('Average usage over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average usage rate')",
                "plt.legend()",
                "#plt.show()"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in attitude_list_2.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in attitude_list_3.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "ASSIGN = []",
                "ASSIGN = []",
                "for key, value in attitude_list_4.items():",
                "ASSIGN.append(key)",
                "ASSIGN.append(value)",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 4')",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 3')",
                "plt.plot(ASSIGN, ASSIGN, label = 'sem 2')",
                "plt.plot(weeks, att, label = 'sem 1')",
                "plt.title('Average attitude over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude level')",
                "plt.legend()",
                "plt.show()"
            ],
            "output_type": "display_data",
            "content_old": [
                "week2 = []",
                "att2 = []",
                "",
                "for key, value in attitude_list_2.items():",
                "  week2.append(key)",
                "  att2.append(value)",
                "",
                "week3 = []",
                "att3 = []",
                "",
                "for key, value in attitude_list_3.items():",
                "  week3.append(key)",
                "  att3.append(value)",
                "",
                "week4 = []",
                "att4 = []",
                "",
                "for key, value in attitude_list_4.items():",
                "  week4.append(key)",
                "  att4.append(value)",
                "",
                "plt.plot(week4, att4, label = 'sem 4')",
                "plt.plot(week3, att3, label = 'sem 3')",
                "plt.plot(week2, att2, label = 'sem 2')",
                "plt.plot(weeks, att, label = 'sem 1')",
                "plt.title('Average attitude over the sem')",
                "plt.xlabel('Week')",
                "plt.ylabel('Average attitude level')",
                "plt.legend()",
                "plt.show()"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "content": [
                "ASSIGN = []",
                "ASSIGN = []",
                "for student in student_list:",
                "ASSIGN.append(len(student.friends))",
                "ASSIGN.append(student.shyness)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "friendship_len = []",
                "shyness_score = []",
                "",
                "for student in student_list:",
                "  friendship_len.append(len(student.friends))",
                "  shyness_score.append(student.shyness)"
            ]
        },
        {
            "tags": [
                "visualize_data"
            ],
            "content": [
                "plt.scatter(shyness_score, friendship_len)"
            ],
            "output_type": "execute_result",
            "content_old": [
                "plt.scatter(shyness_score, friendship_len)"
            ]
        },
        {
            "tags": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "content": [
                "SETUP",
                "def calc_logloss(targets, outputs, eps=1e-6):",
                "ASSIGN = [log_loss(np.floor(targets[:,i]), np.clip(outputs[:,i], eps, 1-eps)) for i in range(6)]",
                "return np.average(ASSIGN, weights=[2,1,1,1,1,1])",
                "warnings.filterwarnings(\"ignore\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = ASSIGN.merge(dup, on = 'SOPInstanceUID', how = 'left')"
            ],
            "output_type": "not_existent",
            "content_old": [
                "from sklearn.metrics import log_loss\n",
                "def calc_logloss(targets, outputs, eps=1e-6):\n",
                "    logloss_classes = [log_loss(np.floor(targets[:,i]), np.clip(outputs[:,i], eps, 1-eps)) for i in range(6)]\n",
                "    return np.average(logloss_classes, weights=[2,1,1,1,1,1])\n",
                "\n",
                "import pandas as pd\n",
                "import pickle\n",
                "import os\n",
                "import numpy as np\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "dup = pd.read_csv(\"../input/stage1-test-gt/dup_s1_test.csv\")\n",
                "test = pd.read_csv(\"../input/stage1-test-gt/s1_test_results.csv\")\n",
                "test = test.merge(dup, on = 'SOPInstanceUID', how = 'left')"
            ]
        },
        {
            "tags": [
                "ingest_data",
                "process_data"
            ],
            "content": [
                "def get_split_result(filename, test, eps, rm_dup=False):",
                "ASSIGN = pd.read_csv(filename)",
                "ASSIGN['type'] = ASSIGN['ID'].apply(lambda x: x.split('_')[2])",
                "ASSIGN['name'] = ASSIGN['ID'].apply(lambda x: x.split('_')[1])",
                "ASSIGN = f1[['ASSIGN']]",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'epidural']",
                "ASSIGN.columns = ['ASSIGN','epidural']",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'intraparenchymal']",
                "ASSIGN.columns = ['ASSIGN','intraparenchymal']",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'intraventricular']",
                "ASSIGN.columns = ['ASSIGN','intraventricular']",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'subarachnoid']",
                "ASSIGN.columns = ['ASSIGN','subarachnoid']",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'subdural']",
                "ASSIGN.columns = ['ASSIGN','subdural']",
                "ASSIGN = f1[['name','Label']][f1['type'] == 'any']",
                "ASSIGN.columns = ['ASSIGN','any']",
                "ASSIGN = ASSIGN.merge(f1_any, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.merge(f1_epidural, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.merge(f1_intraparenchymal, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.merge(f1_intraventricular, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.merge(f1_subarachnoid, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.merge(f1_subdural, on = 'ASSIGN', how = 'left')",
                "ASSIGN = ASSIGN.drop_duplicates()",
                "ASSIGN.rename(columns = {'ASSIGN': 'SOPInstanceUID'}, inplace=True)",
                "ASSIGN = 'ID_' + ASSIGN",
                "ASSIGN = ASSIGN.merge(test, on = 'SOPInstanceUID', how = 'left')",
                "if rm_dup:",
                "ASSIGN = name[name['dup'].isnull() == True]",
                "else:",
                "ASSIGN = name.copy()",
                "ASSIGN = name_use[['any_y',",
                "'epidural_y', 'subdural_y', 'subarachnoid_y', 'intraventricular_y',",
                "'intraparenchymal_y']].values",
                "ASSIGN = name_use[['any',",
                "'epidural', 'subdural', 'subarachnoid', 'intraventricular',",
                "'intraparenchymal']].values",
                "return calc_logloss(ASSIGN, ASSIGN, eps=eps)"
            ],
            "output_type": "not_existent",
            "content_old": [
                "def get_split_result(filename, test, eps, rm_dup=False):\n",
                "    f1 = pd.read_csv(filename)\n",
                "\n",
                "    f1['type'] = f1['ID'].apply(lambda x: x.split('_')[2])\n",
                "    f1['name'] = f1['ID'].apply(lambda x: x.split('_')[1])\n",
                "\n",
                "    name = f1[['name']]\n",
                "\n",
                "    f1_epidural = f1[['name','Label']][f1['type'] == 'epidural']\n",
                "    f1_epidural.columns = ['name','epidural']\n",
                "    f1_intraparenchymal = f1[['name','Label']][f1['type'] == 'intraparenchymal']\n",
                "    f1_intraparenchymal.columns = ['name','intraparenchymal']\n",
                "    f1_intraventricular = f1[['name','Label']][f1['type'] == 'intraventricular']\n",
                "    f1_intraventricular.columns = ['name','intraventricular']\n",
                "    f1_subarachnoid = f1[['name','Label']][f1['type'] == 'subarachnoid']\n",
                "    f1_subarachnoid.columns = ['name','subarachnoid']\n",
                "    f1_subdural = f1[['name','Label']][f1['type'] == 'subdural']\n",
                "    f1_subdural.columns = ['name','subdural']\n",
                "    f1_any = f1[['name','Label']][f1['type'] == 'any']\n",
                "    f1_any.columns = ['name','any']\n",
                "\n",
                "    name = name.merge(f1_any, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_epidural, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_intraparenchymal, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_intraventricular, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_subarachnoid, on = 'name', how = 'left')\n",
                "    name = name.merge(f1_subdural, on = 'name', how = 'left')\n",
                "    name = name.drop_duplicates()\n",
                "    name.rename(columns = {'name': 'SOPInstanceUID'}, inplace=True)\n",
                "    name['SOPInstanceUID'] = 'ID_' + name['SOPInstanceUID']\n",
                "    \n",
                "    name = name.merge(test, on = 'SOPInstanceUID', how = 'left')\n",
                "    \n",
                "    if rm_dup:\n",
                "        name_use = name[name['dup'].isnull() == True] #remove duplicate patientID\n",
                "    else:\n",
                "        name_use = name.copy()  #all test\n",
                "    gt = name_use[['any_y',\n",
                "           'epidural_y', 'subdural_y', 'subarachnoid_y', 'intraventricular_y',\n",
                "           'intraparenchymal_y']].values\n",
                "    pred = name_use[['any',\n",
                "               'epidural', 'subdural', 'subarachnoid', 'intraventricular',\n",
                "               'intraparenchymal']].values\n",
                "    return calc_logloss(gt, pred, eps=eps)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "output_type": "execute_result",
            "content": [
                "get_split_result(\"..path\", test, 1e-6)"
            ],
            "content_old": [
                "#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3\n",
                "get_split_result(\"../input/kernel-0076/submission.csv\", test, 1e-6)"
            ]
        },
        {
            "tags": [
                "process_data"
            ],
            "output_type": "execute_result",
            "content": [
                "get_split_result(\"..path\", test, 1e-6, rm_dup=True)"
            ],
            "content_old": [
                "get_split_result(\"../input/kernel-0076/submission.csv\", test, 1e-6, rm_dup=True)"
            ]
        }
    ]
}