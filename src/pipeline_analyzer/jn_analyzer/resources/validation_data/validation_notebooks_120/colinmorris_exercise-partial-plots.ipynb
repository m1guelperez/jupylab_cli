{"cells":[{"metadata":{"_uuid":"f109509b0a221ad6f9e9fffda157436bb9dcd03f"},"cell_type":"markdown","source":"# Exercises\n\n## Set Up\n\nToday you will create partial dependence plots and practice building insights with data from the [Taxi Fare Prediction](https://www.kaggle.com/c/new-york-city-taxi-fare-prediction) competition.\n\nWe have again provided code to do the basic loading, review and model-building. Run the cell below to set everything up:"},{"metadata":{"_uuid":"66747dc04948e408afb1e43b509ae76ead9d0ed6","trusted":true},"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n# Environment Set-Up for feedback system.\nimport sys\nsys.path.append('../input/ml-insights-tools')\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom ex3 import *\nprint(\"Setup Complete\")\n\n# Data manipulation code below here\ndata = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)\n\n# Remove data with extreme outlier coordinates or negative fares\ndata = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +\n                  'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +\n                  'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +\n                  'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +\n                  'fare_amount > 0'\n                  )\n\ny = data.fare_amount\n\nbase_features = ['pickup_longitude',\n                 'pickup_latitude',\n                 'dropoff_longitude',\n                 'dropoff_latitude']\n\nX = data[base_features]\n\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\nfirst_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y)\nprint(\"Data sample:\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b4e3b65dedfa2e80f13693a07a5d5bf836dfc2a6","trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1ba75e2085c59e1845d2c6251166ab770042b4bd"},"cell_type":"markdown","source":"## Question 1\n\nHere is the code to plot the partial dependence plot for pickup_longitude.  Run the following cell."},{"metadata":{"_uuid":"bd4e80f6e5f7caa72fb625bc01a2df60f67a7e5b","trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom pdpbox import pdp, get_dataset, info_plots\n\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52226b7a57145841e71c2674a534be429494c32e"},"cell_type":"markdown","source":"Why does the partial dependence plot have this U-shape?\n\nDoes your explanation suggest what shape to expect in the partial dependence plots for the other features?\n\nCreate all other partial plots in a for-loop below (slightly modifying the code from the above cell."},{"metadata":{"_uuid":"b52611fa468eddf0210b28ab2a85d0329da20c86","trusted":true},"cell_type":"code","source":"for feat_name in base_features:\n    pdp_dist = pdp.pdp_isolate(model=first_model, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp.pdp_plot(pdp_dist, feat_name)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8af2c7ed9383c94019e5b17202d479ed98f7533"},"cell_type":"markdown","source":"Do the shapes match your expectations for what shapes they would have? Can you explain the shape now that you've seen them? \n\nUncomment the following line to check your intuition."},{"metadata":{"_uuid":"059847ab1d4a820a416933ff40fe07982f685b6f","trusted":true},"cell_type":"code","source":"q_1.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dc4d485508d0f9234c7bf9fd837ae8dc422cc5c3"},"cell_type":"markdown","source":"## Question 2\n\nNow create a 2D partial dependence plot.  As a reminder, here is the code from the tutorial.  \n\n```\ninter1  =  pdp.pdp_interact(model=my_model, dataset=val_X, model_features=feature_names, features=['Goal Scored', 'Distance Covered (Kms)'])\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=['Goal Scored', 'Distance Covered (Kms)'], plot_type='contour')\nplt.show()\n```\n\nHow do you interpret that shape?"},{"metadata":{"_uuid":"50f16498428053dabf8a13ce09fa1e1090a3e78a","trusted":true},"cell_type":"code","source":"# Add your code here\nfeats = ['pickup_longitude', 'dropoff_longitude']\ninter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"622dedda4d9e16d7eb36dfa4e28987e877d6dbc0"},"cell_type":"code","source":"feats = ['pickup_latitude', 'dropoff_latitude']\ninter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1dfbbed2db82238dc8a5e5988643142f5daa87d7"},"cell_type":"code","source":"feats = ['pickup_latitude', 'dropoff_longitude']\nfeats = ['pickup_longitude', 'dropoff_longitude']\ninter1  =  pdp.pdp_interact(model=first_model, dataset=val_X, model_features=base_features, features=feats)\n\npdp.pdp_interact_plot(pdp_interact_out=inter1, feature_names=feats, plot_type='contour')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b770606d81b121983feaa8c5abac1f7011fcbd69"},"cell_type":"markdown","source":"Uncomment the line below to see the solution and explanation for how one might reason about the plot shape."},{"metadata":{"_uuid":"b89dfd95094c6f5a88495e604551d0c2c1108d11","trusted":true},"cell_type":"code","source":"q_2.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9a31c88a87638a027b59420bdd98b73a18f957"},"cell_type":"markdown","source":"## Question 3\nConsider a ride starting at longitude -73.92 and ending at longitude -74. Using the graph from the last question, estimate how much money the rider would have saved if they'd started the ride at longitude -73.98 instead?"},{"metadata":{"_uuid":"6357a288cbb4d9ddb69e69ad45d357d0c291b4ed","trusted":true},"cell_type":"code","source":"savings_from_shorter_trip = 14\n\n# Uncomment the line below to check your answer\nq_3.check()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd5271b8de874cf82ab2dcbc985cf39e3ad595e"},"cell_type":"markdown","source":"For a solution or hint, uncomment the appropriate line below."},{"metadata":{"_uuid":"1d6120304286ae664e70b7d65ef6caaac690b0e6","trusted":true},"cell_type":"code","source":"q_3.hint()\nq_3.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0ed0aa5932b0e34d0a7e82b1be271a34319d2b35"},"cell_type":"markdown","source":"## Question 4\nIn the PDP's you've seen so far, location features have primarily served as a proxy to capture distance traveled. In the permutation importance lessons, you added the features `abs_lon_change` and `abs_lat_change` as a more direct measure of distance.\n\nCreate these features again here. You only need to fill in the top two lines.  Then run the following cell.  **After you run it, identify the most important difference between this partial dependence plot and the one you got above without the absolute value features.**\n\n---"},{"metadata":{"_uuid":"3b60eba142294be35b3e686fc567766d04b21840","trusted":true},"cell_type":"code","source":"# create new features\ndata['abs_lon_change'] = abs(data.dropoff_longitude - data.pickup_longitude)\ndata['abs_lat_change'] = abs(data.dropoff_latitude - data.pickup_latitude)\n\nfeatures_2  = ['pickup_longitude',\n               'pickup_latitude',\n               'dropoff_longitude',\n               'dropoff_latitude',\n               'abs_lat_change',\n               'abs_lon_change']\n\nX = data[features_2]\nnew_train_X, new_val_X, new_train_y, new_val_y = train_test_split(X, y, random_state=1)\nsecond_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(new_train_X, new_train_y)\n\nfeat_name = 'pickup_longitude'\npdp_dist = pdp.pdp_isolate(model=second_model, dataset=new_val_X, model_features=features_2, feature=feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()\n\n# uncomment the line below to check your answer\nq_4.check()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51be46efa0cc0de24edc887ecc18cbe942ae14c1"},"cell_type":"markdown","source":"Uncomment the lines below to see a hint or the solution (including an explanation of the important differences between the plots)."},{"metadata":{"_uuid":"283a89d6c4b771d65399b1acbbe92c47b0268cd9","trusted":true},"cell_type":"code","source":"q_4.hint()\nq_4.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a00a342b0103fd2746eb250f4b61cd9fb0beda7"},"cell_type":"markdown","source":"## Question 5\nConsider a scenario where you have only 2 predictive features, which we will call `feat_A` and `feat_B`. Both features have minimum values of -1 and maximum values of 1.  The partial dependence plot for `feat_A` increases steeply over its whole range, whereas the partial dependence plot for feature B increases at a slower rate (less steeply) over its whole range.\n\nDoes this guarantee that `feat_A` will have a higher permutation importance than `feat_B`.  Why or why not?\n\nAfter you've thought about it, uncomment the line below for the solution."},{"metadata":{"_uuid":"b15b34ffa308f7be3b2c1ef1d2eb206041efd504","trusted":true},"cell_type":"code","source":"q_5.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52a5904afa0cbafb6c6bd3115d8a4c679acfa0c5"},"cell_type":"markdown","source":"## Question 6\nCreate a dataset with the following characteristics:\n- There are 2 predictive features and a target\n- Each predictive feature has a range from -2 to 2.\n- The PDP of your first feature (called `X1`) has a positive slope from [-1,1], and a negative slope everywhere else when run in a decision tree model.\n\n*Note: You are supplied the code to create the predictive features, and you only need to create y*"},{"metadata":{"_uuid":"c3eba0d258ce6e430d0481be17446434c1f7c5a4","trusted":true},"cell_type":"code","source":"from numpy.random import rand\nimport numpy as np\n\nn_samples = 1000\n\n# Create array holding predictive feature\nX1 = 4 * rand(n_samples) - 2\nX2 = 4 * rand(n_samples) - 2\n# Create y. you should have X in the expression for y\ny = X1.copy()\ny[X1 < -1] *= -1\ny[X1 > 1] *= -1\n\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1')\n\n# visualize your results\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\n\nprint(\"Scatter plot of X1 vs y:\")\nplt.scatter(X1, y)\nplt.show()\n\n# uncomment line below to check your solution\nq_6.check()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8964d870cae3a4f6b979efc978abba5ccaddc49a"},"cell_type":"markdown","source":"Uncomment the lines below for a hint or solution"},{"metadata":{"_uuid":"b1b6b23acbd562804e601114b27952191db997e2","trusted":true},"cell_type":"code","source":"q_6.hint()\nq_6.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea17013216f294a19c4066cfba0773a48e2254d2"},"cell_type":"markdown","source":"## Question 7\nCreate a dataset with 2 features and a target, such that the pdp of the first feature is flat, but its permutation importance is high.  We will use a RandomForest for the model.\n\n*Note: You only need to supply the lines that create the variables X1, X2 and y. The code to check your data is provided*."},{"metadata":{"trusted":true,"_uuid":"404a35efbf4db60dc850515551983bab8c865aec"},"cell_type":"code","source":"(np.random.rand(10) < .5).astype(float)\nnp.random.choice([1, -1], 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3cc54c8d7d5aef8d0c955c0255b02ac151d97cc9","trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nn_samples = 10000\n\n# Create array holding predictive feature\nX2 = np.random.choice([1., -1.], n_samples)\nX1 = np.random.rand(n_samples)\n# Create y. you should have X in the expression for y\ny = (X1 * 1000) % 2\n\nX1 = np.random.randint(0, 9, n_samples)\nX1 = np.random.rand(n_samples) * 999\nX2 = np.random.rand(n_samples)\n\nfor i in range(n_samples):\n    if X1[i]%2 < 1:\n        y[i] = X2[i]\n    else:\n        y[i] = -X2[i]\n\n# create dataframe because pdp_isolate expects a dataFrame as an argument\nmy_df = pd.DataFrame({'X1': X1, 'X2': X2, 'y': y})\npredictors_df = my_df.drop(['y'], axis=1)\n\nmy_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(predictors_df, my_df.y)\n\n\npdp_dist = pdp.pdp_isolate(model=my_model, dataset=my_df, model_features=['X1', 'X2'], feature='X1', num_grid_points=10)\npdp.pdp_plot(pdp_dist, 'X1')\nplt.show()\n\n#perm = PermutationImportance(my_model).fit(predictors_df, my_df.y)\n\n# Uncomment the following line to check your answer.\n# q_7.check()\n\n#q_7.check()\n\n# show the weights for the permutation importance you just calculated\n#eli5.show_weights(perm, feature_names = ['X1', 'X2'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dcc5549735b894cd40bea80a0a93c2f3a33e2edf","trusted":true},"cell_type":"code","source":"# Uncomment the following lines for the hint or solution\nq_7.hint()\nq_7.solution()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"52f2813642d5f02f138a94734bfedffe0a842452"},"cell_type":"markdown","source":"## Congrats\n\nPartial dependence plots can be really interesting. We have a [discussion thread](https://www.kaggle.com/learn-forum/65782) to talk about what real-world topics or questions you'd be curious to see addressed with partial dependence plots. \n\nNext up is **SHAP values** which help you understand the logic for each individual prediction."}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}