{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Linear Models deep Understanding (with implementation from scratch)\nby Tareq Albeesh"},{"metadata":{},"cell_type":"markdown","source":"### Motivation:\n~2.5 years ago it was the first time I heard the term 'Machine Learning', and I had a hard time to find an article that explains how the simplest models in ML work mathematically, and how to code it from scratch, so in this article I will try hopefully to explain how linear models work in the simplest way possible, and I'll code all the models we will learn from scratch using python, so we will start from math and ends up with a model classifies images, so buckle up and enjoy the ride..."},{"metadata":{},"cell_type":"markdown","source":"first I'll give an intuition about the linear function and how it separates the space geometrically,this will give us imagination about what happens when we use the linear models in regression and classification."},{"metadata":{},"cell_type":"markdown","source":"### Line Equation"},{"metadata":{},"cell_type":"markdown","source":"Line equation is a linear equation that takes the formula:  \n$\\qquad$$\\qquad$ $ y = w_{1} x + b $  \nand this equation represents a line in the 2D space  \nwhere:\n- $w_{1}$: is called the 'coefficient' or 'slope'\n- $b$: is called the bias or 'intercept'"},{"metadata":{},"cell_type":"markdown","source":"###### numerical example about the line equation:\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport random\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = range(1000)\n#changing our list to numpy array to benifit from numpy's broadcasting\nX = np.asarray(X)\nw1 = 2\nb  = 500\ndef line_function(X):\n    y = w1 * X + b\n    return y\ny = line_function(X)\n\nplt.plot(X,y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"line equation can be used to represent many problems, \nfor instance, lets say we want to calculate the weather temperature in Fahrenheit  knowing the temperature in Celsius, the linear function that gives us the temperature in F is given as:  \n$\\qquad$$\\qquad$$F = \\frac{9}{5}  C + 32$  \nwhere:  \n- F is the temperature in Fahrenheit.\n- C is the temperature in Celsius.\n- $\\frac{9}{5}$ is the value of the slope $(w_{1})$ \n- $32$ is the value of the bias $(b)$\n\nso if the known temperature in Celsius is 29, The equivalent temperature in Fahrenheit  will be:  \n\n$\\qquad$$\\qquad$$F = \\frac{9}{5}  29 + 32 = 84.2$  \n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"C = range(1000)\nC = np.asarray(C)\nw1 = 9/2\nb  = 32\nF = line_function(C)\n\nfig = plt.figure(figsize=(4,3))\nax = fig.add_subplot(111)\nax.set_title('change of F with respect to C')\n# ax.scatter(x=data[:,0],y=data[:,1],label='Data')\nplt.plot(C,F)\nax.set_xlabel('Celsius (C)')\nax.set_ylabel('Fahrenheit (F)')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"as we saw above we used the Line equation, which is a linear equation to represent a real-life problem,  \nbut what if the linear problem that we are trying solve wasn't in 2D, in other words, what if the input parameters of our problem were multiple parameters instead of just a single input value?,  \nthe solution is to use the Hyperplane equation instead of line equation,\nbut what are 'Hyperplanes' ?\n</br>\n"},{"metadata":{},"cell_type":"markdown","source":"### Hyperplane"},{"metadata":{},"cell_type":"markdown","source":"the Affine hyperplane is a linear function that represents a 'Slice' from the space that it's in,so it separates the space that it's in into two 'half spaces', so the hyperplane in 3D space is just a slice from the 3d space (as we can see below), and the hyperplane in 2d space is just a line as we saw above, so it's safe to say that Hyperplanes are a generalization to lines."},{"metadata":{},"cell_type":"markdown","source":"hyperplanes equation:  \n$\\qquad$$\\qquad$    $w_{1}x_{1}+w_{2}x_{2}+\\cdots +w_{n}x_{n} + b = 0$  \nwhere:\n- $w_{1}\\cdots w_{n}$: are the coefficients (or commonly called 'weights')\n- $b$: is the bias\n- $x_{1}\\cdots x_{n}$: are the function features or variables"},{"metadata":{},"cell_type":"markdown","source":"An example of a 2d hyperplane in 3d space (2d hyperplanes also called 'planes'):"},{"metadata":{"trusted":true},"cell_type":"code","source":"X1 = range(1000)\nX2 = range(1000)\nX1 = np.asarray(X1)\nX2 = np.asarray(X2)\nw1 = 8\nw2 = 6\nWs = [w1,w2]\nb  = 500\ndef hyperplane(Xs):\n    y=b\n    for (w,x) in zip(Ws,Xs):\n        y+=w*x\n    return y\nax = plt.axes(projection='3d')\nxv, yv = np.meshgrid(X1, X2)\nXs = [xv, yv]\ny = hyperplane(Xs)\nax.plot_surface(xv, yv,y);\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Linear Models:"},{"metadata":{},"cell_type":"markdown","source":"Linear models are the functions that take the formula:  \n$a(x_{1},\\ldots ,x_{k})=w_{1}x_{1}+w_{2}x_{2}+\\cdots +w_{n}x_{n} + b $  \nwhere:\n- $w_{1}\\cdots w_{n}$: are the coefficients (or commonly called 'weights')\n- $b$: is the bias\n- $x_{1}\\cdots x_{n}$: are the function features or variables\n- every linear function represents a Hyperplane in the n+1 space"},{"metadata":{},"cell_type":"markdown","source":"### the vector form of linear models:"},{"metadata":{},"cell_type":"markdown","source":"the vector form of linear models:  \n$\\qquad$$\\qquad$ $a(X) = X w$  \nwhere:\n- w:  the weights vector with shape $(m\\times 1)$, where m is (number of features + 1) where the '+1' is for the bias\n- X: the samples matrix that takes the shape  $(n\\times m)$, where each row represents one sample and each column represents one feature."},{"metadata":{},"cell_type":"markdown","source":"### Linear Regression:"},{"metadata":{},"cell_type":"markdown","source":"linear regression is the operation of building a linear model that fits a set of data, so when the model is given an input x, the model will predict the output for this input, so the result of the linear regression belongs to the real numbers $(a(x) = y \\in \\rm I\\!{R})$"},{"metadata":{},"cell_type":"markdown","source":"so if we had a data set that looks like the figure below, the mission that linear regression will try to do is to fit the given data using a line so if a new input **x** is given, the model will try to predict what result **y** would be."},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/random-linear-regression/train.csv')\nplt.scatter(df.x,df.y,s = 4) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so the objective of linear regression is to find a linear model that fits the data best as the plot below, but How?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/random-linear-regression/train.csv')\nplt.scatter(df.x,df.y,s = 4) \nX= range(100)\nY= X\nplt.plot(X,Y,c='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### How to find the best linear model that fits the data"},{"metadata":{},"cell_type":"markdown","source":"first, we will put a random linear model like the one below"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.x,df.y,s = 4) \nX= np.asarray((range(100)))\nY= -X\nplt.plot(X,Y,c='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"secondly, we need to measure how well did it fit the data, so we'll need to measure the loss of the model (or how bad was it),  \nthere are many functions we can use to measure the loss of the linear model, but one of the most common functions is the MSE (mean squared error), which takes the formula:  \n$\\qquad$ $\\qquad$ $ L(w) = \\frac{1}{n} \\sum\\limits_{i=1}^{n}(w^{T}x_{i}-y_{i})^{2}$  \nor in vector form:  \n$\\qquad$ $\\qquad$ $ L(w) = \\frac{1}{n}   ||Xw - y||^{2} $"},{"metadata":{},"cell_type":"markdown","source":"so after we measured the loss function of our linear model, we need to update the weights 'w' (coefficients) of the models so that it fits the data better, and we need to do it in a way the loss function is minimized:  \n$\\qquad$ $\\qquad$ $L(w) -> \\underset{w}{min}$   \n"},{"metadata":{},"cell_type":"markdown","source":"given the loss function above we can solve the problem analytically with a single equation which takes the form:   \n$\\qquad$ $\\qquad$ $w = (X^{T}X)^{-1}X^{T}y$  \nThis is great we can find the best line that minimizes our loss function with a single equation, but there is a catch…\nas we can see in the formula there is an inversion to a matrix, in case of a small number of parameters it will not be a big deal, but if we had many dimensions it will be computationally infeasible to calculate the inverse of the matrix, so we will resort to other methods that are computationally feasible."},{"metadata":{},"cell_type":"markdown","source":"one of the most common methods to find the best coefficients that minimize the loss functions is Gradient Descent,but what is 'Gradient Descent'? "},{"metadata":{},"cell_type":"markdown","source":"### Gradient Descent"},{"metadata":{},"cell_type":"markdown","source":"gradient descent is an iterative optimization algorithm that helps us finding local minimum, but the Error function that we are trying to find it's local minimum must be differentiable with respect to the parameter that we are trying to optimize which is (w), because GD uses the derivative of the error function with respect to (w) to find the local minimum,  \nso for our error function:  \n$ L(w) = \\frac{1}{n}   ||Xw - y||^{2} $  \nwe can see that L is differentiable with respect to 'w', so we can us GD to find it's local minimum,  \nso as we can see in the figure below each iteration in gradient descent will get us closer to the local minimum or hopefully to the global minimum if we were lucky enough."},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://cdn-images-1.medium.com/max/600/1*iNPHcCxIvcm7RwkRaMTx1g.jpeg\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" />\n"},{"metadata":{},"cell_type":"markdown","source":"##### Gradient Descent Algorithm:"},{"metadata":{},"cell_type":"markdown","source":"1- generate a **random weights vector**:  \n   $w^{0} = random(m \\times 1)$  \n2- **find the loss function's gradients vector** with respect to the parameter that we are trying to optimize (weights):  \n$\\qquad$$\\qquad$ \n$  \\nabla L(w^{0}) =  (\\frac{\\partial L w^{0}}{ w_{1}} , \\dots,\\frac{\\partial L w^{0}}{ w_{m}} ) $   \n\nwhere m is number of features (wights + 1 for bias)  \n\n     \n\n3- the gradients vector points to the steepest ascent direction of the loss function so to decrease the loss function we **move in the opposite direction of the gradients vector**:  \n\n$w^{1} = w^{0} - \\eta_{1} \\nabla L(w^{0})  $ \n\n4- check **if** the weights didn't change that much (the change didn't exceed a threshold that we put '$\\epsilon$' ) **then** break,  \n**else** go to the step 2 again:  \n     $ if  \\big( ||w^{t} - w^{t-1}|| < \\epsilon \\big)\\: then \\: break $  \n               $else: \\:  go \\: to \\: step \\: 2 $\n     \n     \n\n\n   "},{"metadata":{},"cell_type":"markdown","source":"Linear regression example:  \nwe'll try to fit the data above using linear regression code without using off-the-shelf libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"#loading data from the data set \nimport pandas as pd\ndf = pd.read_csv('../input/random-linear-regression/train.csv')\ndf = df.dropna()\nplt.scatter(df.x,df.y,s = 4) ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First lets try to find the optimal solution using the analytical approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"def getXYfromDF(df):\n    X = []\n    for i in list(df.x):\n        if(type(i)!=list):\n            i = [i]\n        i.append(1)\n        X.append(i)\n    X = np.asarray(X)\n    y = np.asarray(df.y)\n    return X,y\ndef randomWeights(m):\n    w= []\n    for i in range(m):\n        w.append(random.randint(1,9))\n    w = np.asarray(w)\n    return w\n\n\nX,y = getXYfromDF(df) \n\n\nn = X.shape[0]\nm = X.shape[1]\n\nw = randomWeights(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The analytical optimal solution:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#finding the best wights analytically\nw = np.dot(np.dot(np.linalg.inv(np.dot((X.T),X)),(X.T)),y) \nw","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plotTheLineWithData(X,w):\n    plt.scatter(df.x,df.y,s = 4) \n    #this X is to generate test samples\n    X=[]\n    for i in range(100):\n        X.append([i,1])\n    X = np.asarray(X)\n    predicted_y = np.dot(X,w) \n    plt.plot(X[:,0],predicted_y,c='red')\nplotTheLineWithData(X,w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now lets apply the gradient descent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = getXYfromDF(df) \nw = randomWeights(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MSE(y,y_predicted):\n    return ((y- y_predicted)**2).mean()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): \n    w_history  = []\n    loss_hostory = []\n    for iteration in range(max_iteration):\n        predicted_y = np.dot(X,w)\n        loss =  MSE(y,predicted_y)\n        loss = round(loss,9)\n        w_history.append(w)\n        loss_hostory.append(loss)\n        derivative = -(2/y.shape[0])* X.dot(loss).sum()\n        w = w + lr * derivative\n    return w_history,loss_hostory\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w_history,loss_hostory = gradient_descent(X,y,w,lr = 0.0000001)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"perfect_i = loss_hostory.index(min(loss_hostory)) \nperfect_w = w_history[perfect_i]\nw= perfect_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loss_hostory","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plotTheLineWithData(X,w)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"so as we saw above by using a loss function to measure the error of the model and using gradient descent we were able to find a good line that can fit the data pretty well"},{"metadata":{},"cell_type":"markdown","source":"now that we know how to optimize a function to find the best parameters using GD it'll be easy to understand the linear classification,"},{"metadata":{},"cell_type":"markdown","source":"### Linear Classification:"},{"metadata":{},"cell_type":"markdown","source":"##### Linear Binary Classification:"},{"metadata":{},"cell_type":"markdown","source":"linear binary classification is the operation of finding a line or hyperplane that can separate the data into two classes (positive and negative)"},{"metadata":{},"cell_type":"markdown","source":"so if we had a data like the plot below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')\nplt.scatter(df.X1,df.X2,c= df.Y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the linear classification objective is to find the best possible line that can separate the yellow points from the purple ones, like the red line in the plot below:"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')\nplt.scatter(df.X1,df.X2,c= df.Y)\nX = range(1000)\ny= X\nplt.plot(X,y,c='red')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Intuition:**"},{"metadata":{},"cell_type":"markdown","source":"Imagine a line in 2d space like the line below with the equation:  \n$2x + 3y = 0$  \nwhich is equivalent to   \n$ y =  - \\frac{2}{3} x$\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.asarray(range(1000))\ny = -(2/3)*X \nplt.plot(X,y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"what if we try to insert a random point, let's say(800,-400), and put it int the equation of our line: \n$2x + 3y = 0$  \n$2(800) + 3(-400) = 400 > 0$  \nthat geometrically means that the point (800,-400) is above our line:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.asarray(range(1000))\ny = -(2/3)*X \nplt.plot(X,y)\nplt.plot(800,-400,'+',c='green')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"in a similar way, if we took a point (400,-500):  \n$2x + 3y = 0$  \n$2(400) + 3(-500) = -700 < 0$  \nthat geometrically means that the point (400,-500) is under the line:"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.asarray(range(1000))\ny = -(2/3)*X \nplt.plot(X,y)\nplt.plot(400,-500,'_',c='red')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using line equation we can tell if a point in space is below or above the line, or speaking more generally, if we  apply any point in a space with N dimensions, to a linear equation with N-1 dimensions it will be either in the positive side of the hyperplane that represents the linear equation, or in the negative side of it, or in the surface of the hyper plane, and we can represent these cases like:   \n$ X w > 0 $, the point is on the positive side of the hyperplane  \n$ X w < 0 $, the point is on the negative side of the hyperplane  \n$ X w = 0 $, the point is on the surface of the hyperplane\n\n\n\n"},{"metadata":{},"cell_type":"markdown","source":"now that we understand why are we using the linear equation in classification, lets see how can we find the best line/hyperplane that separates two classes,"},{"metadata":{},"cell_type":"markdown","source":"#### Perceptron:"},{"metadata":{},"cell_type":"markdown","source":"perceptron is the simplest linear model for classification, it simple updates the weights in case of miss classification using this simple algorithm:    \n\n$w_{0} = random weights$  \nwhile some input vectors remain unclassified **do**:</br>\nlet $x_{i}$ be a miss classified input vector:  \n$if\\:\\:(x_{i}. w).\\: y_{i} < 0  \\: and  \\: y_{i} < 0 $:   \n$\\qquad$$w = w -\\eta  x_{i}$  \n$if\\:\\:(x_{i}. w) .\\:y_{i} < 0  \\: and  \\: y_{i} > 0 $:   \n$\\qquad$$w = w +  \\eta x_{i}$  \n\n       \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getXYfromDF(df):\n    X = []\n    for i in df[['X1','X2']].values.tolist():\n        if(type(i)!=list):\n            i = [i]\n        i.append(1)\n        X.append(i)\n    X = np.asarray(X)\n    y = np.asarray(df.Y)\n    return X,y\ndef randomWeights(m):\n    w= []\n    for i in range(m):\n        w.append(random.randint(1,9)/100)\n    w = np.asarray(w)\n    return w\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X,y = getXYfromDF(df)\nw= randomWeights(3)\nprint(w)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"using this random weights we got the line:"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.X1,df.X2,c=df.Y)\nX12 = np.column_stack((range(1000),np.ones(1000)))\nprint(X12.shape)\nprint(w[1:2].shape)\ny0 =  -np.divide(np.dot(X12,w[1:3]),w[0])\nres = [sub[0] for sub in X12] \nX1 = res\nplt.plot(X1,y0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def equales(list1,list2):\n    if(len(list1)!=len(list2)):\n        return False\n    else: \n        for i in range(len(list1)):\n            if(list1[i]!=list2[i]):\n                return False\n    return True\ndef perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):\n    for iteration in range(max_iterations):\n        prev_w = w\n        for i in range(w.shape[0]):\n            if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):\n                w=w- learning_rate * X[i]\n                \n            elif(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]>0):\n                w=w+ learning_rate * X[i]\n        if(equales(prev_w,w)):\n            print('prev_w == w in ',iteration)\n            break\n        \n        \n    return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_w = perceptron(X,y,w,learning_rate=0.000001,max_iterations= 100000)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w= new_w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.X1,df.X2,c=df.Y)\nX12 = np.column_stack((range(1000),np.ones(1000)))\nprint(X12.shape)\nprint(w[1:2].shape)\ny0 =  -np.divide(np.dot(X12,w[1:3]),w[0])\nres = [sub[0] for sub in X12] \nX1 = res\nplt.plot(X1,y0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"perceptron did an acceptable job in binary classification, but what if we had a multiple classes?, lets talk about multi classes classification using linear models...."},{"metadata":{},"cell_type":"markdown","source":"## Multi-Classes Linear Classification:"},{"metadata":{},"cell_type":"markdown","source":"a single linear function can only separate the data into two classes (positive and negative),so we cant classify multiple classes using a single linear model,but what if we used multiple linear models, each one of them measures the \"degree of belonging\" to one of the classes, and the result will be the maximum \"degree of belonging\" from all linear models, so if have k classes ($y \\in \\{ 1,\\dots,K \\}$) the model will be :   \n$\\qquad$$\\qquad$$a(x) = arg \\underset{k \\in \\{ 1,\\dots,K \\} }{max}  \\big( w_{k}^{T} x \\big)  $   \nwhere:  \n- w is the weights matrix and it's shape is: $K \\times m $ where:    \n -  $K$ is number of linear models (number of classes)\n -  $m$ number of coefficients + 1 (for bias) for each linear model\n- and this formula shortly means:$\\:$ find me the the linear function that the example x belongs to the most.  \n"},{"metadata":{},"cell_type":"markdown","source":"this is great, we used multiple linear models to classify multiple classes, now we need a way to measure how good the model did,  \ncurrently we just get the biggest value of the K linear functions, let change this values to probabilistic values each value represents the probability that and example $x_{i}$ belongs to each one of the classes,  \n$ \\qquad$$\\qquad$$\\qquad$$z  = (w_{1}^{T}x,\\dots,w_{k}^{T}x) $  \n\n$ \\qquad$$\\qquad$$\\qquad$$\\qquad$$ (e^{z_{1}},\\dots,e^{z_{K}})$  \n\n$ \\qquad$$\\qquad$$\\sigma(z) = \\big(  {{\\frac {e^{{\\mathsf {z_{1}}}}}{\\sum _{k=1}^{K}e^{{\\mathsf {z_{k}}}}}}},\\dots,{{\\frac {e^{{\\mathsf {z_{K}}}}}{\\sum _{k=1}^{K}e^{{\\mathsf {z_{k}}}}}}} \\big)$  \n$\\sigma(z) $ will give us the probability of each class of the classes K , and this transformation called 'Softmax Transformation'\n"},{"metadata":{},"cell_type":"markdown","source":"numerical example:  \nz = (3,12,-5,0)  \n$\\sigma(z) = (\\: 0 \\:, 0.9\\:, 0 \\:, 0\\: , 0.1\\:)$ \n"},{"metadata":{"trusted":true},"cell_type":"code","source":"def softmax(z):\n    e_z = np.exp(z)\n    return e_z / e_z.sum()\nz = (3,12,-5,0,10) \nnp.round(softmax(z),1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"now using the Softmax Transformation we were able to find the probability of each class from the K classes instead of just having numerical output from the K linear models,"},{"metadata":{},"cell_type":"markdown","source":"Target values for class probabilities:  \n$\\qquad$$\\qquad$$p = ([y=1],\\dots,[y=K]) $   \nthis vector represents a 'One-hot' vector that all of it's elements are 0 except the right class is 1,\nso if the right class were the second class, the vector p will be:  \n$\\qquad$$\\qquad$$p=[0,1,0,0,0]$  \nThe similarity between z and p can be measured using the **cross-entropy**:  \n\n$\\qquad$$\\qquad$$ -\\sum_{k=1}^{K}[y=k]log  \\frac {e^{{\\mathsf {z_{k}}}}}{\\sum _{j=1}^{K}e^{{\\mathsf {z_{j}}}}} = \n- \\frac {e^{{\\mathsf {z_{y}}}}}{\\sum _{j=1}^{K}e^{{\\mathsf {z_{j}}}}}\n$  \n\nso in the previous example if the true class is the second class, the cross-entropy will be:  \n\n$p=[0,1,0,0,0]$  \n\n- $\\sigma(z) = (\\: 0 \\:, 0.9\\:, 0 \\:, 0\\: , 0.1\\:)$  \n$ -0*log(0)-1*log(0.9)-0*log(0)-0*log(0)-0*log(0.1) \\approx 0.04$  \n\nin this example the cross-entropy value was 0.04 and this means that the prediction was almost right because the cross-entropy is relatively low.\n\n\n- $\\sigma(z) = (\\: 0 \\:, 1\\:, 0 \\:, 0\\: , 0\\:)$  \n$ -0*log(0)-1*log(1)-0*log(0)-0*log(0)-0*log(0) = 0$\n\nin this example the cross-entropy was 0, and this is great because it means that the prediction was absolutely right,\n\n- $\\sigma(z) = (\\: 0 \\:,0 \\:, 0 \\:, 0\\: , 1\\:)$  \n$ -0*log(0)-1*log(0)-0*log(0)-0*log(0)-0*log(1) = +\\infty$\n\nin this example the loss (cross-entropy) value was infinity, and this should be expected because our prediction is confidence in the wrong direction.\n\nso as we saw above, the cross-entropy is great for finding the loss of our model because we are using a probability distribution approach in our output and cross-entropy born to find the difference between probability distributions,  \nanother thing that's great about cross-entropy as a loss function, is that it's differentiable, and as we saw above when we tried finding the local minimum (and hopefully the global minimum) using gradient descent, the  loss function that we are using to optimize our weights must be differentiable with respect to the weights, and cross-entropy satisfies this condition,  \n"},{"metadata":{},"cell_type":"markdown","source":"the previous formula for the cross entropy was for a single sample from the data, and the generalized formula to compute the loss for the data is:  \n$ \\qquad$$\\qquad$$L(w) = -\\sum_{i=1}^{n}\\sum_{k=1}^{K}[y_{i}=k]log  \\frac {e^{{\\mathsf {w_{k}^{T}x_{i}}}}}{\\sum _{j=1}^{K}e^{{\\mathsf {w_{j}^{T}x_{i}}}}} \n$  \n"},{"metadata":{},"cell_type":"markdown","source":"and now that we have a differentiable loss functions we can use gradients descent to find a good W matrix, \nand the derivative of our loss function for every class k from the **K** classes :  \n \n$\\qquad$$\\qquad$$\\nabla w_{k} L(W) = - \\frac{1}{n} \\sum_{i=0}^{n}[x^{(i)}(y_{i},\\hat{y_{i}})]$\n\nwhere:\n- n is number of examples.\n- $ k \\in {\\{1,2,\\dots,K\\}}$\n- $y_{i}$ is the true value of the output.\n- $\\hat{y_{i}}$ is the predicted value.\n\nand then we update the wights,like :  \n\n$\\qquad$$\\qquad$$w_{k} = w_{k} - \\eta\\nabla w_{k} L(W) $  \nwhere:  \n- $\\eta$ is the learning rate"},{"metadata":{},"cell_type":"markdown","source":"this model often often referred to as 'Softmax Classifier':"},{"metadata":{},"cell_type":"markdown","source":"<img src=\"https://drive.google.com/uc?export=view&id=1332HxpjY9cdpcH8VQe-79HAYxn_SOSm_\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" />"},{"metadata":{},"cell_type":"markdown","source":"now lets code what we've learned:"},{"metadata":{},"cell_type":"markdown","source":"first lets try to code a 'Softmax Classifier' model that separates the simple data that we saw previously,"},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the data \ndf = pd.read_csv('../input/simple-binary-classification-data/binary_classification_simple.xls')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#this function changes the output from numerical values to one hot vectors\n#example:\n#[1,-1,1] becomes: \n#[[0,1],[1,0],[0,1]\n#this function is not general it's just for the case if the data has -1 and 1 cases only,\n#the generalized version will be coded next.\ndef getOneHot(y):\n    newY = []\n    for i in range(y.shape[0]):\n        if(y[i]==-1):\n            newY.append([1,0])\n        else:\n            newY.append([0,1])\n    return np.asarray(newY)\n#this function loads the data to X and y vectors\ndef getXYfromDF(df):\n    X = []\n    for i in df[['X1','X2']].values.tolist():\n        if(type(i)!=list):\n            i = [i]\n        i.append(1)\n        X.append(i)\n    X = np.asarray(X)\n    y = np.asarray(df.Y)\n    return X,y\n#this function generates random weights to initailize the weights(+ biases ofcourse)\ndef randomWeights(m,k):\n    w= []\n    for i in range(m):\n        temp = []\n        for j in range(k):\n            temp.append(random.randint(1,9))\n        w.append(temp)\n    w = np.asarray(w)\n    return w\n\n\nX,y = getXYfromDF(df) \ny = getOneHot(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = X.shape[0] #number of data samples\nm = X.shape[1] #number of features for each sample \nk = 2 #number of classes\nw = randomWeights(m,k)\nw=np.asarray(w,'float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The plot below shows the the two linear models and how they fit the data with the current initial weights"},{"metadata":{"trusted":true},"cell_type":"code","source":"#visualize the data that we are trying to fit, and plotting the current lines with the random weights\nplt.scatter(df.X1,df.X2,c=df.Y)\nX12 = np.column_stack((range(1000),np.ones(1000)))\ny0 =  -np.divide(np.dot(X12,w[1:3]),w[0])\nres = [sub[0] for sub in X12] \nX1 = np.asarray(res)\nprint(X1.shape) \nprint(y0.shape)\n#plot the first line that represents the first linear model\nplt.plot(X1,y0[:,0],c='blue')\n#plot the second line that represents the second linear model\nplt.plot(X1,y0[:,1],c='red')\nplt.show","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#the softmax that we use previously was right, but numerically it wasn't stable\n#this 'edited softmax' is more numerically stable \ndef softmax(x):\n    temp = np.exp(x - np.max(x))  # for numerical stability\n    return temp / temp.sum(axis=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EPS = 1e-9\n#same as in softmax,the first line in this function just gives numerical stability for cross entropy \ndef cross_entropy(y, y_hat):\n    y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability\n    return -np.sum(y * np.log(y_hat)/n)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the algorithm implementation:"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = [] #loss history \nnumberOfRounds =1000 # max number of times the optimization algorithm will run\nlearningRate = 0.1\nfor _ in range(numberOfRounds):\n    z = np.dot(X,w)\n    y_hat = []\n    for i in range(n):\n        y_hat.append(softmax(z[i]))\n    y_hat = np.asarray(y_hat)\n    history.append(cross_entropy(y,y_hat))\n\n    for j in range(k):\n        deltaTemp=0 \n        #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,\n        #and it works fine in case of offline training and smalle number of samples) \n        for i in range(n):\n            deltaTemp += np.dot(X.T,(y-y_hat))\n        deltaTemp  = - deltaTemp/n\n        deltaTemp = np.asarray(deltaTemp)\n        w-=learningRate*deltaTemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history)\nplt.title('the change of loss with iterations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the plot below shows the two lines that represents the two linear models that we used in the classification operation"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.scatter(df.X1,df.X2,c=df.Y)\nX12 = np.column_stack((range(1000),np.ones(1000)))\nprint(X12.shape)\nprint(w[1:2].shape)\ny0 =  -np.divide(np.dot(X12,w[1:3]),w[0])\nres = [sub[0] for sub in X12] \nX1 = res\nplt.plot(X1,y0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the implementation above gave a good imagination about the linear models that are in the 'Softmax Classifier', but now lets get more serious and try to implement an image classifier that classifies images:"},{"metadata":{},"cell_type":"markdown","source":"the data that we will be using is 1797 images that contain digits from 0 to 9"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.datasets import load_digits\ndigits = load_digits()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = digits.data\ny = digits.target","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some examples from the dataset:"},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(2, 3, sharex='col', sharey='row')\ncurrNum = 0\nfor i in range(2):\n    for j in range(3):\n        img = X[currNum,0:64]\n        currNum += 1\n        img = np.array(img, dtype='float')\n        pixels = img.reshape((8, 8))\n        ax[i, j].imshow(pixels, cmap='gray')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"first, lets define some useful functions:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#simple functions to add 1 to each sample's vector for the bias\ndef add_bias(X):\n    newX = [] \n    for i in range(X.shape[0]):\n        newX.append(np.append(X[i],1))\n    return np.asarray(newX)\nX = add_bias(X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#change the form of the target values from a single digit to a onehot so we can apply out algorithm\ntargets=[0,1,2,3,4,5,6,7,8,9]\ndef oneHot(y,targets):\n    newY = []\n    for i in range(y.shape[0]): \n        temp = [] \n        for j in targets:\n            if(y[i]==targets[j]):\n                temp.append(1)\n            else:\n                temp.append(0)\n        newY.append(temp)\n    return np.asarray(newY)\ny = oneHot(y,targets)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n = X.shape[0] #number of data samples\nm = X.shape[1] #number of features for each sample \nk = 10 #number of classes\nw = randomWeights(m,k)\nw=np.asarray(w,'float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"w.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = []\nmaxNumOfIterations = 20\nfor iteration in range(maxNumOfIterations): \n    print('iteration: ',iteration)\n    z = np.dot(X,w)\n    y_hat = []\n    for i in range(n):\n        y_hat.append(softmax(z[i]))\n    y_hat = np.asarray(y_hat)\n    history.append(cross_entropy(y,y_hat))\n    for j in range(k):\n        deltaTemp=0\n        for i in range(n):\n            deltaTemp += np.dot(X.T,(y-y_hat))\n        deltaTemp  = - deltaTemp/n\n        deltaTemp = np.asarray(deltaTemp)\n        w-=0.1*deltaTemp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history)\nplt.title('the change of loss with iterations')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"That's great we implemented a numbers classifier from scratch!!!!!!!"},{"metadata":{},"cell_type":"markdown","source":"now lets try to classify some samples:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def giveMeValueFromOneHot(y_hat):\n    return np.where(y_hat == 1)[0][0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predictDis(x):\n    img = np.array(x, dtype='float')\n    pixels = x[0:64].reshape((8, 8))\n    plt.imshow(pixels, cmap='gray')\n    z = np.dot(x,w)\n    print (\"the class of this Image is: \",giveMeValueFromOneHot(softmax(z)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = X[0]\npredictDis(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = X[1]\npredictDis(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = X[3]\npredictDis(x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"well that was satisfying, as we saw in this article, starting from simple math and some optimization algorithms  we were able to build an image classifier that give a reasonable accuracy."},{"metadata":{},"cell_type":"markdown","source":"some notes:\n- this article is mainly meant to be an introduction to ML for those who are new to the field so there are a lot of things that I purposely didn't mention like 'the accuracy measures', 'why used cross entropy not MSE in classification', 'how to optimize the Gradient descent',...\n- this article's Notebook notebook can be found on kaggle here: https://www.kaggle.com/tareqalbeesh/linear-models-deep-understanding"},{"metadata":{},"cell_type":"markdown","source":"**finally**:"},{"metadata":{},"cell_type":"markdown","source":"I hope the article/notebook was useful and gave you better understanding and more imagination about how linear models work, and will appreciate it if you gave me notes about the article/Notebook to help me write better in the future."},{"metadata":{},"cell_type":"markdown","source":"**References**: "},{"metadata":{},"cell_type":"markdown","source":"[1] Introduction to Machine Learning by Ethem Alpaydın  \n[2] https://mc.ai/an-introduction-to-gradient-descent-2/  \n[3] https://www.kaggle.com/tareqalbeesh/simple-binary-classification-data?select=binary_classification_simple.xls  \n[4] https://towardsdatascience.com/analytical-solution-of-linear-regression-a0e870b038d5  \n[5] https://www.kaggle.com/andonians/random-linear-regression?select=train.csv  "},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}