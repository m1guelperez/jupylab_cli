{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom scipy.stats import skew\nfrom scipy import stats\nfrom sklearn.metrics import mean_squared_error, make_scorer\n%matplotlib inline\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":26,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","scrolled":true,"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv', header=0, sep=',')\ntrain_df = train_df.loc[:,'MSSubClass':]\n#print('Training data\\n', train_df.head())\nprint('Training data columns:', train_df.columns)\nprint('Training data shape', train_df.shape)\n\ntest_df = pd.read_csv('../input/test.csv', header=0, sep=',')\ntest_df = test_df.loc[:,'MSSubClass':]\n#print('Test data\\n', test_df.head())\nprint('Training data columns:', test_df.columns)\nprint('Test data shape\\n', test_df.shape)","execution_count":27,"outputs":[]},{"metadata":{"_cell_guid":"8cb313ab-5498-4f16-87f0-ee242bfe65df","_uuid":"3da40041a72f914b7c9254c138b7fc4f2aeab4be"},"cell_type":"markdown","source":"Nice, after selecting for features which show min 0.2 pearson correlation with SalePrice we see that OverallQual has a very high pos corr with SalePrice. Here we can see that there are few features e.g. **GarageCars and GarageArea** are **highly correlated**. We can decide on taking only one of them to reduce dimensionality by taking out highly correlative featres. I will take GarageCars as it gives better correlation with SalePrice. "},{"metadata":{"_cell_guid":"332d51a3-a3fd-4939-9fc5-b3915c756945","_uuid":"ec4ce24a07129551671d24c8bef30034aab3efbf","scrolled":true,"trusted":true},"cell_type":"code","source":"# Missing data\nprint('Missing data points in every features')\ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data[:20])","execution_count":28,"outputs":[]},{"metadata":{"_cell_guid":"d37b229e-e862-4bd4-97f9-3dc7d5ad7cf9","_uuid":"81e71949997925a271e8ace9640ce8af18c07cc9"},"cell_type":"markdown","source":"So we have some features with NULL values *damn!!*, good thing there are a few of them. We can decide if we want to fill the null values, remove the data point or remove the feature. First we will remove all the columns with missing data and train a model."},{"metadata":{"_cell_guid":"aa6d74e6-d7f7-467a-9a10-ba57dc66e1b0","_uuid":"e50f91737155608c2042555e9fc441150e41bea2","trusted":true},"cell_type":"code","source":"# Findout how many of the columns are caregorical or numerical\nnan_columns = missing_data[missing_data['Percent'] > 0.0].index\nfilr_train_df = train_df[missing_data[missing_data['Percent'] <= 0.0].index]\nnumr_cols = filr_train_df.dtypes[filr_train_df.dtypes != \"object\"].index # Numerical columns\ncatg_cols = filr_train_df.dtypes[filr_train_df.dtypes == \"object\"].index # Categorical columns\nprint(catg_cols)","execution_count":29,"outputs":[]},{"metadata":{"_cell_guid":"c9969c24-6ee1-4e8f-b20a-902e63da650a","_uuid":"18b9a8248009ab50289a618d6d892d73969ecd7d","trusted":true},"cell_type":"code","source":"#dealing with missing data\nfor ind, col in filr_train_df[catg_cols].iteritems(): # What kind of data in categorical columns\n    print(ind, set(list(col)))","execution_count":30,"outputs":[]},{"metadata":{"_cell_guid":"54d379d5-3707-43c2-9b86-bc4824cbca89","_uuid":"f23e6bdcd86559c9dc0691d05572bd39a1a15ebf","scrolled":false,"trusted":true},"cell_type":"code","source":"# Corrilation of numerical features with sales prize\nprint('Number of features:', len(filr_train_df.columns))\nfilr_train_df_corr = filr_train_df.corr()\nhighly_corr = filr_train_df_corr[(filr_train_df_corr['SalePrice'] > 0.1)].index # No negative correlation in data\ncorr_sale = filr_train_df.loc[:,highly_corr].corr()\nprint('Number of features with corr:', len(corr_sale))\nplt.figure(figsize=(10,10))\nsns.heatmap(corr_sale, annot=True, square=True)","execution_count":31,"outputs":[]},{"metadata":{"_cell_guid":"d6ac052b-a08b-4fe5-9dad-9b13ee2a2c72","_uuid":"ef1b267a45e5a59ca340d2e433c35fb29693f198"},"cell_type":"markdown","source":"We see some features showing a nice correlation with SalePrice, that is really good."},{"metadata":{"_cell_guid":"539e799d-3ddf-447d-803d-6c7ba46b8b69","_uuid":"b7b8ad3fd123844b6f03e6f5ce72c5b5cb1efb1d","trusted":true},"cell_type":"code","source":"# Now check for skewnwss of numerical features\nskewed_feats = filr_train_df[highly_corr].apply(lambda x: skew(x.dropna())) #compute skewness\nprint('Skewness in feature data\\n',skewed_feats)","execution_count":32,"outputs":[]},{"metadata":{"_cell_guid":"ff119d1d-9c37-4a8f-aa9f-291629848a2b","_uuid":"e9bfe9db3d3b1b8fb3a6943bb5c99967ba8a3b27"},"cell_type":"markdown","source":"Now when we look at skewness we see there are multiple features with **positive skewness**. This can be improved with log transformation and take care of zeros in the data we will + 1 and logtransform. But i think we should only log transform numerical (non categorical) features. "},{"metadata":{"_cell_guid":"700d5976-9a9d-49b4-bfe2-c307f9da2d0b","_uuid":"37c65f5031b0a64335ba134658736552011c0c5f","trusted":true},"cell_type":"code","source":"# Correct skewness for features with positive skewness > 0.5 with log1p transformation\nLT_columns = []\nnumLT_train_df = pd.DataFrame()\nfor ind, skew in skewed_feats.iteritems():\n    if (skew > 0.5):\n        numLT_train_df = pd.concat([numLT_train_df, np.log1p(filr_train_df[ind])], axis=1)\n        LT_columns.append(ind)\n    else:\n        numLT_train_df = pd.concat([numLT_train_df, filr_train_df[ind]], axis=1)\n        \n# Example of skewness\nprint('Example feature:', LT_columns[0])\nskew_ex = pd.DataFrame({'Not_trsf': filr_train_df[LT_columns[0]], 'log_trsf':numLT_train_df[LT_columns[0]]})\nskew_ex.hist()","execution_count":33,"outputs":[]},{"metadata":{"_cell_guid":"674f8f46-b843-4eaf-a43c-7e9f2cbbbf20","_uuid":"b793e0886c7c3fd62fece328322dd5eae41cf6eb","trusted":true},"cell_type":"code","source":"# Can also look at normal prob plot\n#histogram and normal probability plot\nsns.set()\nsns.distplot(numLT_train_df[LT_columns[0]], fit=norm);\nfig = plt.figure()\nres = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)","execution_count":34,"outputs":[]},{"metadata":{"_cell_guid":"d4a9a87a-98da-475b-a454-80bb7ec91cad","_uuid":"d66e3b95435071390955381c23898a2ccd5ee830","trusted":true},"cell_type":"code","source":"print(numLT_train_df.shape)\nsns.pairplot(numLT_train_df.iloc[:,:5])","execution_count":35,"outputs":[]},{"metadata":{"_cell_guid":"6666d7de-278c-444a-9ee8-b77bcd5ad236","_uuid":"2d0ec73931669466d704e97366b361e3e2ae9d23"},"cell_type":"markdown","source":""},{"metadata":{"_cell_guid":"29fcb519-dc6a-4ca8-aeec-0a0683407294","_uuid":"92db54c924a91fc09d3347183066a58b8873f8d6"},"cell_type":"markdown","source":"Based on descriptive statistics, I would like to manually select features for training the model. \n- First I would like to remove highly correlative features\n- Features Remove: 'LotFrontage', 'GarageArea', 'GarageyearBuilt', 'TotRmsAbvGrd', 'TotalBsmtSF'\n- Predicted Log1p: 'SalePrice'\n- Create dummy variable for categorical features.\n"},{"metadata":{"_cell_guid":"66ac0a96-3602-41dc-bc01-ba1a3446af52","_uuid":"452ae3866adcba55a4da06138371c688434036ba"},"cell_type":"markdown","source":"**Start the modelling:** Ok, so untill now I have learned about data and gathered information to preprocess it. Now we will start the the modelling."},{"metadata":{"_cell_guid":"d0d55bd0-1701-4786-91d2-e786e026f5d1","_uuid":"9c2b050793ae5fa8457882e175fb77d43a00dcf0","trusted":true},"cell_type":"code","source":"# What features we want to take\ncatg_cols # these are our categorical features\nhighly_corr # numeric features which we have selected based on correlation\nLT_columns # numeric features which needs to be transformed","execution_count":36,"outputs":[]},{"metadata":{"_cell_guid":"4312fcf5-94a7-4db7-91dd-50a83c4b05ef","_uuid":"92f8bf8306ed3e4beb233e34ed872cda2a4df5cb","scrolled":true,"trusted":true},"cell_type":"code","source":"# Concat test and training for preprocessing\ntot_data = pd.concat([train_df.loc[:,'MSSubClass':], test_df.loc[:,'MSSubClass':]], ignore_index=True)\nprint(tot_data.shape)\nprint(tot_data.head())","execution_count":37,"outputs":[]},{"metadata":{"_cell_guid":"40713d89-1dea-42ae-9ec2-6e68502a2775","_uuid":"875d673830868d0c9a71c83ccb7dc81f4337da17","scrolled":true,"trusted":true},"cell_type":"code","source":"# Create dummy variable for categorical features\ntot_data_cat = pd.get_dummies(tot_data[catg_cols])\ntot_data_cat.head()","execution_count":38,"outputs":[]},{"metadata":{"_cell_guid":"26cc20c9-c8e2-453c-9c46-68e504ac29ec","_uuid":"31d73dd62549f8c5f113ace9c79639ed1457741b","trusted":true},"cell_type":"code","source":"# Log transform numerical data\ntot_data_num = tot_data[highly_corr]\nprint(tot_data_num.shape)\nfor cols in LT_columns:\n    tot_data_num.loc[:,cols] = np.log1p(tot_data_num.loc[:,cols])\nprint(tot_data_num.shape)\ntot_data_num.head()","execution_count":39,"outputs":[]},{"metadata":{"_cell_guid":"2308e33f-c32e-4375-aa72-d07adfeeafe3","_uuid":"8de144d7228cbd60bbc1c348c6be25e6ec3e1f5d","scrolled":true,"trusted":true},"cell_type":"code","source":"# Combining preprocessed data\ntot_data_pro = pd.concat([tot_data_cat, tot_data_num],axis=1)\ntot_data_pro.head()","execution_count":40,"outputs":[]},{"metadata":{"_cell_guid":"92e7a0e5-f019-49f7-b852-3090df5aa025","_uuid":"a72e6dea75c653c6732b4fdf61ca81b3820198d8","trusted":true},"cell_type":"code","source":"# Creating matrix for sklern \npr_trainData = tot_data_pro[:1000]\npr_testData = tot_data_pro[1000:1400]\npr_testData = pr_testData.fillna(pr_testData.mean())\nprint(pr_trainData.shape, pr_testData.shape)\nY = pr_trainData['SalePrice']\nX_trainData = pr_trainData.drop('SalePrice', axis=1)\nX_testData = pr_testData.drop('SalePrice', axis=1)\nprint(X_trainData.shape, X_testData.shape)","execution_count":60,"outputs":[]},{"metadata":{"_cell_guid":"313ed677-e2b9-4145-8b7e-9bdc63224ef8","_uuid":"110a76b3218708b0fe7ea95e8d23eb02f51b8aaa"},"cell_type":"markdown","source":"# Modelling"},{"metadata":{"_cell_guid":"b3cf5feb-4c1f-445d-8ca8-1b14d9450ecd","_uuid":"d095fd810a94d971aa72ea5d4e3a2eefad812dae"},"cell_type":"markdown","source":"Now we are going to use regularized linear regression models from the scikit learn module. I'm going to try both l_1(Lasso) and l_2(Ridge) regularization. I'll also define a function that returns the cross-validation rmse error so we can evaluate our models and pick the best tuning par"},{"metadata":{"_cell_guid":"a6a41f76-eaf3-466b-85f3-0b1b0262d05f","_uuid":"e2b6873f587a98a36759140b5f7a77b7cb282a43","trusted":true,"collapsed":true},"cell_type":"code","source":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\n\n# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_trainData, Y, scoring = scorer, cv = 5))\n    return(rmse)","execution_count":42,"outputs":[]},{"metadata":{"_cell_guid":"5ac35a73-e4f1-43b6-ada8-0c9c9b05102e","_uuid":"b8ba9ec191ef98810882495b5d3871be3c50c838"},"cell_type":"markdown","source":"**RidgeCV**\n\nRegularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.\nRidge regression is an L2 penalized model where we simply add the squared sum of the weights to our cost function."},{"metadata":{"_cell_guid":"03eeee2b-7435-40cc-9c2d-ddcb05f96b77","_uuid":"6f400adbacaaac58395255e19f74765e8eb3d84d","scrolled":false,"trusted":true},"cell_type":"code","source":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_trainData, Y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 5)\nridge.fit(X_trainData, Y)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\ny_train_rdg = ridge.predict(X_trainData)\ny_test_rdg = ridge.predict(X_testData)\n# Plot residuals\nsns.set()\nplt.scatter(y_train_rdg, y_train_rdg - Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)\nplt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, Y, c = \"blue\", marker = \"o\", label = \"Training data\", alpha=0.7)\nplt.scatter(y_test_rdg, pr_testData['SalePrice'], c = \"green\", marker = \"o\", label = \"Validation data\", alpha=0.7)\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_trainData.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","execution_count":61,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4c0b110e908351e52622f25a29a06640a78a2ce6"},"cell_type":"code","source":"# Here we can see the predicted price\n#saleprice = pd.Series(y_test_rdg, index=X_testData.index, name='SalePrice')\n#pd.concat([X_testData, saleprice], axis=1).head()","execution_count":59,"outputs":[]},{"metadata":{"_cell_guid":"849cb19c-1789-4701-8116-8d512c36bc97","_uuid":"57cbb3a3b37d9820916ae91755b11da334064624"},"cell_type":"markdown","source":"\n**Linear Regression with Lasso regularization (L1 penalty)**\n\nLASSO stands for Least Absolute Shrinkage and Selection Operator. It is an alternative regularization method, where we simply replace the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.\n\nWe can suspect that it should be more efficient than Ridge here.\n"},{"metadata":{"_cell_guid":"be47970d-c602-4521-93f2-1286cbbcf81d","_uuid":"444f08bf12dc125b5b8b6496fe739f60b972d3e3","scrolled":false,"trusted":true},"cell_type":"code","source":"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_trainData, Y)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_trainData, Y)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\ny_train_las = lasso.predict(X_trainData)\ny_test_las = lasso.predict(X_testData)\n\n# Plot residuals\nplt.scatter(y_train_las, y_train_las - Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)\nplt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_las, Y, c = \"blue\", marker = \"s\", label = \"Training data\", alpha=0.7)\nplt.scatter(y_test_las, pr_testData['SalePrice'], c = \"green\", marker = \"s\", label = \"Validation data\", alpha=0.7)\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()\n\n# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_trainData.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","execution_count":25,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}