content,tag,output_type,original_content,y_pred
df_train['price_doc'].hist(bins=50),1,execute_result,df_train['price_doc'].hist(bins=50),0
df['v2a1'].hist(),1,not_existent,df['v2a1'].hist(),0
df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),1,not_existent,df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),0
df_hwoh['v2a1'].hist(),1,not_existent,df_hwoh['v2a1'].hist(),0
df['escolari'].hist(),1,not_existent,df['escolari'].hist(),0
df['overcrowding'].hist(),1,not_existent,df['overcrowding'].hist(),0
df['Target'].hist(),1,not_existent,df['Target'].hist(),0
"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",1,not_existent,"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",0
"full_report(train_df, target_column='logerror')",1,display_data,"full_report(train_df, target_column='logerror')",0
"SETUP CHECKPOINT sns.set(style=""white"", color_codes=True) warnings.filterwarnings(""ignore"") print(os.listdir()) pio.renderers.default = ""browser""",0,stream,"# This Python 3 environment comes with many helpful analytics libraries installed  # Loading datasets required for analysis    import numpy as np # linear algebra  import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)  import seaborn as sns  import matplotlib.pyplot as plt  sns.set(style=""white"", color_codes=True)  import warnings # current version of seaborn generates a bunch of warnings that we'll ignore  warnings.filterwarnings(""ignore"")    # Input data files are available in the ""../input/"" directory.  import os  print(os.listdir(""./""))  #print(os.listdir(""../input/input""))  %load_ext autoreload  %autoreload 2  import plotly.io as pio  pio.renderers.default = ""browser""",1
"ASSIGN = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, items.family.value_counts().values), ASSIGN = False))) ASSIGN = go.Bar( ASSIGN = items.family.value_counts().values, ASSIGN = items.family.value_counts().index ) ASSIGN = dict( ASSIGN='Counts of items per family category', ASSIGN = 900, height = 600, ASSIGN=dict( ASSIGN = True, ASSIGN = True, ASSIGN = True )) ASSIGN = go.Figure(data=[trace2]) ASSIGN['ASSIGN'].update(ASSIGN) py.iplot(ASSIGN, filename='plots')",1,display_data,"# BAR PLOT FOR ITEMS V/S FAMILY TYPE  x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index,                                            items.family.value_counts().values),                                           reverse = False)))  trace2 = go.Bar(      y = items.family.value_counts().values,      x = items.family.value_counts().index  )    layout = dict(      title='Counts of items per family category',       width = 900, height = 600,      yaxis=dict(          showgrid = True,          showline = True,          showticklabels = True      ))    fig1 = go.Figure(data=[trace2])  fig1['layout'].update(layout)  py.iplot(fig1, filename='plots')",0
display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT)) +' customers in the list who have more balance than the credit limit assigned. ' +'It may be due to more payament than usage andpath<path>')),1,display_data,display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))               +' customers in the list who have more balance than the credit limit assigned. '               +'It may be due to more payament than usage and/or continuous pre-payment.</h4>')),0
"X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)",1,execute_result,"X.boxplot(figsize = (30,25),grid=True,fontsize=25,rot=90)",0
ASSIGN = model.fit_predict(X) display(HTML('<b>The model has converged :<path>'+str(model.converged_))) display(HTML('<b>The model has taken iterations :<path>'+str(model.n_iter_))),1,display_data,clusters = model.fit_predict(X)  display(HTML('<b>The model has converged :</b>'+str(model.converged_)))  display(HTML('<b>The model has taken iterations :</b>'+str(model.n_iter_))),0
for i in range(7): display(HTML('<h2>Cluster'+str(i)+'<path>')) cc1[cc1.cluster == i].describe(),1,display_data,"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T  for i in range(7):      display(HTML('<h2>Cluster'+str(i)+'</h2>'))      cc1[cc1.cluster == i].describe()",0
grafico('Sex'),1,display_data,grafico('Sex'),0
grafico('Pclass'),1,display_data,grafico('Pclass'),0
grafico('SibSp'),1,display_data,grafico('SibSp'),0
grafico('Parch'),1,display_data,grafico('Parch'),0
grafico('Embarked'),1,display_data,grafico('Embarked'),0
grafico('Title'),1,display_data,grafico('Title'),0
"ASSIGN = sb.FacetGrid(train, hue=""Survived"", aspect=4) ASSIGN.map(sb.kdeplot, 'Age', shade=True) ASSIGN.set(xlim=(0, train['Age'].max())) ASSIGN.add_legend() pl.show()",1,display_data,"facet = sb.FacetGrid(train, hue=""Survived"", aspect=4) facet.map(sb.kdeplot, 'Age', shade=True) facet.set(xlim=(0, train['Age'].max())) facet.add_legend() pl.show()",0
grafico('Age'),1,display_data,grafico('Age'),0
"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts() Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts() Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",0,execute_result,"Pclass1 = train[train['Pclass']==1]['Embarked'].value_counts() Pclass2 = train[train['Pclass']==2]['Embarked'].value_counts() Pclass3 = train[train['Pclass']==3]['Embarked'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))",1
"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",0,execute_result,"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))",1
"ASSIGN = ['Ticket', 'SibSp', 'Parch'] ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(['PassengerId'], axis=1) ASSIGN = train.drop('Survived', axis=1) ASSIGN = train['Survived'] train_data.shape, target.shape",0,execute_result,"features_drop = ['Ticket', 'SibSp', 'Parch'] train = train.drop(features_drop, axis=1) test = test.drop(features_drop, axis=1) train = train.drop(['PassengerId'], axis=1) train_data = train.drop('Survived', axis=1) target = train['Survived']  train_data.shape, target.shape",1
msno.matrix(df_train.sample(250));,1,display_data,msno.matrix(df_train.sample(250));,0
show_batch(train_dl),1,display_data,show_batch(train_dl),0
"ASSIGN=[] ASSIGN=[] for dirname, _, filenames in os.walk('..path'): for filename in filenames[0:9]: ASSIGN.append( cv2.imread(dirname+""path""+ filename,0)) ASSIGN.append(""1"") DisplayImage(ASSIGN,ASSIGN,3)",1,display_data,"images=[] titles=[] for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):     for filename in filenames[0:9]:                 images.append( cv2.imread(dirname+""/""+ filename,0))         titles.append(""1"")                   DisplayImage(images,titles,3)",0
"twoplot(train, 'magic1')",1,display_data,"twoplot(train, 'magic1')",0
"twoplot(train, 'magic2')",1,display_data,"twoplot(train, 'magic2')",0
"twoplot(train, 'magic3')",1,display_data,"twoplot(train, 'magic3')",0
"twoplot(train, 'magic4')",1,display_data,"twoplot(train, 'magic4')",0
"twoplot(train, 'Duration')",1,display_data,"twoplot(train, 'Duration')",0
"twoplot(train, 'StartTime')",1,display_data,"twoplot(train, 'StartTime')",0
"twoplot(train, 'EndTime')",1,display_data,"twoplot(train, 'EndTime')",0
ShowTrainingData2(5),1,display_data,ShowTrainingData2(5),0
"ShowPredictions(""CNN"")",1,display_data,"  ShowPredictions(""CNN"")   ",0
"def display_all(df): ''' input: dataframe description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen ''' with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers. display(df)",1,not_existent,"def display_all(df):      '''      input: dataframe      description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen      '''      with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers.          display(df)",0
display_all(df),1,display_data,display_all(df),0
"SETUP CHECKPOINT sns.set_style(""whitegrid"") plt.style.use(""fivethirtyeight"") for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename)) warnings.filterwarnings('ignore')",0,stream,"import pandas as pd import numpy as np import matplotlib.pyplot as plt %matplotlib inline import seaborn as sns sns.set_style(""whitegrid"") plt.style.use(""fivethirtyeight"")  #Showing full path of datasets import os for dirname, _, filenames in os.walk('/kaggle/input'):     for filename in filenames:         print(os.path.join(dirname, filename))           #Disable warnings import warnings warnings.filterwarnings('ignore')",1
SETUP ASSIGN = Image(filename='path') display(ASSIGN),1,error,from IPython.display import Image   pil_img = Image(filename='/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/s/color_18_0100.png')    display(pil_img),0
"plt.scatter(shyness_score, friendship_len)",1,execute_result,"plt.scatter(shyness_score, friendship_len)",0
compare_models(),1,display_data,compare_models(),0
ASSIGN = create_model('ASSIGN'),1,display_data,lr = create_model('lr'),0
evaluate_model(lr),1,display_data,evaluate_model(lr),0
ASSIGN = pd.read_csv('path') display(ASSIGN.head(5)),1,display_data,# Read in meat DataFrame  meat = pd.read_csv('/kaggle/input/week6dataset/meat.csv')    # Review the first five lines of the meat DataFrame  display(meat.head(5)),0
ASSIGN = pd.to_datetime(ASSIGN ) ASSIGN = ASSIGN.set_index('date') display(ASSIGN.describe()),1,display_data,# Convert the date column to a datestamp type  meat['date'] = pd.to_datetime(meat['date'] )    # Set the date column as the index of your DataFrame meat  meat = meat.set_index('date')    # Print the summary statistics of the DataFrame  display(meat.describe()),0
"CHECKPOINT ASSIGN = pd.read_csv('path', parse_dates = ['datestamp']) display(ASSIGN.head(5)) print(ASSIGN.dtypes) ASSIGN = pd.to_datetime(ASSIGN) ASSIGN = ASSIGN.set_index('datestamp') display(ASSIGN.isnull().sum())",1,display_data,"# Read in jobs file  jobs = pd.read_csv('/kaggle/input/week6dataset/employment.csv', parse_dates = ['datestamp'])    # Review the first five lines of your DataFrame  display(jobs.head(5))    # Review the type of each column in your DataFrame  print(jobs.dtypes)    # Convert datestamp column to a datetime object  jobs['datestamp'] = pd.to_datetime(jobs['datestamp'])    # Set the datestamp columns as the index of your DataFrame  jobs = jobs.set_index('datestamp')    # Check the number of missing values in each column  display(jobs.isnull().sum())",0
SETUP sns.countplot(Y_train),1,execute_result,import seaborn as sns  %matplotlib inline  sns.countplot(Y_train),0
SETUP train.profile_report(),1,display_data,import pandas_profiling     train.profile_report(),0
"SETUP def display_category(urls, category_name): ASSIGN = ""width: 180px; margin: 0px; float: left; border: 1px solid black;"" ASSIGN = ''.join([f""<img style='{img_style}' src='{u}' path>"" for _, u in urls.head(12).iteritems()]) display(HTML(ASSIGN)) ASSIGN = train['landmark_id'].value_counts().keys()[0] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"from IPython.display import Image from IPython.core.display import HTML   def display_category(urls, category_name):     img_style = ""width: 180px; margin: 0px; float: left; border: 1px solid black;""     images_list = ''.join([f""<img style='{img_style}' src='{u}' />"" for _, u in urls.head(12).iteritems()])      display(HTML(images_list))  category = train['landmark_id'].value_counts().keys()[0] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")      ",0
"ASSIGN = train['landmark_id'].value_counts().keys()[1] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"category = train['landmark_id'].value_counts().keys()[1] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")",0
"ASSIGN = train['landmark_id'].value_counts().keys()[2] ASSIGN = train[train['landmark_id'] == category]['url'] display_category(ASSIGN, """")",1,not_existent,"category = train['landmark_id'].value_counts().keys()[2] urls = train[train['landmark_id'] == category]['url'] display_category(urls, """")",0
"display(df_belem.shape, df_curitiba.shape)",1,display_data,"#Questão 1  display(df_belem.shape, df_curitiba.shape)",0
"df_belem.set_index('YEAR',inplace=True) df_curitiba.set_index('YEAR',inplace=True) display(df_belem.head()) display(df_curitiba.head())",1,display_data,"#Questão 2  df_belem.set_index('YEAR',inplace=True)  df_curitiba.set_index('YEAR',inplace=True)  display(df_belem.head())  display(df_curitiba.head())",0
display(df_belem['JAN'].value_counts()) display(df_curitiba['JAN'].value_counts()),1,display_data,#mostra a quantidade de valores únicos no mẽs de janeiro  #verificando os valores únicos confirmamos que o valor 999.90 é o único outlier  display(df_belem['JAN'].value_counts())  display(df_curitiba['JAN'].value_counts()),0
"ASSIGN = df_belem.replace(999.90,np.nan) ASSIGN = ASSIGN.fillna(ASSIGN.mean()) display(ASSIGN) ASSIGN = df_curitiba.replace(999.90,np.nan) ASSIGN = ASSIGN.fillna(ASSIGN.mean()) display(ASSIGN)",1,display_data,"#exercício 4  #para tratar os outliers, podemos excluir os dados ausentes (999.90) ou substituí-lo pela média do ano anterior e posterior.  #adotarei a solução de substituir os nulos pela média.      #cria um novo dataset transformando o outlier em nulo para aplicação das funções de tratamento  df_belem_t = df_belem.replace(999.90,np.nan)  #substitui os valores nulos restantes pela média do ano anterior e posterior  df_belem_t = df_belem_t.fillna(df_belem_t.mean())  display(df_belem_t)    #cria um novo dataset transformando o outlier em nulo para aplicação das funções de tratamento  df_curitiba_t = df_curitiba.replace(999.90,np.nan)  #substitui os valores nulos restantes pela média do ano anterior e posterior  df_curitiba_t = df_curitiba_t.fillna(df_curitiba_t.mean())  display(df_curitiba_t) ",0
"display(df_curitiba_t['JUL'].describe()) display(df_belem_t['JUL'].describe()) stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])",1,display_data,"#questão 6  display(df_curitiba_t['JUL'].describe())  display(df_belem_t['JUL'].describe())  stats.f_oneway(df_belem_t['JUL'], df_curitiba_t['JUL'])",0
"ASSIGN = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN']) ASSIGN['A1'] = ASSIGN['JAN'].shift(1) ASSIGN['A2'] = ASSIGN['JAN'].shift(2) ASSIGN['A3'] = ASSIGN['JAN'].shift(3) ASSIGN = ASSIGN.dropna() display(ASSIGN.head()) X_train, X_test, y_train, y_test = model_selection.train_test_split(ASSIGN.drop(columns=['JAN']),ASSIGN['JAN'],test_size=0.25, random_state=33)",1,display_data,"#exercício 7  df_curitiba_jan = pd.DataFrame(df_curitiba_t['JAN'],columns=['JAN'])  #cria o dataset de previsão com os valores dos 3 anos anteriores  df_curitiba_jan['A1'] = df_curitiba_jan['JAN'].shift(1)  df_curitiba_jan['A2'] = df_curitiba_jan['JAN'].shift(2)  df_curitiba_jan['A3'] = df_curitiba_jan['JAN'].shift(3)  #dropa os primeiros anos (que não tem anos anteriores para montar o dataset)  df_curitiba_jan = df_curitiba_jan.dropna()  display(df_curitiba_jan.head())  #separa em conjuntos de teste e treinamento  X_train, X_test, y_train, y_test = model_selection.train_test_split(df_curitiba_jan.drop(columns=['JAN']),df_curitiba_jan['JAN'],test_size=0.25, random_state=33) ",0
"CHECKPOINT ASSIGN = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths']) plot",0,execute_result,"plot = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths'])  plot",1
"ASSIGN = df_india_data.groupby(['Date', 'Statepath'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False) ex.line(ASSIGN, x=""Date"", y=""TotalConfirmed"", color='Statepath', title='ASSIGN over time', height=600)",1,display_data,"Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)    ex.line(Spread, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', title='Spread over time', height=600)",0
"ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), ASSIGN=""TotalConfirmed"", y=""Statepath"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",1,display_data,"Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True),                x=""TotalConfirmed"", y=""State/UnionTerritory"", title='Confirmed Cases', text='TotalConfirmed', orientation='h',                width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])  Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Confirmed_bar.show()",0
"ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), ASSIGN=""Deaths"", y=""Statepath"", title='Death in each state', text='Deaths', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",1,display_data,"Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True),                x=""Deaths"", y=""State/UnionTerritory"", title='Death in each state', text='Deaths', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])  Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Death_rate_bar.show()",0
"ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), ASSIGN=""Cured"", y=""Statepath"", title='Cured cases', text='Cured', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",1,display_data,"cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True),                x=""Cured"", y=""State/UnionTerritory"", title='Cured cases', text='Cured', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])  cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  cure_bar.show()",0
"ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), ASSIGN=""Active"", y=""Statepath"", title='Active cases', text='Active', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",1,display_data,"Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True),                x=""Active"", y=""State/UnionTerritory"", title='Active cases', text='Active', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Active'])+10])  Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Active_cases.show()",0
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",1,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)  Top_50 = latest_date[latest_date['TotalConfirmed']>20]  Top_50 = Top_50.sort_values('Death Rate', ascending=False)    Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),                x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h',                width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')  Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')  Plot.show()",0
"ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= cum_var_exp, ASSIGN='lines+markers', ASSIGN=""'Cumulative Explained Variance'"", ASSIGN=dict( ASSIGN='spline', ASSIGN = 'goldenrod' ) ) ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= var_exp, ASSIGN='lines+markers', ASSIGN=""'Individual Explained Variance'"", ASSIGN=dict( ASSIGN='linear', ASSIGN = 'black' ) ) ASSIGN = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}], ASSIGN=True) ASSIGN.append_trace(ASSIGN, 1, 1) ASSIGN.append_trace(ASSIGN,1,1) ASSIGN.layout.title = 'Explained Variance plots - Full and Zoomed-in' ASSIGN.layout.xaxis = dict(range=[0, 80], title = 'Feature columns') ASSIGN.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')",1,stream,"trace1 = go.Scatter(      x=list(range(784)),      y= cum_var_exp,      mode='lines+markers',      name=""'Cumulative Explained Variance'"",  #     hoverinfo= cum_var_exp,      line=dict(          shape='spline',          color = 'goldenrod'      )  )  trace2 = go.Scatter(      x=list(range(784)),      y= var_exp,      mode='lines+markers',      name=""'Individual Explained Variance'"",  #     hoverinfo= var_exp,      line=dict(          shape='linear',          color = 'black'      )  )  fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],                            print_grid=True)    fig.append_trace(trace1, 1, 1)  fig.append_trace(trace2,1,1)  fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'  fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')  fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')  # fig['data'] = []  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))    # fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]  # fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]    # # fig['data'] = data  # # fig['layout'] = layout  # # fig['data'] += data2  # # fig['layout'] += layout2  # py.iplot(fig, filename='inset example')",0
"8 ASSIGN = go.Scatter( ASSIGN = X_LDA_2D[:,0], ASSIGN = X_LDA_2D[:,1], ASSIGN = 'markers', ASSIGN = Target, ASSIGN = True, ASSIGN = dict( ASSIGN = 8, ASSIGN = Target, ASSIGN ='Jet', ASSIGN = False, ASSIGN = dict( ASSIGN = 2, ASSIGN = 'rgb(255, 255, 255)' ), ASSIGN = 0.8 ) ) ASSIGN = [traceLDA] ASSIGN = go.Layout( ASSIGN= 'Linear Discriminant Analysis (LDA)', ASSIGN= 'closest', ASSIGN= dict( ASSIGN= 'First Linear Discriminant', ASSIGN= 5, ASSIGN= False, ASSIGN= 2, ), ASSIGN=dict( ASSIGN= 'Second Linear Discriminant', ASSIGN= 5, ASSIGN= 2, ), ASSIGN= False ) ASSIGN = dict(data=data, layout=layout) py.iplot(ASSIGN, filename='styled-scatter')",1,display_data,"8# Using the Plotly library again  traceLDA = go.Scatter(      x = X_LDA_2D[:,0],      y = X_LDA_2D[:,1],  #     name = Target,  #     hoveron = Target,      mode = 'markers',      text = Target,      showlegend = True,      marker = dict(          size = 8,          color = Target,          colorscale ='Jet',          showscale = False,          line = dict(              width = 2,              color = 'rgb(255, 255, 255)'          ),          opacity = 0.8      )  )  data = [traceLDA]    layout = go.Layout(      title= 'Linear Discriminant Analysis (LDA)',      hovermode= 'closest',      xaxis= dict(           title= 'First Linear Discriminant',          ticklen= 5,          zeroline= False,          gridwidth= 2,      ),      yaxis=dict(          title= 'Second Linear Discriminant',          ticklen= 5,          gridwidth= 2,      ),      showlegend= False  )    fig = dict(data=data, layout=layout)  py.iplot(fig, filename='styled-scatter')",0
"plotTheLineWithData(X,w)",1,not_existent,"plotTheLineWithData(X,w)",0
"SETUP init_notebook_mode(connected=True) ASSIGN = pd.read_csv(""path"") ASSIGN.head() ASSIGN = ASSIGN.rename(columns={'Countrypath':'Country'}) ASSIGN = ASSIGN.rename(columns={'ObservationDate':'Date'}) ASSIGN = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False) ASSIGN = ASSIGN.drop_duplicates(subset = ['Country']) ASSIGN = ASSIGN[ASSIGN['Confirmed']>0] ASSIGN = df[df['Confirmed']>0] ASSIGN = ASSIGN.groupby(['Date','Country']).sum().reset_index() df_countrydate ASSIGN = px.choropleth(df_countrydate, ASSIGN=""Country"", ASSIGN = ""country names"", ASSIGN=""Confirmed"", ASSIGN=""Country"", ASSIGN=""Date"" ) ASSIGN.update_layout( ASSIGN = 'Global Spread of Coronavirus', ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",1,display_data,"# Import libraries import numpy as np  import pandas as pd  import plotly as py import plotly.express as px import plotly.graph_objs as go from plotly.subplots import make_subplots from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot init_notebook_mode(connected=True) # Read Data df = pd.read_csv(""/kaggle/input/novel-corona-virus-2019-dataset/covid_19_data.csv"") df.head()  # Rename columns df = df.rename(columns={'Country/Region':'Country'}) df = df.rename(columns={'ObservationDate':'Date'}) # Manipulate Dataframe df_countries = df.groupby(['Country', 'Date']).sum().reset_index().sort_values('Date', ascending=False) df_countries = df_countries.drop_duplicates(subset = ['Country']) df_countries = df_countries[df_countries['Confirmed']>0] df_countrydate = df[df['Confirmed']>0] df_countrydate = df_countrydate.groupby(['Date','Country']).sum().reset_index() df_countrydate # Creating the visualization fig = px.choropleth(df_countrydate,                      locations=""Country"",                      locationmode = ""country names"",                     color=""Confirmed"",                      hover_name=""Country"",                      animation_frame=""Date""                    ) fig.update_layout(     title_text = 'Global Spread of Coronavirus',     title_x = 0.5,     geo=dict(         showframe = False,         showcoastlines = False,     ))      fig.show()",0
"ASSIGN = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] ASSIGN = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True), ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True) ,ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True),ASSIGN.value_counts(normalize=True)], ASSIGN=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59' ,'age60-69','age70-79','age80+']).T ASSIGN = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35))",1,not_existent," age_18_21 = mulcA.Q5[(mulcA.Q2 == '18-21')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_22_24 = mulcA.Q5[(mulcA.Q2 == '22-24')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_25_29 = mulcA.Q5[(mulcA.Q2 == '25-29')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_30_34 = mulcA.Q5[(mulcA.Q2 == '30-34')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_35_39 = mulcA.Q5[(mulcA.Q2 == '35-39')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_40_44 = mulcA.Q5[(mulcA.Q2 == '40-44')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_45_49 = mulcA.Q5[(mulcA.Q2 == '45-49')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_50_54 = mulcA.Q5[(mulcA.Q2 == '50-54')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_55_59 = mulcA.Q5[(mulcA.Q2 == '55-59')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_60_69 = mulcA.Q5[(mulcA.Q2 == '60-69')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_70_79 = mulcA.Q5[(mulcA.Q2 == '70-79')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')] age_80m = mulcA.Q5[(mulcA.Q2 == '80+')&(mulcA.Q5 != 'Other')&(mulcA.Q5 != 'I never declared a major')]  df_arol = pd.DataFrame([age_18_21.value_counts(normalize=True),age_22_24.value_counts(normalize=True),age_25_29.value_counts(normalize=True),age_30_34.value_counts(normalize=True),                         age_35_39.value_counts(normalize=True),age_40_44.value_counts(normalize=True),age_45_49.value_counts(normalize=True),age_50_54.value_counts(normalize=True)                         ,age_55_59.value_counts(normalize=True),age_60_69.value_counts(normalize=True),age_70_79.value_counts(normalize=True),age_80m.value_counts(normalize=True)],                        index=['age18-21','age22-24','age25-29','age30-34','age35-39','age40-44','age45-49','age50-54','age55-59'                               ,'age60-69','age70-79','age80+']).T  axes = df_arol.plot.barh(rot=0, subplots=True,figsize=(15,35)) ",0
"ASSIGN = 20 ASSIGN = np.random.randint(0,len(x_train),n_samples) display_images(x_train[ASSIGN], y_train[ASSIGN])",1,display_data,"n_samples  = 20  idx_sample = np.random.randint(0,len(x_train),n_samples)  display_images(x_train[idx_sample], y_train[idx_sample])",0
"ASSIGN = imageio.get_reader('..path') ASSIGN = reader.get_meta_data()['ASSIGN'] ASSIGN = [] try: for im in ASSIGN: ASSIGN.append(im) except RuntimeError: pass ASSIGN.close() ASSIGN = [resize(frame, (256, 256))[..., :3] for frame in ASSIGN] HTML(display(ASSIGN).to_html5_video())",1,execute_result,"reader = imageio.get_reader('../input/digitsinnoise-video/Test.mp4')  fps = reader.get_meta_data()['fps']  driving_video = []  try:      for im in reader:          driving_video.append(im)  except RuntimeError:      pass  reader.close()    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]        HTML(display(driving_video).to_html5_video())",0
SETUP ASSIGN = Image.open('path') display(ASSIGN),1,display_data,"from PIL import Image, ImageDraw  from IPython.display import display    # Sample Image  virat_img = Image.open('/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces/Kohli-Williamson.jpg')  display(virat_img)",0
"CHECKPOINT for filename in os.listdir(UNKNOWN_FACES_DIR): print(f'Filename {filename}', end='') ASSIGN = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}path{filename}') ASSIGN = face_recognition.ASSIGN(unknown_image) ASSIGN = face_recognition.ASSIGN(unknown_image, face_locations) ASSIGN = Image.fromarray(unknown_image) ASSIGN = ImageDraw.Draw(pil_image) for (top, right, bottom, left), face_encoding in zip(ASSIGN, ASSIGN): ASSIGN = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE) ASSIGN = ""Unknown"" ASSIGN = face_recognition.face_distance(known_faces, face_encoding) ASSIGN = np.argmin(face_distances) if ASSIGN[ASSIGN]: ASSIGN = known_names[best_match_index] ASSIGN.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255)) ASSIGN = draw.textsize(name) ASSIGN.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255)) ASSIGN.text((left + 6, bottom - text_height - 5), ASSIGN, fill=(255, 255, 255, 255)) del draw display(ASSIGN)",1,stream,"for filename in os.listdir(UNKNOWN_FACES_DIR):        # Load image      print(f'Filename {filename}', end='')      unknown_image = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}/{filename}')      # Load an image with an unknown face      #unknown_image = face_recognition.load_image_file(""/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces/Virat_Kohli/gettyimages-463104486-2048x2048.jpg"")        # Find all the faces and face encodings in the unknown image      face_locations = face_recognition.face_locations(unknown_image)      face_encodings = face_recognition.face_encodings(unknown_image, face_locations)        # Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library      # See http://pillow.readthedocs.io/ for more about PIL/Pillow      pil_image = Image.fromarray(unknown_image)      # Create a Pillow ImageDraw Draw instance to draw with      draw = ImageDraw.Draw(pil_image)        # Loop through each face found in the unknown image      for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):          # See if the face is a match for the known face(s)          matches = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)            name = ""Unknown""            # Or instead, use the known face with the smallest distance to the new face          face_distances = face_recognition.face_distance(known_faces, face_encoding)          best_match_index = np.argmin(face_distances)          if matches[best_match_index]:              name = known_names[best_match_index]            # Draw a box around the face using the Pillow module          draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))            # Draw a label with a name below the face          text_width, text_height = draw.textsize(name)          draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))          draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))        # Remove the drawing library from memory as per the Pillow docs      del draw        # Display the resulting image      display(pil_image)",0
