content,tag,output_type,original_content,y_pred
"ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN.head()",0,execute_result,"df_train = pd.read_csv(""../input/train.csv"", parse_dates=['timestamp'])  df_test = pd.read_csv(""../input/test.csv"", parse_dates=['timestamp'])  df_macro = pd.read_csv(""../input/macro.csv"", parse_dates=['timestamp'])    df_train.head()",1
df_train['price_doc'].hist(bins=50),0,execute_result,df_train['price_doc'].hist(bins=50),1
"ASSIGN = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)",0,not_existent,"model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)",1
ASSIGN=[],0,not_existent,drop_column=[],1
ASSIGN=[],0,not_existent,cat_column=[],1
ASSIGN=[],0,not_existent,Num_col=[],1
for col in cat_enc_col: feature.append(col),1,not_existent,for col in cat_enc_col:      feature.append(col),0
"CHECKPOINT ASSIGN=(preds_1+preds_2)path print(,mean_absolute_error(y_valid, ASSIGN))",1,stream,"preds_3=(preds_1+preds_2)/2  print(""MAE:"",mean_absolute_error(y_valid, preds_3))",0
pd.options.display.max_columns = num_of_cols,0,not_existent,pd.options.display.max_columns = num_of_cols,1
df['parentesco1'].loc[df.parentesco1 == 1].describe(),0,not_existent,df['parentesco1'].loc[df.parentesco1 == 1].describe(),1
(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']path(df))*100,0,not_existent,(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']/len(df))*100,1
"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)",0,not_existent,"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)",1
"CHECKPOINT print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])",0,not_existent,"print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4']) ",1
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",0,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",1
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",0,not_existent,"# remove these two rows... df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",1
CHECKPOINT cols_electronics.remove('Target') cols_electronics,1,not_existent,cols_electronics.remove('Target') cols_electronics,0
total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv') len(total_features),1,not_existent,total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv')  len(total_features),0
total_features.remove('escolari') len(total_features),1,not_existent,total_features.remove('escolari') len(total_features),0
"CHECKPOINT for col in nan_cols: if col != 'v2a1': print(col, df[col].unique())",0,not_existent,"for col in nan_cols:     if col != 'v2a1':         print(col, df[col].unique())",1
"len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",0,not_existent,"# number of rows where income and rez_esc has values len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",1
"ASSIGN = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(ASSIGN, data=df, kind=""kde"")",0,not_existent,"x, y = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(x, y, data=df, kind=""kde"")",1
total_features.remove('hogar_total') len(total_features),1,not_existent,"# use hhsize, ignore 'hogar_total', total_features.remove('hogar_total')  len(total_features)",0
total_features.remove('female') len(total_features),1,not_existent,# removing female total_features.remove('female')  len(total_features),0
total_features.remove('r4t3'),1,not_existent,"# removing 'r4t3', as 'hhsize' is of almost same distribution total_features.remove('r4t3')",0
total_features.append('edjefa') total_features.append('edjefe'),1,not_existent,total_features.append('edjefa') total_features.append('edjefe'),0
total_features.remove('abastaguano') total_features.remove('abastaguafuera') len(total_features),1,not_existent,total_features.remove('abastaguano') total_features.remove('abastaguafuera')  len(total_features),0
total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother') len(total_features),1,not_existent,# removing these features -> inc by 0.002 total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother')  len(total_features),0
df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],0,not_existent,df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],1
df['v2a1'].loc[df.parentesco1 == 1].plot.line(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.line(),1
df['v2a1'].loc[df.parentesco1 == 1].plot.hist(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.hist(),1
"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",0,not_existent,"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",1
"ASSIGN = xgb.XGBClassifier( ASSIGN =0.1, ASSIGN=1000, ASSIGN=5, ASSIGN=1, ASSIGN=0, ASSIGN=0.8, ASSIGN=0.8, ASSIGN= 'binary:logistic', ASSIGN=ASSIGN, ASSIGN=1, ASSIGN=27)",0,not_existent,"#Let's do a little Gridsearch, Hyperparameter Tunning model3 = xgb.XGBClassifier(  learning_rate =0.1,  n_estimators=1000,  max_depth=5,  min_child_weight=1,  gamma=0,  subsample=0.8,  colsample_bytree=0.8,  objective= 'binary:logistic',  n_jobs=n_jobs,  scale_pos_weight=1,  seed=27)",1
for f in less_imp_features: if f in total_features: total_features.remove(f) len(total_features),1,not_existent,for f in less_imp_features:     if f in total_features:         total_features.remove(f)  len(total_features),0
CHECKPOINT ASSIGN = sort(model3.feature_importances_) thresholds,1,not_existent,thresholds = sort(model3.feature_importances_) thresholds,0
"df_final[cols].to_csv('sample_submission.csv', index=False)",0,not_existent,"df_final[cols].to_csv('sample_submission.csv', index=False)",1
"SETUP ASSIGN = Sequential([Dense(units=1, input_shape=[1])]) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ASSIGN = np.array([-3, -1, 1, 3, 5, 7], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500)",0,not_existent,"### Try Keras from keras import Sequential from keras.layers import Dense  # Define model model = Sequential([Dense(units=1, input_shape=[1])])  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ys = np.array([-3, -1, 1, 3, 5, 7], dtype=float)  # Train model model.fit(xs, ys, epochs=500)",1
"SETUP ASSIGN = Sequential() ASSIGN.add(Dense(units=1, input_shape=[1])) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([0,1,2,3,4,5,6], dtype=float) ASSIGN = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500) ASSIGN.predict([7])",1,not_existent,"### House price from keras import Sequential from keras.layers import Dense  # Define model model = Sequential() model.add(Dense(units=1, input_shape=[1]))  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([0,1,2,3,4,5,6], dtype=float) ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)  # Train model model.fit(xs, ys, epochs=500)  # Predict model.predict([7])",0
"SETUP SETUP https:path\ -O path ASSIGN = InceptionV3(weights=None, ASSIGN=False, ASSIGN=(INPUT_SIZE[0], INPUT_SIZE[1], 3)) ASSIGN = 'path' ASSIGN.load_weights(ASSIGN) ASSIGN.trainable = False ASSIGN.summary()",0,not_existent,"### Define pretrained model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions BATCH_SIZE = 20 INPUT_SIZE = (150,150)  # Get local weight !wget --no-check-certificate \     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5      # Define inception model base_model = InceptionV3(weights=None,                           include_top=False,                           input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))  # Load local weight local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' base_model.load_weights(local_weight_file)  # Freeze all layer base_model.trainable = False  # Summary of the model base_model.summary()",1
"CHECKPOINT ASSIGN = base_model.get_layer('mixed7') ASSIGN = last_layer.output print('Last layer shape :',ASSIGN.output_shape)",1,not_existent,"### Get specific layer from pre-trained model  last_layer = base_model.get_layer('mixed7') last_output = last_layer.output  print('Last layer shape :',last_layer.output_shape)",0
SETUP CHECKPOINT ASSIGN = Tokenizer() ASSIGN.fit_on_texts(label) ASSIGN = tokenizer_label.word_index ASSIGN = tokenizer_label.texts_to_sequences(label) print(ASSIGN) print(ASSIGN),1,not_existent,### Tokenize the label data from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer_label = Tokenizer()  # Get the token dict from data tokenizer_label.fit_on_texts(label) label_word_index = tokenizer_label.word_index label_seq = tokenizer_label.texts_to_sequences(label)  print(label_seq) print(label_word_index),0
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",0
ASSIGN = model.layers[0] ASSIGN = e.get_weights()[0],1,not_existent,### Get the embedding weight for visualization e = model.layers[0] weights = e.get_weights()[0],0
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",0
"full_report(train_df, target_column='logerror')",0,display_data,"full_report(train_df, target_column='logerror')",1
codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],0,execute_result,codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],1
"SETUP ASSIGN = KFold(n_splits=5, shuffle=False, random_state=None)",1,not_existent,"from sklearn.model_selection import KFold  cv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)",0
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_equidistant)      cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_balanced) ASSIGN = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_balanced)      cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_balanced))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"SETUP CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"from sklearn.ensemble import RandomForestClassifier  Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_equidistant)      cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_balanced) ASSIGN = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_balanced).score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_balanced)      cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"SETUP ASSIGN = Pipeline([ ('scaler', None), ('knn', KNeighborsClassifier( ASSIGN=-1 )) ])",0,not_existent,"from sklearn.neighbors import KNeighborsClassifier  from sklearn.pipeline import Pipeline  from sklearn.preprocessing import StandardScaler    # Create k-NN classifier  pipe = Pipeline([      #('scaler', StandardScaler()), # With standardization      ('scaler', None), # Better performance without standardization!      ('knn', KNeighborsClassifier(          n_jobs=-1 # As many parallel jobs as possible      ))  ]) ",1
"ASSIGN = codiv_country_analyze['category_equidistant'] ASSIGN = codiv_country_analyze['category_balanced'] ASSIGN = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) sns.countplot(ASSIGN)",0,execute_result,"y_equidistant = codiv_country_analyze['category_equidistant']  y_balanced = codiv_country_analyze['category_balanced']  X = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1)   sns.countplot(y_balanced)",1
"CHECKPOINT ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_equidistant)      cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_equidistant)      cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]),0,not_existent,#Recovered  Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])   #deaths     Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])   #confirmed    Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    #codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country/Region')  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    ,1
CHECKPOINT print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1)) print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1)),0,error,print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))  print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1)),1
"CHECKPOINT ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Training data shape', ASSIGN.shape) ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Test data shape\n', ASSIGN.shape)",0,not_existent,"train_df = pd.read_csv('../input/train.csv', header=0, sep=',') train_df = train_df.loc[:,'MSSubClass':] #print('Training data\n', train_df.head()) print('Training data columns:', train_df.columns) print('Training data shape', train_df.shape)  test_df = pd.read_csv('../input/test.csv', header=0, sep=',') test_df = test_df.loc[:,'MSSubClass':] #print('Test data\n', test_df.head()) print('Training data columns:', test_df.columns) print('Test data shape\n', test_df.shape)",1
"ASSIGN = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, items.family.value_counts().values), ASSIGN = False))) ASSIGN = go.Bar( ASSIGN = items.family.value_counts().values, ASSIGN = items.family.value_counts().index ) ASSIGN = dict( ASSIGN='Counts of items per family category', ASSIGN = 900, height = 600, ASSIGN=dict( ASSIGN = True, ASSIGN = True, ASSIGN = True )) ASSIGN = go.Figure(data=[trace2]) ASSIGN['ASSIGN'].update(ASSIGN) py.iplot(ASSIGN, filename='plots')",0,display_data,"# BAR PLOT FOR ITEMS V/S FAMILY TYPE  x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index,                                            items.family.value_counts().values),                                           reverse = False)))  trace2 = go.Bar(      y = items.family.value_counts().values,      x = items.family.value_counts().index  )    layout = dict(      title='Counts of items per family category',       width = 900, height = 600,      yaxis=dict(          showgrid = True,          showline = True,          showticklabels = True      ))    fig1 = go.Figure(data=[trace2])  fig1['layout'].update(layout)  py.iplot(fig1, filename='plots')",1
for i in range(7): display(HTML('<h2>Cluster'+str(i)+'<path>')) cc1[cc1.cluster == i].describe(),0,display_data,"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T  for i in range(7):      display(HTML('<h2>Cluster'+str(i)+'</h2>'))      cc1[cc1.cluster == i].describe()",1
"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",1,execute_result,"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))",0
"ASSIGN = sb.FacetGrid(train, hue=""Survived"", aspect=4) ASSIGN.map(sb.kdeplot, 'FamilySize', shade= True) ASSIGN.set(xlim=(0, train['FamilySize'].max())) ASSIGN.add_legend()",0,execute_result,"facet = sb.FacetGrid(train, hue=""Survived"", aspect=4) facet.map(sb.kdeplot, 'FamilySize', shade= True) facet.set(xlim=(0, train['FamilySize'].max())) facet.add_legend()",1
"SETUP ASSIGN = KFold(n_splits=10, shuffle=True, random_state=0)",1,not_existent,"from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score k_fold = KFold(n_splits=10, shuffle=True, random_state=0)",0
"CHECKPOINT ASSIGN = SVC() ASSIGN.fit(train_data, target) ASSIGN = test.drop(""PassengerId"", axis=1).copy() ASSIGN = clf.predict(test_data) ASSIGN = pd.read_csv('path') ASSIGN = clf.predict(test_data2) print(ASSIGN)",1,stream,"clf = SVC()  clf.fit(train_data, target)  test_data = test.drop(""PassengerId"", axis=1).copy()  prediction = clf.predict(test_data)  test_data2 = pd.read_csv('/kaggle/input/testes/teste.csv') prediction2 = clf.predict(test_data2)  print(prediction2)",0
"CHECKPOINT ASSIGN = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score } dict1",1,execute_result,"dict1 = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score }  dict1",0
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",0,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",1
"def get_default_device(): """"""Pick GPU if available, else CPU"""""" if torch.cuda.is_available(): return torch.device('cuda') else: return torch.device('cpu') def to_device(data, device): """"""Move tensor(s) to chosen device"""""" if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) class DeviceDataLoader(): """"""Wrap a dataloader to move data to a device"""""" def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): """"""Yield a batch of data after moving it to device"""""" for b in self.dl: yield to_device(b, self.device) def __len__(self): """"""Number of batches"""""" return len(self.dl)",1,not_existent,"def get_default_device():     """"""Pick GPU if available, else CPU""""""     if torch.cuda.is_available():         return torch.device('cuda')     else:         return torch.device('cpu')      def to_device(data, device):     """"""Move tensor(s) to chosen device""""""     if isinstance(data, (list,tuple)):         return [to_device(x, device) for x in data]     return data.to(device, non_blocking=True)  class DeviceDataLoader():     """"""Wrap a dataloader to move data to a device""""""     def __init__(self, dl, device):         self.dl = dl         self.device = device              def __iter__(self):         """"""Yield a batch of data after moving it to device""""""         for b in self.dl:              yield to_device(b, self.device)      def __len__(self):         """"""Number of batches""""""         return len(self.dl)",0
"def conv_block(in_channels, out_channels, pool=False): ASSIGN = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)] if pool: ASSIGN.append(nn.MaxPool2d(2)) return nn.Sequential(*ASSIGN) class ResNet9(ImageClassificationBase): def __init__(self, in_channels, num_classes): super().__init__() self.conv1 = conv_block(in_channels, 64) self.conv2 = conv_block(64, 128, pool=True) self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) self.conv3 = conv_block(128, 256, pool=True) self.conv4 = conv_block(256, 512, pool=True) self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) self.classifier = nn.Sequential(nn.MaxPool2d(4), nn.Flatten(), nn.Linear(512, num_classes)) def forward(self, xb): ASSIGN = self.conv1(xb) ASSIGN = self.conv2(ASSIGN) ASSIGN = self.res1(ASSIGN) + ASSIGN ASSIGN = self.conv3(ASSIGN) ASSIGN = self.conv4(ASSIGN) ASSIGN = self.res2(ASSIGN) + ASSIGN ASSIGN = self.classifier(ASSIGN) return out",0,not_existent,"def conv_block(in_channels, out_channels, pool=False):     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),                nn.BatchNorm2d(out_channels),                nn.ReLU(inplace=True)]     if pool: layers.append(nn.MaxPool2d(2))     return nn.Sequential(*layers)  class ResNet9(ImageClassificationBase):     def __init__(self, in_channels, num_classes):         super().__init__()                  self.conv1 = conv_block(in_channels, 64)         self.conv2 = conv_block(64, 128, pool=True)         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))                  self.conv3 = conv_block(128, 256, pool=True)         self.conv4 = conv_block(256, 512, pool=True)         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))                  self.classifier = nn.Sequential(nn.MaxPool2d(4),                                          nn.Flatten(),                                          nn.Linear(512, num_classes))              def forward(self, xb):         out = self.conv1(xb)         out = self.conv2(out)         out = self.res1(out) + out         out = self.conv3(out)         out = self.conv4(out)         out = self.res2(out) + out         out = self.classifier(out)         return out",1
ASSIGN = 8 ASSIGN = 0.01 ASSIGN = 0.1 ASSIGN = 1e-4 ASSIGN = torch.optim.Adam,0,not_existent,epochs = 8 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam,1
"CHECKPOINT ASSIGN = 9106 ASSIGN = model.predict_generator(test_generator) ASSIGN = np.argmax(Y_pred_test, axis=1) y_pred_test",1,execute_result,"#prediction on test data     num = 9106    Y_pred_test = model.predict_generator(test_generator)  y_pred_test = np.argmax(Y_pred_test, axis=1)  y_pred_test ",0
"ASSIGN=[] ASSIGN=[] for dirname, _, filenames in os.walk('..path'): for filename in filenames[0:9]: ASSIGN.append( cv2.imread(dirname+""path""+ filename,0)) ASSIGN.append(""1"") DisplayImage(ASSIGN,ASSIGN,3)",0,display_data,"images=[] titles=[] for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):     for filename in filenames[0:9]:                 images.append( cv2.imread(dirname+""/""+ filename,0))         titles.append(""1"")                   DisplayImage(images,titles,3)",1
"CHECKPOINT def twoplot(df, col, xaxis=None): ''' scatter plot a feature split into response values as two subgraphs ''' if col not in df.columns.values: print('ERROR: %s not a column' % col) ASSIGN = pd.DataFrame(index = df.index) ASSIGN = ASSIGN ASSIGN = ASSIGN if xaxis else df.index ASSIGN = ASSIGN ASSIGN = sns.FacetGrid(ndf, col=""Response"", hue=""Response"") ASSIGN.map(plt.scatter, xaxis, col, alpha=.7, s=1) ASSIGN.add_legend(); del ndf",1,not_existent,"def twoplot(df, col, xaxis=None):      ''' scatter plot a feature split into response values as two subgraphs '''      if col not in df.columns.values:          print('ERROR: %s not a column' % col)      ndf = pd.DataFrame(index = df.index)      ndf[col] = df[col]      ndf[xaxis] = df[xaxis] if xaxis else df.index      ndf['Response'] = df['Response']            g = sns.FacetGrid(ndf, col=""Response"", hue=""Response"")      g.map(plt.scatter, xaxis, col, alpha=.7, s=1)      g.add_legend();            del ndf",0
"CHECKPOINT ASSIGN = [] for dirname, _, filenames in os.walk('path'): for filename in filenames: ASSIGN.append(os.path.join(dirname, filename)) ASSIGN = pd.concat([pd.read_csv(filepath) for filepath in metadata_files]) print(f'Line data shape: {ASSIGN.shape}') ASSIGN.head(10)",1,stream,"# Get line data    metadata_files = []  for dirname, _, filenames in os.walk('/kaggle/input/tractable_ds_excercise_data/metadata'):      for filename in filenames:          metadata_files.append(os.path.join(dirname, filename))    line_data = pd.concat([pd.read_csv(filepath) for filepath in metadata_files])  print(f'Line data shape: {line_data.shape}')  line_data.head(10)",0
for i in Ktrain_cat1: encode(Ktrain[i]) for i in Ktest_cat1: encode(Ktest[i]),1,not_existent,for i in Ktrain_cat1:      encode(Ktrain[i])  for i in Ktest_cat1:      encode(Ktest[i]),0
for i in Ktrain_cat2: encode(Ktrain[i]) for i in Ktest_cat2: encode(Ktest[i]),1,not_existent,for i in Ktrain_cat2:      encode(Ktrain[i])  for i in Ktest_cat2:      encode(Ktest[i]),0
SETUP del Ktest_identity del Ktest_transaction del Ktrain_identity del Ktrain_transaction del Ktrain_cat1 del Ktest_cat1 del Ktrain_cat2 del Ktest_cat2 gc.collect(),0,execute_result,import gc  del Ktest_identity  del Ktest_transaction  del Ktrain_identity  del Ktrain_transaction  del Ktrain_cat1  del Ktest_cat1  del Ktrain_cat2  del Ktest_cat2  gc.collect(),1
"''' ASSIGN=StandardScaler().fit_transform(X) ASSIGN=PCA().fit(X_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() ASSIGN=StandardScaler().fit_transform(X_Ktest) ASSIGN=PCA().fit(X_Ktest_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() '''",1,execute_result,"'''  X_fit=StandardScaler().fit_transform(X)  X_pca=PCA().fit(X_fit)  plt.plot(np.cumsum(X_pca.explained_variance_ratio_))  plt.title('All columns included', color='gray')  plt.xlabel(""Number of Component"", color='green')  plt.ylabel(""Cumulative Variance Ratio"", color='green')  plt.grid(color='gray', linestyle='-', linewidth=0.3)  plt.show()    X_Ktest_fit=StandardScaler().fit_transform(X_Ktest)  X_Ktest_pca=PCA().fit(X_Ktest_fit)  plt.plot(np.cumsum(X_Ktest_pca.explained_variance_ratio_))  plt.title('All columns included', color='gray')  plt.xlabel(""Number of Component"", color='green')  plt.ylabel(""Cumulative Variance Ratio"", color='green')  plt.grid(color='gray', linestyle='-', linewidth=0.3)  plt.show()  '''",0
ASSIGN = KNeighborsRegressor(38),0,not_existent,knn = KNeighborsRegressor(38),1
(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar'),0,not_existent,(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar'),1
ASSIGN = GaussianNB(),0,not_existent,NB = GaussianNB() ,1
"ASSIGN = pd.DataFrame(index = test.index) ASSIGN = Ytest ASSIGN.to_csv('submission.csv',index = True)",1,not_existent,"pred = pd.DataFrame(index = test.index) pred['ham'] = Ytest pred.to_csv('submission.csv',index = True)",0
"inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})",1,not_existent,"inputs.Sex = inputs.Sex.map({'male': 1, 'female': 2})",0
"SETUP ASSIGN = np.load(DSPATH+""olivetti_faces.npy"") ASSIGN = np.load(DSPATH+""olivetti_faces_target.npy"") ThiefImage={} SLICE= image.imread(""..path"") SLICE=image.imread(""..path"")",0,not_existent,"DSPATH=""../input/olivetti-faces/"" X = np.load(DSPATH+""olivetti_faces.npy"") y = np.load(DSPATH+""olivetti_faces_target.npy"")    ThiefImage={} ThiefImage[""False""]= image.imread(""../input/thief-images/False.jpg"") ThiefImage[""True""]=image.imread(""../input/thief-images/True.jpg"")       ",1
df[df.x==0],0,execute_result,df[df.x==0],1
"ASSIGN = ReduceLROnPlateau(monitor='val_acc', ASSIGN=3, ASSIGN=1, ASSIGN=0.7, ASSIGN=0.00000000001) ASSIGN = EarlyStopping(patience=2)",0,not_existent,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',                                              patience=3,                                              verbose=1,                                              factor=0.7,                                              min_lr=0.00000000001) early_stopping_monitor = EarlyStopping(patience=2)",1
df.describe(include='object'),1,execute_result,#Including Categorical features with include object df.describe(include='object'),0
"def Mrmse(y_true,y_pred): ASSIGN = np.log(ASSIGN) ASSIGN = math.sqrt(mean_squared_error(y_true, y_pred)) return rmse",0,not_existent,"def Mrmse(y_true,y_pred):     y_true = np.log(y_true)     #y_pred = np.log(y_pred)     rmse = math.sqrt(mean_squared_error(y_true, y_pred))     return rmse",1
"ASSIGN = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) ASSIGN.to_csv('submission.csv', index=False)",0,not_existent,"my_submission = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) # you could use any filename. We choose submission here my_submission.to_csv('submission.csv', index=False) #my_submission",1
"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""d.i khan"")",1,not_existent,"# use the function we just wrote to replace close matches to ""d.i khan"" with ""d.i khan"" replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""d.i khan"")",0
"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)",1,not_existent,"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)",0
"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])",0,stream,"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])",1
"app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()), ""ID""]",0,execute_result,"# Check if there are missing or 0 ID's  app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()),                ""ID""]",1
"app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]",0,execute_result,"# Check if there are null values in the Size column  app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]",1
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",0,execute_result,"#Amount of games that have only the English language  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",1
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",0,execute_result,"#Amount of games that have only one language and is not English  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",1
df['CHAS'].value_counts(dropna=False),0,execute_result,df['CHAS'].value_counts(dropna=False),1
"ASSIGN = X[:,:13] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt = X[:,:13]   regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[1,3,5,7,8,9,10,11,12]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11,12]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[1,3,5,7,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[3,5,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[3,5,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[0,1,4,10,14]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,error,"X_opt =X[:,[0,1,4,10,14]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""chicago_crime"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref)",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""chicago_crime"" dataset  dataset_ref = client.dataset(""chicago_crime"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)",0
preprocessing.StandardScaler().fit(full).transform(full.astype(float)),1,execute_result,#Data Standardization  preprocessing.StandardScaler().fit(full).transform(full.astype(float)),0
"ASSIGN=test['PassengerId'] ASSIGN=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) ASSIGN.to_csv('submission.csv',index=False) ASSIGN.head()",0,execute_result,"Id=test['PassengerId'] sub_df=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) sub_df.to_csv('submission.csv',index=False) sub_df.head()",1
"SETUP def giveMeFeatures(image): ASSIGN = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2') return res",1,not_existent,"from skimage.feature import hog  def giveMeFeatures(image):      res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')  #     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)      return res           ",0
ASSIGN = models.Sequential(),0,not_existent,model = models.Sequential(),1
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] ASSIGN=0 ASSIGN=0 for typ in ASSIGN: for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([1,0]) ASSIGN+=1 for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([0,1]) ASSIGN+=1 print('A: ', ASSIGN) print('B: ', ASSIGN)",1,stream,"import glob  import cv2  directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'  directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'  directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_s = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/s'    ADirectories = []  BDirectories = []    ADirectories.append(directory_a)  ADirectories.append(directory_s)  BDirectories.append(directory_b)  BDirectories.append(directory_5)      X = []  y = []  types = ['*.png', '*.jpg']  countA=0  countB=0  for typ in types:      for directory in ADirectories:          for filename in glob.glob(os.path.join(directory, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(28,28))              X.append(im1)              y.append([1,0])              countA+=1      for directory in BDirectories:          for filename in glob.glob(os.path.join(directory, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(28,28))              X.append(im1)              y.append([0,1])              countB+=1  print('A: ', countA)  print('B: ', countB) ",0
X=Xpath,1,not_existent,X=X/255,0
"ASSIGN = ImageDataGenerator( ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=10, ASSIGN = 0.1, ASSIGN=0.1, ASSIGN=0.1, ASSIGN=False, ASSIGN=False) ASSIGN.fit(X_train)",1,error,"# With data augmentation to prevent overfitting    datagen = ImageDataGenerator(          featurewise_center=False,  # set input mean to 0 over the dataset          samplewise_center=False,  # set each sample mean to 0          featurewise_std_normalization=False,  # divide inputs by std of the dataset          samplewise_std_normalization=False,  # divide each input by its std          zca_whitening=False,  # apply ZCA whitening          rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)          zoom_range = 0.1, # Randomly zoom image           width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)          height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)          horizontal_flip=False,  # randomly flip images          vertical_flip=False)  # randomly flip images      datagen.fit(X_train)",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob  import cv2  directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_2 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/2'  directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          for filename in glob.glob(os.path.join(directory_5, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          for filename in glob.glob(os.path.join(directory_unk, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)                 ",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob  import cv2  directory_1 = '/kaggle/input/3shapesdataset/resized/1'  directory_2 = '/kaggle/input/3shapesdataset/resized/2'  directory_3 = '/kaggle/input/3shapesdataset/resized/3'  #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_1, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          for filename in glob.glob(os.path.join(directory_3, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)  #         for filename in glob.glob(os.path.join(directory_unk, typ)):  #             im1 =cv2.imread(filename,0)  #             im1 = cv2.resize(im1,(64,64))  #             X.append(im1)  #             y.append(3)                               ",0
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(3) print('finished')",1,stream,"import glob  import cv2  directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'  directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'  directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'  # directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'  # directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'  #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_1, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          print('finished')          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          print('finished')          for filename in glob.glob(os.path.join(directory_3, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)          print('finished')          for filename in glob.glob(os.path.join(directory_unk, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(3)          print('finished')                               ",0
ASSIGN = [] ASSIGN = [],0,not_existent,chromosome = []  fitval = [] ,1
"CHECKPOINT generateCromosome() for i in range(1000): print(,i+1) evaluateSolution() c1,c2,bstval=selection() ASSIGN==5.0: break crossover(c1,c2) mutation()",0,stream,"generateCromosome()  for i in range(1000):      print(""Iteration"",i+1)      evaluateSolution()      c1,c2,bstval=selection()      if bstval==5.0:          break      crossover(c1,c2)      mutation()   ",1
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""hacker_news"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""comments"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""hacker_news"" dataset  dataset_ref = client.dataset(""hacker_news"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)    # Construct a reference to the ""comments"" table  table_ref = dataset_ref.table(""comments"")    # API request - fetch the table  table = client.get_table(table_ref)    # Preview the first five lines of the ""comments"" table  client.list_rows(table, max_results=5).to_dataframe()",0
ASSIGN = 0 ASSIGN = 0 ASSIGN = 0.01 ASSIGN = 10000 ASSIGN = float(len(x)),0,not_existent,b0 = 0  b1 = 0  alpha = 0.01  count = 10000  n = float(len(x)),1
"CHECKPOINT for i in range(count): ASSIGN = b1*x + b0 ASSIGN = ASSIGN - (alphapath)*sum(x*(y_bar-y)) ASSIGN = ASSIGN - (alphapath)*sum(y_bar-y) print(ASSIGN,ASSIGN)",1,stream,"for i in range(count):      y_bar = b1*x + b0      b1 = b1 - (alpha/n)*sum(x*(y_bar-y))      b0 = b0 - (alpha/n)*sum(y_bar-y)            print(b0,b1)",0
"CHECKPOINT ASSIGN = np.array(df['MSE']).reshape(-1,1) ASSIGN = np.array(df['ESE']).reshape(-1,1) ASSIGN = LinearRegression() ASSIGN.fit(ASSIGN,ASSIGN) print(ASSIGN.coef_) print(ASSIGN.intercept_) ASSIGN = lr.predict(X) ASSIGN = RSE(Y,yp) print(ASSIGN)",1,stream,"X = np.array(df['MSE']).reshape(-1,1)  Y = np.array(df['ESE']).reshape(-1,1)    lr = LinearRegression()  lr.fit(X,Y)    print(lr.coef_)  print(lr.intercept_)    yp = lr.predict(X)  rse = RSE(Y,yp)    print(rse)",0
ASSIGN=10000,0,not_existent,sampleNumber=10000,1
ASSIGN = X_std.dot(matrix_w),1,not_existent,Y = X_std.dot(matrix_w),0
SETUP CHECKPOINT ASSIGN = sm.tsa.seasonal_decompose(co2_levels) print(ASSIGN.seasonal),1,stream,# Import statsmodels.api as sm  import statsmodels.api as sm    # Perform time series decompositon  decomposition = sm.tsa.seasonal_decompose(co2_levels)    # Print the seasonality component  print(decomposition.seasonal),0
"ASSIGN = pd.read_csv('path', parse_dates = ['Month'], index_col = 'Month') ASSIGN.head()",0,execute_result,"airline = pd.read_csv('/kaggle/input/week6dataset/airline_passengers.csv', parse_dates = ['Month'], index_col = 'Month')  airline.head()",1
ASSIGN = pd.to_datetime(ASSIGN ) ASSIGN = ASSIGN.set_index('date') display(ASSIGN.describe()),0,display_data,# Convert the date column to a datestamp type  meat['date'] = pd.to_datetime(meat['date'] )    # Set the date column as the index of your DataFrame meat  meat = meat.set_index('date')    # Print the summary statistics of the DataFrame  display(meat.describe()),1
"CHECKPOINT print(meat[['beef', 'pork']].corr(method='spearman'))",0,stream,"# Print the correlation matrix between the beef and pork columns using the spearman method  print(meat[['beef', 'pork']].corr(method='spearman'))",1
"CHECKPOINT print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))",0,stream,"# Print the correlation matrix between the pork, veal and turkey columns using the pearson method  print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))",1
"ASSIGN = model.predict(test_data) ASSIGN = np.argmax(ASSIGN,axis = 1) ASSIGN = pd.Series(ASSIGN,name=""Label"")",1,not_existent,"# predict results  results = model.predict(test_data)    # select the indix with the maximum probability  results = np.argmax(results,axis = 1)    results = pd.Series(results,name=""Label"")",0
"ASSIGN = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1) ASSIGN.to_csv(""mySubmission.csv"",index=False)",0,not_existent,"submission = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1)    submission.to_csv(""mySubmission.csv"",index=False)",1
corr['blueWins'].sort_values(ascending=False),0,execute_result,corr['blueWins'].sort_values(ascending=False),1
SETUP ASSIGN = 7 np.random.ASSIGN(ASSIGN),0,not_existent,import matplotlib.pyplot as plt import numpy as np import pandas as pd seed = 7 np.random.seed(seed),1
ASSIGN = deepCNN(),0,not_existent,CNNmodel = deepCNN(),1
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""stackoverflow"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""posts_questions"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery  # Create a ""Client"" object client = bigquery.Client()  # Construct a reference to the ""stackoverflow"" dataset dataset_ref = client.dataset(""stackoverflow"", project=""bigquery-public-data"")  # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  # Construct a reference to the ""posts_questions"" table table_ref = dataset_ref.table(""posts_questions"")  # API request - fetch the table table = client.get_table(table_ref)  # Preview the first five lines of the table client.list_rows(table, max_results=5).to_dataframe()",0
"ASSIGN = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)",0,stream,"knn_grid = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)",1
"ASSIGN = load(""titanic_SVM_02"", with_metadata=True)",0,stream,"svc_grid = load(""titanic_SVM_02"", with_metadata=True)",1
"ASSIGN = load(""titanic_RandomForest_02"", with_metadata=True)",0,stream,"random_forest_grid = load(""titanic_RandomForest_02"", with_metadata=True)",1
"ASSIGN = pd.read_csv(""..path"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)",0,not_existent,"train = pd.read_csv(""../input/cs-challenge/training_set.csv"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)",1
"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",0,not_existent,"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",1
"get_split_result(""..path"", test, 1e-6)",1,execute_result,"#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3  get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6)",0
"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}path{len(train['landmark_id'].unique())}""",0,not_existent,"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}/{len(train['landmark_id'].unique())}""",1
"os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]",0,not_existent," os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]",1
"CHECKPOINT ASSIGN = math.sqrt(mean_squared_error(p_train, y_train)) print('Pontuação para o treinamento: %.2f RMSE' % (ASSIGN)) ASSIGN = math.sqrt(mean_squared_error(p_test, y_test)) print('Pontuação para o teste: %.2f RMSE' % (ASSIGN))",0,stream,"#calcula os erros de previsão  trainScore = math.sqrt(mean_squared_error(p_train, y_train))  print('Pontuação para o treinamento: %.2f RMSE' % (trainScore))  testScore = math.sqrt(mean_squared_error(p_test, y_test))  print('Pontuação para o teste: %.2f RMSE' % (testScore))",1
adult_train.describe(exclude = [np.number]),0,execute_result,adult_train.describe(exclude = [np.number]),1
"SETUP ASSIGN = adult_train.copy() ASSIGN = LabelEncoder() ASSIGN[""income""] = ASSIGN.fit_transform(ASSIGN['income']) plt.figure(figsize=(10,10)) ASSIGN = np.zeros_like(adult_copy.corr(), dtype=np.bool) ASSIGN[np.triu_indices_from(ASSIGN)] = True sns.heatmap(ASSIGN.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, ASSIGN=ASSIGN) plt.show()",1,display_data,"adult_copy = adult_train.copy()  from sklearn.preprocessing import LabelEncoder  le = LabelEncoder()  adult_copy[""income""] = le.fit_transform(adult_copy['income'])    #heat map:  plt.figure(figsize=(10,10))  mask = np.zeros_like(adult_copy.corr(), dtype=np.bool)  mask[np.triu_indices_from(mask)] = True  sns.heatmap(adult_copy.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, mask=mask)  plt.show()",0
"ASSIGN = pd.read_csv('path',parse_dates=['Date'], dayfirst=True) ASSIGN = pd.read_csv('path') ASSIGN = pd.read_excel('path',sheet_name='India', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Italy', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Korea', parse_dates=['Date'])",0,not_existent,"df_india = pd.read_csv('/kaggle/input/covid19-in-india/covid_19_india.csv',parse_dates=['Date'], dayfirst=True)  df_coordinates = pd.read_csv('/kaggle/input/coronavirus-cases-in-india/Indian Coordinates.csv')  df_India_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='India', parse_dates=['Date'])  df_Italy_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Italy', parse_dates=['Date'])  df_Korea_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Korea', parse_dates=['Date'])",1
"CHECKPOINT ASSIGN = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths']) plot",0,execute_result,"plot = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths'])  plot",1
"CHECKPOINT ASSIGN = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4) for i in range(0, len(df_india)): folium.Circle( ASSIGN=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']], ASSIGN='crimson', ASSIGN = '<li><bold>Statepath: '+str(df_india.iloc[i]['Statepath'])+ '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+ '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+ '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+ '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']), ASSIGN=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India) India",1,execute_result,"#India  India = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)  for i in range(0, len(df_india)):      folium.Circle(          location=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],                    color='crimson',                    tooltip = '<li><bold>State/UnionTerritory : '+str(df_india.iloc[i]['State/UnionTerritory'])+                              '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+                              '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+                              '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+                              '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),                    radius=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)  India    #The output is not 100% correct as there was some issue with the cordinates.",0
"Cure_over_Death = df_india.groupby('Date').sum().reset_index() Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death = Cure_over_Death.melt(id_vars ='Date', ASSIGN=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True, ASSIGN='Cure_over_Death', color_discrete_sequence=[deth,cure]) ASSIGN.show()",0,display_data,"Cure_over_Death = df_india.groupby('Date').sum().reset_index()    Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100  Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100    Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',                            value_vars=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],                            var_name='Ratio',                            value_name='Value')    fig = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,               title='Cure_over_Death', color_discrete_sequence=[deth,cure])    fig.show()",1
"ASSIGN = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['Statepath'].unique().apply(len) ASSIGN = pd.DataFrame(ASSIGN).reset_index() ASSIGN = ex.line(spread, x='Date', y='Statepath', text='Statepath', ASSIGN='Number of Statepath', ASSIGN=[conf,deth, cure]) ASSIGN.update_traces(textposition='top center') ASSIGN.show()",0,display_data,"  spread = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['State/UnionTerritory'].unique().apply(len)  spread = pd.DataFrame(spread).reset_index()    spread_graph = ex.line(spread, x='Date', y='State/UnionTerritory', text='State/UnionTerritory',                title='Number of State/UnionTerritory to which COVID-19 spread over the time',                color_discrete_sequence=[conf,deth, cure])  spread_graph.update_traces(textposition='top center')  spread_graph.show()",1
"ASSIGN = df_india_data.groupby(['Date', 'Statepath'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False) ex.line(ASSIGN, x=""Date"", y=""TotalConfirmed"", color='Statepath', title='ASSIGN over time', height=600)",0,display_data,"Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)    ex.line(Spread, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', title='Spread over time', height=600)",1
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)  Top_50 = latest_date[latest_date['TotalConfirmed']>20]  Top_50 = Top_50.sort_values('Death Rate', ascending=False)    Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),                x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h',                width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')  Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')  Plot.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Confirmed  Date_vs_confirmed = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_confirmed = Date_vs_confirmed.reset_index()    Date_vs_confirmed_fig = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_confirmed_fig.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Cured  Date_vs_cured = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_cured = Date_vs_cured.reset_index()    Date_vs_cured_fig = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_cured_fig.show()",1
"Date_vs_Deaths = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() Date_vs_Deaths = Date_vs_Deaths.reset_index() Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm) Date_vs_Deaths_fig.show()",0,display_data,"#Date vs Active  Date_vs_Deaths = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_Deaths = Date_vs_Deaths.reset_index()    Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_Deaths_fig.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured'] ASSIGN = ASSIGN.sum().diff().reset_index() ASSIGN = new_cases['Statepath'] != new_cases['Statepath'].shift(1) ASSIGN.loc[ASSIGN, 'TotalConfirmed'] = np.nan ASSIGN.loc[ASSIGN, 'Deaths'] = np.nan ASSIGN.loc[ASSIGN, 'Cured'] = np.nan ASSIGN = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='Statepath',title='New cases') ASSIGN.show()",0,display_data,"new_cases = df_india_data.groupby(['State/UnionTerritory', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']  new_cases = new_cases.sum().diff().reset_index()    mat = new_cases['State/UnionTerritory'] != new_cases['State/UnionTerritory'].shift(1)    new_cases.loc[mat, 'TotalConfirmed'] = np.nan  new_cases.loc[mat, 'Deaths'] = np.nan  new_cases.loc[mat, 'Cured'] = np.nan    New_cases_plot = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory',title='New cases')  New_cases_plot.show()",1
"ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= cum_var_exp, ASSIGN='lines+markers', ASSIGN=""'Cumulative Explained Variance'"", ASSIGN=dict( ASSIGN='spline', ASSIGN = 'goldenrod' ) ) ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= var_exp, ASSIGN='lines+markers', ASSIGN=""'Individual Explained Variance'"", ASSIGN=dict( ASSIGN='linear', ASSIGN = 'black' ) ) ASSIGN = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}], ASSIGN=True) ASSIGN.append_trace(ASSIGN, 1, 1) ASSIGN.append_trace(ASSIGN,1,1) ASSIGN.layout.title = 'Explained Variance plots - Full and Zoomed-in' ASSIGN.layout.xaxis = dict(range=[0, 80], title = 'Feature columns') ASSIGN.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')",0,stream,"trace1 = go.Scatter(      x=list(range(784)),      y= cum_var_exp,      mode='lines+markers',      name=""'Cumulative Explained Variance'"",  #     hoverinfo= cum_var_exp,      line=dict(          shape='spline',          color = 'goldenrod'      )  )  trace2 = go.Scatter(      x=list(range(784)),      y= var_exp,      mode='lines+markers',      name=""'Individual Explained Variance'"",  #     hoverinfo= var_exp,      line=dict(          shape='linear',          color = 'black'      )  )  fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],                            print_grid=True)    fig.append_trace(trace1, 1, 1)  fig.append_trace(trace2,1,1)  fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'  fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')  fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')  # fig['data'] = []  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))    # fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]  # fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]    # # fig['data'] = data  # # fig['layout'] = layout  # # fig['data'] += data2  # # fig['layout'] += layout2  # py.iplot(fig, filename='inset example')",1
"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ASSIGN = [] ASSIGN = [] for iteration in range(max_iteration): ASSIGN = np.dot(X,w) ASSIGN = MSE(y,predicted_y) ASSIGN = round(ASSIGN,9) ASSIGN.append(w) ASSIGN.append(ASSIGN) ASSIGN = -(2path[0])* X.dot(loss).sum() ASSIGN = ASSIGN + lr * derivative return w_history,loss_hostory",0,not_existent,"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001):      w_history  = []     loss_hostory = []     for iteration in range(max_iteration):         predicted_y = np.dot(X,w)         loss =  MSE(y,predicted_y)         loss = round(loss,9)         w_history.append(w)         loss_hostory.append(loss)         derivative = -(2/y.shape[0])* X.dot(loss).sum()         w = w + lr * derivative     return w_history,loss_hostory ",1
"ASSIGN = [] ASSIGN =1000 ASSIGN = 0.1 for _ in range(ASSIGN): ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=ASSIGN*ASSIGN",0,not_existent,"history = [] #loss history  numberOfRounds =1000 # max number of times the optimization algorithm will run learningRate = 0.1 for _ in range(numberOfRounds):     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))      for j in range(k):         deltaTemp=0          #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,         #and it works fine in case of offline training and smalle number of samples)          for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=learningRate*deltaTemp",1
"CHECKPOINT ASSIGN = [] ASSIGN = 20 for iteration in range(ASSIGN): print('iteration: ',iteration) ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=0.1*ASSIGN",0,not_existent,"history = [] maxNumOfIterations = 20 for iteration in range(maxNumOfIterations):      print('iteration: ',iteration)     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))     for j in range(k):         deltaTemp=0         for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=0.1*deltaTemp",1
"SETUP ASSIGN="".path"" os.mkdir(ASSIGN) for dirname, _, filenames in os.walk('..path'): for filename in filenames: ASSIGN = ""..path""+filename ASSIGN = test_set+os.path.splitext(filename)[0]+"".wav"" ASSIGN = AudioSegment.from_mp3(src) ASSIGN = ASSIGN.set_frame_rate(8000) ASSIGN.export(ASSIGN, format=""wav"")",1,not_existent,"import os  from os import path from pydub import AudioSegment      test_set=""./test_set/""    os.mkdir(test_set)     for dirname, _, filenames in os.walk('../input/quran-asr-challenge/test_set'):     for filename in filenames:         # files                                                                                  src = ""../input/quran-asr-challenge/test_set/""+filename         dst = test_set+os.path.splitext(filename)[0]+"".wav""         # convert wav to mp3                                                                     sound = AudioSegment.from_mp3(src)         sound = sound.set_frame_rate(8000)         sound.export(dst, format=""wav"") ",0
"ASSIGN = mulcA[round(mulcA.iloc[:,0].astype(int) path) <= 3].index ASSIGN = ASSIGN.drop(fast, axis=0) ASSIGN = mulcA.Q5 ASSIGN.value_counts(normalize=True).plot(kind='bar')",0,not_existent,"fast = mulcA[round(mulcA.iloc[:,0].astype(int) / 60) <= 3].index mulcA = mulcA.drop(fast, axis=0) rol = mulcA.Q5 rol.value_counts(normalize=True).plot(kind='bar')",1
"mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6, '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12, '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17, '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran', 'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea', 'Czech Republic':'Czech R.'}},inplace = True) ASSIGN = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] ASSIGN = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java', 'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'Cpath++','Q16_Part_9':'MATLAB', 'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C 'Q16_Part_15':'Ruby','Q16_Part_16':'SASpath'} ASSIGN= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) ASSIGN.pop('Q16_Part_17') ASSIGN.pop('Q16_Part_18') ASSIGN.pop('Q16_OTHER_TEXT') ASSIGN = list(q16_lim.iloc[:0]) for i in ASSIGN: SLICE= ASSIGN['{}'.format(i)] ASSIGN = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(), ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),ASSIGN,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities', 'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))",0,not_existent,"# mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,                        '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,                        '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,                                  '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',                        'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',                       'Czech Republic':'Czech R.'}},inplace = True)  q16 = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] q16_col = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',            'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'C/C++','Q16_Part_9':'MATLAB',            'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C#/.NET','Q16_Part_14':'PHP',            'Q16_Part_15':'Ruby','Q16_Part_16':'SAS/STATA'}  q16_lim= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) q16_lim.pop('Q16_Part_17') q16_lim.pop('Q16_Part_18') q16_lim.pop('Q16_OTHER_TEXT') lab = list(q16_lim.iloc[:0]) for i in lab:     mulcA[i]= q16_lim['{}'.format(i)] # com_sci = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] eng_nco = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] mat_sta = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] biu_dis = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] phy_ast = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] inf_tec = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] med_sci = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] soc_sci = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] hum_tie = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] env_sci = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] fin_art = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]   rem_prom = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),             soc_sci.Q9.mean(),hum_tie.Q9.mean(),env_sci.Q9.mean(),fin_art.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),rem_prom,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',                             'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))",1
"ASSIGN = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(), mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['Cpath++'].sum(),mulcA['MATLAB'].sum(), mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SASpath'].sum()],index=lab) ASSIGN.plot(kind='bar',color='brown')",0,not_existent,"lengs = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),                            mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['C/C++'].sum(),mulcA['MATLAB'].sum(),                            mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C#/.NET'].sum(),                            mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SAS/STATA'].sum()],index=lab) lengs.plot(kind='bar',color='brown')",1
"CHECKPOINT def count_unique_values(df) : ASSIGN = list(df.columns) print('Count unique values :') for i in ASSIGN : ASSIGN = len(df[i].unique()) print(i,':',ASSIGN) def check_missing_values(df) : ASSIGN = len(df) ASSIGN = list(df.columns) ASSIGN = [] ASSIGN = [] print('Variable with missing values :') for i in ASSIGN : ASSIGN = np.sum(df[i].isna()) ASSIGN = round(count*100path, 2) if ASSIGN > 0 : print(i,':',ASSIGN,'path',ASSIGN,'%') ASSIGN.append(i) ASSIGN.append(ASSIGN) return missing_var, missing_count def stepwise_selection(X, y, ASSIGN=[], ASSIGN=0.01, ASSIGN = 0.05, ASSIGN=True): ASSIGN = list(initial_list) while True: ASSIGN=False ASSIGN = list(set(X.columns)-set(included)) ASSIGN = pd.Series(index=excluded) for new_column in ASSIGN: ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN[new_column] = ASSIGN.pvalues[new_column] ASSIGN = new_pval.min() if ASSIGN < ASSIGN: ASSIGN = new_pval.argmin() ASSIGN.append(ASSIGN) ASSIGN=True if ASSIGN: print('Add {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN = model.ASSIGN.iloc[1:] ASSIGN = pvalues.max() if ASSIGN > ASSIGN: ASSIGN=True ASSIGN = pvalues.argmax() ASSIGN.remove(ASSIGN) if ASSIGN: print('Drop {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) if not ASSIGN: break return included def dataset_ready(x_train, y_train) : ASSIGN = pd.get_dummies(x_train) ASSIGN = [1]*len(X) ASSIGN['gdp_pop'] = np.log(ASSIGN['gdp_per_capita']*ASSIGN['population']) ASSIGN = ['gdp_per_capita','population'] for i in ASSIGN : ASSIGN = np.log(ASSIGN) ASSIGN = pd.Series(X.columns) ASSIGN = list(X.filter(like='continent').columns) for i in ASSIGN : ASSIGN = i+'_gdp' ASSIGN = ASSIGN*ASSIGN for i in ASSIGN : ASSIGN = i+'_population' ASSIGN = ASSIGN*ASSIGN ASSIGN = y_train return X,Y",1,not_existent,"# Function used in this notebook def count_unique_values(df) :     var = list(df.columns)     print('Count unique values :')          for i in var :         count = len(df[i].unique())         print(i,':',count)  def check_missing_values(df) :     n = len(df)     var = list(df.columns)     missing_var = []     missing_count = []     print('Variable with missing values :')          for i in var :         count = np.sum(df[i].isna())         count_percentage = round(count*100/n, 2)         if count > 0 :             print(i,':',count,'//',count_percentage,'%')             missing_var.append(i)             missing_count.append(count_percentage)          return missing_var, missing_count  def stepwise_selection(X, y,                         initial_list=[],                         threshold_in=0.01,                         threshold_out = 0.05,                         verbose=True):       included = list(initial_list)     while True:         changed=False         # forward step         excluded = list(set(X.columns)-set(included))         new_pval = pd.Series(index=excluded)         for new_column in excluded:             model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))                                 ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()             new_pval[new_column] = model.pvalues[new_column]         best_pval = new_pval.min()         if best_pval < threshold_in:             best_feature = new_pval.argmin()             included.append(best_feature)             changed=True             if verbose:                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))          # backward step         model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))                             ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()         # use all coefs except intercept         pvalues = model.pvalues.iloc[1:]         worst_pval = pvalues.max() # null if pvalues is empty         if worst_pval > threshold_out:             changed=True             worst_feature = pvalues.argmax()             included.remove(worst_feature)             if verbose:                 print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))         if not changed:             break     return included  def dataset_ready(x_train, y_train) :     # Make dummy variable for categorical variable     X = pd.get_dummies(x_train)      # Make Intercept     X['Intercept'] = [1]*len(X)      # Make interaction between 'gdp_per_capita' and 'population'     X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])      # Scale continuous variable with log function     cont_var = ['gdp_per_capita','population']     for i in cont_var :         X[i] = np.log(X[i])      # Make interaction between 'continent' and 'gdp'     col = pd.Series(X.columns)     var1 = list(X.filter(like='continent').columns)     for i in var1 :         string = i+'_gdp'         X[string] = X[i]*X['gdp_per_capita']         # Make interaction between 'continent' and 'population'     for i in var1 :         string = i+'_population'         X[string] = X[i]*X['population']        # Target variable     Y = y_train          return X,Y  # I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn",0
"ASSIGN = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index, train_filtered.groupby(['Pclass']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Sex']).sum().index, train_filtered.groupby(['Sex']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Age']).sum().index, train_filtered.groupby(['Age']).sum()['Survived'] ) ASSIGN = sns.jointplot(train_filtered.groupby(['Fare']).sum().index, train_filtered.groupby(['Fare']).sum()['Survived'] )",0,not_existent,"# Survival by pclass g = sns.jointplot(train_filtered.groupby(['Pclass']).sum().index,                   train_filtered.groupby(['Pclass']).sum()['Survived']     )  # Survival by sex g = sns.jointplot(train_filtered.groupby(['Sex']).sum().index,                   train_filtered.groupby(['Sex']).sum()['Survived']     )  # Survival by age g = sns.jointplot(train_filtered.groupby(['Age']).sum().index,                   train_filtered.groupby(['Age']).sum()['Survived']     )  # Survival by fare g = sns.jointplot(train_filtered.groupby(['Fare']).sum().index,                   train_filtered.groupby(['Fare']).sum()['Survived']     )",1
"CHECKPOINT X_train, X_test, y_train, y_test = train_test_split( train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33) ASSIGN = LR().fit(y=y_train,X=X_train) ASSIGN = model.predict(X_test) ASSIGN = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual']) ASSIGN['Survived predicted'] = ASSIGN ASSIGN=m.confusion_matrix(res['Survived actual'],res['Survived predicted']) sns.heatmap(ASSIGN,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix') print('Accuracy: '+str(m.accuracy_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted']))) print('Precision: '+str(m.precision_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted']))) print('Recall: '+str(m.recall_score(ASSIGN['Survived actual'],ASSIGN['Survived predicted'])))",1,not_existent,"# Split data into test and train sets based on the train data set X_train, X_test, y_train, y_test = train_test_split(     train_filtered.drop('Survived', axis=1), train_filtered['Survived'], test_size=0.33)  # Train model model = LR().fit(y=y_train,X=X_train)  # Predict results results = model.predict(X_test)  # Add results to a data frame res = pd.DataFrame(data=y_test.tolist(),columns=['Survived actual']) res['Survived predicted'] = results  # Confusion matrix confmatrix=m.confusion_matrix(res['Survived actual'],res['Survived predicted']) sns.heatmap(confmatrix,annot=True,fmt='d',cbar=0).set_title('Confusion Matrix') #   True negatives (tn)     True positives (tp) #   False negatives (fn)    False positives (fp)  print('Accuracy: '+str(m.accuracy_score(res['Survived actual'],res['Survived predicted']))) # percent of accurate classification print('Precision: '+str(m.precision_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fp), 0 is worst, 1 is best print('Recall: '+str(m.recall_score(res['Survived actual'],res['Survived predicted']))) # tp / (tp + fn), 0 is worst, 1 is best",0
(train == '?').any(),0,not_existent,(train == '?').any(),1
train[train == '?'].count(),0,not_existent,train[train == '?'].count(),1
"ASSIGN = pd.read_csv('..path') ASSIGN = np.array([ imread(test_dir+p)path]) ASSIGN = np.array(ASSIGN) ASSIGN = getProb(model, x_test) ASSIGN['has_cactus'] = ASSIGN ASSIGN.to_csv('cactus_net_submission.csv', index=False)",1,not_existent,"df_test = pd.read_csv('../input/sample_submission.csv')  x_test  = np.array([ imread(test_dir+p)/255 for p in df_test.id.values])  x_test  = np.array(x_test)    # test prediction  y_prob_test = getProb(model, x_test)    df_test['has_cactus'] = y_prob_test  df_test.to_csv('cactus_net_submission.csv', index=False)",0
"SETUP CHECKPOINT sys.path.append('..path') binder.bind(globals()) print() ASSIGN = pd.read_csv('..path', nrows=50000) ASSIGN = ASSIGN.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' + 'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' + 'pickup_longitude > -74 and pickup_longitude < -73.9 and ' + 'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' + 'fare_amount > 0' ) ASSIGN = data.fare_amount ASSIGN = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'] ASSIGN = data[base_features] train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1) ASSIGN = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print() ASSIGN.head()",1,not_existent,"import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  # Environment Set-Up for feedback system. import sys sys.path.append('../input/ml-insights-tools') from learntools.core import binder binder.bind(globals()) from ex3 import * print(""Setup Complete"")  # Data manipulation code below here data = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)  # Remove data with extreme outlier coordinates or negative fares data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +                   'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +                   'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +                   'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +                   'fare_amount > 0'                   )  y = data.fare_amount  base_features = ['pickup_longitude',                  'pickup_latitude',                  'dropoff_longitude',                  'dropoff_latitude']  X = data[base_features]   train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print(""Data sample:"") data.head()",0
"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)",1,not_existent,"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)",0
CHECKPOINT print('Loading known faces...') ASSIGN = [] ASSIGN = [],0,stream,print('Loading known faces...')  known_faces = []  known_names = [],1
"CHECKPOINT for filename in os.listdir(UNKNOWN_FACES_DIR): print(f'Filename {filename}', end='') ASSIGN = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}path{filename}') ASSIGN = face_recognition.ASSIGN(unknown_image) ASSIGN = face_recognition.ASSIGN(unknown_image, face_locations) ASSIGN = Image.fromarray(unknown_image) ASSIGN = ImageDraw.Draw(pil_image) for (top, right, bottom, left), face_encoding in zip(ASSIGN, ASSIGN): ASSIGN = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE) ASSIGN = ""Unknown"" ASSIGN = face_recognition.face_distance(known_faces, face_encoding) ASSIGN = np.argmin(face_distances) if ASSIGN[ASSIGN]: ASSIGN = known_names[best_match_index] ASSIGN.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255)) ASSIGN = draw.textsize(name) ASSIGN.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255)) ASSIGN.text((left + 6, bottom - text_height - 5), ASSIGN, fill=(255, 255, 255, 255)) del draw display(ASSIGN)",0,stream,"for filename in os.listdir(UNKNOWN_FACES_DIR):        # Load image      print(f'Filename {filename}', end='')      unknown_image = face_recognition.load_image_file(f'{UNKNOWN_FACES_DIR}/{filename}')      # Load an image with an unknown face      #unknown_image = face_recognition.load_image_file(""/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/known_faces/Virat_Kohli/gettyimages-463104486-2048x2048.jpg"")        # Find all the faces and face encodings in the unknown image      face_locations = face_recognition.face_locations(unknown_image)      face_encodings = face_recognition.face_encodings(unknown_image, face_locations)        # Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library      # See http://pillow.readthedocs.io/ for more about PIL/Pillow      pil_image = Image.fromarray(unknown_image)      # Create a Pillow ImageDraw Draw instance to draw with      draw = ImageDraw.Draw(pil_image)        # Loop through each face found in the unknown image      for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):          # See if the face is a match for the known face(s)          matches = face_recognition.compare_faces(known_faces, face_encoding,TOLERANCE)            name = ""Unknown""            # Or instead, use the known face with the smallest distance to the new face          face_distances = face_recognition.face_distance(known_faces, face_encoding)          best_match_index = np.argmin(face_distances)          if matches[best_match_index]:              name = known_names[best_match_index]            # Draw a box around the face using the Pillow module          draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))            # Draw a label with a name below the face          text_width, text_height = draw.textsize(name)          draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))          draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))        # Remove the drawing library from memory as per the Pillow docs      del draw        # Display the resulting image      display(pil_image)",1
