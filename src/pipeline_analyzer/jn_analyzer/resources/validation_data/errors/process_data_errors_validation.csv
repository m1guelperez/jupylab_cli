content,tag,output_type,original_content,y_pred
"ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN = pd.read_csv(""..path"", parse_dates=['timestamp']) ASSIGN.head()",0,execute_result,"df_train = pd.read_csv(""../input/train.csv"", parse_dates=['timestamp'])  df_test = pd.read_csv(""../input/test.csv"", parse_dates=['timestamp'])  df_macro = pd.read_csv(""../input/macro.csv"", parse_dates=['timestamp'])    df_train.head()",1
df_train['price_doc'].hist(bins=50),0,execute_result,df_train['price_doc'].hist(bins=50),1
"ASSIGN = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)",0,not_existent,"model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_round)",1
"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)",1,not_existent,"(np.random.rand(10) < .5).astype(float) np.random.choice([1, -1], 10)",0
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""stackoverflow"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""posts_questions"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery  # Create a ""Client"" object client = bigquery.Client()  # Construct a reference to the ""stackoverflow"" dataset dataset_ref = client.dataset(""stackoverflow"", project=""bigquery-public-data"")  # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  # Construct a reference to the ""posts_questions"" table table_ref = dataset_ref.table(""posts_questions"")  # API request - fetch the table table = client.get_table(table_ref)  # Preview the first five lines of the table client.list_rows(table, max_results=5).to_dataframe()",0
"for index, target in enumerate(target_cols): ASSIGN = models[index] finalize_model_pipeline(ASSIGN,target)",1,not_existent,"for index, target in enumerate(target_cols):     model = models[index]     finalize_model_pipeline(model,target)",0
"ASSIGN = pd.read_csv('path',parse_dates=['Date'], dayfirst=True) ASSIGN = pd.read_csv('path') ASSIGN = pd.read_excel('path',sheet_name='India', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Italy', parse_dates=['Date']) ASSIGN = pd.read_excel('path',sheet_name='Korea', parse_dates=['Date'])",0,not_existent,"df_india = pd.read_csv('/kaggle/input/covid19-in-india/covid_19_india.csv',parse_dates=['Date'], dayfirst=True)  df_coordinates = pd.read_csv('/kaggle/input/coronavirus-cases-in-india/Indian Coordinates.csv')  df_India_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='India', parse_dates=['Date'])  df_Italy_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Italy', parse_dates=['Date'])  df_Korea_perday = pd.read_excel('/kaggle/input/coronavirus-cases-in-india/per_day_cases.xlsx',sheet_name='Korea', parse_dates=['Date'])",1
"VALIDATION ASSIGN = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths']) plot",0,execute_result,"plot = slate.melt(id_vars=""Date"", value_vars=['Active','Cured','Deaths'])  plot",1
"ASSIGN = ex.treemap(plot, path=['variable'], values=""value"", height=500, width=800, ASSIGN=[acti,cure,deth]) ASSIGN.show()",0,display_data,"matt = ex.treemap(plot, path=['variable'], values=""value"", height=500, width=800,                  color_discrete_sequence=[acti,cure,deth])  matt.show() ",1
"VALIDATION ASSIGN = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4) for i in range(0, len(df_india)): folium.Circle( ASSIGN=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']], ASSIGN='crimson', ASSIGN = '<li><bold>Statepath: '+str(df_india.iloc[i]['Statepath'])+ '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+ '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+ '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+ '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']), ASSIGN=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India) India",1,execute_result,"#India  India = folium.Map(location=[20.5937,78.9629], tiles='cartodbpositron', min_zoom=4, max_zoom=10, zoom_start=4)  for i in range(0, len(df_india)):      folium.Circle(          location=[df_india.iloc[i]['Latitude'],df_india.iloc[i]['Longitude']],                    color='crimson',                    tooltip = '<li><bold>State/UnionTerritory : '+str(df_india.iloc[i]['State/UnionTerritory'])+                              '<li><bold>ConfirmedIndianNational : '+str(df_india.iloc[i]['ConfirmedIndianNational'])+                              '<li><bold>ConfirmedForeignNational : '+str(df_india.iloc[i]['ConfirmedForeignNational'])+                              '<li><bold>Deaths : '+str(df_india.iloc[i]['Deaths'])+                              '<li><bold>Cured : '+str(df_india.iloc[i]['Cured']),                    radius=int(df_india.iloc[i]['ConfirmedIndianNational'])**1.1).add_to(India)  India    #The output is not 100% correct as there was some issue with the cordinates.",0
"ASSIGN = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'], ASSIGN='Case', value_name='Count') ASSIGN.head() ASSIGN=ex.area(graph, x='Date', y='Count', color='Case', ASSIGN = 'Cases over time', color_discrete_sequence=[cure, deth, acti]) ASSIGN.show()",0,display_data,"graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()  graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],           var_name='Case', value_name='Count')  graph.head()    fig=ex.area(graph, x='Date', y='Count', color='Case',             title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])  fig.show()",1
"Cure_over_Death = df_india.groupby('Date').sum().reset_index() Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']path(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100 Cure_over_Death = Cure_over_Death.melt(id_vars ='Date', ASSIGN=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True, ASSIGN='Cure_over_Death', color_discrete_sequence=[deth,cure]) ASSIGN.show()",0,display_data,"Cure_over_Death = df_india.groupby('Date').sum().reset_index()    Cure_over_Death['No. of Deaths to 100 Confirmed Cases'] = round(Cure_over_Death['Deaths']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100  Cure_over_Death['No. of Recovered to 100 Confirmed Cases'] = round(Cure_over_Death['Cured']/(Cure_over_Death['ConfirmedIndianNational']+Cure_over_Death['ConfirmedForeignNational']),3)*100    Cure_over_Death = Cure_over_Death.melt(id_vars ='Date',                            value_vars=['No. of Deaths to 100 Confirmed Cases','No. of Recovered to 100 Confirmed Cases'],                            var_name='Ratio',                            value_name='Value')    fig = ex.line(Cure_over_Death, x='Date', y='Value', color='Ratio', log_y=True,               title='Cure_over_Death', color_discrete_sequence=[deth,cure])    fig.show()",1
"ASSIGN = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['Statepath'].unique().apply(len) ASSIGN = pd.DataFrame(ASSIGN).reset_index() ASSIGN = ex.line(spread, x='Date', y='Statepath', text='Statepath', ASSIGN='Number of Statepath', ASSIGN=[conf,deth, cure]) ASSIGN.update_traces(textposition='top center') ASSIGN.show()",0,display_data,"  spread = df_india_data[df_india_data['TotalConfirmed']!=0].groupby('Date')['State/UnionTerritory'].unique().apply(len)  spread = pd.DataFrame(spread).reset_index()    spread_graph = ex.line(spread, x='Date', y='State/UnionTerritory', text='State/UnionTerritory',                title='Number of State/UnionTerritory to which COVID-19 spread over the time',                color_discrete_sequence=[conf,deth, cure])  spread_graph.update_traces(textposition='top center')  spread_graph.show()",1
"ASSIGN = df_india_data.groupby(['Date', 'Statepath'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False) ex.line(ASSIGN, x=""Date"", y=""TotalConfirmed"", color='Statepath', title='ASSIGN over time', height=600)",0,display_data,"Spread = df_india_data.groupby(['Date', 'State/UnionTerritory'])['TotalConfirmed'].sum().reset_index().sort_values('TotalConfirmed', ascending=False)    ex.line(Spread, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', title='Spread over time', height=600)",1
"ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), ASSIGN=""TotalConfirmed"", y=""Statepath"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True),                x=""TotalConfirmed"", y=""State/UnionTerritory"", title='Confirmed Cases', text='TotalConfirmed', orientation='h',                width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])  Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Confirmed_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), ASSIGN=""Deaths"", y=""Statepath"", title='Death in each state', text='Deaths', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True),                x=""Deaths"", y=""State/UnionTerritory"", title='Death in each state', text='Deaths', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])  Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Death_rate_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), ASSIGN=""Cured"", y=""Statepath"", title='Cured cases', text='Cured', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True),                x=""Cured"", y=""State/UnionTerritory"", title='Cured cases', text='Cured', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])  cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  cure_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), ASSIGN=""Active"", y=""Statepath"", title='Active cases', text='Active', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True),                x=""Active"", y=""State/UnionTerritory"", title='Active cases', text='Active', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Active'])+10])  Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Active_cases.show()",1
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)  Top_50 = latest_date[latest_date['TotalConfirmed']>20]  Top_50 = Top_50.sort_values('Death Rate', ascending=False)    Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),                x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h',                width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')  Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')  Plot.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Confirmed  Date_vs_confirmed = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_confirmed = Date_vs_confirmed.reset_index()    Date_vs_confirmed_fig = ex.bar(Date_vs_confirmed, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Confirmed', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_confirmed_fig.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() ASSIGN = ASSIGN.reset_index() ASSIGN = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm) ASSIGN.show()",0,display_data,"#Date vs Cured  Date_vs_cured = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_cured = Date_vs_cured.reset_index()    Date_vs_cured_fig = ex.bar(Date_vs_cured, x=""Date"", y=""Cured"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Cured', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_cured_fig.show()",1
"Date_vs_Deaths = df_india_data.groupby(['Statepath', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum() Date_vs_Deaths = Date_vs_Deaths.reset_index() Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='Statepath', orientation='v', height=600, ASSIGN='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm) Date_vs_Deaths_fig.show()",0,display_data,"#Date vs Active  Date_vs_Deaths = df_india_data.groupby(['State/UnionTerritory', 'Date'])['TotalConfirmed', 'Deaths', 'Cured'].sum()  Date_vs_Deaths = Date_vs_Deaths.reset_index()    Date_vs_Deaths_fig = ex.bar(Date_vs_Deaths, x=""Date"", y=""Deaths"", color='State/UnionTerritory', orientation='v', height=600,                          title='Date vs Active', color_discrete_sequence = ex.colors.cyclical.mygbm)  Date_vs_Deaths_fig.show()",1
"ASSIGN = df_india_data.groupby(['Statepath', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured'] ASSIGN = ASSIGN.sum().diff().reset_index() ASSIGN = new_cases['Statepath'] != new_cases['Statepath'].shift(1) ASSIGN.loc[ASSIGN, 'TotalConfirmed'] = np.nan ASSIGN.loc[ASSIGN, 'Deaths'] = np.nan ASSIGN.loc[ASSIGN, 'Cured'] = np.nan ASSIGN = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='Statepath',title='New cases') ASSIGN.show()",0,display_data,"new_cases = df_india_data.groupby(['State/UnionTerritory', 'Date', ])['TotalConfirmed', 'Deaths', 'Cured']  new_cases = new_cases.sum().diff().reset_index()    mat = new_cases['State/UnionTerritory'] != new_cases['State/UnionTerritory'].shift(1)    new_cases.loc[mat, 'TotalConfirmed'] = np.nan  new_cases.loc[mat, 'Deaths'] = np.nan  new_cases.loc[mat, 'Cured'] = np.nan    New_cases_plot = ex.bar(new_cases, x=""Date"", y=""TotalConfirmed"", color='State/UnionTerritory',title='New cases')  New_cases_plot.show()",1
"ASSIGN = px.pie(last20, values = 'Confirmed',names='Countrypath', height=600) ASSIGN.update_traces(textposition='inside', textinfo='percent+label') ASSIGN.update_layout( ASSIGN = 0.5, ASSIGN=dict( ASSIGN = False, ASSIGN = False, )) ASSIGN.show()",0,display_data,"pie_chart_last20 = px.pie(last20, values = 'Confirmed',names='Country/Region', height=600)  pie_chart_last20.update_traces(textposition='inside', textinfo='percent+label')    pie_chart_last20.update_layout(      title_x = 0.5,      geo=dict(          showframe = False,          showcoastlines = False,      ))    pie_chart_last20.show()",1
"ASSIGN = corona_data.groupby('ObservationDate').sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='ObservationDate', ASSIGN=['Confirmed', 'Recovered', 'Deaths'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio', ASSIGN='Confirmed cases, Recovered cases, and Death Over Time') ASSIGN.show()",0,display_data,"line_data = corona_data.groupby('ObservationDate').sum().reset_index()    line_data = line_data.melt(id_vars='ObservationDate',                    value_vars=['Confirmed',                                'Recovered',                                'Deaths'],                    var_name='Ratio',                    value_name='Value')    line_fig = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio',                 title='Confirmed cases, Recovered cases, and Death Over Time')  line_fig.show()",1
"ASSIGN = mulcA[round(mulcA.iloc[:,0].astype(int) path) <= 3].index ASSIGN = ASSIGN.drop(fast, axis=0) ASSIGN = mulcA.Q5 ASSIGN.value_counts(normalize=True).plot(kind='bar')",0,not_existent,"fast = mulcA[round(mulcA.iloc[:,0].astype(int) / 60) <= 3].index mulcA = mulcA.drop(fast, axis=0) rol = mulcA.Q5 rol.value_counts(normalize=True).plot(kind='bar')",1
"mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6, '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12, '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17, '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran', 'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea', 'Czech Republic':'Czech R.'}},inplace = True) ASSIGN = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] ASSIGN = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java', 'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'Cpath++','Q16_Part_9':'MATLAB', 'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C 'Q16_Part_15':'Ruby','Q16_Part_16':'SASpath'} ASSIGN= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) ASSIGN.pop('Q16_Part_17') ASSIGN.pop('Q16_Part_18') ASSIGN.pop('Q16_OTHER_TEXT') ASSIGN = list(q16_lim.iloc[:0]) for i in ASSIGN: SLICE= ASSIGN['{}'.format(i)] ASSIGN = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] ASSIGN = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(), ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean(),ASSIGN.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),ASSIGN,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities', 'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))",0,not_existent,"# mulcA.replace({'Q9':{'0-10,000':1,'10-20,000':2,'20-30,000':3,'30-40,000':4,'40-50,000':5,'50-60,000':6,                        '60-70,000':7,'70-80,000':8,'80-90,000':9,'90-100,000':10,'100-125,000':11,'125-150,000':12,                        '150-200,000':13,'200-250,000':14,'250-300,000':15,'300-400,000':16,'400-500,000':17,                                  '500,000+':18}},inplace = True) mulcA.replace({'Q3':{'United Kingdom of Great Britain and Northern Ireland':'Great B.','Iran, Islamic Republic of...':'Iran',                        'United States of America':'USA','Hong Kong (S.A.R.)':'Hong Kong','Republic of Korea':'R. Korea',                       'Czech Republic':'Czech R.'}},inplace = True)  q16 = multiple.filter(regex=""(Q{t}$|Q{t}_)"".format(t = 16))[1:] q16_col = {'Q16_Part_1':'Python','Q16_Part_2':'R','Q16_Part_3':'SQL','Q16_Part_4': 'Bash','Q16_Part_5':'Java',            'Q16_Part_6':'Javascript','Q16_Part_7':'VBA','Q16_Part_8':'C/C++','Q16_Part_9':'MATLAB',            'Q16_Part_10':'Scala','Q16_Part_11':'Julia','Q16_Part_12':'Go','Q16_Part_13':'C#/.NET','Q16_Part_14':'PHP',            'Q16_Part_15':'Ruby','Q16_Part_16':'SAS/STATA'}  q16_lim= q16.rename(columns=q16_col).fillna(0).replace('[^\\d]',1, regex=True) q16_lim.pop('Q16_Part_17') q16_lim.pop('Q16_Part_18') q16_lim.pop('Q16_OTHER_TEXT') lab = list(q16_lim.iloc[:0]) for i in lab:     mulcA[i]= q16_lim['{}'.format(i)] # com_sci = mulcA[(mulcA.Q5 == 'Computer science (software engineering, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] eng_nco = mulcA[(mulcA.Q5 == 'Engineering (non-computer focused)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] mat_sta = mulcA[(mulcA.Q5 == 'Mathematics or statistics') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] biu_dis = mulcA[(mulcA.Q5 == 'A business discipline (accounting, economics, finance, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] phy_ast = mulcA[(mulcA.Q5 == 'Physics or astronomy') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] inf_tec = mulcA[(mulcA.Q5 == 'Information technology, networking, or system administration') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] med_sci = mulcA[(mulcA.Q5 == 'Medical or life sciences (biology, chemistry, medicine, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] soc_sci = mulcA[(mulcA.Q5 == 'Social sciences (anthropology, psychology, sociology, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] hum_tie = mulcA[(mulcA.Q5 == 'Humanities (history, literature, philosophy, etc.)') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] env_sci = mulcA[(mulcA.Q5 == 'Environmental science or geology') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')] fin_art = mulcA[(mulcA.Q5 == 'Fine arts or performing arts') & (mulcA.Q9 != 'I do not wish to disclose my approximate yearly compensation')]   rem_prom = [com_sci.Q9.mean(),eng_nco.Q9.mean(),mat_sta.Q9.mean(),biu_dis.Q9.mean(),phy_ast.Q9.mean(),inf_tec.Q9.mean(),med_sci.Q9.mean(),             soc_sci.Q9.mean(),hum_tie.Q9.mean(),env_sci.Q9.mean(),fin_art.Q9.mean()] plt.figure(figsize=(20,10)) plt.bar(np.arange(11),rem_prom,color=['dodgerblue','c','tomato','silver','midnightblue','tan']) plt.xticks(np.arange(11), ('Com. Science', 'Engieneering', 'Mathematics', 'Economics', 'Physics','Inf. Tecnologist','Medics','Social sciences','Humanities',                             'Env. Sciense','Arts')) plt.yticks(np.arange(10),('$10000','$20000','$30000','$40000','$50000','$60000','$70000','$80000','$90000','$100000'))",1
"ASSIGN = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(), mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['Cpath++'].sum(),mulcA['MATLAB'].sum(), mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SASpath'].sum()],index=lab) ASSIGN.plot(kind='bar',color='brown')",0,not_existent,"lengs = pd.DataFrame([mulcA['Python'].sum(),mulcA['R'].sum(),mulcA['SQL'].sum(),mulcA['Bash'].sum(),mulcA['Java'].sum(),                            mulcA['Javascript'].sum(),mulcA['VBA'].sum(),mulcA['C/C++'].sum(),mulcA['MATLAB'].sum(),                            mulcA['Scala'].sum(),mulcA['Julia'].sum(),mulcA['Go'].sum(),mulcA['C#/.NET'].sum(),                            mulcA['PHP'].sum(),mulcA['Ruby'].sum(),mulcA['SAS/STATA'].sum()],index=lab) lengs.plot(kind='bar',color='brown')",1
SETUP VALIDATION ASSIGN = sm.tsa.seasonal_decompose(co2_levels) print(ASSIGN.seasonal),1,stream,# Import statsmodels.api as sm  import statsmodels.api as sm    # Perform time series decompositon  decomposition = sm.tsa.seasonal_decompose(co2_levels)    # Print the seasonality component  print(decomposition.seasonal),0
"ASSIGN = pd.read_csv('path', parse_dates = ['Month'], index_col = 'Month') ASSIGN.head()",0,execute_result,"airline = pd.read_csv('/kaggle/input/week6dataset/airline_passengers.csv', parse_dates = ['Month'], index_col = 'Month')  airline.head()",1
ASSIGN = pd.to_datetime(ASSIGN ) ASSIGN = ASSIGN.set_index('date') display(ASSIGN.describe()),0,display_data,# Convert the date column to a datestamp type  meat['date'] = pd.to_datetime(meat['date'] )    # Set the date column as the index of your DataFrame meat  meat = meat.set_index('date')    # Print the summary statistics of the DataFrame  display(meat.describe()),1
"VALIDATION print(meat[['beef', 'pork']].corr(method='spearman'))",0,stream,"# Print the correlation matrix between the beef and pork columns using the spearman method  print(meat[['beef', 'pork']].corr(method='spearman'))",1
"VALIDATION print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))",0,stream,"# Print the correlation matrix between the pork, veal and turkey columns using the pearson method  print(meat[['pork', 'veal', 'turkey']].corr(method='pearson'))",1
"VALIDATION def count_unique_values(df) : ASSIGN = list(df.columns) print('Count unique values :') for i in ASSIGN : ASSIGN = len(df[i].unique()) print(i,':',ASSIGN) def check_missing_values(df) : ASSIGN = len(df) ASSIGN = list(df.columns) ASSIGN = [] ASSIGN = [] print('Variable with missing values :') for i in ASSIGN : ASSIGN = np.sum(df[i].isna()) ASSIGN = round(count*100path, 2) if ASSIGN > 0 : print(i,':',ASSIGN,'path',ASSIGN,'%') ASSIGN.append(i) ASSIGN.append(ASSIGN) return missing_var, missing_count def stepwise_selection(X, y, ASSIGN=[], ASSIGN=0.01, ASSIGN = 0.05, ASSIGN=True): ASSIGN = list(initial_list) while True: ASSIGN=False ASSIGN = list(set(X.columns)-set(included)) ASSIGN = pd.Series(index=excluded) for new_column in ASSIGN: ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN[new_column] = ASSIGN.pvalues[new_column] ASSIGN = new_pval.min() if ASSIGN < ASSIGN: ASSIGN = new_pval.argmin() ASSIGN.append(ASSIGN) ASSIGN=True if ASSIGN: print('Add {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN = model.ASSIGN.iloc[1:] ASSIGN = pvalues.max() if ASSIGN > ASSIGN: ASSIGN=True ASSIGN = pvalues.argmax() ASSIGN.remove(ASSIGN) if ASSIGN: print('Drop {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) if not ASSIGN: break return included def dataset_ready(x_train, y_train) : ASSIGN = pd.get_dummies(x_train) ASSIGN = [1]*len(X) ASSIGN['gdp_pop'] = np.log(ASSIGN['gdp_per_capita']*ASSIGN['population']) ASSIGN = ['gdp_per_capita','population'] for i in ASSIGN : ASSIGN = np.log(ASSIGN) ASSIGN = pd.Series(X.columns) ASSIGN = list(X.filter(like='continent').columns) for i in ASSIGN : ASSIGN = i+'_gdp' ASSIGN = ASSIGN*ASSIGN for i in ASSIGN : ASSIGN = i+'_population' ASSIGN = ASSIGN*ASSIGN ASSIGN = y_train return X,Y",1,not_existent,"# Function used in this notebook def count_unique_values(df) :     var = list(df.columns)     print('Count unique values :')          for i in var :         count = len(df[i].unique())         print(i,':',count)  def check_missing_values(df) :     n = len(df)     var = list(df.columns)     missing_var = []     missing_count = []     print('Variable with missing values :')          for i in var :         count = np.sum(df[i].isna())         count_percentage = round(count*100/n, 2)         if count > 0 :             print(i,':',count,'//',count_percentage,'%')             missing_var.append(i)             missing_count.append(count_percentage)          return missing_var, missing_count  def stepwise_selection(X, y,                         initial_list=[],                         threshold_in=0.01,                         threshold_out = 0.05,                         verbose=True):       included = list(initial_list)     while True:         changed=False         # forward step         excluded = list(set(X.columns)-set(included))         new_pval = pd.Series(index=excluded)         for new_column in excluded:             model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))                                 ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()             new_pval[new_column] = model.pvalues[new_column]         best_pval = new_pval.min()         if best_pval < threshold_in:             best_feature = new_pval.argmin()             included.append(best_feature)             changed=True             if verbose:                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))          # backward step         model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))                             ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()         # use all coefs except intercept         pvalues = model.pvalues.iloc[1:]         worst_pval = pvalues.max() # null if pvalues is empty         if worst_pval > threshold_out:             changed=True             worst_feature = pvalues.argmax()             included.remove(worst_feature)             if verbose:                 print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))         if not changed:             break     return included  def dataset_ready(x_train, y_train) :     # Make dummy variable for categorical variable     X = pd.get_dummies(x_train)      # Make Intercept     X['Intercept'] = [1]*len(X)      # Make interaction between 'gdp_per_capita' and 'population'     X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])      # Scale continuous variable with log function     cont_var = ['gdp_per_capita','population']     for i in cont_var :         X[i] = np.log(X[i])      # Make interaction between 'continent' and 'gdp'     col = pd.Series(X.columns)     var1 = list(X.filter(like='continent').columns)     for i in var1 :         string = i+'_gdp'         X[string] = X[i]*X['gdp_per_capita']         # Make interaction between 'continent' and 'population'     for i in var1 :         string = i+'_population'         X[string] = X[i]*X['population']        # Target variable     Y = y_train          return X,Y  # I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn",0
"SETUP ASSIGN = Sequential([Dense(units=1, input_shape=[1])]) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ASSIGN = np.array([-3, -1, 1, 3, 5, 7], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500)",0,not_existent,"### Try Keras from keras import Sequential from keras.layers import Dense  # Define model model = Sequential([Dense(units=1, input_shape=[1])])  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([-1, 0, 1, 2, 3, 4], dtype=float) ys = np.array([-3, -1, 1, 3, 5, 7], dtype=float)  # Train model model.fit(xs, ys, epochs=500)",1
"SETUP ASSIGN = Sequential() ASSIGN.add(Dense(units=1, input_shape=[1])) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([0,1,2,3,4,5,6], dtype=float) ASSIGN = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500) ASSIGN.predict([7])",1,not_existent,"### House price from keras import Sequential from keras.layers import Dense  # Define model model = Sequential() model.add(Dense(units=1, input_shape=[1]))  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([0,1,2,3,4,5,6], dtype=float) ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)  # Train model model.fit(xs, ys, epochs=500)  # Predict model.predict([7])",0
"SETUP SETUP ASSIGN = ResNet50(weights='imagenet', ASSIGN=False, ASSIGN=(HEIGHT, WIDTH, 3)) ASSIGN = tf.keras.applications.resnet50.preprocess_input ASSIGN = tf.keras.applications.resnet50.decode_predictions",0,not_existent,"# Make pre train model from tensorflow.keras.applications.resnet50 import ResNet50 from tensorflow.keras.applications.resnet50 import preprocess_input,decode_predictions  HEIGHT = 150 WIDTH = 150 base_model = ResNet50(weights='imagenet',                   include_top=False,                   input_shape=(HEIGHT, WIDTH, 3))  prec_input = tf.keras.applications.resnet50.preprocess_input decode = tf.keras.applications.resnet50.decode_predictions",1
"ASSIGN = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(32, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Conv2D(64, (3,3), activation='relu'), tf.keras.layers.MaxPooling2D(2,2), tf.keras.layers.Flatten(), tf.keras.layers.Dense(512, activation='relu'), tf.keras.layers.Dense(2, activation='softmax') ]) ASSIGN.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])",0,not_existent,"# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS # USE AT LEAST 3 CONVOLUTION LAYERS model = tf.keras.models.Sequential([     tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),     tf.keras.layers.MaxPooling2D(2,2),     tf.keras.layers.Conv2D(32, (3,3), activation='relu'),     tf.keras.layers.MaxPooling2D(2,2),      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),      tf.keras.layers.MaxPooling2D(2,2),     tf.keras.layers.Flatten(),      tf.keras.layers.Dense(512, activation='relu'),      tf.keras.layers.Dense(2, activation='softmax')    # YOUR CODE HERE ])  model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['acc'])",1
"SETUP SETUP https:path\ -O path ASSIGN = InceptionV3(weights=None, ASSIGN=False, ASSIGN=(INPUT_SIZE[0], INPUT_SIZE[1], 3)) ASSIGN = 'path' ASSIGN.load_weights(ASSIGN) ASSIGN.trainable = False ASSIGN.summary()",0,not_existent,"### Define pretrained model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions BATCH_SIZE = 20 INPUT_SIZE = (150,150)  # Get local weight !wget --no-check-certificate \     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5      # Define inception model base_model = InceptionV3(weights=None,                           include_top=False,                           input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))  # Load local weight local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' base_model.load_weights(local_weight_file)  # Freeze all layer base_model.trainable = False  # Summary of the model base_model.summary()",1
"VALIDATION ASSIGN = base_model.get_layer('mixed7') ASSIGN = last_layer.output print('Last layer shape :',ASSIGN.output_shape)",1,not_existent,"### Get specific layer from pre-trained model  last_layer = base_model.get_layer('mixed7') last_output = last_layer.output  print('Last layer shape :',last_layer.output_shape)",0
"SETUP SETUP ASSIGN = np.expand_dims(ASSIGN, 3) ASSIGN = ImageDataGenerator(rescale = 1.path, ASSIGN = 40, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = True) ASSIGN = train_datagen.flow(x=train_img, y=train_label, ASSIGN = BATCH_SIZE) ASSIGN = np.expand_dims(ASSIGN, 3) ASSIGN = ImageDataGenerator(rescale = 1path) ASSIGN = val_datagen.flow(x=val_img, y=val_label, ASSIGN = BATCH_SIZE)",0,not_existent,"### Make the image generator from tensorflow.keras.preprocessing.image import ImageDataGenerator BATCH_SIZE = 32 INPUT_SIZE = (28,28) TRAIN_DIR = '/training' VALID_DIR = '/validating'  # Make image generator for training dataset train_img = np.expand_dims(train_img, 3) train_datagen = ImageDataGenerator(rescale = 1./255.,                                    rotation_range = 40,                                    width_shift_range = 0.2,                                    height_shift_range = 0.2,                                    shear_range = 0.2,                                    zoom_range = 0.2,                                    horizontal_flip = True)  train_gen = train_datagen.flow(x=train_img, y=train_label,                               batch_size = BATCH_SIZE)    # Make image generator for validation dataset val_img = np.expand_dims(val_img, 3) val_datagen = ImageDataGenerator(rescale = 1/255. )  val_gen = val_datagen.flow(x=val_img, y=val_label,                               batch_size = BATCH_SIZE) ",1
SETUP VALIDATION ASSIGN = Tokenizer() ASSIGN.fit_on_texts(label) ASSIGN = tokenizer_label.word_index ASSIGN = tokenizer_label.texts_to_sequences(label) print(ASSIGN) print(ASSIGN),1,not_existent,### Tokenize the label data from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences tokenizer_label = Tokenizer()  # Get the token dict from data tokenizer_label.fit_on_texts(label) label_word_index = tokenizer_label.word_index label_seq = tokenizer_label.texts_to_sequences(label)  print(label_seq) print(label_word_index),0
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK, num_words=VOCAB_SIZE)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",0
ASSIGN = model.layers[0] ASSIGN = e.get_weights()[0],1,not_existent,### Get the embedding weight for visualization e = model.layers[0] weights = e.get_weights()[0],0
"SETUP ASSIGN = Tokenizer(oov_token=OOV_TOK) ASSIGN.fit_on_texts(train_sentences) ASSIGN = text_tokenizer.ASSIGN ASSIGN = text_tokenizer.texts_to_sequences(train_sentences) ASSIGN = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",1,not_existent,"### Tokenize the train sentences from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences text_tokenizer = Tokenizer(oov_token=OOV_TOK)  # Get the token dict from data text_tokenizer.fit_on_texts(train_sentences) word_index = text_tokenizer.word_index  # Pad the data train_sequences = text_tokenizer.texts_to_sequences(train_sentences) train_padded = pad_sequences(train_sequences, padding=PADDING_TYPE, truncating=TRUNC_TYPE, maxlen=MAX_LENGTH)",0
(train == '?').any(),0,not_existent,(train == '?').any(),1
train[train == '?'].count(),0,not_existent,train[train == '?'].count(),1
"VALIDATION ASSIGN = pd.read_csv('..path', na_values='?', index_col='Id') test",0,not_existent,"test = pd.read_csv('../input/dataset/test_data.csv', na_values='?', index_col='Id') test",1
ASSIGN = KNeighborsRegressor(38),0,not_existent,knn = KNeighborsRegressor(38),1
(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar'),0,not_existent,(train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[54:57]].plot(kind = 'bar'),1
ASSIGN = GaussianNB(),0,not_existent,NB = GaussianNB() ,1
"ASSIGN = pd.DataFrame(index = test.index) ASSIGN = Ytest ASSIGN.to_csv('submission.csv',index = True)",1,not_existent,"pred = pd.DataFrame(index = test.index) pred['ham'] = Ytest pred.to_csv('submission.csv',index = True)",0
SETUP ASSIGN = 7 np.random.ASSIGN(ASSIGN),0,not_existent,import matplotlib.pyplot as plt import numpy as np import pandas as pd seed = 7 np.random.seed(seed),1
"VALIDATION ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = np.load(""..path"") ASSIGN = pd.read_csv(""..path"") tlabel",0,not_existent,"tpure = np.load(""../input/train_images_pure.npy"") tnoisy = np.load(""../input/train_images_noisy.npy"") trotated = np.load(""../input/train_images_rotated.npy"") tboth = np.load(""../input/train_images_both.npy"") tlabel = pd.read_csv(""../input/train_labels.csv"") tlabel",1
ASSIGN = deepCNN(),0,not_existent,CNNmodel = deepCNN(),1
"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",0,not_existent,"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",1
ASSIGN = KNeighborsClassifier(n_neighbors=10),0,not_existent,knn = KNeighborsClassifier(n_neighbors=10),1
"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])",0,stream,"pipe.fit(train_non_re.drop('TARGET', axis=1),train_non_re['TARGET'])",1
"ASSIGN = pd.read_csv(""..path"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)",0,not_existent,"train = pd.read_csv(""../input/cs-challenge/training_set.csv"", index_col=""ID"").drop(""MAC_CODE"", axis = 1)",1
VALIDATION print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1)) print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1)),0,error,print(train[train['IsHoliday']==True]['Weekly_Sales'].describe().round(1))  print(train[train['IsHoliday']==False]['Weekly_Sales'].describe().round(1)),1
"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() ASSIGN = pd.DataFrame([Pclass1,Pclass2,Pclass3]) ASSIGN.index = ['1st class', '2nd class', '3rd class'] ASSIGN.plot(kind='bar', stacked=True, figsize=(10,5))",1,execute_result,"Pclass1 = train[train['Pclass']==1]['Cabin'].value_counts() Pclass2 = train[train['Pclass']==2]['Cabin'].value_counts() Pclass3 = train[train['Pclass']==3]['Cabin'].value_counts() df = pd.DataFrame([Pclass1,Pclass2,Pclass3]) df.index = ['1st class', '2nd class', '3rd class'] df.plot(kind='bar', stacked=True, figsize=(10,5))",0
"SETUP ASSIGN = KFold(n_splits=10, shuffle=True, random_state=0)",1,not_existent,"from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score k_fold = KFold(n_splits=10, shuffle=True, random_state=0)",0
"VALIDATION ASSIGN = math.sqrt(mean_squared_error(p_train, y_train)) print('Pontuação para o treinamento: %.2f RMSE' % (ASSIGN)) ASSIGN = math.sqrt(mean_squared_error(p_test, y_test)) print('Pontuação para o teste: %.2f RMSE' % (ASSIGN))",0,stream,"#calcula os erros de previsão  trainScore = math.sqrt(mean_squared_error(p_train, y_train))  print('Pontuação para o treinamento: %.2f RMSE' % (trainScore))  testScore = math.sqrt(mean_squared_error(p_test, y_test))  print('Pontuação para o teste: %.2f RMSE' % (testScore))",1
"app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()), ""ID""]",0,execute_result,"# Check if there are missing or 0 ID's  app_df_cut.loc[(app_df_cut[""ID""] == 0) | (app_df_cut[""ID""].isnull()),                ""ID""]",1
"app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]",0,execute_result,"# Check if there are null values in the Size column  app_df_cut[(app_df_cut[""Size""].isnull()) | (app_df_cut['Size'] == 0)]",1
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",0,execute_result,"#Amount of games that have only the English language  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",1
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",0,execute_result,"#Amount of games that have only one language and is not English  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",1
adult_train.describe(exclude = [np.number]),0,execute_result,adult_train.describe(exclude = [np.number]),1
"SETUP ASSIGN = adult_train.copy() ASSIGN = LabelEncoder() ASSIGN[""income""] = ASSIGN.fit_transform(ASSIGN['income']) plt.figure(figsize=(10,10)) ASSIGN = np.zeros_like(adult_copy.corr(), dtype=np.bool) ASSIGN[np.triu_indices_from(ASSIGN)] = True sns.heatmap(ASSIGN.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, ASSIGN=ASSIGN) plt.show()",1,display_data,"adult_copy = adult_train.copy()  from sklearn.preprocessing import LabelEncoder  le = LabelEncoder()  adult_copy[""income""] = le.fit_transform(adult_copy['income'])    #heat map:  plt.figure(figsize=(10,10))  mask = np.zeros_like(adult_copy.corr(), dtype=np.bool)  mask[np.triu_indices_from(mask)] = True  sns.heatmap(adult_copy.corr(), square=True, vmin=-1, vmax=1, annot = True, linewidths=.5, mask=mask)  plt.show()",0
"VALIDATION ASSIGN = ""This is the euro symbol: €"" ASSIGN = before.encode(""ascii"", errors = ""replace"") print(ASSIGN.decode())",1,not_existent,"# start with a string before = ""This is the euro symbol: €""  # encode it to a different encoding, replacing characters that raise errors after = before.encode(""ascii"", errors = ""replace"")  # convert it back to utf-8 print(after.decode(""ascii""))  # We've lost the original underlying byte string! It's been  # replaced with the underlying byte string for the unknown character :(",0
"VALIDATION ASSIGN = ""i'll try the recommended $, #, 你好 and नमस्ते and see what happens."" ASSIGN = my_text.encode(""ascii"", errors = ""replace"") print(ASSIGN.decode())",0,not_existent,"# Your turn! Try encoding and decoding different symbols to ASCII and # see what happens. I'd recommend $, #, 你好 and नमस्ते but feel free to # try other characters. What happens? When would this cause problems?  # start with a string my_text = ""i'll try the recommended $, #, 你好 and नमस्ते  and see what happens.""  # encode it to a different encoding, replacing characters that raise errors my_text_encoded = my_text.encode(""ascii"", errors = ""replace"")  # convert it back to utf-8 print(my_text_encoded.decode(""ascii"")) ",1
"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)",1,not_existent,"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match=""kuram agency"", min_ratio = 94)",0
"ASSIGN = { ""KNeighborsClassifier"": KNeighborsClassifier(), ""RandomForest"": RandomForestClassifier(), ""SVM"": SVC(), } ASSIGN = { ""KNeighborsClassifier"": { ""n_neighbors"": randint(low=1, high=30), }, ""RandomForest"": { ""n_estimators"": randint(low=1, high=200), ""max_features"": randint(low=1, high=8), }, ""SVM"": { ""kernel"": [""linear"", ""rbf""], ""C"": reciprocal(0.1, 200000), ""gamma"": expon(scale=1.0), } }",0,not_existent,"models = {      ""KNeighborsClassifier"": KNeighborsClassifier(),      ""RandomForest"": RandomForestClassifier(),      ""SVM"": SVC(),  }    randomized_params = {      ""KNeighborsClassifier"": {          ""n_neighbors"": randint(low=1, high=30),      },      ""RandomForest"": {          ""n_estimators"": randint(low=1, high=200),          ""max_features"": randint(low=1, high=8),      },      ""SVM"": {          ""kernel"": [""linear"", ""rbf""],          ""C"": reciprocal(0.1, 200000),          ""gamma"": expon(scale=1.0),      }  }",1
"ASSIGN = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)",0,stream,"knn_grid = load(""titanic_KNeighborsClassifier_02"", with_metadata=True)",1
"ASSIGN = load(""titanic_SVM_02"", with_metadata=True)",0,stream,"svc_grid = load(""titanic_SVM_02"", with_metadata=True)",1
"ASSIGN = load(""titanic_RandomForest_02"", with_metadata=True)",0,stream,"random_forest_grid = load(""titanic_RandomForest_02"", with_metadata=True)",1
"def Mrmse(y_true,y_pred): ASSIGN = np.log(ASSIGN) ASSIGN = math.sqrt(mean_squared_error(y_true, y_pred)) return rmse",0,not_existent,"def Mrmse(y_true,y_pred):     y_true = np.log(y_true)     #y_pred = np.log(y_pred)     rmse = math.sqrt(mean_squared_error(y_true, y_pred))     return rmse",1
"ASSIGN = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) ASSIGN.to_csv('submission.csv', index=False)",0,not_existent,"my_submission = pd.DataFrame({'Id': test_final_id['Id'], 'SalePrice': y_pred_final}) # you could use any filename. We choose submission here my_submission.to_csv('submission.csv', index=False) #my_submission",1
ASSIGN = 0 ASSIGN = 0 ASSIGN = 0.01 ASSIGN = 10000 ASSIGN = float(len(x)),0,not_existent,b0 = 0  b1 = 0  alpha = 0.01  count = 10000  n = float(len(x)),1
"VALIDATION for i in range(count): ASSIGN = b1*x + b0 ASSIGN = ASSIGN - (alphapath)*sum(x*(y_bar-y)) ASSIGN = ASSIGN - (alphapath)*sum(y_bar-y) print(ASSIGN,ASSIGN)",1,stream,"for i in range(count):      y_bar = b1*x + b0      b1 = b1 - (alpha/n)*sum(x*(y_bar-y))      b0 = b0 - (alpha/n)*sum(y_bar-y)            print(b0,b1)",0
"VALIDATION def twoplot(df, col, xaxis=None): ''' scatter plot a feature split into response values as two subgraphs ''' if col not in df.columns.values: print('ERROR: %s not a column' % col) ASSIGN = pd.DataFrame(index = df.index) ASSIGN = ASSIGN ASSIGN = ASSIGN if xaxis else df.index ASSIGN = ASSIGN ASSIGN = sns.FacetGrid(ndf, col=""Response"", hue=""Response"") ASSIGN.map(plt.scatter, xaxis, col, alpha=.7, s=1) ASSIGN.add_legend(); del ndf",1,not_existent,"def twoplot(df, col, xaxis=None):      ''' scatter plot a feature split into response values as two subgraphs '''      if col not in df.columns.values:          print('ERROR: %s not a column' % col)      ndf = pd.DataFrame(index = df.index)      ndf[col] = df[col]      ndf[xaxis] = df[xaxis] if xaxis else df.index      ndf['Response'] = df['Response']            g = sns.FacetGrid(ndf, col=""Response"", hue=""Response"")      g.map(plt.scatter, xaxis, col, alpha=.7, s=1)      g.add_legend();            del ndf",0
"ASSIGN = pd.read_csv('..path') ASSIGN = np.array([ imread(test_dir+p)path]) ASSIGN = np.array(ASSIGN) ASSIGN = getProb(model, x_test) ASSIGN['has_cactus'] = ASSIGN ASSIGN.to_csv('cactus_net_submission.csv', index=False)",1,not_existent,"df_test = pd.read_csv('../input/sample_submission.csv')  x_test  = np.array([ imread(test_dir+p)/255 for p in df_test.id.values])  x_test  = np.array(x_test)    # test prediction  y_prob_test = getProb(model, x_test)    df_test['has_cactus'] = y_prob_test  df_test.to_csv('cactus_net_submission.csv', index=False)",0
"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}path{len(train['landmark_id'].unique())}""",0,not_existent,"f""Number of classes under 10 occurences : {(train['landmark_id'].value_counts() <= 10).sum()}/{len(train['landmark_id'].unique())}""",1
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",0,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",1
"VALIDATION ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Training data shape', ASSIGN.shape) ASSIGN = pd.read_csv('..path', header=0, sep=',') ASSIGN = ASSIGN.loc[:,'MSSubClass':] print('Training data columns:', ASSIGN.columns) print('Test data shape\n', ASSIGN.shape)",0,not_existent,"train_df = pd.read_csv('../input/train.csv', header=0, sep=',') train_df = train_df.loc[:,'MSSubClass':] #print('Training data\n', train_df.head()) print('Training data columns:', train_df.columns) print('Training data shape', train_df.shape)  test_df = pd.read_csv('../input/test.csv', header=0, sep=',') test_df = test_df.loc[:,'MSSubClass':] #print('Test data\n', test_df.head()) print('Training data columns:', test_df.columns) print('Test data shape\n', test_df.shape)",1
"ASSIGN = model.predict_generator(validation_generator, num_of_test_samples) ASSIGN = np.argmax(Y_pred, axis=1) ASSIGN[200]",0,execute_result,"  Y_pred = model.predict_generator(validation_generator, num_of_test_samples)  y_pred = np.argmax(Y_pred, axis=1)    y_pred[200]   ",1
"ASSIGN = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0path) ASSIGN = test_datagen.flow_from_directory(""..path"", ASSIGN = 'categorical', ASSIGN = (150, 150))",0,stream,"#Extract the test data => I didnt find a way without creating a new folder on colab    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)    test_generator = test_datagen.flow_from_directory(""../input/animal-ori/animal_dataset_intermediate"",                                                                                                            class_mode = 'categorical',                                                       target_size = (150, 150))",1
"VALIDATION ASSIGN = 9106 ASSIGN = model.predict_generator(test_generator) ASSIGN = np.argmax(Y_pred_test, axis=1) y_pred_test",1,execute_result,"#prediction on test data     num = 9106    Y_pred_test = model.predict_generator(test_generator)  y_pred_test = np.argmax(Y_pred_test, axis=1)  y_pred_test ",0
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""hacker_news"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""comments"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""hacker_news"" dataset  dataset_ref = client.dataset(""hacker_news"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)    # Construct a reference to the ""comments"" table  table_ref = dataset_ref.table(""comments"")    # API request - fetch the table  table = client.get_table(table_ref)    # Preview the first five lines of the ""comments"" table  client.list_rows(table, max_results=5).to_dataframe()",0
pd.options.display.max_columns = num_of_cols,0,not_existent,pd.options.display.max_columns = num_of_cols,1
df['parentesco1'].loc[df.parentesco1 == 1].describe(),0,not_existent,df['parentesco1'].loc[df.parentesco1 == 1].describe(),1
(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']path(df))*100,0,not_existent,(df['parentesco1'].loc[df.parentesco1 == 1].describe()['count']/len(df))*100,1
"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)",0,not_existent,"df[['parentesco1','idhogar']].loc[df.parentesco1 == 1].head(5)",1
"VALIDATION print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4'])",0,not_existent,"print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '09b195e7a']) print(df[['Id','v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == 'f2bfa75c4']) ",1
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",0,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",1
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",0,not_existent,"# remove these two rows... df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",1
VALIDATION cols_electronics.remove('Target') cols_electronics,1,not_existent,cols_electronics.remove('Target') cols_electronics,0
total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv') len(total_features),1,not_existent,total_features.remove('r4t3') total_features.remove('tamhog') total_features.remove('tamviv')  len(total_features),0
total_features.remove('escolari') len(total_features),1,not_existent,total_features.remove('escolari') len(total_features),0
df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar(),0,not_existent,df.loc[df.Target == 1].groupby('overcrowding').SQBescolari.value_counts().unstack().plot.bar(),1
"VALIDATION for col in nan_cols: if col != 'v2a1': print(col, df[col].unique())",0,not_existent,"for col in nan_cols:     if col != 'v2a1':         print(col, df[col].unique())",1
"len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",0,not_existent,"# number of rows where income and rez_esc has values len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",1
"ASSIGN = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(ASSIGN, data=df, kind=""kde"")",0,not_existent,"x, y = df['rez_esc'].fillna(0), df['v2a1'].fillna(0) sns.jointplot(x, y, data=df, kind=""kde"")",1
total_features.remove('hogar_total') len(total_features),1,not_existent,"# use hhsize, ignore 'hogar_total', total_features.remove('hogar_total')  len(total_features)",0
total_features.remove('female') len(total_features),1,not_existent,# removing female total_features.remove('female')  len(total_features),0
total_features.remove('r4t3'),1,not_existent,"# removing 'r4t3', as 'hhsize' is of almost same distribution total_features.remove('r4t3')",0
total_features.append('edjefa') total_features.append('edjefe'),1,not_existent,total_features.append('edjefa') total_features.append('edjefe'),0
total_features.remove('abastaguano') total_features.remove('abastaguafuera') len(total_features),1,not_existent,total_features.remove('abastaguano') total_features.remove('abastaguafuera')  len(total_features),0
total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother') len(total_features),1,not_existent,# removing these features -> inc by 0.002 total_features.remove('pisonatur') total_features.remove('pisonotiene') total_features.remove('pisoother')  len(total_features),0
df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],0,not_existent,df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],1
df['v2a1'].loc[df.parentesco1 == 1].plot.line(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.line(),1
df['v2a1'].loc[df.parentesco1 == 1].plot.hist(),0,not_existent,df['v2a1'].loc[df.parentesco1 == 1].plot.hist(),1
"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",0,not_existent,"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",1
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",0,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",1
"ASSIGN = xgb.XGBClassifier( ASSIGN =0.1, ASSIGN=1000, ASSIGN=5, ASSIGN=1, ASSIGN=0, ASSIGN=0.8, ASSIGN=0.8, ASSIGN= 'binary:logistic', ASSIGN=ASSIGN, ASSIGN=1, ASSIGN=27)",0,not_existent,"#Let's do a little Gridsearch, Hyperparameter Tunning model3 = xgb.XGBClassifier(  learning_rate =0.1,  n_estimators=1000,  max_depth=5,  min_child_weight=1,  gamma=0,  subsample=0.8,  colsample_bytree=0.8,  objective= 'binary:logistic',  n_jobs=n_jobs,  scale_pos_weight=1,  seed=27)",1
for f in less_imp_features: if f in total_features: total_features.remove(f) len(total_features),1,not_existent,for f in less_imp_features:     if f in total_features:         total_features.remove(f)  len(total_features),0
VALIDATION ASSIGN = sort(model3.feature_importances_) thresholds,1,not_existent,thresholds = sort(model3.feature_importances_) thresholds,0
df_final.index.name = None df_final.head(7),1,not_existent,df_final.index.name = None df_final.head(7),0
"VALIDATION ASSIGN = ("""""" -- Select all the columns we want in our joined table SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python FROM `bigquery-public-data.github_repos.sample_commits` as sc -- Table to merge into sample_files INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf ON sc.repo_name = sf.repo_name -- what columns should we join on? WHERE sf.path LIKE '%.py' GROUP BY sf.repo_name ORDER BY number_of_commits_in_python DESC """""") print(github.estimate_query_size(ASSIGN)) ASSIGN = github.query_to_pandas_safe(query, max_gb_scanned=6)",1,not_existent,"#How many commits (recorded in the ""sample_commits"" table) have been made in repos written in the Python programming language? (I'm looking for the number of commits per repo for all the repos written in Python.  query = (""""""         -- Select all the columns we want in our joined table         SELECT sf.repo_name, count(sc.commit) number_of_commits_in_python         FROM `bigquery-public-data.github_repos.sample_commits` as sc         -- Table to merge into sample_files         INNER JOIN `bigquery-public-data.github_repos.sample_files` as sf              ON sc.repo_name =  sf.repo_name -- what columns should we join on?         WHERE sf.path LIKE '%.py'         GROUP BY sf.repo_name         ORDER BY number_of_commits_in_python DESC         """""")  print(github.estimate_query_size(query)) file_count_by_python_files = github.query_to_pandas_safe(query, max_gb_scanned=6)",0
"def get_default_device(): """"""Pick GPU if available, else CPU"""""" if torch.cuda.is_available(): return torch.device('cuda') else: return torch.device('cpu') def to_device(data, device): """"""Move tensor(s) to chosen device"""""" if isinstance(data, (list,tuple)): return [to_device(x, device) for x in data] return data.to(device, non_blocking=True) class DeviceDataLoader(): """"""Wrap a dataloader to move data to a device"""""" def __init__(self, dl, device): self.dl = dl self.device = device def __iter__(self): """"""Yield a batch of data after moving it to device"""""" for b in self.dl: yield to_device(b, self.device) def __len__(self): """"""Number of batches"""""" return len(self.dl)",1,not_existent,"def get_default_device():     """"""Pick GPU if available, else CPU""""""     if torch.cuda.is_available():         return torch.device('cuda')     else:         return torch.device('cpu')      def to_device(data, device):     """"""Move tensor(s) to chosen device""""""     if isinstance(data, (list,tuple)):         return [to_device(x, device) for x in data]     return data.to(device, non_blocking=True)  class DeviceDataLoader():     """"""Wrap a dataloader to move data to a device""""""     def __init__(self, dl, device):         self.dl = dl         self.device = device              def __iter__(self):         """"""Yield a batch of data after moving it to device""""""         for b in self.dl:              yield to_device(b, self.device)      def __len__(self):         """"""Number of batches""""""         return len(self.dl)",0
"def conv_block(in_channels, out_channels, pool=False): ASSIGN = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)] if pool: ASSIGN.append(nn.MaxPool2d(2)) return nn.Sequential(*ASSIGN) class ResNet9(ImageClassificationBase): def __init__(self, in_channels, num_classes): super().__init__() self.conv1 = conv_block(in_channels, 64) self.conv2 = conv_block(64, 128, pool=True) self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) self.conv3 = conv_block(128, 256, pool=True) self.conv4 = conv_block(256, 512, pool=True) self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) self.classifier = nn.Sequential(nn.MaxPool2d(4), nn.Flatten(), nn.Linear(512, num_classes)) def forward(self, xb): ASSIGN = self.conv1(xb) ASSIGN = self.conv2(ASSIGN) ASSIGN = self.res1(ASSIGN) + ASSIGN ASSIGN = self.conv3(ASSIGN) ASSIGN = self.conv4(ASSIGN) ASSIGN = self.res2(ASSIGN) + ASSIGN ASSIGN = self.classifier(ASSIGN) return out",0,not_existent,"def conv_block(in_channels, out_channels, pool=False):     layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),                nn.BatchNorm2d(out_channels),                nn.ReLU(inplace=True)]     if pool: layers.append(nn.MaxPool2d(2))     return nn.Sequential(*layers)  class ResNet9(ImageClassificationBase):     def __init__(self, in_channels, num_classes):         super().__init__()                  self.conv1 = conv_block(in_channels, 64)         self.conv2 = conv_block(64, 128, pool=True)         self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))                  self.conv3 = conv_block(128, 256, pool=True)         self.conv4 = conv_block(256, 512, pool=True)         self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))                  self.classifier = nn.Sequential(nn.MaxPool2d(4),                                          nn.Flatten(),                                          nn.Linear(512, num_classes))              def forward(self, xb):         out = self.conv1(xb)         out = self.conv2(out)         out = self.res1(out) + out         out = self.conv3(out)         out = self.conv4(out)         out = self.res2(out) + out         out = self.classifier(out)         return out",1
ASSIGN = 8 ASSIGN = 0.01 ASSIGN = 0.1 ASSIGN = 1e-4 ASSIGN = torch.optim.Adam,0,not_existent,epochs = 8 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam,1
ASSIGN=KNeighborsClassifier(n_neighbors=7),0,not_existent,knn=KNeighborsClassifier(n_neighbors=7),1
"SETUP """"""author    s_agnik1511"""""" ASSIGN = pd.read_csv('path') ASSIGN = trd.SalePrice ASSIGN = ['LotArea'] ASSIGN = trd[predictor_cols] ASSIGN = RandomForestRegressor() ASSIGN.fit(ASSIGN, ASSIGN)",0,execute_result,"""""""author    s_agnik1511""""""  import numpy as np  import pandas as pd  from sklearn.ensemble import RandomForestRegressor  trd = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')  train_y = trd.SalePrice  predictor_cols = ['LotArea']  train_X = trd[predictor_cols]  my_model = RandomForestRegressor()  my_model.fit(train_X, train_y)",1
"ASSIGN = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, items.family.value_counts().values), ASSIGN = False))) ASSIGN = go.Bar( ASSIGN = items.family.value_counts().values, ASSIGN = items.family.value_counts().index ) ASSIGN = dict( ASSIGN='Counts of items per family category', ASSIGN = 900, height = 600, ASSIGN=dict( ASSIGN = True, ASSIGN = True, ASSIGN = True )) ASSIGN = go.Figure(data=[trace2]) ASSIGN['ASSIGN'].update(ASSIGN) py.iplot(ASSIGN, filename='plots')",0,display_data,"# BAR PLOT FOR ITEMS V/S FAMILY TYPE  x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index,                                            items.family.value_counts().values),                                           reverse = False)))  trace2 = go.Bar(      y = items.family.value_counts().values,      x = items.family.value_counts().index  )    layout = dict(      title='Counts of items per family category',       width = 900, height = 600,      yaxis=dict(          showgrid = True,          showline = True,          showticklabels = True      ))    fig1 = go.Figure(data=[trace2])  fig1['layout'].update(layout)  py.iplot(fig1, filename='plots')",1
"VALIDATION ASSIGN = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False) ASSIGN=np.sort(ASSIGN) print('lines to skip:', len(ASSIGN)) ASSIGN = pd.read_csv(""..path"", skiprows=skiplines)",1,not_existent,"skiplines = np.random.choice(np.arange(1, lines), size=lines-1-1000000, replace=False) skiplines=np.sort(skiplines) print('lines to skip:', len(skiplines))  data = pd.read_csv(""../input/data.csv"", skiprows=skiplines)",0
"ASSIGN=[] ASSIGN=[] for dirname, _, filenames in os.walk('..path'): for filename in filenames[0:9]: ASSIGN.append( cv2.imread(dirname+""path""+ filename,0)) ASSIGN.append(""1"") DisplayImage(ASSIGN,ASSIGN,3)",0,display_data,"images=[] titles=[] for dirname, _, filenames in os.walk('../input/digits-in-noise/Test'):     for filename in filenames[0:9]:                 images.append( cv2.imread(dirname+""/""+ filename,0))         titles.append(""1"")                   DisplayImage(images,titles,3)",1
"SETUP ASSIGN = np.load(DSPATH+""olivetti_faces.npy"") ASSIGN = np.load(DSPATH+""olivetti_faces_target.npy"") ThiefImage={} SLICE= image.imread(""..path"") SLICE=image.imread(""..path"")",0,not_existent,"DSPATH=""../input/olivetti-faces/"" X = np.load(DSPATH+""olivetti_faces.npy"") y = np.load(DSPATH+""olivetti_faces_target.npy"")    ThiefImage={} ThiefImage[""False""]= image.imread(""../input/thief-images/False.jpg"") ThiefImage[""True""]=image.imread(""../input/thief-images/True.jpg"")       ",1
df[df.x==0],0,execute_result,df[df.x==0],1
"ASSIGN = ReduceLROnPlateau(monitor='val_acc', ASSIGN=3, ASSIGN=1, ASSIGN=0.7, ASSIGN=0.00000000001) ASSIGN = EarlyStopping(patience=2)",0,not_existent,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',                                              patience=3,                                              verbose=1,                                              factor=0.7,                                              min_lr=0.00000000001) early_stopping_monitor = EarlyStopping(patience=2)",1
"ASSIGN = cv2.CascadeClassifier('..path') ASSIGN =cv2.imread( ""..path"",0)",0,not_existent,"face_cascade = cv2.CascadeClassifier('../input/opencv-haarcascade/data/haarcascades/haarcascade_frontalface_default.xml') gray =cv2.imread( ""../input/opencv-samples-images/data/lena.jpg"",0)",1
ASSIGN=10000,0,not_existent,sampleNumber=10000,1
"VALIDATION ASSIGN=getpatterns(""O"") ASSIGN=getpatterns(""X"") TicTecBoard=[""-"" for i in range(9)] ASSIGN=""O"" try: while True: ValidIndexList=getValidPlace(TicTecBoard) ASSIGN==""O"": while True: printTicTecBoard(TicTecBoard) ASSIGN=input("" \n Enter Cell Number from Valid Index List ""+str(ValidIndexList )+"" : \n"") if ASSIGN in ValidIndexList: ASSIGN=int(ASSIGN)-1 break else: print() else: ASSIGN=0 for place in ValidIndexList: ASSIGN=list(TicTecBoard) ASSIGN=int(place)-1 SLICE=ASSIGN if checkPatterns(ASSIGN,ASSIGN)>0: ASSIGN=testIndex ASSIGN=1 ASSIGN==0: for place in ValidIndexList: ASSIGN=list(TicTecBoard) ASSIGN=int(place)-1 SLICE=""O"" if checkPatterns(ASSIGN,ASSIGN)>0: ASSIGN=testIndex ASSIGN=-1 ASSIGN==0: ASSIGN=4 if ""5"" in ValidIndexList else int(random.choice(ValidIndexList))-1 SLICE=ASSIGN if checkPatterns(getpatterns(ASSIGN),TicTecBoard)>0: printTicTecBoard(TicTecBoard) print('\x1b[6;30;42m' +ASSIGN++ '\x1b[0m') break elif checkPatterns(""-"",TicTecBoard)==0: printTicTecBoard(TicTecBoard) print() break else: ASSIGN=""X"" if ASSIGN==""O"" else ""O"" except: print()",0,stream," patternO=getpatterns(""O"") patternX=getpatterns(""X"") TicTecBoard=[""-"" for i in range(9)] player=""O"" try:        while True:             ValidIndexList=getValidPlace(TicTecBoard)         if player==""O"":             while True:                   printTicTecBoard(TicTecBoard)                 index=input("" \n Enter Cell Number from Valid Index List  ""+str(ValidIndexList )+"" : \n"")                  if index in ValidIndexList:                     index=int(index)-1                     break                 else:                     print(""Plz Enter Valied Place"")           else:             machineWin=0             for place in ValidIndexList:                 testTicTecBoard=list(TicTecBoard)                 testIndex=int(place)-1                 testTicTecBoard[testIndex]=player                 if checkPatterns(patternX,testTicTecBoard)>0:                      index=testIndex                     machineWin=1             if machineWin==0:                 for place in ValidIndexList:                     testTicTecBoard=list(TicTecBoard)                     testIndex=int(place)-1                     testTicTecBoard[testIndex]=""O""                     if checkPatterns(patternO,testTicTecBoard)>0:                          index=testIndex                         machineWin=-1             if machineWin==0:                 index=4 if ""5"" in ValidIndexList else int(random.choice(ValidIndexList))-1            TicTecBoard[index]=player           if checkPatterns(getpatterns(player),TicTecBoard)>0:              printTicTecBoard(TicTecBoard)             print('\x1b[6;30;42m'  +player+"" is Win ""+ '\x1b[0m')               break         elif checkPatterns(""-"",TicTecBoard)==0:              printTicTecBoard(TicTecBoard)             print(""\033[93m Game is Draw  \033[0m"")             break          else:             player=""X"" if player==""O"" else ""O""    except:     print(""Plz Enter Valied Index"")    ",1
preprocessing.StandardScaler().fit(full).transform(full.astype(float)),1,execute_result,#Data Standardization  preprocessing.StandardScaler().fit(full).transform(full.astype(float)),0
"ASSIGN=test['PassengerId'] ASSIGN=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) ASSIGN.to_csv('submission.csv',index=False) ASSIGN.head()",0,execute_result,"Id=test['PassengerId'] sub_df=pd.DataFrame({'PassengerId':Id,'Survived':MLPClassifier_y_pred}) sub_df.to_csv('submission.csv',index=False) sub_df.head()",1
"os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]",0,not_existent," os.environ['KAGGLE_USERNAME'] = API[""username""] os.environ['KAGGLE_KEY'] = API[""key""]",1
"SETUP def giveMeFeatures(image): ASSIGN = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2') return res",1,not_existent,"from skimage.feature import hog  def giveMeFeatures(image):      res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')  #     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)      return res           ",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(1) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.random.RandomState(321) ASSIGN = rand.permutation(len(X)) ASSIGN = ASSIGN[shuffle] ASSIGN = ASSIGN[shuffle]",1,not_existent,"import glob  import cv2  directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'  directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'    X = []  y = []    for filename in glob.glob(os.path.join(directory_a, '*.png')):      im1 =cv2.imread(filename,0)      im1 = cv2.resize(im1,(64,64))      features = giveMeFeatures(im1)      X.append(features)      y.append(0)    for filename in glob.glob(os.path.join(directory_b, '*.png')):      im1 =cv2.imread(filename,0)      im1 = cv2.resize(im1,(64,64))      features = giveMeFeatures(im1)      X.append(features)      y.append(1)        X = np.array(np.float32(X))  y = np.array(np.float32(y))      rand = np.random.RandomState(321)  shuffle = rand.permutation(len(X))  X = X[shuffle]  y = y[shuffle]             ",0
ASSIGN = models.Sequential(),0,not_existent,model = models.Sequential(),1
X=Xpath,1,not_existent,X=X/255,0
"ASSIGN = ImageDataGenerator( ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=10, ASSIGN = 0.1, ASSIGN=0.1, ASSIGN=0.1, ASSIGN=False, ASSIGN=False) ASSIGN.fit(X_train)",1,error,"# With data augmentation to prevent overfitting    datagen = ImageDataGenerator(          featurewise_center=False,  # set input mean to 0 over the dataset          samplewise_center=False,  # set each sample mean to 0          featurewise_std_normalization=False,  # divide inputs by std of the dataset          samplewise_std_normalization=False,  # divide each input by its std          zca_whitening=False,  # apply ZCA whitening          rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)          zoom_range = 0.1, # Randomly zoom image           width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)          height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)          horizontal_flip=False,  # randomly flip images          vertical_flip=False)  # randomly flip images      datagen.fit(X_train)",0
label_binarizer.transform(np.asarray([0])),0,execute_result,label_binarizer.transform(np.asarray([0])),1
"VALIDATION ASSIGN = ASSIGN.reshape(-1,64,64,1) X.shape",1,execute_result,"X = X.reshape(-1,64,64,1)  X.shape",0
"SETUP VALIDATION ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(3) print('finished')",1,stream,"import glob  import cv2  directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'  directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'  directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'  # directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'  # directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'  #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_1, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          print('finished')          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          print('finished')          for filename in glob.glob(os.path.join(directory_3, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)          print('finished')          for filename in glob.glob(os.path.join(directory_unk, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(3)          print('finished')                               ",0
"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ASSIGN = [] ASSIGN = [] for iteration in range(max_iteration): ASSIGN = np.dot(X,w) ASSIGN = MSE(y,predicted_y) ASSIGN = round(ASSIGN,9) ASSIGN.append(w) ASSIGN.append(ASSIGN) ASSIGN = -(2path[0])* X.dot(loss).sum() ASSIGN = ASSIGN + lr * derivative return w_history,loss_hostory",0,not_existent,"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001):      w_history  = []     loss_hostory = []     for iteration in range(max_iteration):         predicted_y = np.dot(X,w)         loss =  MSE(y,predicted_y)         loss = round(loss,9)         w_history.append(w)         loss_hostory.append(loss)         derivative = -(2/y.shape[0])* X.dot(loss).sum()         w = w + lr * derivative     return w_history,loss_hostory ",1
"ASSIGN = [] ASSIGN =1000 ASSIGN = 0.1 for _ in range(ASSIGN): ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=ASSIGN*ASSIGN",0,not_existent,"history = [] #loss history  numberOfRounds =1000 # max number of times the optimization algorithm will run learningRate = 0.1 for _ in range(numberOfRounds):     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))      for j in range(k):         deltaTemp=0          #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,         #and it works fine in case of offline training and smalle number of samples)          for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=learningRate*deltaTemp",1
"VALIDATION ASSIGN = [] ASSIGN = 20 for iteration in range(ASSIGN): print('iteration: ',iteration) ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=0.1*ASSIGN",0,not_existent,"history = [] maxNumOfIterations = 20 for iteration in range(maxNumOfIterations):      print('iteration: ',iteration)     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))     for j in range(k):         deltaTemp=0         for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=0.1*deltaTemp",1
"full_report(train_df, target_column='logerror')",0,display_data,"full_report(train_df, target_column='logerror')",1
"ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= cum_var_exp, ASSIGN='lines+markers', ASSIGN=""'Cumulative Explained Variance'"", ASSIGN=dict( ASSIGN='spline', ASSIGN = 'goldenrod' ) ) ASSIGN = go.Scatter( ASSIGN=list(range(784)), ASSIGN= var_exp, ASSIGN='lines+markers', ASSIGN=""'Individual Explained Variance'"", ASSIGN=dict( ASSIGN='linear', ASSIGN = 'black' ) ) ASSIGN = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}], ASSIGN=True) ASSIGN.append_trace(ASSIGN, 1, 1) ASSIGN.append_trace(ASSIGN,1,1) ASSIGN.layout.title = 'Explained Variance plots - Full and Zoomed-in' ASSIGN.layout.xaxis = dict(range=[0, 80], title = 'Feature columns') ASSIGN.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')",0,stream,"trace1 = go.Scatter(      x=list(range(784)),      y= cum_var_exp,      mode='lines+markers',      name=""'Cumulative Explained Variance'"",  #     hoverinfo= cum_var_exp,      line=dict(          shape='spline',          color = 'goldenrod'      )  )  trace2 = go.Scatter(      x=list(range(784)),      y= var_exp,      mode='lines+markers',      name=""'Individual Explained Variance'"",  #     hoverinfo= var_exp,      line=dict(          shape='linear',          color = 'black'      )  )  fig = tls.make_subplots(insets=[{'cell': (1,1), 'l': 0.7, 'b': 0.5}],                            print_grid=True)    fig.append_trace(trace1, 1, 1)  fig.append_trace(trace2,1,1)  fig.layout.title = 'Explained Variance plots - Full and Zoomed-in'  fig.layout.xaxis = dict(range=[0, 80], title = 'Feature columns')  fig.layout.yaxis = dict(range=[0, 60], title = 'Explained Variance')  # fig['data'] = []  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))  # fig['data'].append(go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance'))    # fig['data'] = go.Scatter(x= list(range(784)) , y=cum_var_exp, xaxis='x2', yaxis='y2', name = 'Cumulative Explained Variance')]  # fig['data'] += [go.Scatter(x=list(range(784)), y=var_exp, xaxis='x2', yaxis='y2',name = 'Individual Explained Variance')]    # # fig['data'] = data  # # fig['layout'] = layout  # # fig['data'] += data2  # # fig['layout'] += layout2  # py.iplot(fig, filename='inset example')",1
ASSIGN = X_std.dot(matrix_w),1,not_existent,Y = X_std.dot(matrix_w),0
ASSIGN = [] ASSIGN = [],0,not_existent,chromosome = []  fitval = [] ,1
"VALIDATION generateCromosome() for i in range(1000): print(,i+1) evaluateSolution() c1,c2,bstval=selection() ASSIGN==5.0: break crossover(c1,c2) mutation()",0,stream,"generateCromosome()  for i in range(1000):      print(""Iteration"",i+1)      evaluateSolution()      c1,c2,bstval=selection()      if bstval==5.0:          break      crossover(c1,c2)      mutation()   ",1
"ASSIGN = model.predict(test_data) ASSIGN = np.argmax(ASSIGN,axis = 1) ASSIGN = pd.Series(ASSIGN,name=""Label"")",1,not_existent,"# predict results  results = model.predict(test_data)    # select the indix with the maximum probability  results = np.argmax(results,axis = 1)    results = pd.Series(results,name=""Label"")",0
"ASSIGN = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1) ASSIGN.to_csv(""mySubmission.csv"",index=False)",0,not_existent,"submission = pd.concat([pd.Series(range(1,28001),name = ""ImageId""),results],axis = 1)    submission.to_csv(""mySubmission.csv"",index=False)",1
VALIDATION print('Loading known faces...') ASSIGN = [] ASSIGN = [],0,stream,print('Loading known faces...')  known_faces = []  known_names = [],1
ASSIGN=[],0,not_existent,drop_column=[],1
ASSIGN=[],0,not_existent,cat_column=[],1
"SETUP ASSIGN = train_test_split(X_train, Y_train, ASSIGN=0.8, test_size=0.2, ASSIGN=0) ASSIGN=test_data",1,not_existent,"from sklearn.model_selection import train_test_split  x_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train,                                                        train_size=0.8, test_size=0.2,                                                        random_state=0)  x_test=test_data",0
ASSIGN=[],0,not_existent,Num_col=[],1
for col in cat_enc_col: feature.append(col),1,not_existent,for col in cat_enc_col:      feature.append(col),0
"VALIDATION ASSIGN=(preds_1+preds_2)path print(,mean_absolute_error(y_valid, ASSIGN))",1,stream,"preds_3=(preds_1+preds_2)/2  print(""MAE:"",mean_absolute_error(y_valid, preds_3))",0
"ASSIGN = {'undamaged': 0, 'repair': 1, 'replace': 2} data['operation_rank'] = data['operation'].apply(lambda x: ASSIGN[x]) def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold): ASSIGN = int(urr_score > repair_threshold) + int(urr_score > replace_threshold) return abs(ASSIGN - operation_rank) assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2) assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0) assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)",0,not_existent,"operation_ranks = {'undamaged': 0,                     'repair': 1,                     'replace': 2}    data['operation_rank'] = data['operation'].apply(lambda x: operation_ranks[x])    def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):      classified_outcome_rank = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)        return abs(classified_outcome_rank - operation_rank)    assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)  assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)  assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)      ",1
"VALIDATION ASSIGN = data[(data['set']==2)][['urr_score', 'operation_rank']] def mae(thresholds): return mae_dataset(ASSIGN, thresholds[0], thresholds[1]) ASSIGN = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True) print(f'Best thresholds: {ASSIGN.x}') print(f'Best average mse: {ASSIGN.fun}')",0,stream,"# Use only the test set to evaluate the best thresholds    test_set = data[(data['set']==2)][['urr_score', 'operation_rank']]    def mae(thresholds):      return mae_dataset(test_set, thresholds[0], thresholds[1])    # Calculating mse is somewhat expensive at a couple of seconds a time, so use an optimizer and small number of iterations  # Takes about 2.5 minutes  opt = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)    print(f'Best thresholds: {opt.x}')  print(f'Best average mse: {opt.fun}')",1
codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],0,execute_result,codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],1
"SETUP ASSIGN = KFold(n_splits=5, shuffle=False, random_state=None)",1,not_existent,"from sklearn.model_selection import KFold  cv_strategy = KFold(n_splits=5, shuffle=False, random_state=None)",0
"VALIDATION ASSIGN = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan) print(ASSIGN)",0,stream,"matrix = confusion_matrix(y_true=y_te_balan, y_pred=y_pred_balan)  print(matrix)",1
"VALIDATION ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_equidistant)      cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"VALIDATION ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_balanced) ASSIGN = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_balanced)      cv_val_scores = cross_val_score(DecisionTree, X, y_balanced, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_balanced))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(depth,cv_val_scores.mean(),DecisionTree.score(X, y_balanced))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
"SETUP ASSIGN = Pipeline([ ('scaler', None), ('knn', KNeighborsClassifier( ASSIGN=-1 )) ])",0,not_existent,"from sklearn.neighbors import KNeighborsClassifier  from sklearn.pipeline import Pipeline  from sklearn.preprocessing import StandardScaler    # Create k-NN classifier  pipe = Pipeline([      #('scaler', StandardScaler()), # With standardization      ('scaler', None), # Better performance without standardization!      ('knn', KNeighborsClassifier(          n_jobs=-1 # As many parallel jobs as possible      ))  ]) ",1
"ASSIGN = codiv_country_analyze['category_equidistant'] ASSIGN = codiv_country_analyze['category_balanced'] ASSIGN = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1) sns.countplot(ASSIGN)",0,execute_result,"y_equidistant = codiv_country_analyze['category_equidistant']  y_balanced = codiv_country_analyze['category_balanced']  X = codiv_country_analyze.drop(['Deaths Ratio','category_balanced','category_equidistant'], axis=1)   sns.countplot(y_balanced)",1
"VALIDATION ASSIGN=range(1,15) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for depth in ASSIGN: DecisionTree = DecisionTreeClassifier(criterion='gini', random_state=0, max_depth=depth) DecisionTree.fit(X, y_equidistant) ASSIGN = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(DecisionTree.score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",1,stream,"depths=range(1,15)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for depth in depths:      DecisionTree = DecisionTreeClassifier(criterion='gini',  random_state=0, max_depth=depth)      DecisionTree.fit(X, y_equidistant)      cv_val_scores = cross_val_score(DecisionTree, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(DecisionTree.score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f std %f score %f""%(depth,cv_val_scores.mean(),cv_val_scores.std(),DecisionTree.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",0
ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_Restrictions['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]) ASSIGN=1 for countryInd in codiv_country_without_Restrictions_qurantine['Country']: if not codiv_country_short[codiv_country_short['Country']==countryInd].empty: ASSIGN==1: ASSIGN=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd] ASSIGN=0 else: ASSIGN=ASSIGN.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Countrypath']==countryInd]),0,not_existent,#Recovered  Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_recovered=codiv_country_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_recovered=codiv_country_Restrictions_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_recovered=codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_recovered=codiv_country_without_Restrictions_qurantine_recovered.append(codiv_time_recovered_grouped_sum[codiv_time_recovered_grouped_sum['Country/Region']==countryInd])   #deaths     Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_deaths=codiv_country_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_deaths=codiv_country_Restrictions_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_deaths=codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_deaths=codiv_country_without_Restrictions_qurantine_deaths.append(codiv_time_deaths_grouped_sum[codiv_time_deaths_grouped_sum['Country/Region']==countryInd])   #confirmed    Start=1  for countryInd in codiv_country_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_qurantine_confirmed=codiv_country_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    Start=1  for countryInd in codiv_country_Restrictions['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_Restrictions_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    #codiv_country_Restrictions_confirmed=codiv_country_Restrictions_confirmed.set_index('Country/Region')  #  Start=1  for countryInd in codiv_country_without_Restrictions_qurantine['Country']:      if not codiv_country_short[codiv_country_short['Country']==countryInd].empty:          if Start==1:              codiv_country_without_Restrictions_qurantine_confirmed=codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd]              Start=0          else:              codiv_country_without_Restrictions_qurantine_confirmed=codiv_country_without_Restrictions_qurantine_confirmed.append(codiv_time_confirmed_grouped_sum[codiv_time_confirmed_grouped_sum['Country/Region']==countryInd])    ,1
"class SimpleLinearRegression: ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 def fit(self, x_train, y_train): ASSIGN = sum(x_train) ASSIGN = sum(y_train) ASSIGN = np.sum(np.square(x_train)) ASSIGN = np.sum(np.square(y_train)) ASSIGN = np.dot(x_train,y_train) ASSIGN = len(x_train) ASSIGN = sum_of_x2 - sum_of_x * sum_of_xpath ASSIGN = sum_of_y2 - sum_of_y * sum_of_ypath ASSIGN = length * dotproduct - sum_of_x * sum_of_y ASSIGN = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y)) ASSIGN = dotproduct - sum_of_x * sum_of_y path self.ASSIGN = np.square(ASSIGN path(ASSIGN)) self.ASSIGN = ASSIGN path((ASSIGN path) * sum_of_xpath) self.ASSIGN = ASSIGN path def predict(self,x_test): return x_test * self.coef + self.intercept",1,not_existent,"class SimpleLinearRegression:     coef = 0     intercept = 0     rsquared = 0     def fit(self, x_train, y_train):         sum_of_x = sum(x_train)         sum_of_y = sum(y_train)         sum_of_x2 = np.sum(np.square(x_train))         sum_of_y2 = np.sum(np.square(y_train))         dotproduct = np.dot(x_train,y_train)         length = len(x_train)         dif_x = sum_of_x2 - sum_of_x * sum_of_x/length         dif_y = sum_of_y2 - sum_of_y * sum_of_y/length         numerator = length * dotproduct - sum_of_x * sum_of_y         denom = (length * sum_of_x2 - sum_of_x * sum_of_x) * (length * sum_of_y2 - (sum_of_y * sum_of_y))         co = dotproduct - sum_of_x * sum_of_y / length         self.rsquared = np.square(numerator / np.sqrt(denom))         self.intercept = sum_of_y / length - ((co / dif_x) * sum_of_x/length)         self.coef = co / dif_x     def predict(self,x_test):         return x_test * self.coef + self.intercept         ",0
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""chicago_crime"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref)",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""chicago_crime"" dataset  dataset_ref = client.dataset(""chicago_crime"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)",0
df['CHAS'].value_counts(dropna=False),0,execute_result,df['CHAS'].value_counts(dropna=False),1
"ASSIGN = X[:,:13] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt = X[:,:13]   regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[1,3,5,7,8,9,10,11,12]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11,12]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[1,3,5,7,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[3,5,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,execute_result,"X_opt =X[:,[3,5,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
"ASSIGN =X[:,[0,1,4,10,14]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",0,error,"X_opt =X[:,[0,1,4,10,14]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",1
for i in range(7): display(HTML('<h2>Cluster'+str(i)+'<path>')) cc1[cc1.cluster == i].describe(),0,display_data,"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T  for i in range(7):      display(HTML('<h2>Cluster'+str(i)+'</h2>'))      cc1[cc1.cluster == i].describe()",1
for i in Ktrain_cat1: encode(Ktrain[i]) for i in Ktest_cat1: encode(Ktest[i]),1,not_existent,for i in Ktrain_cat1:      encode(Ktrain[i])  for i in Ktest_cat1:      encode(Ktest[i]),0
for i in Ktrain_cat2: encode(Ktrain[i]) for i in Ktest_cat2: encode(Ktest[i]),1,not_existent,for i in Ktrain_cat2:      encode(Ktrain[i])  for i in Ktest_cat2:      encode(Ktest[i]),0
SETUP del Ktest_identity del Ktest_transaction del Ktrain_identity del Ktrain_transaction del Ktrain_cat1 del Ktest_cat1 del Ktrain_cat2 del Ktest_cat2 gc.collect(),0,execute_result,import gc  del Ktest_identity  del Ktest_transaction  del Ktrain_identity  del Ktrain_transaction  del Ktrain_cat1  del Ktest_cat1  del Ktrain_cat2  del Ktest_cat2  gc.collect(),1
"''' ASSIGN=StandardScaler().fit_transform(X) ASSIGN=PCA().fit(X_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() ASSIGN=StandardScaler().fit_transform(X_Ktest) ASSIGN=PCA().fit(X_Ktest_fit) plt.plot(np.cumsum(ASSIGN.explained_variance_ratio_)) plt.title('All columns included', color='gray') plt.xlabel(""Number of Component"", color='green') plt.ylabel(""Cumulative Variance Ratio"", color='green') plt.grid(color='gray', linestyle='-', linewidth=0.3) plt.show() '''",1,execute_result,"'''  X_fit=StandardScaler().fit_transform(X)  X_pca=PCA().fit(X_fit)  plt.plot(np.cumsum(X_pca.explained_variance_ratio_))  plt.title('All columns included', color='gray')  plt.xlabel(""Number of Component"", color='green')  plt.ylabel(""Cumulative Variance Ratio"", color='green')  plt.grid(color='gray', linestyle='-', linewidth=0.3)  plt.show()    X_Ktest_fit=StandardScaler().fit_transform(X_Ktest)  X_Ktest_pca=PCA().fit(X_Ktest_fit)  plt.plot(np.cumsum(X_Ktest_pca.explained_variance_ratio_))  plt.title('All columns included', color='gray')  plt.xlabel(""Number of Component"", color='green')  plt.ylabel(""Cumulative Variance Ratio"", color='green')  plt.grid(color='gray', linestyle='-', linewidth=0.3)  plt.show()  '''",0
"get_split_result(""..path"", test, 1e-6)",1,execute_result,"#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3  get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6)",0
