content,tag,output_type,original_content,y_pred
"ASSIGN = plt.subplots(1, 1, figsize=(8, 16)) xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)",0,execute_result,"fig, ax = plt.subplots(1, 1, figsize=(8, 16))  xgb.plot_importance(model, max_num_features=50, height=0.5, ax=ax)",1
SETUP ASSIGN = tree.DecisionTreeClassifier(),1,not_existent,from sklearn import tree model = tree.DecisionTreeClassifier(),0
"model.score(inputs_n,target)",0,not_existent,"model.score(inputs_n,target)",1
"SETUP ASSIGN = PolynomialFeatures(len(X)) ASSIGN = poly.fit_transform(X_shaped) ASSIGN = LinearRegression() ASSIGN.fit(ASSIGN, y_shaped)",1,execute_result,"from sklearn.preprocessing import PolynomialFeatures   from sklearn.linear_model import LinearRegression      poly = PolynomialFeatures(len(X))  X_poly = poly.fit_transform(X_shaped)      # fit the transformed features to Linear Regression  poly_model = LinearRegression()  poly_model.fit(X_poly, y_shaped) ",0
"ASSIGN = target_cols[0] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[0] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )",0
"ASSIGN = target_cols[1] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[1] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )",0
"ASSIGN = target_cols[2] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[2] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )",0
"ASSIGN = target_cols[3] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[3] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )",0
"ASSIGN = target_cols[4] ASSIGN = get_train_data(target) ASSIGN = setup( ASSIGN = train_df, ASSIGN = ASSIGN, ASSIGN=0.8, ASSIGN = 'mean', ASSIGN = True ) compare_models( ASSIGN = blacklist_models, ASSIGN = 10, ASSIGN = 'MAE', ASSIGN = True )",1,not_existent,"target = target_cols[4] train_df = get_train_data(target)  setup_reg = setup(     data = train_df,     target = target,     train_size=0.8,     numeric_imputation = 'mean',     silent = True )  compare_models(     blacklist = blacklist_models,     fold = 10,     sort = 'MAE',     turbo = True )",0
"VALIDATION ASSIGN = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score } dict1",0,execute_result,"dict1 = {""Logistic Regression"" : lr_score,""Random Forest"" : rf_score,""K-Nearest Neighbour"" : knn_score ,""Naive Bayes"": nb_score ,""K-Nearest Neighbour"" : knn_score }  dict1",1
"VALIDATION def count_unique_values(df) : ASSIGN = list(df.columns) print('Count unique values :') for i in ASSIGN : ASSIGN = len(df[i].unique()) print(i,':',ASSIGN) def check_missing_values(df) : ASSIGN = len(df) ASSIGN = list(df.columns) ASSIGN = [] ASSIGN = [] print('Variable with missing values :') for i in ASSIGN : ASSIGN = np.sum(df[i].isna()) ASSIGN = round(count*100path, 2) if ASSIGN > 0 : print(i,':',ASSIGN,'path',ASSIGN,'%') ASSIGN.append(i) ASSIGN.append(ASSIGN) return missing_var, missing_count def stepwise_selection(X, y, ASSIGN=[], ASSIGN=0.01, ASSIGN = 0.05, ASSIGN=True): ASSIGN = list(initial_list) while True: ASSIGN=False ASSIGN = list(set(X.columns)-set(included)) ASSIGN = pd.Series(index=excluded) for new_column in ASSIGN: ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN[new_column] = ASSIGN.pvalues[new_column] ASSIGN = new_pval.min() if ASSIGN < ASSIGN: ASSIGN = new_pval.argmin() ASSIGN.append(ASSIGN) ASSIGN=True if ASSIGN: print('Add {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) ASSIGN = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included])) ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit() ASSIGN = model.ASSIGN.iloc[1:] ASSIGN = pvalues.max() if ASSIGN > ASSIGN: ASSIGN=True ASSIGN = pvalues.argmax() ASSIGN.remove(ASSIGN) if ASSIGN: print('Drop {:30} with p-value {:.6}'.format(ASSIGN, ASSIGN)) if not ASSIGN: break return included def dataset_ready(x_train, y_train) : ASSIGN = pd.get_dummies(x_train) ASSIGN = [1]*len(X) ASSIGN['gdp_pop'] = np.log(ASSIGN['gdp_per_capita']*ASSIGN['population']) ASSIGN = ['gdp_per_capita','population'] for i in ASSIGN : ASSIGN = np.log(ASSIGN) ASSIGN = pd.Series(X.columns) ASSIGN = list(X.filter(like='continent').columns) for i in ASSIGN : ASSIGN = i+'_gdp' ASSIGN = ASSIGN*ASSIGN for i in ASSIGN : ASSIGN = i+'_population' ASSIGN = ASSIGN*ASSIGN ASSIGN = y_train return X,Y",0,not_existent,"# Function used in this notebook def count_unique_values(df) :     var = list(df.columns)     print('Count unique values :')          for i in var :         count = len(df[i].unique())         print(i,':',count)  def check_missing_values(df) :     n = len(df)     var = list(df.columns)     missing_var = []     missing_count = []     print('Variable with missing values :')          for i in var :         count = np.sum(df[i].isna())         count_percentage = round(count*100/n, 2)         if count > 0 :             print(i,':',count,'//',count_percentage,'%')             missing_var.append(i)             missing_count.append(count_percentage)          return missing_var, missing_count  def stepwise_selection(X, y,                         initial_list=[],                         threshold_in=0.01,                         threshold_out = 0.05,                         verbose=True):       included = list(initial_list)     while True:         changed=False         # forward step         excluded = list(set(X.columns)-set(included))         new_pval = pd.Series(index=excluded)         for new_column in excluded:             model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))                                 ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()             new_pval[new_column] = model.pvalues[new_column]         best_pval = new_pval.min()         if best_pval < threshold_in:             best_feature = new_pval.argmin()             included.append(best_feature)             changed=True             if verbose:                 print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))          # backward step         model = sm.genmod.GLM(y, sm.add_constant(pd.DataFrame(X[included]))                             ,family=sm.genmod.families.Gamma(link=sm.genmod.families.links.log)).fit()         # use all coefs except intercept         pvalues = model.pvalues.iloc[1:]         worst_pval = pvalues.max() # null if pvalues is empty         if worst_pval > threshold_out:             changed=True             worst_feature = pvalues.argmax()             included.remove(worst_feature)             if verbose:                 print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))         if not changed:             break     return included  def dataset_ready(x_train, y_train) :     # Make dummy variable for categorical variable     X = pd.get_dummies(x_train)      # Make Intercept     X['Intercept'] = [1]*len(X)      # Make interaction between 'gdp_per_capita' and 'population'     X['gdp_pop'] = np.log(X['gdp_per_capita']*X['population'])      # Scale continuous variable with log function     cont_var = ['gdp_per_capita','population']     for i in cont_var :         X[i] = np.log(X[i])      # Make interaction between 'continent' and 'gdp'     col = pd.Series(X.columns)     var1 = list(X.filter(like='continent').columns)     for i in var1 :         string = i+'_gdp'         X[string] = X[i]*X['gdp_per_capita']         # Make interaction between 'continent' and 'population'     for i in var1 :         string = i+'_population'         X[string] = X[i]*X['population']        # Target variable     Y = y_train          return X,Y  # I use stepwise algorith from this link https://datascience.stackexchange.com/questions/24405/how-to-do-stepwise-regression-using-sklearn",1
"SETUP ASSIGN = ImageDataGenerator(rescale=1path) ASSIGN = ImageDataGenerator(rescale=1path) ASSIGN = train_datagen.flow_from_directory(train_dir, ASSIGN=20, ASSIGN='binary', ASSIGN=(150,150)) ASSIGN = val_datagen.flow_from_directory(val_dir, ASSIGN=20, ASSIGN='binary', ASSIGN=(150,150))",1,not_existent,"### Preprocess the image from keras.preprocessing.image import ImageDataGenerator  # Define generator train_datagen = ImageDataGenerator(rescale=1/255) val_datagen = ImageDataGenerator(rescale=1/255)  # Define flow for train gen train_gen = train_datagen.flow_from_directory(train_dir,                                               batch_size=20,                                               class_mode='binary',                                               target_size=(150,150))  # Define flow for validation gen val_gen = val_datagen.flow_from_directory(val_dir,                                           batch_size=20,                                           class_mode='binary',                                           target_size=(150,150))",0
"SETUP SETUP ASSIGN = ImageDataGenerator(rescale = 1.path, ASSIGN = 40, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = 0.2, ASSIGN = True) ASSIGN = train_datagen.flow_from_directory(TRAIN_DIR, ASSIGN = BATCH_SIZE, ASSIGN = 'binary', ASSIGN = INPUT_SIZE) ASSIGN = ImageDataGenerator(rescale = 1path) ASSIGN = val_datagen.flow_from_directory(VALID_DIR, ASSIGN = BATCH_SIZE, ASSIGN = 'binary', ASSIGN = INPUT_SIZE)",1,not_existent,"### Set the image generator from tensorflow.keras.preprocessing.image import ImageDataGenerator TRAIN_DIR = '/training' VALID_DIR = '/validating'  # Make image generator for training dataset train_datagen = ImageDataGenerator(rescale = 1./255.,                                    rotation_range = 40,                                    width_shift_range = 0.2,                                    height_shift_range = 0.2,                                    shear_range = 0.2,                                    zoom_range = 0.2,                                    horizontal_flip = True)  train_gen = train_datagen.flow_from_directory(TRAIN_DIR,                                               batch_size = BATCH_SIZE,                                               class_mode = 'binary',                                                target_size = INPUT_SIZE)    # Make image generator for validation dataset val_datagen = ImageDataGenerator(rescale = 1/255. )  val_gen =  val_datagen.flow_from_directory(VALID_DIR,                                             batch_size  = BATCH_SIZE,                                             class_mode  = 'binary',                                              target_size = INPUT_SIZE)",0
ASSIGN = model.layers[0] ASSIGN = e.get_weights()[0],0,not_existent,### Get the embedding weight for visualization e = model.layers[0] weights = e.get_weights()[0],1
"ASSIGN = [] for i in range(1,25): ASSIGN = KNeighborsClassifier(n_neighbors = i) ASSIGN = cross_val_score(knn, Xtrain, Ytrain, cv=10) ASSIGN.append(ASSIGN.mean())",1,not_existent,"scores_array = [] for i in range(1,25):     knn = KNeighborsClassifier(n_neighbors = i)     scores = cross_val_score(knn, Xtrain, Ytrain, cv=10)     scores_array.append(scores.mean())      ",0
ASSIGN = KNeighborsRegressor(38),1,not_existent,knn = KNeighborsRegressor(38),0
ASSIGN = GaussianNB(),1,not_existent,NB = GaussianNB() ,0
ASSIGN = deepCNN(),1,not_existent,CNNmodel = deepCNN(),0
ASSIGN = KNeighborsClassifier(n_neighbors=10),1,not_existent,knn = KNeighborsClassifier(n_neighbors=10),0
"ASSIGN = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)",0,not_existent,"scores = cross_val_score(knn, Xtrain_h, Ytrain_h, cv=10)",1
"SETUP ASSIGN = ColumnTransformer([ ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])], ASSIGN = StandardScaler()) ASSIGN = RidgeCV(cv=5) ASSIGN = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True)",1,not_existent,"from sklearn.preprocessing import OneHotEncoder, StandardScaler  from sklearn.linear_model import RidgeCV  from sklearn.pipeline import Pipeline  from sklearn.compose import ColumnTransformer      col_transformer = ColumnTransformer([      ('MAC_CODE', OneHotEncoder(dtype='int'),['MAC_CODE'])],      remainder = StandardScaler())    reg = RidgeCV(cv=5)    pipe = Pipeline([('col_transformer', col_transformer),('reg', reg)], verbose=True) ",0
"ASSIGN = Pipeline([(""bow no func"", CountVectorizer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_pipeline = Pipeline([(""bow no func"", CountVectorizer()),                         (""NB_classifier"", MultinomialNB())])",0
"ASSIGN = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_func_pipeline = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)),                              (""NB_classifier"", MultinomialNB())])",0
"ASSIGN = Pipeline([(""bow no func"", CountVectorizer()), (""tfidf"", TfidfTransformer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_tfidf_pipeline = Pipeline([(""bow no func"", CountVectorizer()),                                (""tfidf"", TfidfTransformer()),                                (""NB_classifier"", MultinomialNB())])",0
"ASSIGN = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)), (""tfidf"", TfidfTransformer()), (""NB_classifier"", MultinomialNB())])",1,not_existent,"NB_func_tfidf_pipeline = Pipeline([(""bow with func"", CountVectorizer(analyzer=text_process)),                                (""tfidf"", TfidfTransformer()),                                (""NB_classifier"", MultinomialNB())])",0
"ASSIGN = { ""KNeighborsClassifier"": KNeighborsClassifier(), ""RandomForest"": RandomForestClassifier(), ""SVM"": SVC(), } ASSIGN = { ""KNeighborsClassifier"": { ""n_neighbors"": randint(low=1, high=30), }, ""RandomForest"": { ""n_estimators"": randint(low=1, high=200), ""max_features"": randint(low=1, high=8), }, ""SVM"": { ""kernel"": [""linear"", ""rbf""], ""C"": reciprocal(0.1, 200000), ""gamma"": expon(scale=1.0), } }",1,not_existent,"models = {      ""KNeighborsClassifier"": KNeighborsClassifier(),      ""RandomForest"": RandomForestClassifier(),      ""SVM"": SVC(),  }    randomized_params = {      ""KNeighborsClassifier"": {          ""n_neighbors"": randint(low=1, high=30),      },      ""RandomForest"": {          ""n_estimators"": randint(low=1, high=200),          ""max_features"": randint(low=1, high=8),      },      ""SVM"": {          ""kernel"": [""linear"", ""rbf""],          ""C"": reciprocal(0.1, 200000),          ""gamma"": expon(scale=1.0),      }  }",0
"sns.set() sns.distplot(numLT_train_df[LT_columns[0]], fit=norm); ASSIGN = plt.figure() ASSIGN = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)",1,not_existent,"# Can also look at normal prob plot #histogram and normal probability plot sns.set() sns.distplot(numLT_train_df[LT_columns[0]], fit=norm); fig = plt.figure() res = stats.probplot(numLT_train_df[LT_columns[0]], plot=plt)",0
"VALIDATION ASSIGN = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 5) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = ridge.predict(X_trainData) ASSIGN = ridge.predict(X_testData) sns.set() plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(ridge.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()",1,not_existent,"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,                            alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,                           alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],                  cv = 5) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Ridge RMSE on Training set :"", rmse_cv_train(ridge).mean()) y_train_rdg = ridge.predict(X_trainData) y_test_rdg = ridge.predict(X_testData) # Plot residuals sns.set() plt.scatter(y_train_rdg, y_train_rdg - Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_rdg, Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(ridge.coef_, index = X_trainData.columns) print(""Ridge picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()",0
"VALIDATION ASSIGN = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, ASSIGN * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = lasso.predict(X_trainData) ASSIGN = lasso.predict(X_testData) plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(lasso.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()",1,not_existent,"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8,                            alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05,                            alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35,                            alpha * 1.4],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Lasso RMSE on Training set :"", rmse_cv_train(lasso).mean()) y_train_las = lasso.predict(X_trainData) y_test_las = lasso.predict(X_testData)  # Plot residuals plt.scatter(y_train_las, y_train_las - Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_las, Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(lasso.coef_, index = X_trainData.columns) print(""Lasso picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()",0
"SETUP ASSIGN = '..path' ASSIGN = '..path' ASSIGN = 150 ASSIGN = 150 ASSIGN = 100 ASSIGN = 32 ASSIGN = 5736 ASSIGN = 2460 ASSIGN = ImageDataGenerator(rescale=1. path, ASSIGN=40, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=0.2, ASSIGN=True, ASSIGN='nearest') ASSIGN = ImageDataGenerator(rescale=1. path) ASSIGN = train_datagen.flow_from_directory(train_data_path, ASSIGN=(img_rows, img_cols), ASSIGN=ASSIGN, ASSIGN='categorical') ASSIGN = test_datagen.flow_from_directory(test_data_path, ASSIGN=(img_rows, img_cols), ASSIGN=ASSIGN, ASSIGN='categorical')",1,stream,"import numpy as np  from keras import backend as K  from keras.models import Sequential  from keras.layers.core import Dense, Dropout, Activation, Flatten  from keras.layers.convolutional import Convolution2D, MaxPooling2D  from keras.preprocessing.image import ImageDataGenerator  from sklearn.metrics import classification_report, confusion_matrix    #Start  train_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/train'  test_data_path = '../input/animal1209/animal_dataset_intermediate_new/train_split/val'  img_rows = 150  img_cols = 150  epochs = 100  batch_size = 32  num_of_train_samples = 5736  num_of_test_samples = 2460    #Image Generator  train_datagen = ImageDataGenerator(rescale=1. / 255,                                     rotation_range=40,                                     width_shift_range=0.2,                                     height_shift_range=0.2,                                     shear_range=0.2,                                     zoom_range=0.2,                                     horizontal_flip=True,                                     fill_mode='nearest')    test_datagen = ImageDataGenerator(rescale=1. / 255)    train_generator = train_datagen.flow_from_directory(train_data_path,                                                      target_size=(img_rows, img_cols),                                                      batch_size=batch_size,                                                      class_mode='categorical')    validation_generator = test_datagen.flow_from_directory(test_data_path,                                                          target_size=(img_rows, img_cols),                                                          batch_size=batch_size,                                                          class_mode='categorical') ",0
"ASSIGN = xgb.XGBClassifier( ASSIGN =0.1, ASSIGN=1000, ASSIGN=5, ASSIGN=1, ASSIGN=0, ASSIGN=0.8, ASSIGN=0.8, ASSIGN= 'binary:logistic', ASSIGN=ASSIGN, ASSIGN=1, ASSIGN=27)",1,not_existent,"#Let's do a little Gridsearch, Hyperparameter Tunning model3 = xgb.XGBClassifier(  learning_rate =0.1,  n_estimators=1000,  max_depth=5,  min_child_weight=1,  gamma=0,  subsample=0.8,  colsample_bytree=0.8,  objective= 'binary:logistic',  n_jobs=n_jobs,  scale_pos_weight=1,  seed=27)",0
"ASSIGN = plt.subplots(figsize=(23, 17)) plot_importance(model3, ax=ax)",0,not_existent,"fig, ax = plt.subplots(figsize=(23, 17)) plot_importance(model3, ax=ax)",1
VALIDATION train_model6,0,not_existent,train_model6,1
VALIDATION ASSIGN = sort(model3.feature_importances_) thresholds,0,not_existent,thresholds = sort(model3.feature_importances_) thresholds,1
ASSIGN = 8 ASSIGN = 0.01 ASSIGN = 0.1 ASSIGN = 1e-4 ASSIGN = torch.optim.Adam,1,not_existent,epochs = 8 max_lr = 0.01 grad_clip = 0.1 weight_decay = 1e-4 opt_func = torch.optim.Adam,0
ASSIGN=KNeighborsClassifier(n_neighbors=7),1,not_existent,knn=KNeighborsClassifier(n_neighbors=7),0
"VALIDATION ASSIGN = accuracy_score(y_train,y_pred_train) print(.format(ASSIGN))",0,stream,"#Check accuracy of our model with train set  predict_train = accuracy_score(y_train,y_pred_train) print(""Accuracy of our model on train set :: {}"".format(predict_train))",1
"ASSIGN = ReduceLROnPlateau(monitor='val_acc', ASSIGN=3, ASSIGN=1, ASSIGN=0.7, ASSIGN=0.00000000001) ASSIGN = EarlyStopping(patience=2)",1,not_existent,"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',                                              patience=3,                                              verbose=1,                                              factor=0.7,                                              min_lr=0.00000000001) early_stopping_monitor = EarlyStopping(patience=2)",0
"plot_model(lr,""confusion_matrix"")",0,display_data,"plot_model(lr,""confusion_matrix"")",1
evaluate_model(lr),0,display_data,evaluate_model(lr),1
"ASSIGN = predict_model(lr, data = X_test)",0,not_existent,"lr_pred = predict_model(lr, data = X_test) #new_data is pd dataframe",1
"SETUP MLPClassifierModel = MLPClassifier(activation='logistic', ASSIGN='lbfgs', ASSIGN='adaptive', ASSIGN= False, ASSIGN=0.1 ,hidden_layer_sizes=(52, 3),random_state=33) MLPClassifierModel.fit(X_train, y_train) ASSIGN = MLPClassifierModel.predict(X_test) MLPClassifierModel.fit(X, y) ASSIGN= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])",1,stream,"from sklearn.neural_network import MLPClassifier MLPClassifierModel = MLPClassifier(activation='logistic', # can be also identity , tanh,logistic , relu                                    solver='lbfgs',  # can be lbfgs also sgd , adam                                    learning_rate='adaptive', # can be constant also invscaling , adaptive                                    early_stopping= False,                                    alpha=0.1 ,hidden_layer_sizes=(52, 3),random_state=33) MLPClassifierModel.fit(X_train, y_train)  MLPClassifier_y_pred = MLPClassifierModel.predict(X_test) MLPClassifierModel.fit(X, y) MLPClassifier_y_pred= MLPClassifierModel.predict(test[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']])",0
"SETUP def giveMeFeatures(image): ASSIGN = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2') return res",0,not_existent,"from skimage.feature import hog  def giveMeFeatures(image):      res = hog(image, orientations=8, pixels_per_cell=(16,16),cells_per_block=(4, 4),block_norm= 'L2')  #     =  hog(img, orientations=9, pixels_per_cell=(6, 6),cells_per_block=(2, 2),block_norm='L1', visualize=False,transform_sqrt=False,feature_vector=True)      return res           ",1
ASSIGN = models.Sequential(),1,not_existent,model = models.Sequential(),0
"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001): ASSIGN = [] ASSIGN = [] for iteration in range(max_iteration): ASSIGN = np.dot(X,w) ASSIGN = MSE(y,predicted_y) ASSIGN = round(ASSIGN,9) ASSIGN.append(w) ASSIGN.append(ASSIGN) ASSIGN = -(2path[0])* X.dot(loss).sum() ASSIGN = ASSIGN + lr * derivative return w_history,loss_hostory",1,not_existent,"def gradient_descent(X,y,w,max_iteration=1000,lr=0.00001):      w_history  = []     loss_hostory = []     for iteration in range(max_iteration):         predicted_y = np.dot(X,w)         loss =  MSE(y,predicted_y)         loss = round(loss,9)         w_history.append(w)         loss_hostory.append(loss)         derivative = -(2/y.shape[0])* X.dot(loss).sum()         w = w + lr * derivative     return w_history,loss_hostory ",0
"VALIDATION def equales(list1,list2): if(len(list1)!=len(list2)): return False else: for i in range(len(list1)): if(list1[i]!=list2[i]): return False return True def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000): for iteration in range(max_iterations): ASSIGN = w for i in range(w.shape[0]): if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0): ASSIGN=ASSIGN- learning_rate * X[i] elif(np.dot(np.dot(X[i],ASSIGN),y[i]) < 0 and y[i]>0): ASSIGN=ASSIGN+ learning_rate * X[i] if(equales(ASSIGN,ASSIGN)): print('ASSIGN == ASSIGN in ',iteration) break return w",1,not_existent,"def equales(list1,list2):     if(len(list1)!=len(list2)):         return False     else:          for i in range(len(list1)):             if(list1[i]!=list2[i]):                 return False     return True def perceptron(X,y,w,learning_rate = 0.0001,max_iterations= 1000):     for iteration in range(max_iterations):         prev_w = w         for i in range(w.shape[0]):             if(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]<0):                 w=w- learning_rate * X[i]                              elif(np.dot(np.dot(X[i],w),y[i]) < 0 and y[i]>0):                 w=w+ learning_rate * X[i]         if(equales(prev_w,w)):             print('prev_w == w in ',iteration)             break                       return w",0
"ASSIGN = [] ASSIGN =1000 ASSIGN = 0.1 for _ in range(ASSIGN): ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=ASSIGN*ASSIGN",1,not_existent,"history = [] #loss history  numberOfRounds =1000 # max number of times the optimization algorithm will run learningRate = 0.1 for _ in range(numberOfRounds):     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))      for j in range(k):         deltaTemp=0          #deltaTemp is the loss derivative , and we aggregate it from all n samples (in the simple form of gradient descent,         #and it works fine in case of offline training and smalle number of samples)          for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=learningRate*deltaTemp",0
"VALIDATION ASSIGN = [] ASSIGN = 20 for iteration in range(ASSIGN): print('iteration: ',iteration) ASSIGN = np.dot(X,w) ASSIGN = [] for i in range(n): ASSIGN.append(softmax(ASSIGN[i])) ASSIGN = np.asarray(ASSIGN) ASSIGN.append(cross_entropy(y,ASSIGN)) for j in range(k): ASSIGN=0 for i in range(n): ASSIGN += np.dot(X.T,(y-ASSIGN)) ASSIGN = - deltaTemppath ASSIGN = np.asarray(ASSIGN) w-=0.1*ASSIGN",1,not_existent,"history = [] maxNumOfIterations = 20 for iteration in range(maxNumOfIterations):      print('iteration: ',iteration)     z = np.dot(X,w)     y_hat = []     for i in range(n):         y_hat.append(softmax(z[i]))     y_hat = np.asarray(y_hat)     history.append(cross_entropy(y,y_hat))     for j in range(k):         deltaTemp=0         for i in range(n):             deltaTemp += np.dot(X.T,(y-y_hat))         deltaTemp  = - deltaTemp/n         deltaTemp = np.asarray(deltaTemp)         w-=0.1*deltaTemp",0
"ASSIGN = 30 ASSIGN = PCA(n_components=n_components).fit(train.values) ASSIGN = pca.components_.reshape(n_components, 28, 28) ASSIGN = pca.components_",0,not_existent,"# Invoke SKlearn's PCA method  n_components = 30  pca = PCA(n_components=n_components).fit(train.values)    eigenvalues = pca.components_.reshape(n_components, 28, 28)    # Extracting the PCA components ( eignevalues )  #eigenvalues = pca.components_.reshape(n_components, 28, 28)  eigenvalues = pca.components_",1
"for countryindex in codiv_country_Restrictions_active.T: ASSIGN==""Japan"": ASSIGN=4500 else: ASSIGN=250 coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""restrictions"",ASSIGN=ASSIGN) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN = np.polyval(coefs_Country_prev, x_values) ASSIGN=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40) if ymaxchina>ASSIGN: ASSIGN=ymaxchina ASSIGN = plt.figure(figsize= (18, 10)) plt.scatter(x_tr_China, y_tr_China, s=10) plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"") ASSIGN!=""China"": plt.scatter(x_tr_Country, y_tr_Country, s=10) plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=""magenta"") plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"") PredictionName='%s-prediction'%(countryindex,) plt.plot(x_values, ASSIGN, label=PredictionName,c=""brown"",ls='dashed') plt.ylim(-5,ASSIGN) plt.ylabel(""Number"") plt.grid(True) ASSIGN = range(0,len(dfdate),10) plt.xticks(ticks = ASSIGN ,labels = dfdate[""date""], rotation = 75) ASSIGN!=""China"": ASSIGN='%s-prediction - Restrictions'%(countryindex,) plt.title(ASSIGN) else: plt.title(""China"") ASSIGN="""" ASSIGN==""Ireland"": ASSIGN=""Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?"" ASSIGN==""Japan"": ASSIGN=""Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher"" ASSIGN==""Canada"" or countryindex==""Netherlands"" or countryindex==""Peru"" or countryindex==""Colombia""or countryindex==""Portugal""or countryindex==""Sweden""or countryindex==""United Kingdom"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Norway"" or countryindex==""Poland"": ASSIGN="" The model wait for the peak to fit better,they take long time to reach it"" plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15) plt.legend() plt.show()",1,display_data,"for countryindex in codiv_country_Restrictions_active.T:  #codiv_country_quarantine_active.T:      # China is only in quarantine group      if countryindex==""Japan"":          trigger=4500      else:          trigger=250      coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""restrictions"",trigger=trigger)      y_Country = np.polyval(coefs_Country, x_values)      y_Country_prev = np.polyval(coefs_Country_prev, x_values)      ymax=int(max(y_Country_prev[StartCountryIndex:lendataChina])*1.40)      #adjust the ymax limit      if ymaxchina>ymax:          ymax=ymaxchina      #      fig = plt.figure(figsize= (18, 10))      plt.scatter(x_tr_China, y_tr_China, s=10)      plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"")      if countryindex!=""China"":          plt.scatter(x_tr_Country, y_tr_Country, s=10)          plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=""magenta"")          plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"")          PredictionName='%s-prediction'%(countryindex,)          plt.plot(x_values, y_Country_prev, label=PredictionName,c=""brown"",ls='dashed')      plt.ylim(-5,ymax)      plt.ylabel(""Number"")      plt.grid(True)      tickvalues = range(0,len(dfdate),10)      plt.xticks(ticks = tickvalues ,labels = dfdate[""date""], rotation = 75)      if countryindex!=""China"":          Title='%s-prediction - Restrictions'%(countryindex,)          plt.title(Title)      else:          plt.title(""China"")      ##      Texte=""""      if countryindex==""Ireland"":          Texte=""Ireland had no clear data, the model need the next days data to fit better. Are the information trustfull?""      if countryindex==""Japan"":          Texte=""Japan tried long time to contain the epidemy, after it gone up. the start threshold should be higher""       if countryindex==""Canada"" or countryindex==""Netherlands"" or countryindex==""Peru"" or countryindex==""Colombia""or countryindex==""Portugal""or countryindex==""Sweden""or countryindex==""United Kingdom"":          Texte="" The model wait for the peak to fit better""           if countryindex==""Norway"" or countryindex==""Poland"":          Texte="" The model wait for the peak to fit better,they take long time to reach it""       plt.text(1, ymax-10000, Texte, fontsize=15)      ##      plt.legend()      plt.show()",0
"for countryindex in codiv_country_without_Restrictions_qurantine_active.T: ASSIGN==""Singapore"" or countryindex==""Qatar"" or countryindex==""Kuwait"": ASSIGN=2000 else: ASSIGN=300 coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""NoQuaNoRES"",ASSIGN=ASSIGN) ASSIGN = np.polyval(coefs_Country, x_values) ASSIGN = np.polyval(coefs_Country_prev, x_values) ASSIGN==""US"": ASSIGN=1500000 else: ASSIGN==""Brazil"": ASSIGN=180000 else: ASSIGN=60000 ASSIGN = plt.figure(figsize= (18, 10)) plt.scatter(x_tr_China, y_tr_China, s=10) plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"") ASSIGN!=""China"": plt.scatter(x_tr_Country, y_tr_Country, s=10) plt.plot(x_values[0:lendataChina], ASSIGN[0:lendataChina], label=countryindex,c=""magenta"") plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"") PredictionName='%s-prediction'%(countryindex,) plt.plot(x_values, ASSIGN, label=PredictionName,c=""brown"",ls='dashed') plt.ylim(-5,ASSIGN) plt.ylabel(""Number"") plt.grid(True) ASSIGN = range(0,len(dfdate),10) plt.xticks(ticks = ASSIGN ,labels = dfdate[""date""], rotation = 75) ASSIGN!=""China"": ASSIGN='%s-prediction - No-Restrictions No-quarantine'%(countryindex,) plt.title(ASSIGN) else: plt.title(""China"") ASSIGN="""" ASSIGN==""Armenia"" or countryindex==""Bahrain"" or countryindex==""Bangladesh"" or countryindex==""Belarus""or countryindex==""Brazil""or countryindex==""Chile""or countryindex==""United Kingdom""or countryindex==""US"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Ecuador"" or countryindex==""Indonesia"" or countryindex==""Kuwait"" or countryindex==""Mexico"" or countryindex==""Oman"" or countryindex==""Pakistan"" or countryindex==""Quatar"" or countryindex==""South Africa"": ASSIGN="" The model wait for the peak to fit better"" ASSIGN==""Dominican Republic"" or countryindex==""Ghana"": ASSIGN="" The model wait for more data to fit better"" plt.text(1, ASSIGN-10000, ASSIGN, fontsize=15) plt.legend() plt.show()",1,display_data,"for countryindex in codiv_country_without_Restrictions_qurantine_active.T:  #codiv_country_quarantine_active.T:      # china is only in quarntine group      if countryindex==""Singapore"" or countryindex==""Qatar"" or countryindex==""Kuwait"":          trigger=2000      else:          trigger=300      coefs_Country,x_tr_Country,y_tr_Country,coefs_Country_prev,x_tr_Country_prev,y_tr_Country_prev,StartCountryIndex=Polifq(countryindex,shift=1,Version=""NoQuaNoRES"",trigger=trigger)      y_Country = np.polyval(coefs_Country, x_values)      y_Country_prev = np.polyval(coefs_Country_prev, x_values)      # adjust the ymax graph limit      if countryindex==""US"":          ymax=1500000      else:          if countryindex==""Brazil"":              ymax=180000          else:              ymax=60000      fig = plt.figure(figsize= (18, 10))      plt.scatter(x_tr_China, y_tr_China, s=10)      plt.plot(x_values[0:lendataChina], y_China[0:lendataChina], label='China',c=""blue"")      if countryindex!=""China"":          plt.scatter(x_tr_Country, y_tr_Country, s=10)          plt.plot(x_values[0:lendataChina], y_Country[0:lendataChina], label=countryindex,c=""magenta"")          plt.scatter(x_tr_Country_prev,y_tr_Country_prev, s=10,c=""black"")          PredictionName='%s-prediction'%(countryindex,)          plt.plot(x_values, y_Country_prev, label=PredictionName,c=""brown"",ls='dashed')      plt.ylim(-5,ymax)      plt.ylabel(""Number"")      plt.grid(True)      tickvalues = range(0,len(dfdate),10)      plt.xticks(ticks = tickvalues ,labels = dfdate[""date""], rotation = 75)      if countryindex!=""China"":          Title='%s-prediction - No-Restrictions No-quarantine'%(countryindex,)          plt.title(Title)      else:          plt.title(""China"")      ##      Texte=""""      if countryindex==""Armenia"" or countryindex==""Bahrain"" or countryindex==""Bangladesh"" or countryindex==""Belarus""or countryindex==""Brazil""or countryindex==""Chile""or countryindex==""United Kingdom""or countryindex==""US"":          Texte="" The model wait for the peak to fit better""        if countryindex==""Ecuador"" or countryindex==""Indonesia"" or countryindex==""Kuwait"" or countryindex==""Mexico"" or countryindex==""Oman"" or countryindex==""Pakistan"" or countryindex==""Quatar"" or countryindex==""South Africa"":          Texte="" The model wait for the peak to fit better""      if countryindex==""Dominican Republic"" or countryindex==""Ghana"":          Texte="" The model wait for more data to fit better""      plt.text(1, ymax-10000, Texte, fontsize=15)      plt.legend()      plt.show()",0
"SETUP class GMClusters(GMM, ClusterMixin): def __init__(self, n_clusters=1, **kwargs): kwargs[""n_components""] = n_clusters kwargs['covariance_type'] = 'full' super(GMClusters, self).__init__(**kwargs) def fit(self, X): super(GMClusters, self).fit(X) self.labels_ = self.predict(X) return self ASSIGN = KElbow(GMClusters(), k=(2,12), force_model=True) ASSIGN.fit(X) ASSIGN.show()",1,execute_result,"from sklearn.base import ClusterMixin  from yellowbrick.cluster import KElbow    class GMClusters(GMM, ClusterMixin):        def __init__(self, n_clusters=1, **kwargs):          kwargs[""n_components""] = n_clusters          kwargs['covariance_type'] = 'full'          super(GMClusters, self).__init__(**kwargs)        def fit(self, X):          super(GMClusters, self).fit(X)          self.labels_ = self.predict(X)          return self     oz = KElbow(GMClusters(), k=(2,12), force_model=True)  oz.fit(X)  oz.show()",0
"ASSIGN=OrdinalEncoder() ASSIGN=KNN() def encode(data): '''function to encode non-null data and replace it in the original data''' ASSIGN = np.array(data.dropna()) ASSIGN = nonulls.reshape(-1,1) ASSIGN = encoder.fit_transform(impute_reshape) data.loc[data.notnull()] = np.squeeze(ASSIGN) return data",1,not_existent,"encoder=OrdinalEncoder()  imputer=KNN()    def encode(data):      '''function to encode non-null data and replace it in the original data'''      #retains only non-null values      nonulls = np.array(data.dropna())      #reshapes the data for encoding      impute_reshape = nonulls.reshape(-1,1)      #encode date      impute_ordinal = encoder.fit_transform(impute_reshape)      #Assign back encoded values to non-null values      data.loc[data.notnull()] = np.squeeze(impute_ordinal)      return data",0
"''' ASSIGN=[] for i in range(1,101): ASSIGN.append(""a""+str(i)) ASSIGN= pd.DataFrame(data=X_fit, columns= xlist) ASSIGN= pd.DataFrame(data=X_Ktest_fit, columns= xlist) ASSIGN.head() '''",1,execute_result,"'''  xlist=[]  for i in range(1,101):      xlist.append(""a""+str(i))    X_new= pd.DataFrame(data=X_fit, columns= xlist)  X_Ktest_new= pd.DataFrame(data=X_Ktest_fit, columns= xlist)  X_Ktest_new.head()  '''",0
"class Student: def __init__(self, id, shyness, attitude, cap, usageRate, year): self.id = id self.shyness = shyness ''' attitude towards drinking (normal distribution, mean is MeanAttitude and s.d 1) determines usagerate, whether student may drink alone and addiction ''' self.attitude = attitude ''' updated at the end of the school year if below a threshold, student can't continue next year (inSchool = 0) ''' self.cap = cap self.usageRate = usageRate self.inSchool = True self.year = year self.friends = [] self.host = [] self.guestList = [] self.whenAttend = [] def host_party(self, student_list): ASSIGN = self.whenAttend ASSIGN = (0.2 * self.shyness**2) - (len(attending))path ASSIGN = rd.random() if ASSIGN <= ASSIGN: ASSIGN = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat'] for a in ASSIGN: if a in ASSIGN: ASSIGN.remove(a) ASSIGN = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35]) self.host.append(ASSIGN) for others in student_list: if others.id in self.friends: others.willAttend(ASSIGN) if ASSIGN in others.whenAttend: self.guestList.append(others.id) def willAttend(self, ASSIGN): if ASSIGN not in self.whenAttend: ASSIGN = rd.random() ASSIGN = ['Sun','Sat'] if ASSIGN in ASSIGN: if ASSIGN <= 0.8: self.whenAttend.append(ASSIGN) else: if ASSIGN <= 0.4: self.whenAttend.append(ASSIGN) def partyDrink(self, peer_pressure): ASSIGN = (self.attitudepath) + (2*peer_pressure) - 0.1*(self.usageRatepath) ASSIGN = rd.random() if ASSIGN <= ASSIGN: self.usageRate += 1 self.experience() return 1 return 0 def drinkAlone(self): if self.attitude > 3: ASSIGN = self.attitudepath ASSIGN = rd.random() if ASSIGN <= ASSIGN: self.usageRate += 1 def experience(self): ASSIGN = rd.noramlvariate(meanExperience, stDevExperience) if ASSIGN < -6: ASSIGN = -6 elif ASSIGN > 3: ASSIGN = 3 ASSIGN = rd.random() if ASSIGN < probabilityofBust: ASSIGN -= 3 self.attitude += ASSIGN*0.1 def gradeExperience(self): if self.usageRate > usageRateGradeDrop: ASSIGN = -1*(self.usageRatepath) self.attitude += (ASSIGN * 0.2) def gradeUpdate(self): ASSIGN = 0.2path ASSIGN = rd.normalvariate(0.1,std_dev) if self.usageRate > usageRateGradeDrop: ASSIGN = self.usageRatepath(30*self.year) ASSIGN = 0.5path ASSIGN = rd.normalvariate(mean_drop,s_dev) ASSIGN += ASSIGN student.cap -= ASSIGN",0,not_existent,"### create Student Class class Student:    def __init__(self, id, shyness, attitude, cap, usageRate, year):     self.id = id      ##characteristics (integers)     #low score means more shy (normal distribution, mean 0 and s.d 1)     self.shyness = shyness     '''     attitude towards drinking      (normal distribution, mean is MeanAttitude and s.d 1)     determines usagerate, whether student may drink alone and addiction      '''     self.attitude = attitude     '''     updated at the end of the school year     if below a threshold, student can't continue next year (inSchool = 0)     '''     self.cap = cap     #number of times drinking (in the past 30 days?)>should update every 30 or just keep it for the whole sem?     self.usageRate = usageRate      self.inSchool = True         self.year = year      ##social life (lists)     self.friends = []     #list of days they are hosting a party in the next week, and a list of attendees     self.host = []     self.guestList = []     #list of days student will attend someone else's party     self.whenAttend = []     def host_party(self, student_list):        #determine if hosting that week,     attending = self.whenAttend          p_host = (0.2 * self.shyness**2) - (len(attending))/5     num = rd.random()     if num <= p_host:        #choose which day to host       week = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']       for a in attending:         if a in week:           week.remove(a)       #       day = rd.choices(week, weights = [0.05,0.05,0.05,0.05,0.1,0.35,0.35])       self.host.append(day)           #send out invites              for others in student_list:         #is there a better way to select the respective students based on their id?         #two nested for loops - might get hard over large student populations         if others.id in self.friends:           others.willAttend(day)            if day in others.whenAttend:             self.guestList.append(others.id)      #when student gets invited to a party   def willAttend(self, day):     if day not in self.whenAttend:       num = rd.random()       weekend = ['Sun','Sat']       if day in weekend:         if num <= 0.8:           self.whenAttend.append(day)       else:         if num <= 0.4:           self.whenAttend.append(day)           def partyDrink(self, peer_pressure):     # arbitrary coefficients      # limits - self.attitude = [2, 3]     #        - attitudeUse = [1,1]     #        - peer_pressure = [0, 1]     #        - usageRate = []     #        - usageRateUse = []     # take note of the limits     p_drink = (self.attitude/ attitudeUse) + (2*peer_pressure) - 0.1*(self.usageRate/usageRateUse)     num = rd.random()     if num <= p_drink:       self.usageRate += 1       self.experience()       return 1     return 0                 def drinkAlone(self):     if self.attitude > 3:       p_alone = self.attitude/15       num = rd.random()       if num <= p_alone:         self.usageRate += 1    #after a drink at the party     def experience(self):     num = rd.noramlvariate(meanExperience, stDevExperience)     if num < -6:       num = -6     elif num > 3:       num = 3      #if party gets busted     bust = rd.random()     if bust < probabilityofBust:       num -= 3      #update attitude towards drinking     # attitude update is arbitrary at 0.1     self.attitude += num*0.1 #    self.attitude += num*0.5         #every 3 weeks   def gradeExperience(self):     if self.usageRate > usageRateGradeDrop:       poor_exp = -1*(self.usageRate/15)       self.attitude += (poor_exp * 0.2)    #at the end of the semester   def gradeUpdate(self):     std_dev = 0.2/self.year     change = rd.normalvariate(0.1,std_dev)      #add on to grade drop due to drinking     if self.usageRate > usageRateGradeDrop:       mean_drop = self.usageRate/(30*self.year)       s_dev = 0.5/self.year       grade_drop = rd.normalvariate(mean_drop,s_dev)       change += grade_drop          student.cap -= change    ",1
