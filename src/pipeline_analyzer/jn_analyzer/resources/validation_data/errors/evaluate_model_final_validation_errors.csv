content,tag,output_type,original_content,y_pred
"CHECKPOINT print(,end=) print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))",0,stream,"print(""MAE:"",end="" "")  print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))",1
"CHECKPOINT print( % (accuracy_score(y_test, pred1) * 100)) print( % (accuracy_score(y_test, pred2) * 100))",0,not_existent,"print(""Accuracy for model 1: %.2f"" % (accuracy_score(y_test, pred1) * 100)) print(""Accuracy for model 2: %.2f"" % (accuracy_score(y_test, pred2) * 100))",1
"CHECKPOINT print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))",0,not_existent,"print('Model 3 XGboost Report %r' % (classification_report(y_test, pred3)))",1
"CHECKPOINT print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))",0,not_existent,"print('Model 5 XGboost Report %r' % (classification_report(y_test, pred5)))",1
"CHECKPOINT print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))",0,not_existent,"print('Model 4 XGboost Report %r' % (classification_report(y_test, pred4)))",1
"CHECKPOINT np.random.seed(13) def impact_coding(data, feature, target='y'): ''' In this implementation we get the values and the dictionary as two different steps. This is just because initially we were ignoring the dictionary as a result variable. In this implementation the KFolds use shuffling. If you want reproducibility the cv could be moved to a parameter. ''' ASSIGN = 20 ASSIGN = 10 ASSIGN = pd.Series() ASSIGN = data[target].mean() ASSIGN = KFold(n_splits=n_folds, shuffle=True) ASSIGN = pd.DataFrame() ASSIGN = 0 for infold, oof in ASSIGN.ASSIGN(data[feature]): ASSIGN = pd.Series() ASSIGN = KFold(n_splits=n_inner_folds, shuffle=True) ASSIGN = 0 ASSIGN = pd.DataFrame() ASSIGN = data.iloc[infold][target].mean() for infold_inner, oof_inner in ASSIGN.ASSIGN(data.iloc[infold]): ASSIGN = data.iloc[infold_inner].groupby(by=feature)[target].mean() ASSIGN = ASSIGN.append(data.iloc[infold].apply( lambda x: ASSIGN[x[feature]] if x[feature] in oof_mean.index else oof_default_inner_mean , axis=1)) ASSIGN = ASSIGN.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer') ASSIGN.fillna(value=ASSIGN, inplace=True) ASSIGN += 1 ASSIGN = ASSIGN.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer') ASSIGN.fillna(value=ASSIGN, inplace=True) ASSIGN += 1 ASSIGN = ASSIGN.append(data.iloc[oof].apply( lambda x: ASSIGN.loc[x[feature]].mean() if x[feature] in inner_oof_mean_cv.index else oof_default_mean , axis=1)) return ASSIGN, ASSIGN.mean(axis=1), ASSIGN ASSIGN = {} for f in categorical_features: print(.format(f)) train_data[""impact_encoded_{}"".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f) ASSIGN[f] = (impact_coding_mapping, default_coding) ASSIGN = impact_coding_map[f] test_data[""impact_encoded_{}"".format(f)] = test_data.apply(lambda x: mapping[x[f]] if x[f] in mapping else default_mean , axis=1)",0,stream,"# This way we have randomness and are able to reproduce the behaviour within this cell.  np.random.seed(13)    def impact_coding(data, feature, target='y'):      '''      In this implementation we get the values and the dictionary as two different steps.      This is just because initially we were ignoring the dictionary as a result variable.            In this implementation the KFolds use shuffling. If you want reproducibility the cv       could be moved to a parameter.      '''      n_folds = 20      n_inner_folds = 10      impact_coded = pd.Series()            oof_default_mean = data[target].mean() # Gobal mean to use by default (you could further tune this)      kf = KFold(n_splits=n_folds, shuffle=True)      oof_mean_cv = pd.DataFrame()      split = 0      for infold, oof in kf.split(data[feature]):              impact_coded_cv = pd.Series()              kf_inner = KFold(n_splits=n_inner_folds, shuffle=True)              inner_split = 0              inner_oof_mean_cv = pd.DataFrame()              oof_default_inner_mean = data.iloc[infold][target].mean()              for infold_inner, oof_inner in kf_inner.split(data.iloc[infold]):                  # The mean to apply to the inner oof split (a 1/n_folds % based on the rest)                  oof_mean = data.iloc[infold_inner].groupby(by=feature)[target].mean()                  impact_coded_cv = impact_coded_cv.append(data.iloc[infold].apply(                              lambda x: oof_mean[x[feature]]                                        if x[feature] in oof_mean.index                                        else oof_default_inner_mean                              , axis=1))                    # Also populate mapping (this has all group -> mean for all inner CV folds)                  inner_oof_mean_cv = inner_oof_mean_cv.join(pd.DataFrame(oof_mean), rsuffix=inner_split, how='outer')                  inner_oof_mean_cv.fillna(value=oof_default_inner_mean, inplace=True)                  inner_split += 1                # Also populate mapping              oof_mean_cv = oof_mean_cv.join(pd.DataFrame(inner_oof_mean_cv), rsuffix=split, how='outer')              oof_mean_cv.fillna(value=oof_default_mean, inplace=True)              split += 1                            impact_coded = impact_coded.append(data.iloc[oof].apply(                              lambda x: inner_oof_mean_cv.loc[x[feature]].mean()                                        if x[feature] in inner_oof_mean_cv.index                                        else oof_default_mean                              , axis=1))        return impact_coded, oof_mean_cv.mean(axis=1), oof_default_mean    # Apply the encoding to training and test data, and preserve the mapping  impact_coding_map = {}  for f in categorical_features:      print(""Impact coding for {}"".format(f))      train_data[""impact_encoded_{}"".format(f)], impact_coding_mapping, default_coding = impact_coding(train_data, f)      impact_coding_map[f] = (impact_coding_mapping, default_coding)      mapping, default_mean = impact_coding_map[f]      test_data[""impact_encoded_{}"".format(f)] = test_data.apply(lambda x: mapping[x[f]]                                                                           if x[f] in mapping                                                                           else default_mean                                                                 , axis=1)",1
"SETUP ASSIGN = Sequential() ASSIGN.add(Dense(units=1, input_shape=[1])) ASSIGN.compile(optimizer='sgd', loss='mean_squared_error') ASSIGN = np.array([0,1,2,3,4,5,6], dtype=float) ASSIGN = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float) ASSIGN.fit(ASSIGN, ASSIGN, epochs=500) ASSIGN.predict([7])",1,not_existent,"### House price from keras import Sequential from keras.layers import Dense  # Define model model = Sequential() model.add(Dense(units=1, input_shape=[1]))  # Compile model model.compile(optimizer='sgd', loss='mean_squared_error')  # Define data xs = np.array([0,1,2,3,4,5,6], dtype=float) ys = np.array([0.5,1,1.5,2,2.5,3,3.5], dtype=float)  # Train model model.fit(xs, ys, epochs=500)  # Predict model.predict([7])",0
"model.evaluate(val_img, val_label)",1,not_existent,"### Evaluate model model.evaluate(val_img, val_label)",0
"SETUP CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",0,stream,"from sklearn.ensemble import RandomForestClassifier  Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_equidistant)      cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",1
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_balanced) ASSIGN = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_balanced).score(X, y_balanced)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",0,stream,"Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_balanced)      cv_val_scores = cross_val_score(forest, X, y_balanced, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_balanced).score(X, y_balanced))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_balanced))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",1
"SETUP ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal)) ]) ASSIGN.fit(feature_2, y_equidistant) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"from matplotlib.colors import ListedColormap  from sklearn import neighbors, datasets  from sklearn.metrics import accuracy_score    # Create color maps  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])    feature1=pd.DataFrame(feature_2).loc[:][0]  feature2=pd.DataFrame(feature_2).loc[:][1]    weights='distance'  h = .02  # step size in the mesh  # we create an instance of Neighbours Classifier and fit the data.  # Create a k-NN pipeline  knn_pipe = Pipeline([      ('scaler', StandardScaler()),      ('knn', KNeighborsClassifier(n_neighbors=k_Optimal))      #, weights=weights))  ])  # Fit estimator  knn_pipe.fit(feature_2, y_equidistant)  # Plot the decision boundary. For that, we will assign a color to each  # point in the mesh [x_min, x_max]x[y_min, y_max].  feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1  feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1  xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))  Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])  Zo = Z.reshape(xx.shape)",0
"SETUP CHECKPOINT print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))",0,stream,"from sklearn.metrics import classification_report  print(classification_report(y_true=y_fea_2_te_equid, y_pred=knn_predict))",1
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])  #  feature1=pd.DataFrame(feature_2).loc[:][0]  feature2=pd.DataFrame(feature_2).loc[:][1]  #  weights='distance'  h = .02  # step size in the mesh  # Create a k-NN pipeline  knn_pipe = Pipeline([      ('scaler', StandardScaler()),      ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))  ])  # Fit estimator  knn_pipe.fit(feature_2, y_balanced)  # Plot the decision boundary. For that, we will assign a color to each  # point in the mesh [x_min, x_max]x[y_min, y_max].  feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1  feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1  xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))  Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])  Zo = Z.reshape(xx.shape)",0
"CHECKPOINT ASSIGN=range(100,2000,100) ASSIGN='accuracy' ASSIGN = [] ASSIGN = [] ASSIGN = [] ASSIGN = [] for Estimator in ASSIGN: ASSIGN = RandomForestClassifier(n_estimators=Estimator,random_state=0) ASSIGN.fit(X, y_equidistant) ASSIGN = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN.mean()) ASSIGN.append(ASSIGN.std()) ASSIGN.append(ASSIGN.fit(X, y_equidistant).score(X, y_equidistant)) ASSIGN=""depth %d, cv_val_scores_mean %f score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant)) print(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN) ASSIGN = np.array(ASSIGN)",0,stream,"Estimators=range(100,2000,100)  scoring='accuracy'  cv_val_scores_list = []  cv_val_scores_std = []  cv_val_scores_mean = []  accuracy_scores = []  for Estimator in Estimators:      forest = RandomForestClassifier(n_estimators=Estimator,random_state=0)      forest.fit(X, y_equidistant)      cv_val_scores = cross_val_score(forest, X, y_equidistant, cv=cv_strategy, scoring=scoring)      cv_val_scores_list.append(cv_val_scores)      cv_val_scores_mean.append(cv_val_scores.mean())      cv_val_scores_std.append(cv_val_scores.std())      accuracy_scores.append(forest.fit(X, y_equidistant).score(X, y_equidistant))      Text=""depth %d, cv_val_scores_mean %f  score %f""%(Estimator,cv_val_scores.mean(),forest.score(X, y_equidistant))      print(Text)  cv_val_scores_mean = np.array(cv_val_scores_mean)  cv_val_scores_std = np.array(cv_val_scores_std)  accuracy_scores = np.array(accuracy_scores)",1
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_equidistant) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])    feature1=pd.DataFrame(feature_2).loc[:][0]  feature2=pd.DataFrame(feature_2).loc[:][1]    weights='distance'  h = .02  # step size in the mesh  # we create an instance of Neighbours Classifier and fit the data.  # Create a k-NN pipeline  knn_pipe = Pipeline([      ('scaler', StandardScaler()),      ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))  ])  # Fit estimator  knn_pipe.fit(feature_2, y_equidistant)  # Plot the decision boundary. For that, we will assign a color to each  # point in the mesh [x_min, x_max]x[y_min, y_max].  feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1  feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1  xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))  Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])  Zo = Z.reshape(xx.shape)",0
"CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy) df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins"" df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant))) print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"knn_scores = cross_validate(knn_pipe, feature_2, y_equidistant, cv=cv_strategy)  df_resultat.at[df_resultat.index==6,""method""]=""knn equidistant bins""  df_resultat.at[df_resultat.index==6,""Cross-validation""]=np.mean(knn_scores['test_score'])  print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_equidistant)))  print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score']))) ",0
"ASSIGN = ListedColormap([' ASSIGN = ListedColormap([' ASSIGN=pd.DataFrame(feature_2).loc[:][0] ASSIGN=pd.DataFrame(feature_2).loc[:][1] ASSIGN='distance' ASSIGN = .02 ASSIGN = Pipeline([ ('scaler', StandardScaler()), ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, ASSIGN=ASSIGN)) ]) ASSIGN.fit(feature_2, y_balanced) feature1_min, feature1_max = ASSIGN.min() - 1, ASSIGN.max() + 1 feature2_min, feature2_max = ASSIGN.min() - 1, ASSIGN.max() + 1 ASSIGN = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h)) ASSIGN = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()]) ASSIGN = Z.reshape(xx.shape)",1,not_existent,"# Create color maps  cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF', '#EFFF00', '#B4FCFF', '#FFB4F5'])  cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF', '#FF00F7', '#00FFE6', '#FFA200'])    feature1=pd.DataFrame(feature_2).loc[:][0]  feature2=pd.DataFrame(feature_2).loc[:][1]    weights='distance'  h = .02  # step size in the mesh  # we create an instance of Neighbours Classifier and fit the data.  # Create a k-NN pipeline  knn_pipe = Pipeline([      ('scaler', StandardScaler()),      ('knn', KNeighborsClassifier(n_neighbors=k_Optimal, weights=weights))  ])  # Fit estimator  knn_pipe.fit(feature_2, y_balanced)  # Plot the decision boundary. For that, we will assign a color to each  # point in the mesh [x_min, x_max]x[y_min, y_max].  feature1_min, feature1_max = feature1.min() - 1, feature1.max() + 1  feature2_min, feature2_max = feature2.min() - 1, feature2.max() + 1  xx, yy = np.meshgrid(np.arange(feature1_min, feature1_max, h), np.arange(feature2_min, feature2_max, h))  Z = knn_pipe.predict(np.c_[xx.ravel(), yy.ravel()])  Zo = Z.reshape(xx.shape)",0
"CHECKPOINT ASSIGN = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy) df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins"" df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(ASSIGN['test_score']) print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced))) print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(ASSIGN['test_score'])))",1,stream,"  knn_scores = cross_validate(knn_pipe, feature_2, y_balanced, cv=cv_strategy)  df_resultat.at[df_resultat.index==7,""method""]=""knn balanced bins""  df_resultat.at[df_resultat.index==7,""Cross-validation""]=np.mean(knn_scores['test_score'])  print('knn by equidistant bins of element score {:.3f}'.format(knn_pipe.score(feature_2, y_balanced)))  print('knn by equidistant bins of element - mean test {:.3f}'.format(np.mean(knn_scores['test_score']))) ",0
"SETUP def Polifq(Country,Version=""restrictions"",shift=0,trigger=40): ASSIGN==""restrictions"": ASSIGN=codiv_country_Restrictions_active.T[Country] else: ASSIGN==""quarantine"": ASSIGN=codiv_country_qurantine_active.T[Country] else: ASSIGN=codiv_country_without_Restrictions_qurantine_active.T[Country] StartCountryIndex=0 ShiftIndex=0 ASSIGN=0 ASSIGN=0 ASSIGN=1.0 LargeFactor=1.0 ASSIGN = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std()) ASSIGN = df_codiv.loc[filter0] ASSIGN = ASSIGN.drop(outliers.index, axis=0) ASSIGN=max(df_codiv) ASSIGN = pd.DataFrame(columns = [Country,""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[Country] ASSIGN = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) ASSIGN=0 ASSIGN==1: ASSIGN=1 ASSIGN=max(codiv_country_qurantine_active.T[""China""]) for Index in range(0,len(ASSIGN)): if ASSIGN[Index]>trigger*(ASSIGN*1.0path) and ASSIGN==1 and shift==1: if ASSIGN>ASSIGN: ASSIGN=ASSIGN*(maxCountry*1.5path) StartCountryIndex=Index ASSIGN=0 if ASSIGN[Index]==ASSIGN: ASSIGN=Index LargeCountry=ASSIGN-StartCountryIndex ASSIGN=codiv_country_qurantine_active.T[""China""] ASSIGN = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std()) ASSIGN = df_codiv_China_ref.loc[filter0] ASSIGN = df_codiv_China_ref.drop(outliers.index, axis=0) ASSIGN=max(df_codiv_China_ref) ASSIGN=1 for Index in range(0,len(ASSIGN)): if ASSIGN[Index]>60 and ASSIGN==1 and shift==1: StartChinaIndex=Index ASSIGN=0 if ASSIGN[Index]==ASSIGN: ASSIGN=Index ASSIGN=maxCountry*1.0path LargeChina=ASSIGN-StartChinaIndex LargeFactor=LargeCountry*1.0path ShiftIndex=StartCountryIndex-StartChinaIndex ASSIGN=0.02 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) ASSIGN=0 for runNr in range(1,60,1): if ASSIGN<ASSIGN: LargeFactor=LargeFactor+ASSIGN ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: LargeFactor=LargeFactor+ASSIGN ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: ASSIGN=ASSIGN+step ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) if ASSIGN<ASSIGN: ASSIGN=ASSIGN+step ASSIGN=error1 ASSIGN = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int') for Index in range(0,len(ASSIGN)): ASSIGN = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""]) ASSIGN=ASSIGN.append(df_codiv1, ignore_index = True) ASSIGN=df_country[""Value""] ASSIGN=df_country[""China""] ASSIGN = rmse(y_tr1_China_model,y_tr1_Country) ASSIGN = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex",0,not_existent,"import scipy.stats as stats  import datetime  def Polifq(Country,Version=""restrictions"",shift=0,trigger=40):      # Version= works for restrictions, quarantin and without_Restrictions_qurantine.      if Version==""restrictions"":          df_codiv=codiv_country_Restrictions_active.T[Country]      else:          if Version==""quarantine"":              df_codiv=codiv_country_qurantine_active.T[Country]          else:              df_codiv=codiv_country_without_Restrictions_qurantine_active.T[Country]      #######################################      ######################## parameter setup      StartCountryIndex=0      ShiftIndex=0      x_tr1_China_model=0      y_tr1_China_model=0      highfactor=1.0      LargeFactor=1.0      ##############################################################################      ####################################### fitting the Country ##################      ######################## filter outlier for the Country      filter0 = np.abs(df_codiv - df_codiv.mean()) > (3 * df_codiv.std())      outliers = df_codiv.loc[filter0]      df_codiv = df_codiv.drop(outliers.index, axis=0)      ###########      # max actual value of the country      maxCountry=max(df_codiv)      #######################################      ########### Build the dataframe for the country      df_country = pd.DataFrame(columns = [Country,""Value"",""time""], dtype='int')      for Index in range(0,len(df_codiv)):          df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [Country,""Value"",""time""])          df_country=df_country.append(df_codiv1, ignore_index = True)      #######################################      #################### polyfit 10gr for the country value      x_tr1_Country=df_country[""Value""]      y_tr1_Country=df_country[Country]      #      # * Polyfit with degree 10      coefs_Country_poly10 = np.polyfit(x_tr1_Country, y_tr1_Country, deg=10) # Fit to train data      ##############################################################################      ##############################################################################      coefs_predict_poly10=0      #######################################      ############### Now shifting the china model to fitt to the country      if shift==1:          Start=1          ####### for the country          maxChina=max(codiv_country_qurantine_active.T[""China""])          for Index in range(0,len(df_codiv)):              #look for the start position of the country epidemy, it should be higher then the trigger value, it will be setup only by start of procedure ( STart=1)              if df_codiv[Index]>trigger*(maxCountry*1.0/maxChina) and Start==1 and shift==1:                                    if maxCountry>maxChina:                      trigger=trigger*(maxCountry*1.5/maxChina)                  StartCountryIndex=Index                  Start=0              # register the index of the coutry max position              if df_codiv[Index]==maxCountry:                  maxCountryIndex=Index          # deliver the actual distance between start and maximum for the country          LargeCountry=maxCountryIndex-StartCountryIndex          #################################          #######################################          ####### for china          df_codiv_China_ref=codiv_country_qurantine_active.T[""China""]          #######################################          ###### outiler          filter0 = np.abs(df_codiv_China_ref - df_codiv_China_ref.mean()) > (3 * df_codiv_China_ref.std())          outliers = df_codiv_China_ref.loc[filter0]          df_codiv = df_codiv_China_ref.drop(outliers.index, axis=0)          #######################################          ###### check for the start point by 40 infected          maxChina=max(df_codiv_China_ref)          Start=1          for Index in range(0,len(df_codiv_China_ref)):              if df_codiv_China_ref[Index]>60 and Start==1 and shift==1:                  StartChinaIndex=Index                  Start=0              if df_codiv_China_ref[Index]==maxChina:                  maxChinaIndex=Index          #######################################          ###### start value for the fitting          highfactor=maxCountry*1.0/maxChina          LargeChina=maxChinaIndex-StartChinaIndex          LargeFactor=LargeCountry*1.0/LargeChina          ShiftIndex=StartCountryIndex-StartChinaIndex          ##########OPTIMISATION           step=0.02          ############# the target          # Predictions with the current a,b values          ################# generate the function country is china          df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')          for Index in range(0,len(df_codiv)):              df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])              df_country=df_country.append(df_codiv1, ignore_index = True)          x_tr1_China_model=df_country[""Value""]          y_tr1_China_model=df_country[""China""]                error0 = rmse(y_tr1_China_model,y_tr1_Country)          error1=0          for runNr in range(1,60,1):              if error1<error0:                  LargeFactor=LargeFactor+step                  error0=error1                  ################# generate the function                  df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')                  for Index in range(0,len(df_codiv)):                      df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])                      df_country=df_country.append(df_codiv1, ignore_index = True)                  x_tr1_China_model=df_country[""Value""]                  y_tr1_China_model=df_country[""China""]                   error1 = rmse(y_tr1_China_model,y_tr1_Country)              if error1<error0:                  LargeFactor=LargeFactor+step                  error0=error1                  ################# generate the function                  df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')                  for Index in range(0,len(df_codiv)):                      df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])                      df_country=df_country.append(df_codiv1, ignore_index = True)                  x_tr1_China_model=df_country[""Value""]                  y_tr1_China_model=df_country[""China""]                   error1 = rmse(y_tr1_China_model,y_tr1_Country)              ##############              if error1<error0:                  highfactor=highfactor+step                  error0=error1                  ################# generate the function                  df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')                  for Index in range(0,len(df_codiv)):                      df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])                      df_country=df_country.append(df_codiv1, ignore_index = True)                  x_tr1_China_model=df_country[""Value""]                  y_tr1_China_model=df_country[""China""]                   error1 = rmse(y_tr1_China_model,y_tr1_Country)              #################              if error1<error0:                  highfactor=highfactor+step                  error0=error1                  ################# generate the function                  df_country = pd.DataFrame(columns = [""China"",""Value"",""time""], dtype='int')                  for Index in range(0,len(df_codiv)):                      df_codiv1 = pd.DataFrame ([[int(df_codiv[Index]*highfactor),int((Index)*LargeFactor)+ShiftIndex,str(pd.Timestamp(df_codiv.index[Index])+ pd.Timedelta(days=ShiftIndex))]], columns = [""China"",""Value"",""time""])                      df_country=df_country.append(df_codiv1, ignore_index = True)                  x_tr1_China_model=df_country[""Value""]                  y_tr1_China_model=df_country[""China""]                   error1 = rmse(y_tr1_China_model,y_tr1_Country)          #after having this fitting, we can fit the modified china data  on a 10grade polynome          #          # * Polyfit with degree 10          coefs_predict_poly10 = np.polyfit(x_tr1_China_model, y_tr1_China_model, deg=10) # Fit to train data      return coefs_Country_poly10,x_tr1_Country,y_tr1_Country,coefs_predict_poly10,x_tr1_China_model,y_tr1_China_model,StartCountryIndex",1
"CHECKPOINT ASSIGN = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 5) ASSIGN.fit(X_trainData, Y) ASSIGN = ridge.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = ridge.predict(X_trainData) ASSIGN = ridge.predict(X_testData) sns.set() plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""o"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(ridge.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()",0,not_existent,"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85,                            alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,                           alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4],                  cv = 5) ridge.fit(X_trainData, Y) alpha = ridge.alpha_ print(""Best alpha :"", alpha)  print(""Ridge RMSE on Training set :"", rmse_cv_train(ridge).mean()) y_train_rdg = ridge.predict(X_trainData) y_test_rdg = ridge.predict(X_testData) # Plot residuals sns.set() plt.scatter(y_train_rdg, y_train_rdg - Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, y_test_rdg - pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_rdg, Y, c = ""blue"", marker = ""o"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_rdg, pr_testData['SalePrice'], c = ""green"", marker = ""o"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Ridge regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(ridge.coef_, index = X_trainData.columns) print(""Ridge picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Ridge Model"") plt.show()",1
"CHECKPOINT ASSIGN = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print( + str(ASSIGN)) ASSIGN = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, ASSIGN * .85, ASSIGN * .9, ASSIGN * .95, ASSIGN, ASSIGN * 1.05, ASSIGN * 1.1, ASSIGN * 1.15, ASSIGN * 1.25, ASSIGN * 1.3, ASSIGN * 1.35, ASSIGN * 1.4], ASSIGN = 50000, cv = 10) ASSIGN.fit(X_trainData, Y) ASSIGN = lasso.alpha_ print(, ASSIGN) print(, rmse_cv_train(ASSIGN).mean()) ASSIGN = lasso.predict(X_trainData) ASSIGN = lasso.predict(X_testData) plt.scatter(ASSIGN, ASSIGN - Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, ASSIGN - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show() plt.scatter(ASSIGN, Y, c = ""blue"", marker = ""s"", label = ""Training data"", ASSIGN=0.7) plt.scatter(ASSIGN, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", ASSIGN=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show() ASSIGN = pd.Series(lasso.coef_, index = X_trainData.columns) print( + str(sum(ASSIGN != 0)) + + \ str(sum(ASSIGN == 0)) + "" features"") ASSIGN = pd.concat([coefs.sort_values().head(10), ASSIGN.sort_values().tail(10)]) ASSIGN.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()",0,not_existent,"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Try again for more precision with alphas centered around "" + str(alpha)) lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8,                            alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05,                            alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35,                            alpha * 1.4],                  max_iter = 50000, cv = 10) lasso.fit(X_trainData, Y) alpha = lasso.alpha_ print(""Best alpha :"", alpha)  print(""Lasso RMSE on Training set :"", rmse_cv_train(lasso).mean()) y_train_las = lasso.predict(X_trainData) y_test_las = lasso.predict(X_testData)  # Plot residuals plt.scatter(y_train_las, y_train_las - Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, y_test_las - pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Residuals"") plt.legend(loc = ""upper left"") plt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = ""red"") plt.show()  # Plot predictions plt.scatter(y_train_las, Y, c = ""blue"", marker = ""s"", label = ""Training data"", alpha=0.7) plt.scatter(y_test_las, pr_testData['SalePrice'], c = ""green"", marker = ""s"", label = ""Validation data"", alpha=0.7) plt.title(""Linear regression with Lasso regularization"") plt.xlabel(""Predicted values"") plt.ylabel(""Real values"") plt.legend(loc = ""upper left"") plt.plot([10.5, 13.5], [10.5, 13.5], c = ""red"") plt.show()  # Plot important coefficients coefs = pd.Series(lasso.coef_, index = X_trainData.columns) print(""Lasso picked "" + str(sum(coefs != 0)) + "" features and eliminated the other "" +  \       str(sum(coefs == 0)) + "" features"") imp_coefs = pd.concat([coefs.sort_values().head(10),                      coefs.sort_values().tail(10)]) imp_coefs.plot(kind = ""barh"") plt.title(""Coefficients in the Lasso Model"") plt.show()",1
evaluate_model(estimator=tuned_br_age),1,not_existent,evaluate_model(estimator=tuned_br_age),0
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_lasso_predict))  print(metrics.r2_score(y_true=y_train, y_pred=y_train_lasso_predict))",1
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_ridge_predict))  print(metrics.r2_score(y_true=y_train, y_pred=y_train_ridge_predict))",1
"CHECKPOINT print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict)) print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))",0,stream,"print(metrics.r2_score(y_true=y_test, y_pred=y_test_elasticnet_predict))  print(metrics.r2_score(y_true=y_train, y_pred=y_train_elasticnet_predict))",1
"CHECKPOINT ASSIGN = [evaluate(model, valid_dl)] history",1,error,"history = [evaluate(model, valid_dl)] history",0
"CHECKPOINT ASSIGN = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train'] print(classification_report(validation_generator.labels, y_pred, ASSIGN=ASSIGN))",0,stream,"  target_names = ['elefante_train', 'farfalla_train', 'mucca_train','pecora_train','scoiattolo_train']  print(classification_report(validation_generator.labels, y_pred, target_names=target_names))",1
"ASSIGN = {'undamaged': 0, 'repair': 1, 'replace': 2} data['operation_rank'] = data['operation'].apply(lambda x: ASSIGN[x]) def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold): ASSIGN = int(urr_score > repair_threshold) + int(urr_score > replace_threshold) return abs(ASSIGN - operation_rank) assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2) assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0) assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)",1,not_existent,"operation_ranks = {'undamaged': 0,                     'repair': 1,                     'replace': 2}    data['operation_rank'] = data['operation'].apply(lambda x: operation_ranks[x])    def mae_single_point(urr_score, operation_rank, repair_threshold, replace_threshold):      classified_outcome_rank = int(urr_score > repair_threshold) + int(urr_score > replace_threshold)        return abs(classified_outcome_rank - operation_rank)    assert(mae_single_point(0.9, 0, 0.4, 0.7) == 2)  assert(mae_single_point(0.5, 1, 0.4, 0.7) == 0)  assert(mae_single_point(0.5, 2, 0.4, 0.7) == 1)      ",0
"CHECKPOINT ASSIGN = data[(data['set']==2)][['urr_score', 'operation_rank']] def mae(thresholds): return mae_dataset(ASSIGN, thresholds[0], thresholds[1]) ASSIGN = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True) print(f'Best thresholds: {ASSIGN.x}') print(f'Best average mse: {ASSIGN.fun}')",1,stream,"# Use only the test set to evaluate the best thresholds    test_set = data[(data['set']==2)][['urr_score', 'operation_rank']]    def mae(thresholds):      return mae_dataset(test_set, thresholds[0], thresholds[1])    # Calculating mse is somewhat expensive at a couple of seconds a time, so use an optimizer and small number of iterations  # Takes about 2.5 minutes  opt = gp_minimize(mae, dimensions=[(0.0, 1.0, 'uniform'), (0.0, 1.0, 'uniform')], n_calls=50, verbose=True)    print(f'Best thresholds: {opt.x}')  print(f'Best average mse: {opt.fun}')",0
lasso_score.mean(),1,not_existent,lasso_score.mean(),0
ridge_score.mean(),1,not_existent,ridge_score.mean(),0
"model.score(inputs_n,target)",1,not_existent,"model.score(inputs_n,target)",0
"ASSIGN = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) model.compile(ASSIGN = ASSIGN , loss='sparse_categorical_crossentropy', ASSIGN=['sparse_categorical_accuracy']) ASSIGN = 37 ASSIGN = 20 ASSIGN = ImageDataGenerator( ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=False, ASSIGN=5, ASSIGN = 0.05, ASSIGN=0, ASSIGN=0, ASSIGN=False, ASSIGN=False) ASSIGN.fit(X_train) ASSIGN = model.fit_generator( ASSIGN.flow(X_train,y_train, ASSIGN=ASSIGN), ASSIGN = epoch, ASSIGN = (X_test,y_test), ASSIGN = 2, ASSIGN=X_train.shape[0] path, ASSIGN=[learning_rate_reduction] )",0,stream,"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0) model.compile(optimizer = optimizer , loss='sparse_categorical_crossentropy',             metrics=['sparse_categorical_accuracy']) epoch = 37 batch_size = 20  datagen = ImageDataGenerator(         featurewise_center=False,  # set input mean to 0 over the dataset         samplewise_center=False,  # set each sample mean to 0         featurewise_std_normalization=False,  # divide inputs by std of the dataset         samplewise_std_normalization=False,  # divide each input by its std         zca_whitening=False,  # apply ZCA whitening         rotation_range=5,  # randomly rotate images in the range (degrees, 0 to 180)         zoom_range = 0.05, # Randomly zoom image          width_shift_range=0,  # randomly shift images horizontally (fraction of total width)         height_shift_range=0,  # randomly shift images vertically (fraction of total height)         horizontal_flip=False,  # randomly flip images         vertical_flip=False)  # randomly flip images datagen.fit(X_train)  history = model.fit_generator(                               datagen.flow(X_train,y_train, batch_size=batch_size),                               epochs = epoch,                                validation_data = (X_test,y_test),                               verbose = 2,                                steps_per_epoch=X_train.shape[0] // batch_size,                               callbacks=[learning_rate_reduction]                              )",1
"CHECKPOINT print(history.history.keys()) plt.plot(history.history['sparse_categorical_accuracy']) plt.plot(history.history['val_sparse_categorical_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show()",0,stream,"print(history.history.keys()) # summarize history for accuracy plt.plot(history.history['sparse_categorical_accuracy']) plt.plot(history.history['val_sparse_categorical_accuracy']) plt.title('model accuracy') plt.ylabel('accuracy') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show() # summarize history for loss plt.plot(history.history['loss']) plt.plot(history.history['val_loss']) plt.title('model loss') plt.ylabel('loss') plt.xlabel('epoch') plt.legend(['train', 'test'], loc='upper left') plt.show()",1
"CHECKPOINT print(classification_report(y1_test, NB_pred))",0,stream,"print(classification_report(y1_test, NB_pred))",1
"CHECKPOINT print(classification_report(y2_test, NB_func_pred))",0,stream,"print(classification_report(y2_test, NB_func_pred))",1
"CHECKPOINT print(classification_report(y3_test, NB_tfidf_pred))",0,stream,"print(classification_report(y3_test, NB_tfidf_pred))",1
"CHECKPOINT print(classification_report(y4_test, NB_func_tfidf_pred))",0,stream,"print(classification_report(y4_test, NB_func_tfidf_pred))",1
"SETUP CHECKPOINT print(classification_report(y_test, y_pred_test))",0,stream,"#Classification report from sklearn.metrics import classification_report  print(classification_report(y_test, y_pred_test))",1
"CHECKPOINT print(.format(logreg.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg.score(X_test,y_test)))",1
"CHECKPOINT print(.format(logreg100.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg100.score(X_test,y_test)))",1
"CHECKPOINT print(classification_report(y_test, y_pred_test))",0,stream,"#Classification report print(classification_report(y_test, y_pred_test))",1
"CHECKPOINT print(.format(logreg001.score(X_test,y_test)))",0,stream,"#Overall Accuracy  print(""Accuracy of our model :: {}"".format(logreg001.score(X_test,y_test)))",1
"def Mrmse(y_true,y_pred): ASSIGN = np.log(ASSIGN) ASSIGN = math.sqrt(mean_squared_error(y_true, y_pred)) return rmse",1,not_existent,"def Mrmse(y_true,y_pred):     y_true = np.log(y_true)     #y_pred = np.log(y_pred)     rmse = math.sqrt(mean_squared_error(y_true, y_pred))     return rmse",0
"CHECKPOINT print(Mrmse(y_train,y_pred_train)) print(Mrmse(y_test,y_pred_test))",0,not_existent,"print(Mrmse(y_train,y_pred_train)) print(Mrmse(y_test,y_pred_test))",1
"ASSIGN = pd.read_csv('path', index_col=""ID"") ASSIGN = ASSIGN[[x for x in column_list if x != 'TARGET']] for col in squared_cols: ASSIGN = np.power(ASSIGN,2) ASSIGN = pipe.ASSIGN(test)",1,error,"test = pd.read_csv('/kaggle/input/cs-challenge/test_set.csv', index_col=""ID"")  #test = test[[x for x in non_redundant_cols if x != 'TARGET']]  test = test[[x for x in column_list if x != 'TARGET']]    for col in squared_cols:      test[col] = np.power(test[col],2)    predict = pipe.predict(test)",0
"ASSIGN = predict test['TARGET'].to_csv(""squared_ridge.csv"")",0,error,"test['TARGET'] = predict  test['TARGET'].to_csv(""squared_ridge.csv"")",1
"SETUP classification_report(y_test, prediction)",1,error,"from sklearn.metrics import classification_report,accuracy_score  classification_report(y_test, prediction)",0
"CHECKPOINT print(+str(accuracy_score(y_test, prediction)))",0,error,"print(""Accuracy: ""+str(accuracy_score(y_test, prediction)))  # model1.score(X_test,y_test)",1
"testCNNModel('path',model)",1,execute_result,"testCNNModel('/kaggle/input/testdata2/fist1.jpg',model)",0
"testCNNModel('path',model)",1,error,"testCNNModel('/kaggle/input/testdata2/two14.jpg',model)",0
"testCNNModel('path',model)",1,execute_result,"testCNNModel('/kaggle/input/testdata2/fist5.jpg',model)",0
"testCNNModel('path',model)",1,execute_result,"testCNNModel('/kaggle/input/testdata2/two4.jpg',model)",0
"plt.scatter(shyness_score, friendship_len)",0,execute_result,"plt.scatter(shyness_score, friendship_len)",1
evaluate_model(lr),1,display_data,evaluate_model(lr),0
"CHECKPOINT ASSIGN = np.nonzero(lr_pred[""Label""].values==y_test)[0] ASSIGN = np.nonzero(lr_pred[""Label""].values!=y_test)[0] print(len(ASSIGN),) print(len(ASSIGN),)",0,stream,"correct_predictions = np.nonzero(lr_pred[""Label""].values==y_test)[0] incorrect_predictions = np.nonzero(lr_pred[""Label""].values!=y_test)[0] print(len(correct_predictions),"" classified correctly"") print(len(incorrect_predictions),"" classified incorrectly"")",1
"SETUP CHECKPOINT ASSIGN = LinearRegression() ASSIGN.fit(x_train.reshape(-1,1), y_train.reshape(-1,1)) print(ASSIGN.coef_) print(ASSIGN.intercept_)",0,not_existent,"from sklearn.linear_model import LinearRegression lr = LinearRegression() lr.fit(x_train.reshape(-1,1), y_train.reshape(-1,1)) print(lr.coef_) print(lr.intercept_)",1
"CHECKPOINT ASSIGN = math.sqrt(mean_squared_error(p_train, y_train)) print('Pontuação para o treinamento: %.2f RMSE' % (ASSIGN)) ASSIGN = math.sqrt(mean_squared_error(p_test, y_test)) print('Pontuação para o teste: %.2f RMSE' % (ASSIGN))",1,stream,"#calcula os erros de previsão  trainScore = math.sqrt(mean_squared_error(p_train, y_train))  print('Pontuação para o treinamento: %.2f RMSE' % (trainScore))  testScore = math.sqrt(mean_squared_error(p_test, y_test))  print('Pontuação para o teste: %.2f RMSE' % (testScore))",0
"def MSE(y,y_predicted): return ((y- y_predicted)**2).mean()",1,not_existent,"def MSE(y,y_predicted):     return ((y- y_predicted)**2).mean() ",0
"SETUP def cross_entropy(y, y_hat): ASSIGN = np.clip(ASSIGN, EPS, 1-EPS) return -np.sum(y * np.log(ASSIGN)path)",1,not_existent,"EPS = 1e-9 #same as in softmax,the first line in this function just gives numerical stability for cross entropy  def cross_entropy(y, y_hat):     y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability     return -np.sum(y * np.log(y_hat)/n) ",0
"def predictDis(x): ASSIGN = np.array(x, dtype='float') ASSIGN = x[0:64].reshape((8, 8)) plt.imshow(ASSIGN, cmap='gray') ASSIGN = np.dot(x,w) print (""the class of this Image is: "",giveMeValueFromOneHot(softmax(ASSIGN)))",1,not_existent,"def predictDis(x):     img = np.array(x, dtype='float')     pixels = x[0:64].reshape((8, 8))     plt.imshow(pixels, cmap='gray')     z = np.dot(x,w)     print (""the class of this Image is: "",giveMeValueFromOneHot(softmax(z)))",0
ASSIGN = X[0] predictDis(ASSIGN),1,not_existent,x = X[0] predictDis(x),0
ASSIGN = X[1] predictDis(ASSIGN),1,not_existent,x = X[1] predictDis(x),0
ASSIGN = X[3] predictDis(ASSIGN),1,not_existent,x = X[3] predictDis(x),0
"CHECKPOINT ASSIGN = pd.Series(df_train_v3.columns) ASSIGN = var[~var.isin(['country','suicides_no','suicidespath'])] ASSIGN = 'suicidespath' ASSIGN = df_train_v3[x] ASSIGN = df_train_v3[y] ASSIGN = len(X3) ASSIGN = len(Y3) ASSIGN.loc[ASSIGN+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America'] ASSIGN.loc[ASSIGN+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America'] ASSIGN.loc[ASSIGN+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America'] ASSIGN.loc[ASSIGN+1] = 26.95 ASSIGN.loc[ASSIGN+2] = 6.75 ASSIGN.loc[ASSIGN+3] = 28.35 ASSIGN.loc[ASSIGN+4] = 9.46 ASSIGN.loc[ASSIGN+5] = 21.06 ASSIGN.loc[ASSIGN+6] = 5.42 ASSIGN,ASSIGN = dataset_ready(ASSIGN, ASSIGN) ASSIGN = ASSIGN.loc[nx+1:nx+6] ASSIGN = ASSIGN.loc[ny+1:nx+6] ASSIGN = GLM_result.ASSIGN(X3[best_var]) for i in range(len(ASSIGN)) : print('Option',i+1) print('Predicted suicide rates :',round(ASSIGN.iloc[i],2)) print('Actual suicide rates :',ASSIGN.iloc[i]) print('')",1,not_existent,"# Preparing prediction var = pd.Series(df_train_v3.columns) x = var[~var.isin(['country','suicides_no','suicides/100k pop'])] y = 'suicides/100k pop'  X3 = df_train_v3[x] Y3 = df_train_v3[y]  # Input data for prediction nx = len(X3) ny = len(Y3) X3.loc[nx+1] = [2016, 'male', '25-34 years', 21845000, 57588, 'Millenials', 'North America'] X3.loc[nx+2] = [2016, 'female', '25-34 years', 21917000, 57588, 'Millenials', 'North America'] X3.loc[nx+3] = [2016, 'male', '35-54 years', 40539000, 57588, 'Generation X', 'North America'] X3.loc[nx+4] = [2016, 'female', '35-54 years', 42031000, 57588, 'Generation X', 'North America'] X3.loc[nx+5] = [2016, 'male', '15-24 years', 21719000, 57588, 'Millenials', 'North America'] X3.loc[nx+6] = [2016, 'female', '15-24 years', 21169000, 57588, 'Millenials', 'North America'] Y3.loc[ny+1] = 26.95 Y3.loc[ny+2] = 6.75 Y3.loc[ny+3] = 28.35 Y3.loc[ny+4] = 9.46 Y3.loc[ny+5] = 21.06 Y3.loc[ny+6] = 5.42  # Tranform the data to be ready for precition X3,Y3 = dataset_ready(X3, Y3) X3 = X3.loc[nx+1:nx+6] Y3 = Y3.loc[ny+1:nx+6]  # Predict predict = GLM_result.predict(X3[best_var]) for i in range(len(predict)) :     print('Option',i+1)     print('Predicted suicide rates :',round(predict.iloc[i],2))     print('Actual suicide rates :',Y3.iloc[i])     print('')",0
"SETUP CHECKPOINT sys.path.append('..path') binder.bind(globals()) print() ASSIGN = pd.read_csv('..path', nrows=50000) ASSIGN = ASSIGN.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' + 'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' + 'pickup_longitude > -74 and pickup_longitude < -73.9 and ' + 'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' + 'fare_amount > 0' ) ASSIGN = data.fare_amount ASSIGN = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude'] ASSIGN = data[base_features] train_X, val_X, train_y, val_y = train_test_split(ASSIGN, ASSIGN, random_state=1) ASSIGN = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print() ASSIGN.head()",0,not_existent,"import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split  # Environment Set-Up for feedback system. import sys sys.path.append('../input/ml-insights-tools') from learntools.core import binder binder.bind(globals()) from ex3 import * print(""Setup Complete"")  # Data manipulation code below here data = pd.read_csv('../input/new-york-city-taxi-fare-prediction/train.csv', nrows=50000)  # Remove data with extreme outlier coordinates or negative fares data = data.query('pickup_latitude > 40.7 and pickup_latitude < 40.8 and ' +                   'dropoff_latitude > 40.7 and dropoff_latitude < 40.8 and ' +                   'pickup_longitude > -74 and pickup_longitude < -73.9 and ' +                   'dropoff_longitude > -74 and dropoff_longitude < -73.9 and ' +                   'fare_amount > 0'                   )  y = data.fare_amount  base_features = ['pickup_longitude',                  'pickup_latitude',                  'dropoff_longitude',                  'dropoff_latitude']  X = data[base_features]   train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1) first_model = RandomForestRegressor(n_estimators=30, random_state=1).fit(train_X, train_y) print(""Data sample:"") data.head()",1
