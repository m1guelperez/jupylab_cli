content,tag,output_type,original_content,y_pred
SETUP ASSIGN = get_data('..path') ASSIGN = get_data('..path'),1,not_existent,"### Get data train_img, train_label = get_data('../input/sign-language-mnist/sign_mnist_train.csv') val_img, val_label = get_data('../input/sign-language-mnist/sign_mnist_test.csv')  %time",0
"ASSIGN = cv2.CascadeClassifier('..path') ASSIGN =cv2.imread( ""..path"",0)",1,not_existent,"face_cascade = cv2.CascadeClassifier('../input/opencv-haarcascade/data/haarcascades/haarcascade_frontalface_default.xml') gray =cv2.imread( ""../input/opencv-samples-images/data/lena.jpg"",0)",0
"ASSIGN = df_train.isnull().sum().sort_values(ascending=False) ASSIGN = (df_train.isnull().sum()path().count()).sort_values(ascending=False) ASSIGN = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent']) ASSIGN.head()",0,execute_result,"total = df_train.isnull().sum().sort_values(ascending=False)  percentage = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)  missing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Persent'])  missing_data.head()",1
"ASSIGN = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0path) ASSIGN = test_datagen.flow_from_directory(""..path"", ASSIGN = 'categorical', ASSIGN = (150, 150))",1,stream,"#Extract the test data => I didnt find a way without creating a new folder on colab    test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.)    test_generator = test_datagen.flow_from_directory(""../input/animal-ori/animal_dataset_intermediate"",                                                                                                            class_mode = 'categorical',                                                       target_size = (150, 150))",0
"SETUP ASSIGN = 0 ASSIGN = 20 ASSIGN = 0.2 ASSIGN = transforms.ToTensor() ASSIGN = datasets.MNIST(root='data', train=True, ASSIGN=True, transform=transform) ASSIGN = datasets.MNIST(root='data', train=False, ASSIGN=True, transform=transform) ASSIGN = len(train_data) ASSIGN = list(range(num_train)) np.random.shuffle(ASSIGN) ASSIGN = int(np.floor(valid_size * num_train)) ASSIGN = indices[split:], indices[:split] ASSIGN = SubsetRandomSampler(train_idx) ASSIGN = SubsetRandomSampler(valid_idx) ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ASSIGN=train_sampler, num_workers=num_workers) ASSIGN = torch.utils.data.DataLoader(train_data, batch_size=batch_size, ASSIGN=valid_sampler, num_workers=num_workers) ASSIGN = torch.utils.data.DataLoader(test_data, batch_size=batch_size, ASSIGN=ASSIGN)",1,not_existent,"from torchvision import datasets import torchvision.transforms as transforms from torch.utils.data.sampler import SubsetRandomSampler  # number of subprocesses to use for data loading num_workers = 0 # how many samples per batch to load batch_size = 20 # percentage of training set to use as validation valid_size = 0.2  # convert data to torch.FloatTensor transform = transforms.ToTensor()  # choose the training and test datasets train_data = datasets.MNIST(root='data', train=True,                                    download=True, transform=transform) test_data = datasets.MNIST(root='data', train=False,                                   download=True, transform=transform)  # obtain training indices that will be used for validation num_train = len(train_data) indices = list(range(num_train)) np.random.shuffle(indices) split = int(np.floor(valid_size * num_train)) train_idx, valid_idx = indices[split:], indices[:split]  # define samplers for obtaining training and validation batches train_sampler = SubsetRandomSampler(train_idx) valid_sampler = SubsetRandomSampler(valid_idx)  # prepare data loaders train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,     sampler=train_sampler, num_workers=num_workers) valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,      sampler=valid_sampler, num_workers=num_workers) test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,      num_workers=num_workers)",0
"SETUP ASSIGN = iter(train_loader) ASSIGN = dataiter.next() ASSIGN = ASSIGN.numpy() ASSIGN = plt.figure(figsize=(25, 4)) for idx in np.arange(20): ASSIGN = fig.add_subplot(2, 20path, idx+1, xticks=[], yticks=[]) ASSIGN.imshow(np.squeeze(ASSIGN[idx]), cmap='gray') ASSIGN.set_title(str(labels[idx].item()))",0,not_existent,"import matplotlib.pyplot as plt %matplotlib inline      # obtain one batch of training images dataiter = iter(train_loader) images, labels = dataiter.next() images = images.numpy()  # plot the images in the batch, along with the corresponding labels fig = plt.figure(figsize=(25, 4)) for idx in np.arange(20):     ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])     ax.imshow(np.squeeze(images[idx]), cmap='gray')     # print out the correct label for each image     # .item() gets the value contained in a Tensor     ax.set_title(str(labels[idx].item()))",1
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""chicago_crime"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref)",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""chicago_crime"" dataset  dataset_ref = client.dataset(""chicago_crime"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)",0
"CHECKPOINT for filename in glob.glob(os.path.join(directory_a, '*.png')): ASSIGN =cv2.imread(filename,0) print(ASSIGN.shape)",1,error,"for filename in glob.glob(os.path.join(directory_a, '*.png')):      im1 =cv2.imread(filename,0)      print(im1.shape)",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, '*.png')): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = giveMeFeatures(im1) ASSIGN.append(ASSIGN) ASSIGN.append(1) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN = np.random.RandomState(321) ASSIGN = rand.permutation(len(X)) ASSIGN = ASSIGN[shuffle] ASSIGN = ASSIGN[shuffle]",1,not_existent,"import glob  import cv2  directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'  directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'    X = []  y = []    for filename in glob.glob(os.path.join(directory_a, '*.png')):      im1 =cv2.imread(filename,0)      im1 = cv2.resize(im1,(64,64))      features = giveMeFeatures(im1)      X.append(features)      y.append(0)    for filename in glob.glob(os.path.join(directory_b, '*.png')):      im1 =cv2.imread(filename,0)      im1 = cv2.resize(im1,(64,64))      features = giveMeFeatures(im1)      X.append(features)      y.append(1)        X = np.array(np.float32(X))  y = np.array(np.float32(y))      rand = np.random.RandomState(321)  shuffle = rand.permutation(len(X))  X = X[shuffle]  y = y[shuffle]             ",0
"CHECKPOINT def testModel(path): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN=[] ASSIGN.append(giveMeFeatures(ASSIGN)) ASSIGN = np.array(np.float32(ASSIGN)) ASSIGN =model1.predict(features) if(ASSIGN[0]==0): return 'fist' else: return 'palm' return",1,not_existent,"def testModel(path):      im1 =cv2.imread(path,0)      im1 = cv2.resize(im1,(64,64))      features=[]      features.append(giveMeFeatures(im1))      features = np.array(np.float32(features))          res =model1.predict(features)      if(res[0]==0):          return 'fist'      else:          return 'palm'                return       ",0
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN.append(ASSIGN) ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] ASSIGN=0 ASSIGN=0 for typ in ASSIGN: for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([1,0]) ASSIGN+=1 for directory in ASSIGN: for filename in glob.glob(os.path.join(directory, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN.append(ASSIGN) ASSIGN.append([0,1]) ASSIGN+=1 print('A: ', ASSIGN) print('B: ', ASSIGN)",1,stream,"import glob  import cv2  directory_a = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/a'  directory_b = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/b'  directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_s = '/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/A/s'    ADirectories = []  BDirectories = []    ADirectories.append(directory_a)  ADirectories.append(directory_s)  BDirectories.append(directory_b)  BDirectories.append(directory_5)      X = []  y = []  types = ['*.png', '*.jpg']  countA=0  countB=0  for typ in types:      for directory in ADirectories:          for filename in glob.glob(os.path.join(directory, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(28,28))              X.append(im1)              y.append([1,0])              countA+=1      for directory in BDirectories:          for filename in glob.glob(os.path.join(directory, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(28,28))              X.append(im1)              y.append([0,1])              countB+=1  print('A: ', countA)  print('B: ', countB) ",0
"CHECKPOINT def testCNNModel(path,model): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(28,28)) ASSIGN = [] ASSIGN.append(ASSIGN.reshape(28,28)) ASSIGN = np.asarray(ASSIGN) ASSIGN = ASSIGN.reshape(1,28,28,1) ASSIGN =model.predict(t) return res return",1,not_existent,"def testCNNModel(path,model):      im1 =cv2.imread(path,0)      im1 = cv2.resize(im1,(28,28))      t = []      t.append(im1.reshape(28,28))      t = np.asarray(t)      t = t.reshape(1,28,28,1)      res =model.predict(t)      return res                return ",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob  import cv2  directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_2 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/2'  directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          for filename in glob.glob(os.path.join(directory_5, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          for filename in glob.glob(os.path.join(directory_unk, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)                 ",0
"CHECKPOINT def testCNNModel(path,model): ASSIGN =cv2.imread(path,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN = [] ASSIGN.append(ASSIGN.reshape(64,64)) ASSIGN = np.asarray(ASSIGN) ASSIGN = ASSIGN.reshape(1,64,64,1) ASSIGN =model.predict(t) return res return",1,not_existent,"def testCNNModel(path,model):      im1 =cv2.imread(path,0)      im1 = cv2.resize(im1,(64,64))      t = []      t.append(im1.reshape(64,64))      t = np.asarray(t)      t = t.reshape(1,64,64,1)      res =model.predict(t)      return res                return ",0
"SETUP ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2)",1,not_existent,"import glob  import cv2  directory_1 = '/kaggle/input/3shapesdataset/resized/1'  directory_2 = '/kaggle/input/3shapesdataset/resized/2'  directory_3 = '/kaggle/input/3shapesdataset/resized/3'  #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_1, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          for filename in glob.glob(os.path.join(directory_3, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)  #         for filename in glob.glob(os.path.join(directory_unk, typ)):  #             im1 =cv2.imread(filename,0)  #             im1 = cv2.resize(im1,(64,64))  #             X.append(im1)  #             y.append(3)                               ",0
"SETUP CHECKPOINT ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = 'path' ASSIGN = [] ASSIGN = [] ASSIGN = ['*.png', '*.jpg'] for typ in ASSIGN: for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(0) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(1) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(2) print('finished') for filename in glob.glob(os.path.join(ASSIGN, typ)): ASSIGN =cv2.imread(filename,0) ASSIGN = cv2.resize(ASSIGN,(64,64)) ASSIGN.append(ASSIGN) ASSIGN.append(3) print('finished')",1,stream,"import glob  import cv2  directory_1 = '/kaggle/input/3shapesdatasetunk/All Data/1'  directory_2 = '/kaggle/input/3shapesdatasetunk/All Data/2'  directory_3 = '/kaggle/input/3shapesdatasetunk/All Data/3'  # directory_4 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/4'  # directory_5 = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/5'  directory_unk = '/kaggle/input/3shapesdatasetunk/All Data/unknown'  #directory_unk = '/kaggle/input/sign-language-for-numbers/Sign Language for Numbers/unknown'          X = []  y = []  types = ['*.png', '*.jpg']  for typ in types:          for filename in glob.glob(os.path.join(directory_1, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(0)          print('finished')          for filename in glob.glob(os.path.join(directory_2, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(1)          print('finished')          for filename in glob.glob(os.path.join(directory_3, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(2)          print('finished')          for filename in glob.glob(os.path.join(directory_unk, typ)):              im1 =cv2.imread(filename,0)              im1 = cv2.resize(im1,(64,64))              X.append(im1)              y.append(3)          print('finished')                               ",0
"SETUP CHECKPOINT for dirname, _, filenames in os.walk('path'): for filename in filenames: print(os.path.join(dirname, filename)) ASSIGN = open('path', 'r') for f in ASSIGN: print (f.rstrip()) ASSIGN.close()",1,stream,"# The code below allows you to access your Kaggle data files  import os  for dirname, _, filenames in os.walk('/kaggle/input'):      for filename in filenames:          print(os.path.join(dirname, filename))    # create a file handle called fname to open and hold the contents of the data file  fname = open('/kaggle/input/testtext/test.txt', 'r')    # Add your code below  for f in fname:      # This will print out each line      print (f.rstrip())    # It's good practice to close your file when you are finished. This is in the next line.  fname.close()",0
"CHECKPOINT ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 ASSIGN = open('path', 'r') for f in ASSIGN: ASSIGN=len(f)+ASSIGN ASSIGN=len(f[0])+ASSIGN ASSIGN=(len(f.split()))+ASSIGN print('%d characters'%ASSIGN) print('%d lines'%ASSIGN) print('%d words'%ASSIGN) ASSIGN.close()",1,stream,"numChars = 0  numLines = 0  numWords = 0    # create a file handle called fname to open and hold the contents of the data file  # make sure to upload your test.txt file first  fname = open('/kaggle/input/testtext/test.txt', 'r')    # Add your code below to read each line in the file, count the number of characters, lines, and words  # updating the numChars, numLines, and numWords variables.  for f in fname:            # Cumulative addition of the length of each word      numChars=len(f)+numChars      # Cumulative addition of the first letter in each line      numLines=len(f[0])+numLines      # Cumulative addition of items in each line's list      numWords=(len(f.split()))+numWords        # output code below is provided for you; you should not edit this    print('%d characters'%numChars)  print('%d lines'%numLines)  print('%d words'%numWords)    # It's good practice to close your file when you are finished. This is in the next line.  fname.close()",0
"CHECKPOINT ASSIGN = 0 ASSIGN = 0 ASSIGN = 0 ASSIGN = open('path', 'r') for f in ASSIGN: ASSIGN=len(f)+ASSIGN ASSIGN=len(f[0])+ASSIGN ASSIGN=(len(f.split()))+ASSIGN print('%d characters'%ASSIGN) print('%d lines'%ASSIGN) print('%d words'%ASSIGN) ASSIGN.close()",1,stream,"# Add your code below  numChars = 0  numLines = 0  numWords = 0    # create a file handle called fname to open and hold the contents of the data file  # make sure to upload your test.txt file first  fname = open('/kaggle/input/reading/sherlock.txt', 'r')    # Add your code below to read each line in the file, count the number of characters, lines, and words  # updating the numChars, numLines, and numWords variables.  for f in fname:            # Cumulative addition of the length of each word      numChars=len(f)+numChars      # Cumulative addition of the first letter in each line      numLines=len(f[0])+numLines      # Cumulative addition of items in each line's list      numWords=(len(f.split()))+numWords        # output code below is provided for you; you should not edit this    print('%d characters'%numChars)  print('%d lines'%numLines)  print('%d words'%numWords)    # It's good practice to close your file when you are finished. This is in the next line.  fname.close()",0
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""hacker_news"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""comments"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery    # Create a ""Client"" object  client = bigquery.Client()    # Construct a reference to the ""hacker_news"" dataset  dataset_ref = client.dataset(""hacker_news"", project=""bigquery-public-data"")    # API request - fetch the dataset  dataset = client.get_dataset(dataset_ref)    # Construct a reference to the ""comments"" table  table_ref = dataset_ref.table(""comments"")    # API request - fetch the table  table = client.get_table(table_ref)    # Preview the first five lines of the ""comments"" table  client.list_rows(table, max_results=5).to_dataframe()",0
"plt.figure(figsize=(20, 20)) plt.title(""Original"") plt.imshow(mpimg.imread('..path')) plt.show()",1,display_data,"plt.figure(figsize=(20, 20)) plt.title(""Original"") plt.imshow(mpimg.imread('../input/opencv-samples-images/WaldoBeach.jpg')) plt.show() ",0
"SETUP ASSIGN = bigquery.Client() ASSIGN = client.dataset(""stackoverflow"", project=""bigquery-public-data"") ASSIGN = client.get_dataset(dataset_ref) ASSIGN = dataset_ref.table(""posts_questions"") ASSIGN = client.get_table(table_ref) ASSIGN.list_rows(ASSIGN, max_results=5).to_dataframe()",1,stream,"from google.cloud import bigquery  # Create a ""Client"" object client = bigquery.Client()  # Construct a reference to the ""stackoverflow"" dataset dataset_ref = client.dataset(""stackoverflow"", project=""bigquery-public-data"")  # API request - fetch the dataset dataset = client.get_dataset(dataset_ref)  # Construct a reference to the ""posts_questions"" table table_ref = dataset_ref.table(""posts_questions"")  # API request - fetch the table table = client.get_table(table_ref)  # Preview the first five lines of the table client.list_rows(table, max_results=5).to_dataframe()",0
"SETUP ASSIGN="".path"" os.mkdir(ASSIGN) for dirname, _, filenames in os.walk('..path'): for filename in filenames: ASSIGN = ""..path""+filename ASSIGN = test_set+os.path.splitext(filename)[0]+"".wav"" ASSIGN = AudioSegment.from_mp3(src) ASSIGN = ASSIGN.set_frame_rate(8000) ASSIGN.export(ASSIGN, format=""wav"")",1,not_existent,"import os  from os import path from pydub import AudioSegment      test_set=""./test_set/""    os.mkdir(test_set)     for dirname, _, filenames in os.walk('../input/quran-asr-challenge/test_set'):     for filename in filenames:         # files                                                                                  src = ""../input/quran-asr-challenge/test_set/""+filename         dst = test_set+os.path.splitext(filename)[0]+"".wav""         # convert wav to mp3                                                                     sound = AudioSegment.from_mp3(src)         sound = sound.set_frame_rate(8000)         sound.export(dst, format=""wav"") ",0
"ASSIGN = imageio.get_reader('..path') ASSIGN = reader.get_meta_data()['ASSIGN'] ASSIGN = [] try: for im in ASSIGN: ASSIGN.append(im) except RuntimeError: pass ASSIGN.close() ASSIGN = [resize(frame, (256, 256))[..., :3] for frame in ASSIGN] HTML(display(ASSIGN).to_html5_video())",1,execute_result,"reader = imageio.get_reader('../input/digitsinnoise-video/Test.mp4')  fps = reader.get_meta_data()['fps']  driving_video = []  try:      for im in reader:          driving_video.append(im)  except RuntimeError:      pass  reader.close()    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]        HTML(display(driving_video).to_html5_video())",0
SETUP ASSIGN = Image.open('path') display(ASSIGN),1,display_data,"from PIL import Image, ImageDraw  from IPython.display import display    # Sample Image  virat_img = Image.open('/kaggle/input/virat-kohli-facial-recognition/Virat Kohli Facial Recognition/unknown_faces/Kohli-Williamson.jpg')  display(virat_img)",0
