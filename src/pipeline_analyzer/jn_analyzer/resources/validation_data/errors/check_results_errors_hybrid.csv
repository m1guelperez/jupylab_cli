content,tag,output_type,original_content,y_pred
for col in cat_enc_col: feature.append(col),0,not_existent,for col in cat_enc_col:      feature.append(col),1
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"# columns with null values df_isna = pd.DataFrame(df.isnull().sum()) df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)]",0
df.describe(include='all'),1,not_existent,df.describe(include='all'),0
df['parentesco1'].loc[df.parentesco1 == 1].describe(),1,not_existent,df['parentesco1'].loc[df.parentesco1 == 1].describe(),0
"ASSIGN = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) ASSIGN.sample(5)",1,not_existent,"df_hid = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) df_hid.sample(5)",0
df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),0,not_existent,df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),1
df_hwoh['v2a1'].hist(),0,not_existent,df_hwoh['v2a1'].hist(),1
df['v2a1'].describe().plot(),0,not_existent,df['v2a1'].describe().plot(),1
"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()",0,not_existent,"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()",1
df['v2a1'].max(),1,not_existent,df['v2a1'].max(),0
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]",1,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 > 1000000]",0
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",1,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",0
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",1,not_existent,"# remove these two rows... df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",0
set(df.dtypes),1,not_existent,set(df.dtypes),0
"ASSIGN = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] ASSIGN = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a', 'tamhog', 'hacdor', 'hacapo', 'r4t3', ] ASSIGN = ['age', 'agesq', 'female', 'male',] ASSIGN = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin', 'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] ASSIGN = ['abastaguadentro', 'abastaguafuera', 'abastaguano',] ASSIGN = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] ASSIGN = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] ASSIGN = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] ASSIGN = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] ASSIGN = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] ASSIGN = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] ASSIGN = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] ASSIGN = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother', 'paredpreb', 'paredzinc', 'paredzocalo',] ASSIGN = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',] ASSIGN = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] ASSIGN = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7',] ASSIGN = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] ASSIGN = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] ASSIGN = [ 'eviv1', 'eviv2', 'eviv3',] ASSIGN = [ 'etecho1', 'etecho2', 'etecho3',] ASSIGN = [ 'epared1', 'epared2', 'epared3',] ASSIGN = [ 'dis', 'escolari', 'meaneduc', 'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] ASSIGN = ['coopele', 'noelec', 'planpri', 'public',] ASSIGN = cols_electronics+cols_house_details+cols_person_details+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec len(ASSIGN)",1,not_existent,"cols_electronics = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] cols_house_details = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a',                      'tamhog', 'hacdor', 'hacapo', 'r4t3', ] cols_person_details = ['age', 'agesq', 'female', 'male',] cols_SQ = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin',             'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] cols_water = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]  cols_h = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] cols_r = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] cols_tip = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] cols_roof = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] cols_floor = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] cols_sanitary = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] cols_parents = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',                 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] cols_outside_wall = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother',                'paredpreb', 'paredzinc', 'paredzocalo',] cols_instlevel = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',                   'instlevel7', 'instlevel8', 'instlevel9',] cols_lugar = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] cols_estadoc = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',                  'estadocivil5', 'estadocivil6', 'estadocivil7',] cols_elim = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] cols_energ = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] cols_eviv = [ 'eviv1', 'eviv2', 'eviv3',] cols_etech = [ 'etecho1', 'etecho2', 'etecho3',] cols_pared = [ 'epared1', 'epared2', 'epared3',] cols_unknown = [ 'dis', 'escolari', 'meaneduc',                  'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] cols_elec = ['coopele', 'noelec', 'planpri', 'public',]  total_features = cols_electronics+cols_house_details+cols_person_details+\ cols_SQ+cols_water+cols_h+cols_r+cols_tip+cols_roof+\ cols_floor+cols_sanitary+cols_parents+cols_outside_wall+\ cols_instlevel+cols_lugar+cols_estadoc+cols_elim+cols_energ+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec  len(total_features)",0
df[cols_electronics].plot.area(),0,not_existent,df[cols_electronics].plot.area(),1
ASSIGN = cols_electronics.append('Target') df[cols_electronics].corr(),1,not_existent,cols_electronics_target = cols_electronics.append('Target') df[cols_electronics].corr(),0
"df[['tamhog','r4t3', 'tamviv']].corr()",1,not_existent,"# high correlation between  # no. of persons in the household, # persons living in the household  # and size of the household # we can use any one...!! df[['tamhog','r4t3', 'tamviv']].corr()",0
"df[['r4t3','tamviv']].corr()",1,not_existent,"df[['r4t3','tamviv']].corr()",0
df.groupby('Target').overcrowding.value_counts().unstack().plot.bar(),0,not_existent,df.groupby('Target').overcrowding.value_counts().unstack().plot.bar(),1
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"ff = pd.DataFrame(df.isnull().sum()) ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]",0
"len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",1,not_existent,"# number of rows where income and rez_esc has values len(df.loc[(df.v2a1 >= 0)]), len(df.loc[(df.rez_esc >= 0)])",0
len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)]),1,not_existent,# how many rows with nan values for both income and rez_esc  len(df.loc[(df.v2a1 >= 0) & (df.rez_esc >= 0)]),0
len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)]),1,not_existent,# how many rows with nan values for either income or rez_esc  len(df.loc[(df.v2a1 >= 0) | (df.rez_esc >= 0)]),0
"df[['v2a1','Target']].corr()",1,not_existent,"df[['v2a1','Target']].corr()",0
"df[['v2a1','Target']].fillna(0).corr()",1,not_existent,"df[['v2a1','Target']].fillna(0).corr()",0
"df[['hhsize','Target']].corr()",1,not_existent,"df[['hhsize','Target']].corr()",0
"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",0,not_existent,"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",1
total_features.remove('r4t3'),0,not_existent,"# removing 'r4t3', as 'hhsize' is of almost same distribution total_features.remove('r4t3')",1
total_features.append('edjefa') total_features.append('edjefe'),0,not_existent,total_features.append('edjefa') total_features.append('edjefe'),1
df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],1,not_existent,df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],0
"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",1,not_existent,"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (df.v2a1.isna())].describe(include='all')",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].describe(include='all')",0
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 != 1].describe(include='all')",0
"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[df.parentesco1 == 1].describe(include='all')",0
gc.collect(),0,not_existent,gc.collect(),1
"train_data[['y', 'X0'] + list(train_data.columns[-8:])]",1,execute_result,"train_data[['y', 'X0'] + list(train_data.columns[-8:])]",0
SETUP ASSIGN = os.listdir(SOURCE_DIR) ASSIGN = [fn for fn in list_fn if 'cat' in fn] ASSIGN = [fn for fn in list_fn if 'dog' in fn],0,not_existent,### List file name in train dataset SOURCE_DIR = '/kaggle/working/train/' list_fn = os.listdir(SOURCE_DIR) list_cat_fn = [fn for fn in list_fn if 'cat' in fn] list_dog_fn = [fn for fn in list_fn if 'dog' in fn],1
"SETUP ASSIGN = [] ASSIGN = 2 ASSIGN = 0.2 def make_model() : for l in base_model.layers : l.trainable = False ASSIGN = Sequential() ASSIGN.add(base_model) ASSIGN.add(GlobalAveragePooling2D()) ASSIGN.add(Flatten()) for fc in ASSIGN: ASSIGN.add(Dense(fc, activation=swish)) ASSIGN.add(Dropout(ASSIGN)) ASSIGN.add(Dense(ASSIGN, activation='softmax')) ASSIGN.compile(Adam(), loss='categorical_crossentropy', metrics=['acc']) return model ASSIGN = make_model() ASSIGN.summary()",0,not_existent,"### Add layer and dont train the layer before from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision from tensorflow.keras.activations import swish fc_layers = [] num_classes = 2 dropout = 0.2  def make_model() :          # Freeze layer     for l in base_model.layers :         l.trainable = False      # Make new model     model = Sequential()     model.add(base_model)     model.add(GlobalAveragePooling2D())     model.add(Flatten())     for fc in fc_layers:          # New FC layer, random init         model.add(Dense(fc, activation=swish))         model.add(Dropout(dropout))      model.add(Dense(num_classes, activation='softmax'))      # Compile model     model.compile(Adam(), loss='categorical_crossentropy', metrics=['acc'])          return model   model = make_model() model.summary()",1
"SETUP SETUP https:path\ -O path ASSIGN = InceptionV3(weights=None, ASSIGN=False, ASSIGN=(INPUT_SIZE[0], INPUT_SIZE[1], 3)) ASSIGN = 'path' ASSIGN.load_weights(ASSIGN) ASSIGN.trainable = False ASSIGN.summary()",0,not_existent,"### Define pretrained model from tensorflow.keras.applications.inception_v3 import InceptionV3 from tensorflow.keras.applications.inception_v3 import preprocess_input,decode_predictions BATCH_SIZE = 20 INPUT_SIZE = (150,150)  # Get local weight !wget --no-check-certificate \     https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \     -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5      # Define inception model base_model = InceptionV3(weights=None,                           include_top=False,                           input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))  # Load local weight local_weight_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5' base_model.load_weights(local_weight_file)  # Freeze all layer base_model.trainable = False  # Summary of the model base_model.summary()",1
"SETUP ASSIGN = Sequential() ASSIGN.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)) ASSIGN.add(GlobalAveragePooling1D()) ASSIGN.add(Dense(24, activation='relu')) ASSIGN.add(Dense(6, activation='softmax')) ASSIGN.compile(optimizer = Adam(), ASSIGN = 'sparse_categorical_crossentropy', ASSIGN = 'accuracy') ASSIGN.summary()",0,stream,"### Make simple Embedding MLP model from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D from tensorflow.keras import Sequential, Model from tensorflow.keras.optimizers import SGD, Adam, RMSprop from tensorflow.keras.metrics import Precision  model = Sequential() model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=MAX_LENGTH)) model.add(GlobalAveragePooling1D()) model.add(Dense(24, activation='relu')) model.add(Dense(6, activation='softmax'))  # Compile model model.compile(optimizer = Adam(),               loss = 'sparse_categorical_crossentropy',               metrics = 'accuracy')  # Summary of the model model.summary() ",1
"ASSIGN = pd.read_csv('path', names=['label','id','time','query','handle','text']) ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True) ASSIGN = list(data['text']) ASSIGN = list(data['ASSIGN'].map({0:0, 4:1}))",1,not_existent,"### Load the data data = pd.read_csv('/tmp/training_cleaned.csv', names=['label','id','time','query','handle','text']) data = data.sample(frac=1).reset_index(drop=True) sentences = list(data['text']) label = list(data['label'].map({0:0, 4:1}))",0
plot_history(history_simple),0,not_existent,### Plot simple model performance plot_history(history_simple),1
plot_history(history_single_lstm),0,display_data,### Plot simple model performance plot_history(history_single_lstm),1
plot_history(history_single_gru),0,not_existent,### Plot simple model performance plot_history(history_single_gru),1
plot_history(history_single_conv),0,not_existent,### Plot simple model performance plot_history(history_single_conv),1
plot_history(history),0,not_existent,### Plot simple model performance plot_history(history),1
plot_history(history),0,display_data,### Plot model performance - LSTM no Bidirection plot_history(history),1
"ASSIGN=ASSIGN.drop([ 'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020' ], axis=1) ASSIGN.sample(10)",0,execute_result,"codiv_country=codiv_country.drop([      'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'  ], axis=1)  codiv_country.sample(10)",1
"ASSIGN=pd.read_csv(os.path.join('.path', 'csse_covid_19_daily_reports.csv')) ASSIGN=ASSIGN.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1) ASSIGN.sample(10)",1,execute_result,"codiv_csse=pd.read_csv(os.path.join('./', 'csse_covid_19_daily_reports.csv'))  codiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)  codiv_csse.sample(10)",0
"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000path[""Infected""] codiv_csse.sample(10)",0,execute_result,"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000/codiv_csse[""Infected""]  codiv_csse.sample(10)",1
set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique()),1,execute_result,set(codiv_country['Country'].unique()) - set(codiv_csse['Country_Region'].unique()),0
codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],1,execute_result,codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],0
"ASSIGN = codiv_country.copy() ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int) ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int) ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int) ASSIGN=ASSIGN.drop([ 'Quarantine','Schools','Restrictions', 'Country', 'Total Deaths','Total Infected','Total Active','Total Recovered', ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Deaths Ratio"" ], axis=1) ASSIGN.sample(10)",0,execute_result,"# Working on a copy  codiv_country_analyze = codiv_country.copy()    # Creating categorical variables  codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)  codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)  codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)    #   codiv_country_analyze=codiv_country_analyze.drop([      'Quarantine','Schools','Restrictions', # now categorical      'Country', # not helpful            # Only keep ""Total Infected Log10""      'Total Deaths','Total Infected','Total Active','Total Recovered',       ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Deaths Ratio""  ], axis=1)    codiv_country_analyze.sample(10)",1
"ASSIGN = codiv_country.copy() ASSIGN['Quarantine_cat'] = ASSIGN['Quarantine'].notnull().astype(int) ASSIGN['Restrictions_cat'] = ASSIGN['Restrictions'].notnull().astype(int) ASSIGN['Schools_cat'] = ASSIGN['Schools'].notnull().astype(int) ASSIGN=ASSIGN.drop([ 'Quarantine','Schools','Restrictions', 'Country', 'Total Deaths','Total Infected','Total Active','Total Recovered', ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Total Infected Log10"" ], axis=1) ASSIGN.sample(10)",0,execute_result,"# Working on a copy  codiv_country_analyze = codiv_country.copy()    # Creating categorical variables  codiv_country_analyze['Quarantine_cat'] = codiv_country_analyze['Quarantine'].notnull().astype(int)  codiv_country_analyze['Restrictions_cat'] = codiv_country_analyze['Restrictions'].notnull().astype(int)  codiv_country_analyze['Schools_cat'] = codiv_country_analyze['Schools'].notnull().astype(int)    #   codiv_country_analyze=codiv_country_analyze.drop([      'Quarantine','Schools','Restrictions', # now categorical      'Country', # not helpful      'Total Deaths','Total Infected','Total Active','Total Recovered',       ""Total Deaths Log10"",""Total Recovered Log10"",""Total Active Log10"",""Total Infected Log10""  ], axis=1)    codiv_country_analyze.sample(10)",1
"def rmse(y, y_pred): return np.sqrt(np.mean(np.square(y - y_pred)))",0,not_existent,"# Root mean squared error (RMSE)  def rmse(y, y_pred):      return np.sqrt(np.mean(np.square(y - y_pred)))",1
"ASSIGN=codiv_country_Restrictions_active.shape ASSIGN = np.linspace(0, lendata+40, num=lendata+30) coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70) ASSIGN=max(x_tr_China)-7 ASSIGN = np.polyval(coefs_China, x_values) ASSIGN=int(max(y_tr_China)*1.5)",1,not_existent,"number,lendata=codiv_country_Restrictions_active.shape  x_values = np.linspace(0, lendata+40, num=lendata+30)  coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70)  lendataChina=max(x_tr_China)-7  y_China = np.polyval(coefs_China, x_values)  ymaxchina=int(max(y_tr_China)*1.5)",0
display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT)) +' customers in the list who have more balance than the credit limit assigned. ' +'It may be due to more payament than usage andpath<path>')),0,display_data,display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))               +' customers in the list who have more balance than the credit limit assigned. '               +'It may be due to more payament than usage and/or continuous pre-payment.</h4>')),1
for i in range(7): display(HTML('<h2>Cluster'+str(i)+'<path>')) cc1[cc1.cluster == i].describe(),1,display_data,"#cc1.groupby('cluster').agg({np.min,np.max,np.mean}).T  for i in range(7):      display(HTML('<h2>Cluster'+str(i)+'</h2>'))      cc1[cc1.cluster == i].describe()",0
grafico('Sex'),0,display_data,grafico('Sex'),1
grafico('Pclass'),0,display_data,grafico('Pclass'),1
grafico('SibSp'),0,display_data,grafico('SibSp'),1
grafico('Parch'),0,display_data,grafico('Parch'),1
grafico('Embarked'),0,display_data,grafico('Embarked'),1
grafico('Age'),0,display_data,grafico('Age'),1
"ASSIGN = ['Ticket', 'SibSp', 'Parch'] ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(['PassengerId'], axis=1) ASSIGN = train.drop('Survived', axis=1) ASSIGN = train['Survived'] train_data.shape, target.shape",1,execute_result,"features_drop = ['Ticket', 'SibSp', 'Parch'] train = train.drop(features_drop, axis=1) test = test.drop(features_drop, axis=1) train = train.drop(['PassengerId'], axis=1) train_data = train.drop('Survived', axis=1) target = train['Survived']  train_data.shape, target.shape",0
msno.matrix(df_train.sample(250));,0,display_data,msno.matrix(df_train.sample(250));,1
"ASSIGN = pd.read_csv(""path"") ASSIGN.info()",1,stream,"data = pd.read_csv(""/kaggle/input/fish-market/Fish.csv"")  data.info()",0
"ASSIGN = H.isnull().sum().sort_values(ascending = False) ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False) NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [""ASSIGN"", ""ASSIGN""]) NullValues[NullValues.ASSIGN > 0]",1,execute_result,"Sum = H.isnull().sum().sort_values(ascending = False)  Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)  NullValues = pd.concat([Sum, Percent], axis = 1, keys = [""Sum"", ""Percent""])  NullValues[NullValues.Sum > 0]",0
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",1,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",0
ASSIGN = ASSIGN + ASSIGN + ASSIGN + ASSIGN,1,not_existent,H['TotalPorchArea'] = H['OpenPorchSF'] + H['EnclosedPorch'] + H['3SsnPorch'] + H['ScreenPorch'],0
"CHECKPOINT SETUP def show_batch(dl): for images, labels in dl: ASSIGN = plt.subplots(figsize=(12, 12)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0)) break",0,not_existent,"from torchvision.utils import make_grid  def show_batch(dl):     for images, labels in dl:         fig, ax = plt.subplots(figsize=(12, 12))         ax.set_xticks([]); ax.set_yticks([])         ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))         break",1
"def accuracy(outputs, labels): ASSIGN = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() path(preds)) class ImageClassificationBase(nn.Module): def training_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) return loss def validation_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) ASSIGN = accuracy(out, labels) return {'val_loss': ASSIGN.detach(), 'val_acc': ASSIGN} def validation_epoch_end(self, outputs): ASSIGN = [x['val_loss'] for x in outputs] ASSIGN = torch.stack(batch_losses).mean() ASSIGN = [x['val_acc'] for x in outputs] ASSIGN = torch.stack(batch_accs).mean() return {'val_loss': ASSIGN.item(), 'val_acc': ASSIGN.item()} def epoch_end(self, epoch, result): print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format( epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))",0,not_existent,"def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds))  class ImageClassificationBase(nn.Module):     def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss.detach(), 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format(             epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))",1
plot_accuracies(history),0,not_existent,plot_accuracies(history),1
plot_losses(history),0,not_existent,plot_losses(history),1
plot_lrs(history),0,not_existent,plot_lrs(history),1
"ASSIGN = model.predict_generator(validation_generator, num_of_test_samples) ASSIGN = np.argmax(Y_pred, axis=1) ASSIGN[200]",1,execute_result,"  Y_pred = model.predict_generator(validation_generator, num_of_test_samples)  y_pred = np.argmax(Y_pred, axis=1)    y_pred[200]   ",0
rei.to_csv('final'),0,not_existent,rei.to_csv('final'),1
"twoplot(train, 'magic1')",0,display_data,"twoplot(train, 'magic1')",1
"twoplot(train, 'magic2')",0,display_data,"twoplot(train, 'magic2')",1
"twoplot(train, 'magic3')",0,display_data,"twoplot(train, 'magic3')",1
"twoplot(train, 'magic4')",0,display_data,"twoplot(train, 'magic4')",1
"twoplot(train, 'Duration')",0,display_data,"twoplot(train, 'Duration')",1
"twoplot(train, 'StartTime')",0,display_data,"twoplot(train, 'StartTime')",1
"twoplot(train, 'EndTime')",0,display_data,"twoplot(train, 'EndTime')",1
bucket_counts.sum(axis=1).plot.bar(),0,execute_result,bucket_counts.sum(axis=1).plot.bar(),1
"ASSIGN = corona_data.groupby('ObservationDate').sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='ObservationDate', ASSIGN=['Confirmed', 'Recovered', 'Deaths'], ASSIGN='Ratio', ASSIGN='Value') ASSIGN = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio', ASSIGN='Confirmed cases, Recovered cases, and Death Over Time') ASSIGN.show()",0,display_data,"line_data = corona_data.groupby('ObservationDate').sum().reset_index()    line_data = line_data.melt(id_vars='ObservationDate',                    value_vars=['Confirmed',                                'Recovered',                                'Deaths'],                    var_name='Ratio',                    value_name='Value')    line_fig = px.line(line_data, x=""ObservationDate"", y=""Value"", line_shape=""spline"",color='Ratio',                 title='Confirmed cases, Recovered cases, and Death Over Time')  line_fig.show()",1
for i in Ktrain_cat1: encode(Ktrain[i]) for i in Ktest_cat1: encode(Ktest[i]),0,not_existent,for i in Ktrain_cat1:      encode(Ktrain[i])  for i in Ktest_cat1:      encode(Ktest[i]),1
for i in Ktrain_cat2: encode(Ktrain[i]) for i in Ktest_cat2: encode(Ktest[i]),0,not_existent,for i in Ktrain_cat2:      encode(Ktrain[i])  for i in Ktest_cat2:      encode(Ktest[i]),1
"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')",1,not_existent,"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')",0
ShowTrainingData2(5),0,display_data,ShowTrainingData2(5),1
df[df.x==0],1,execute_result,df[df.x==0],0
"ShowPredictions(""CNN"")",0,display_data,"  ShowPredictions(""CNN"")   ",1
"def display_all(df): ''' input: dataframe description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen ''' with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers. display(df)",0,not_existent,"def display_all(df):      '''      input: dataframe      description: it takes a dataframe and allows use to show a mentioned no. of rows and columns in the screen      '''      with pd.option_context(""display.max_rows"",10,""display.max_columns"",9):  #you might want to change these numbers.          display(df)",1
display_all(df),0,display_data,display_all(df),1
df.describe(include='all'),1,execute_result,#Now including all the features df.describe(include='all'),0
df.skew(),1,execute_result,df.skew(),0
"df.corr().style.background_gradient(cmap=""Reds"")",1,execute_result,"df.corr().style.background_gradient(cmap=""Reds"")",0
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Price""].isnull()].index)",0,not_existent,"# Drop the row with NaN values in the ""Price"" column  app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Price""].isnull()].index)",1
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Languages""].isnull()].index)",0,not_existent,"# Drop the rows with NaN values in the ""Languages"" column  app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Languages""].isnull()].index)",1
"ASSIGN = [year for year in range(2014,2019)] for year in ASSIGN: ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum() ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum() print(""In {year}, {percentage}% games were produced from August to December."" .format(year=year, ASSIGN=round((from_Augustpath)*100, 1)))",1,stream,"#Make a list of years from 2014 to 2018  years_lst = [year for year in range(2014,2019)]    #For loop to get a picture of the amount of games produced from August to December  for year in years_lst:      from_August = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()      total = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum()      print(""In {year}, {percentage}% games were produced from August to December.""            .format(year=year,                    percentage=round((from_August/total)*100, 1)))",0
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",1,execute_result,"#Amount of games that have only the English language  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",0
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",1,execute_result,"#Amount of games that have only one language and is not English  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",0
df['CHAS'].value_counts(dropna=False),1,execute_result,df['CHAS'].value_counts(dropna=False),0
"client.list_rows(table, max_results=5).to_dataframe()",1,execute_result,"# Write the code here to explore the data so you can find the answer  client.list_rows(table, max_results=5).to_dataframe()",0
"SETUP SLICE=SLICE.fillna(mode(SLICE)) full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) ASSIGN = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()",1,execute_result,"#fillna from statistics import mode full['Embarked']=full['Embarked'].fillna(mode(full['Embarked']))  full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) full['Age'] = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()",0
"ASSIGN=pd.get_dummies(data=ASSIGN,columns=['Sex','Embarked'],drop_first=True) ASSIGN.info()",1,stream,"full=pd.get_dummies(data=full,columns=['Sex','Embarked'],drop_first=True) full.info()",0
preprocessing.StandardScaler().fit(full).transform(full.astype(float)),0,execute_result,#Data Standardization  preprocessing.StandardScaler().fit(full).transform(full.astype(float)),1
"ASSIGN = full[full['Survived'].isna()].drop(['Survived'], axis = 1) ASSIGN = full[full['Survived'].notna()] ASSIGN.info()",1,stream,"test = full[full['Survived'].isna()].drop(['Survived'], axis = 1) train = full[full['Survived'].notna()] train.info() ",0
"X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']] ASSIGN=train[['Survived']].astype(np.int8)",1,not_existent,"   X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]  y=train[['Survived']].astype(np.int8)",0
"ASSIGN = pd.DataFrame({ 'hidden_layer': hidden_layer_sizes, 'Score': Scores}) ASSIGN.sort_values(by='Score', ascending=False )",1,execute_result,"models = pd.DataFrame({     'hidden_layer': hidden_layer_sizes,     'Score': Scores}) models.sort_values(by='Score', ascending=False )   ",0
CHECKPOINT path,0,error,/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/a/color_0_0002.png,1
"plt.scatter(shyness_score, friendship_len)",0,execute_result,"plt.scatter(shyness_score, friendship_len)",1
def func(x): return -(x*x)+5,0,not_existent,def func(x):      return -(x*x)+5,1
compare_models(),0,display_data,compare_models(),1
"SETUP ASSIGN = pd.read_csv( ASSIGN='..path', ASSIGN=None, ASSIGN=',') ASSIGN.columns=['A', 'B', 'C', 'D', 'class'] ASSIGN.dropna(how=""all"", inplace=True) # drops the empty line at file-end ASSIGN.tail()",1,execute_result,"import pandas as pd    df = pd.read_csv(      filepath_or_buffer='../input/Seed_Data.csv',      header=None,      sep=',')    df.columns=['A', 'B', 'C', 'D', 'class']  df.dropna(how=""all"", inplace=True) # drops the empty line at file-end    df.tail()",0
corr['blueWins'].sort_values(ascending=False),1,execute_result,corr['blueWins'].sort_values(ascending=False),0
"ASSIGN = ASSIGN + ASSIGN train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()",1,execute_result,"train[""RelativesOnboard""] = train[""SibSp""] + train[""Parch""]  # train[[""RelativesOnboard"", ""Survived""]].groupby(['RelativesOnboard']).mean()  train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()",0
"ASSIGN = ""accuracy"" for model_name in models.keys(): ASSIGN = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100, ASSIGN=ASSIGN, cv=5, verbose=2, random_state=42, n_jobs=-1) ASSIGN.fit(X_train, y_train) ASSIGN = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10, ASSIGN=ASSIGN, verbose=0, n_jobs=-1) ASSIGN = scores.mean() ASSIGN = scores.std() ASSIGN = grid.score(X_test_val, y_test_val) ASSIGN = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores, 'CV Mean': ASSIGN, 'CV STDEV': ASSIGN} ASSIGN = grid.best_estimator_.fit(X_train_val, y_train_val) ASSIGN.score(X_test_val, y_test_val) ASSIGN = clf.predict(X_test_val) ASSIGN = classification_report(y_test_val, y_pred) save(ASSIGN, ASSIGN, ASSIGN, name=""titanic_""+model_name+""_02"", cv_scores=ASSIGN)",0,stream,"scoring = ""accuracy""      for model_name in models.keys():      grid = RandomizedSearchCV(models[model_name], param_distributions=randomized_params[model_name], n_iter=100,                                    scoring=scoring, cv=5, verbose=2, random_state=42,  n_jobs=-1)      grid.fit(X_train, y_train)        scores = cross_val_score(grid.best_estimator_, X_train_val, y_train_val, cv=10,                               scoring=scoring, verbose=0, n_jobs=-1)        CV_scores = scores.mean()      STDev = scores.std()      Test_scores = grid.score(X_test_val, y_test_val)        cv_score = {'Model_Name': model_name, 'Parameters': grid.best_params_, 'Test_Score': Test_scores,                  'CV Mean': CV_scores, 'CV STDEV': STDev}        clf = grid.best_estimator_.fit(X_train_val, y_train_val)      clf.score(X_test_val, y_test_val)      y_pred = clf.predict(X_test_val)      clf_report = classification_report(y_test_val, y_pred)      save(grid, cv_score, clf_report, name=""titanic_""+model_name+""_02"", cv_scores=scores)",1
"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",1,not_existent,"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",0
"get_split_result(""..path"", test, 1e-6)",0,execute_result,"#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3  get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6)",1
"display(df_belem.shape, df_curitiba.shape)",0,display_data,"#Questão 1  display(df_belem.shape, df_curitiba.shape)",1
"df_belem.set_index('YEAR',inplace=True) df_curitiba.set_index('YEAR',inplace=True) display(df_belem.head()) display(df_curitiba.head())",0,display_data,"#Questão 2  df_belem.set_index('YEAR',inplace=True)  df_curitiba.set_index('YEAR',inplace=True)  display(df_belem.head())  display(df_curitiba.head())",1
"ASSIGN = main_data.groupby([""Region""]).mean() ASSIGN.corr()",1,not_existent,"#ok lets start to change our perspective new_data = main_data.groupby([""Region""]).mean() new_data.corr()",0
"ASSIGN = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == state_confirmed['Deaths']+ state_confirmed['Cured']] ASSIGN = ASSIGN[['Statepath','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']] ASSIGN = ASSIGN.sort_values('ConfirmedIndianNational', ascending=False) ASSIGN['Cured'].count()",1,execute_result,"no_recovery = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] ==                                 state_confirmed['Deaths']+ state_confirmed['Cured']]  no_recovery = no_recovery[['State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]  no_recovery = no_recovery.sort_values('ConfirmedIndianNational', ascending=False)  no_recovery['Cured'].count()",0
"ASSIGN = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'], ASSIGN='Case', value_name='Count') ASSIGN.head() ASSIGN=ex.area(graph, x='Date', y='Count', color='Case', ASSIGN = 'Cases over time', color_discrete_sequence=[cure, deth, acti]) ASSIGN.show()",0,display_data,"graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()  graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],           var_name='Case', value_name='Count')  graph.head()    fig=ex.area(graph, x='Date', y='Count', color='Case',             title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])  fig.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), ASSIGN=""TotalConfirmed"", y=""Statepath"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True),                x=""TotalConfirmed"", y=""State/UnionTerritory"", title='Confirmed Cases', text='TotalConfirmed', orientation='h',                width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])  Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Confirmed_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), ASSIGN=""Deaths"", y=""Statepath"", title='Death in each state', text='Deaths', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True),                x=""Deaths"", y=""State/UnionTerritory"", title='Death in each state', text='Deaths', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])  Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Death_rate_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), ASSIGN=""Cured"", y=""Statepath"", title='Cured cases', text='Cured', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True),                x=""Cured"", y=""State/UnionTerritory"", title='Cured cases', text='Cured', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])  cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  cure_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), ASSIGN=""Active"", y=""Statepath"", title='Active cases', text='Active', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True),                x=""Active"", y=""State/UnionTerritory"", title='Active cases', text='Active', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Active'])+10])  Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Active_cases.show()",1
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)  Top_50 = latest_date[latest_date['TotalConfirmed']>20]  Top_50 = Top_50.sort_values('Death Rate', ascending=False)    Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),                x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h',                width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')  Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')  Plot.show()",1
pd.options.display.max_rows = 999 pd.options.display.max_columns=999 train.describe(),1,execute_result,pd.options.display.max_rows = 999  pd.options.display.max_columns=999  train.describe(),0
"pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")",0,execute_result,"  pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")",1
"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")",0,execute_result,"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")",1
"pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")",0,execute_result,"  pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")",1
"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"") ASSIGN=train[[""SalePrice"",""MiscVal""]] ASSIGN.corr()",1,execute_result,"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"")  traincor=train[[""SalePrice"",""MiscVal""]]  traincor.corr()",0
"def MSE(y,y_predicted): return ((y- y_predicted)**2).mean()",0,not_existent,"def MSE(y,y_predicted):     return ((y- y_predicted)**2).mean() ",1
"plotTheLineWithData(X,w)",0,not_existent,"plotTheLineWithData(X,w)",1
"SETUP def cross_entropy(y, y_hat): ASSIGN = np.clip(ASSIGN, EPS, 1-EPS) return -np.sum(y * np.log(ASSIGN)path)",0,not_existent,"EPS = 1e-9 #same as in softmax,the first line in this function just gives numerical stability for cross entropy  def cross_entropy(y, y_hat):     y_hat = np.clip(y_hat, EPS, 1-EPS) # for numerical stability     return -np.sum(y * np.log(y_hat)/n) ",1
(train == '?').any(),1,not_existent,(train == '?').any(),0
train[train == '?'].count(),1,not_existent,train[train == '?'].count(),0
"ASSIGN = 20 ASSIGN = np.random.randint(0,len(x_train),n_samples) display_images(x_train[ASSIGN], y_train[ASSIGN])",0,display_data,"n_samples  = 20  idx_sample = np.random.randint(0,len(x_train),n_samples)  display_images(x_train[idx_sample], y_train[idx_sample])",1
