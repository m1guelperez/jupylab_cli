content,tag,output_type,original_content,y_pred
corr['blueWins'].sort_values(ascending=False),1,execute_result,corr['blueWins'].sort_values(ascending=False),0
"pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")",0,execute_result,"  pl.plot(train[""YearBuilt""], train[""GarageYrBlt""], ""o"")",1
"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")",0,execute_result,"pl.plot(train[""TotalBsmtSF""], train[""1stFlrSF""], ""o"")",1
"pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")",0,execute_result,"  pl.plot(train[""GarageCars""], train[""GarageArea""], ""o"")",1
"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"") ASSIGN=train[[""SalePrice"",""MiscVal""]] ASSIGN.corr()",1,execute_result,"pl.plot(train[""SalePrice""], train[""MiscVal""], ""o"")  traincor=train[[""SalePrice"",""MiscVal""]]  traincor.corr()",0
y_shaped[6][0] -predicted[6][0],1,execute_result,y_shaped[6][0] -predicted[6][0],0
"ASSIGN = pd.read_csv(""path"") ASSIGN.info()",1,stream,"data = pd.read_csv(""/kaggle/input/fish-market/Fish.csv"")  data.info()",0
"ASSIGN = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] == state_confirmed['Deaths']+ state_confirmed['Cured']] ASSIGN = ASSIGN[['Statepath','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']] ASSIGN = ASSIGN.sort_values('ConfirmedIndianNational', ascending=False) ASSIGN['Cured'].count()",1,execute_result,"no_recovery = state_confirmed[state_confirmed['ConfirmedIndianNational']+ state_confirmed['ConfirmedForeignNational'] ==                                 state_confirmed['Deaths']+ state_confirmed['Cured']]  no_recovery = no_recovery[['State/UnionTerritory','ConfirmedIndianNational','ConfirmedForeignNational','Deaths','Cured']]  no_recovery = no_recovery.sort_values('ConfirmedIndianNational', ascending=False)  no_recovery['Cured'].count()",0
"ASSIGN = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index() ASSIGN = ASSIGN.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'], ASSIGN='Case', value_name='Count') ASSIGN.head() ASSIGN=ex.area(graph, x='Date', y='Count', color='Case', ASSIGN = 'Cases over time', color_discrete_sequence=[cure, deth, acti]) ASSIGN.show()",0,display_data,"graph = df_india.groupby('Date')['Cured', 'Deaths', 'Active'].sum().reset_index()  graph = graph.melt(id_vars='Date', value_vars=['Cured', 'Deaths', 'Active'],           var_name='Case', value_name='Count')  graph.head()    fig=ex.area(graph, x='Date', y='Count', color='Case',             title = 'Cases over time', color_discrete_sequence=[cure, deth, acti])  fig.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True), ASSIGN=""TotalConfirmed"", y=""Statepath"", title='Confirmed Cases', text='TotalConfirmed', orientation='h', ASSIGN=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Confirmed_bar = ex.bar(latest_date.sort_values('TotalConfirmed', ascending=False).head(30).sort_values('TotalConfirmed', ascending=True),                x=""TotalConfirmed"", y=""State/UnionTerritory"", title='Confirmed Cases', text='TotalConfirmed', orientation='h',                width=900, height=700, range_x = [0, max(latest_date['TotalConfirmed'])+15])  Confirmed_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Confirmed_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True), ASSIGN=""Deaths"", y=""Statepath"", title='Death in each state', text='Deaths', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Death_rate_bar = ex.bar(latest_date.sort_values('Deaths', ascending=False).head(30).sort_values('Deaths', ascending=True),                x=""Deaths"", y=""State/UnionTerritory"", title='Death in each state', text='Deaths', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Deaths'])+0.5])  Death_rate_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Death_rate_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True), ASSIGN=""Cured"", y=""Statepath"", title='Cured cases', text='Cured', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Cured'])+4]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"cure_bar = ex.bar(latest_date.sort_values('Cured', ascending=False).head(30).sort_values('Cured', ascending=True),                x=""Cured"", y=""State/UnionTerritory"", title='Cured cases', text='Cured', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Cured'])+4])  cure_bar.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  cure_bar.show()",1
"ASSIGN = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True), ASSIGN=""Active"", y=""Statepath"", title='Active cases', text='Active', orientation='h', ASSIGN=800, height=700, range_x = [0, max(latest_date['Active'])+10]) ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"Active_cases = ex.bar(latest_date.sort_values('Active', ascending=False).head(30).sort_values('Active', ascending=True),                x=""Active"", y=""State/UnionTerritory"", title='Active cases', text='Active', orientation='h',                width=800, height=700, range_x = [0, max(latest_date['Active'])+10])  Active_cases.update_traces(marker_color='#46cdcf', opacity=0.8, textposition='outside')  Active_cases.show()",1
"latest_date['Death Rate'] = round((latest_date['Deaths']path['TotalConfirmed'])*20,2) Top_50 = latest_date[latest_date['TotalConfirmed']>20] Top_50 = Top_50.sort_values('Death Rate', ascending=False) ASSIGN = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True), ASSIGN=""Death Rate"", y=""Statepath"", text='Death Rate', orientation='h', ASSIGN=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case') ASSIGN.update_traces(marker_color=' ASSIGN.show()",0,display_data,"latest_date['Death Rate'] = round((latest_date['Deaths']/latest_date['TotalConfirmed'])*20,2)  Top_50 = latest_date[latest_date['TotalConfirmed']>20]  Top_50 = Top_50.sort_values('Death Rate', ascending=False)    Plot = ex.bar(Top_50.sort_values('Death Rate', ascending=False).head(20).sort_values('Death Rate', ascending=True),                x=""Death Rate"", y=""State/UnionTerritory"", text='Death Rate', orientation='h',                width=500, height=500, range_x = [0, 2], title='No. of Deaths Per 20 Confirmed Case')  Plot.update_traces(marker_color='#00a8cc', opacity=0.6, textposition='outside')  Plot.show()",1
ASSIGN = pd.to_datetime(ASSIGN ) ASSIGN = ASSIGN.set_index('date') display(ASSIGN.describe()),0,display_data,# Convert the date column to a datestamp type  meat['date'] = pd.to_datetime(meat['date'] )    # Set the date column as the index of your DataFrame meat  meat = meat.set_index('date')    # Print the summary statistics of the DataFrame  display(meat.describe()),1
"ASSIGN = ['country','year','sex','age','generation'] count_unique_values(df_train[ASSIGN])",0,not_existent,"# Check how many unique values in categorical variable category_var = ['country','year','sex','age','generation'] count_unique_values(df_train[category_var])",1
SETUP SETUP os.mkdir(TRAINING_DIR) os.mkdir(TRAINING_CAT_DIR) os.mkdir(TRAINING_DOG_DIR) os.mkdir(VALIDATION_DIR) os.mkdir(VALIDATION_CAT_DIR) os.mkdir(VALIDATION_DOG_DIR),0,not_existent,### Make directory for ImageGenerator import os TRAINING_DIR = '/kaggle/training/' TRAINING_CAT_DIR = '/kaggle/training/cat/' TRAINING_DOG_DIR = '/kaggle/training/dog/' os.mkdir(TRAINING_DIR) os.mkdir(TRAINING_CAT_DIR) os.mkdir(TRAINING_DOG_DIR)  VALIDATION_DIR = '/kaggle/validation/' VALIDATION_CAT_DIR = '/kaggle/validation/cat/' VALIDATION_DOG_DIR = '/kaggle/validation/dog/' os.mkdir(VALIDATION_DIR) os.mkdir(VALIDATION_CAT_DIR) os.mkdir(VALIDATION_DOG_DIR),1
SETUP ASSIGN = os.listdir(SOURCE_DIR) ASSIGN = [fn for fn in list_fn if 'cat' in fn] ASSIGN = [fn for fn in list_fn if 'dog' in fn],0,not_existent,### List file name in train dataset SOURCE_DIR = '/kaggle/working/train/' list_fn = os.listdir(SOURCE_DIR) list_cat_fn = [fn for fn in list_fn if 'cat' in fn] list_dog_fn = [fn for fn in list_fn if 'dog' in fn],1
"SETUP ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input) ASSIGN = ImageDataGenerator(rescale=1path, preprocessing_function=prec_input) ASSIGN = train_datagen.flow_from_directory(TRAINING_DIR, ASSIGN=BATCH_SIZE, ASSIGN='bicubic', ASSIGN='categorical', ASSIGN=True, ASSIGN=(HEIGHT, WIDTH)) ASSIGN = val_datagen.flow_from_directory(VALIDATION_DIR, ASSIGN=BATCH_SIZE, ASSIGN='bicubic', ASSIGN='categorical', ASSIGN=False, ASSIGN=(HEIGHT, WIDTH))",0,not_existent,"### Preprocess the image from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Define generator train_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input) val_datagen = ImageDataGenerator(rescale=1/255, preprocessing_function=prec_input)  # Define flow for train gen train_gen = train_datagen.flow_from_directory(TRAINING_DIR,                                               batch_size=BATCH_SIZE,                                               interpolation='bicubic',                                               class_mode='categorical',                                               shuffle=True,                                               target_size=(HEIGHT, WIDTH))  # Define flow for validation gen val_gen = val_datagen.flow_from_directory(VALIDATION_DIR,                                           batch_size=BATCH_SIZE,                                           interpolation='bicubic',                                           class_mode='categorical',                                           shuffle=False,                                           target_size=(HEIGHT, WIDTH))",1
"ASSIGN = pd.read_csv('path', names=['label','id','time','query','handle','text']) ASSIGN = ASSIGN.sample(frac=1).reset_index(drop=True) ASSIGN = list(data['text']) ASSIGN = list(data['ASSIGN'].map({0:0, 4:1}))",1,not_existent,"### Load the data data = pd.read_csv('/tmp/training_cleaned.csv', names=['label','id','time','query','handle','text']) data = data.sample(frac=1).reset_index(drop=True) sentences = list(data['text']) label = list(data['label'].map({0:0, 4:1}))",0
plot_history(history_simple),0,not_existent,### Plot simple model performance plot_history(history_simple),1
plot_history(history_single_lstm),0,display_data,### Plot simple model performance plot_history(history_single_lstm),1
plot_history(history_single_gru),0,not_existent,### Plot simple model performance plot_history(history_single_gru),1
plot_history(history_single_conv),0,not_existent,### Plot simple model performance plot_history(history_single_conv),1
plot_history(history),0,not_existent,### Plot simple model performance plot_history(history),1
plot_history(history),0,display_data,### Plot model performance - LSTM no Bidirection plot_history(history),1
(train == '?').any(),1,not_existent,(train == '?').any(),0
train[train == '?'].count(),1,not_existent,train[train == '?'].count(),0
"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')",1,not_existent,"plt.figure(figsize=(15,10)) (train[train['ham'] == True].mean() - train[train['ham'] == False].mean())[train.columns[:54]].plot(kind = 'bar')",0
"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",1,not_existent,"train_hna[(train_hna == 'yes') | (train_hna == 'no')].dropna(axis = 'columns', how='all')",0
grafico('Sex'),0,display_data,grafico('Sex'),1
grafico('Pclass'),0,display_data,grafico('Pclass'),1
grafico('SibSp'),0,display_data,grafico('SibSp'),1
grafico('Parch'),0,display_data,grafico('Parch'),1
grafico('Embarked'),0,display_data,grafico('Embarked'),1
grafico('Age'),0,display_data,grafico('Age'),1
"ASSIGN = ['Ticket', 'SibSp', 'Parch'] ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(features_drop, axis=1) ASSIGN = ASSIGN.drop(['PassengerId'], axis=1) ASSIGN = train.drop('Survived', axis=1) ASSIGN = train['Survived'] train_data.shape, target.shape",1,execute_result,"features_drop = ['Ticket', 'SibSp', 'Parch'] train = train.drop(features_drop, axis=1) test = test.drop(features_drop, axis=1) train = train.drop(['PassengerId'], axis=1) train_data = train.drop('Survived', axis=1) target = train['Survived']  train_data.shape, target.shape",0
"display(df_belem.shape, df_curitiba.shape)",0,display_data,"#Questão 1  display(df_belem.shape, df_curitiba.shape)",1
"df_belem.set_index('YEAR',inplace=True) df_curitiba.set_index('YEAR',inplace=True) display(df_belem.head()) display(df_curitiba.head())",0,display_data,"#Questão 2  df_belem.set_index('YEAR',inplace=True)  df_curitiba.set_index('YEAR',inplace=True)  display(df_belem.head())  display(df_curitiba.head())",1
display(df_belem['JAN'].value_counts()) display(df_curitiba['JAN'].value_counts()),0,display_data,#mostra a quantidade de valores únicos no mẽs de janeiro  #verificando os valores únicos confirmamos que o valor 999.90 é o único outlier  display(df_belem['JAN'].value_counts())  display(df_curitiba['JAN'].value_counts()),1
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Price""].isnull()].index)",0,not_existent,"# Drop the row with NaN values in the ""Price"" column  app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Price""].isnull()].index)",1
"ASSIGN = ASSIGN.drop(ASSIGN.loc[ASSIGN[""Languages""].isnull()].index)",0,not_existent,"# Drop the rows with NaN values in the ""Languages"" column  app_df_cut = app_df_cut.drop(app_df_cut.loc[app_df_cut[""Languages""].isnull()].index)",1
"ASSIGN = [year for year in range(2014,2019)] for year in ASSIGN: ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum() ASSIGN = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum() print(""In {year}, {percentage}% games were produced from August to December."" .format(year=year, ASSIGN=round((from_Augustpath)*100, 1)))",1,stream,"#Make a list of years from 2014 to 2018  years_lst = [year for year in range(2014,2019)]    #For loop to get a picture of the amount of games produced from August to December  for year in years_lst:      from_August = app_df_clean[""Original Release Date""].apply(lambda date: (date.year == year) & (date.month >= 8)).sum()      total = app_df_clean[""Original Release Date""].apply(lambda date: date.year == year).sum()      print(""In {year}, {percentage}% games were produced from August to December.""            .format(year=year,                    percentage=round((from_August/total)*100, 1)))",0
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",1,execute_result,"#Amount of games that have only the English language  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] == ""EN"")])",0
"len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",1,execute_result,"#Amount of games that have only one language and is not English  len(app_df_clean[(app_df_clean[""numLang""] == 1) & (app_df_clean[""Languages""] != ""EN"")])",0
ASSIGN = pd.DataFrame({'Id' : list(range(len(predictions)))}) ASSIGN = pd.DataFrame({'ASSIGN' : predictions}) ASSIGN = income,0,not_existent,id_index = pd.DataFrame({'Id' : list(range(len(predictions)))})  income = pd.DataFrame({'income' : predictions})  result = income,1
"ASSIGN = ASSIGN + ASSIGN train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()",1,execute_result,"train[""RelativesOnboard""] = train[""SibSp""] + train[""Parch""]  # train[[""RelativesOnboard"", ""Survived""]].groupby(['RelativesOnboard']).mean()  train[[""RelativesOnboard"", ""Survived""]].groupby([""RelativesOnboard""]).mean()",0
"ASSIGN = random_forest_grid[""model""].best_estimator_.fit(X_train, y_train)",0,not_existent,"random_forest = random_forest_grid[""model""].best_estimator_.fit(X_train, y_train)",1
ASSIGN = 0 ASSIGN = 0 ASSIGN = 0.01 ASSIGN = 10000 ASSIGN = float(len(x)),0,not_existent,b0 = 0  b1 = 0  alpha = 0.01  count = 10000  n = float(len(x)),1
"twoplot(train, 'magic1')",0,display_data,"twoplot(train, 'magic1')",1
"twoplot(train, 'magic2')",0,display_data,"twoplot(train, 'magic2')",1
"twoplot(train, 'magic3')",0,display_data,"twoplot(train, 'magic3')",1
"twoplot(train, 'magic4')",0,display_data,"twoplot(train, 'magic4')",1
"twoplot(train, 'Duration')",0,display_data,"twoplot(train, 'Duration')",1
"twoplot(train, 'StartTime')",0,display_data,"twoplot(train, 'StartTime')",1
"twoplot(train, 'EndTime')",0,display_data,"twoplot(train, 'EndTime')",1
"ASSIGN = H.isnull().sum().sort_values(ascending = False) ASSIGN = ((H.isnull().sum()*100)path()[0]).sort_values(ascending = False) NullValues = pd.concat([ASSIGN, ASSIGN], axis = 1, keys = [""ASSIGN"", ""ASSIGN""]) NullValues[NullValues.ASSIGN > 0]",1,execute_result,"Sum = H.isnull().sum().sort_values(ascending = False)  Percent = ((H.isnull().sum()*100)/H.count()[0]).sort_values(ascending = False)  NullValues = pd.concat([Sum, Percent], axis = 1, keys = [""Sum"", ""Percent""])  NullValues[NullValues.Sum > 0]",0
"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",1,execute_result,"H[H[""GarageArea""] == 0][['GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageQual','GarageCond']]",0
ASSIGN = ASSIGN + ASSIGN + ASSIGN + ASSIGN,1,not_existent,H['TotalPorchArea'] = H['OpenPorchSF'] + H['EnclosedPorch'] + H['3SsnPorch'] + H['ScreenPorch'],0
"ASSIGN = Sequential() ASSIGN.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Convolution2D(32, (3, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Convolution2D(64, (3, 3), padding='valid')) ASSIGN.add(Activation('relu')) ASSIGN.add(MaxPooling2D(pool_size=(2, 2))) ASSIGN.add(Flatten()) ASSIGN.add(Dense(64)) ASSIGN.add(Activation('relu')) ASSIGN.add(Dropout(0.5)) ASSIGN.add(Dense(5)) ASSIGN.add(Activation('softmax')) ASSIGN.summary() ASSIGN.compile(loss='categorical_crossentropy', ASSIGN='rmsprop', ASSIGN=['accuracy'])",1,stream,"# CNN model    model = Sequential()  model.add(Convolution2D(32, (3, 3), input_shape=(img_rows, img_cols, 3), padding='valid'))  model.add(Activation('relu'))  model.add(MaxPooling2D(pool_size=(2, 2)))    model.add(Convolution2D(32, (3, 3), padding='valid'))  model.add(Activation('relu'))  model.add(MaxPooling2D(pool_size=(2, 2)))    model.add(Convolution2D(64, (3, 3), padding='valid'))  model.add(Activation('relu'))  model.add(MaxPooling2D(pool_size=(2, 2)))    model.add(Flatten())  model.add(Dense(64))  model.add(Activation('relu'))  model.add(Dropout(0.5))  model.add(Dense(5))  model.add(Activation('softmax'))    model.summary()    model.compile(loss='categorical_crossentropy',                optimizer='rmsprop',                metrics=['accuracy'])",0
"ASSIGN = model.predict_generator(validation_generator, num_of_test_samples) ASSIGN = np.argmax(Y_pred, axis=1) ASSIGN[200]",1,execute_result,"  Y_pred = model.predict_generator(validation_generator, num_of_test_samples)  y_pred = np.argmax(Y_pred, axis=1)    y_pred[200]   ",0
rei.to_csv('final'),0,not_existent,rei.to_csv('final'),1
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"# columns with null values df_isna = pd.DataFrame(df.isnull().sum()) df_isna.loc[(df_isna.loc[:, df_isna.dtypes != object] != 0).any(1)]",0
"ASSIGN = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) ASSIGN.sample(5)",1,not_existent,"df_hid = pd.DataFrame(hid_heads, index=None, columns=['idhogar','parentesco1']) df_hid.sample(5)",0
df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),0,not_existent,df['v2a1'].loc[-df['idhogar'].isin(hid_wo_heads)].hist(),1
df_hwoh['v2a1'].hist(),0,not_existent,df_hwoh['v2a1'].hist(),1
df['v2a1'].describe().plot(),0,not_existent,df['v2a1'].describe().plot(),1
"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()",0,not_existent,"df[['v18q1', 'rez_esc', 'meaneduc', 'SQBmeaned']].describe().plot()",1
df['v2a1'].max(),1,not_existent,df['v2a1'].max(),0
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",1,not_existent,"df[['v2a1','idhogar','parentesco1','Target']].loc[df.v2a1 >= 1000000]",0
"df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",1,not_existent,"# remove these two rows... df[['v2a1','idhogar','parentesco1','Target']].loc[df.idhogar == '563cc81b7']",0
set(df.dtypes),1,not_existent,set(df.dtypes),0
"ASSIGN = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] ASSIGN = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a', 'tamhog', 'hacdor', 'hacapo', 'r4t3', ] ASSIGN = ['age', 'agesq', 'female', 'male',] ASSIGN = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin', 'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] ASSIGN = ['abastaguadentro', 'abastaguafuera', 'abastaguano',] ASSIGN = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] ASSIGN = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] ASSIGN = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] ASSIGN = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] ASSIGN = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] ASSIGN = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] ASSIGN = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] ASSIGN = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother', 'paredpreb', 'paredzinc', 'paredzocalo',] ASSIGN = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9',] ASSIGN = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] ASSIGN = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7',] ASSIGN = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] ASSIGN = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] ASSIGN = [ 'eviv1', 'eviv2', 'eviv3',] ASSIGN = [ 'etecho1', 'etecho2', 'etecho3',] ASSIGN = [ 'epared1', 'epared2', 'epared3',] ASSIGN = [ 'dis', 'escolari', 'meaneduc', 'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] ASSIGN = ['coopele', 'noelec', 'planpri', 'public',] ASSIGN = cols_electronics+cols_house_details+cols_person_details+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ ASSIGN+ASSIGN+ASSIGN+ASSIGN+ASSIGN+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec len(ASSIGN)",1,not_existent,"cols_electronics = ['refrig','mobilephone','television','qmobilephone','computer', 'v18q', 'v18q1', ] cols_house_details = ['v2a1', 'area1', 'area2', 'bedrooms','rooms', 'cielorazo', 'v14a',                      'tamhog', 'hacdor', 'hacapo', 'r4t3', ] cols_person_details = ['age', 'agesq', 'female', 'male',] cols_SQ = ['SQBage', 'SQBdependency', 'SQBedjefe', 'SQBescolari', 'SQBhogar_nin',             'SQBhogar_total', 'SQBmeaned', 'SQBovercrowding',] cols_water = ['abastaguadentro', 'abastaguafuera', 'abastaguano',]  cols_h = [ 'hhsize', 'hogar_adul', 'hogar_mayor', 'hogar_nin', 'hogar_total',] cols_r = ['r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3',] cols_tip = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',] cols_roof = ['techocane', 'techoentrepiso', 'techootro', 'techozinc',] cols_floor = ['pisocemento', 'pisomadera', 'pisomoscer', 'pisonatur', 'pisonotiene', 'pisoother',] cols_sanitary = [ 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6',] cols_parents = [ 'parentesco1', 'parentesco10', 'parentesco11', 'parentesco12', 'parentesco2', 'parentesco3',                 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9',] cols_outside_wall = [ 'paredblolad', 'pareddes', 'paredfibras', 'paredmad', 'paredother',                'paredpreb', 'paredzinc', 'paredzocalo',] cols_instlevel = [ 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6',                   'instlevel7', 'instlevel8', 'instlevel9',] cols_lugar = [ 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6',] cols_estadoc = [ 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4',                  'estadocivil5', 'estadocivil6', 'estadocivil7',] cols_elim = ['elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6',] cols_energ = ['energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4',] cols_eviv = [ 'eviv1', 'eviv2', 'eviv3',] cols_etech = [ 'etecho1', 'etecho2', 'etecho3',] cols_pared = [ 'epared1', 'epared2', 'epared3',] cols_unknown = [ 'dis', 'escolari', 'meaneduc',                  'overcrowding', 'rez_esc', 'tamhog', 'tamviv', ] cols_elec = ['coopele', 'noelec', 'planpri', 'public',]  total_features = cols_electronics+cols_house_details+cols_person_details+\ cols_SQ+cols_water+cols_h+cols_r+cols_tip+cols_roof+\ cols_floor+cols_sanitary+cols_parents+cols_outside_wall+\ cols_instlevel+cols_lugar+cols_estadoc+cols_elim+cols_energ+\ cols_eviv+cols_etech+cols_pared+cols_unknown+cols_elec  len(total_features)",0
df[cols_electronics].plot.area(),0,not_existent,df[cols_electronics].plot.area(),1
ASSIGN = cols_electronics.append('Target') df[cols_electronics].corr(),1,not_existent,cols_electronics_target = cols_electronics.append('Target') df[cols_electronics].corr(),0
"df[['tamhog','r4t3', 'tamviv']].corr()",1,not_existent,"# high correlation between  # no. of persons in the household, # persons living in the household  # and size of the household # we can use any one...!! df[['tamhog','r4t3', 'tamviv']].corr()",0
"df[['r4t3','tamviv']].corr()",1,not_existent,"df[['r4t3','tamviv']].corr()",0
df.groupby('Target').overcrowding.value_counts().unstack().plot.bar(),0,not_existent,df.groupby('Target').overcrowding.value_counts().unstack().plot.bar(),1
"ASSIGN = pd.DataFrame(df.isnull().sum()) ASSIGN.loc[(ASSIGN.loc[:, ASSIGN.dtypes != object] != 0).any(1)]",1,not_existent,"ff = pd.DataFrame(df.isnull().sum()) ff.loc[(ff.loc[:, ff.dtypes != object] != 0).any(1)]",0
"df[['v2a1','Target']].corr()",1,not_existent,"df[['v2a1','Target']].corr()",0
"df[['v2a1','Target']].fillna(0).corr()",1,not_existent,"df[['v2a1','Target']].fillna(0).corr()",0
"df[['hhsize','Target']].corr()",1,not_existent,"df[['hhsize','Target']].corr()",0
"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",0,not_existent,"df.groupby('idhogar').sum()[['hogar_adul','hogar_total']].sample(10).plot.bar()",1
total_features.remove('r4t3'),0,not_existent,"# removing 'r4t3', as 'hhsize' is of almost same distribution total_features.remove('r4t3')",1
total_features.append('edjefa') total_features.append('edjefe'),0,not_existent,total_features.append('edjefa') total_features.append('edjefe'),1
df['v2a1'].isna().sum(),1,not_existent,df['v2a1'].isna().sum(),0
df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],1,not_existent,df.loc[(df['v2a1'].isna()) & (df.tipovivi3 == 1)],0
"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",1,not_existent,"df['v2a1'].loc[df.parentesco1 == 1].mean(), df['v2a1'].loc[df.parentesco1 == 1].max(), df['v2a1'].loc[df.parentesco1 == 1].min()",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 == 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0
"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",1,not_existent,"df[['v2a1','idhogar','parentesco1']].loc[(df.parentesco1 != 1) & (-df.v2a1.isna()) & (df.tipovivi3==1) & (df.tipovivi1==0)].mean()",0
gc.collect(),0,not_existent,gc.collect(),1
"VALIDATION SETUP def show_batch(dl): for images, labels in dl: ASSIGN = plt.subplots(figsize=(12, 12)) ax.set_xticks([]); ax.set_yticks([]) ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0)) break",0,not_existent,"from torchvision.utils import make_grid  def show_batch(dl):     for images, labels in dl:         fig, ax = plt.subplots(figsize=(12, 12))         ax.set_xticks([]); ax.set_yticks([])         ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))         break",1
"def accuracy(outputs, labels): ASSIGN = torch.max(outputs, dim=1) return torch.tensor(torch.sum(preds == labels).item() path(preds)) class ImageClassificationBase(nn.Module): def training_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) return loss def validation_step(self, batch): ASSIGN = batch ASSIGN = self(images) ASSIGN = F.cross_entropy(out, labels) ASSIGN = accuracy(out, labels) return {'val_loss': ASSIGN.detach(), 'val_acc': ASSIGN} def validation_epoch_end(self, outputs): ASSIGN = [x['val_loss'] for x in outputs] ASSIGN = torch.stack(batch_losses).mean() ASSIGN = [x['val_acc'] for x in outputs] ASSIGN = torch.stack(batch_accs).mean() return {'val_loss': ASSIGN.item(), 'val_acc': ASSIGN.item()} def epoch_end(self, epoch, result): print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format( epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))",0,not_existent,"def accuracy(outputs, labels):     _, preds = torch.max(outputs, dim=1)     return torch.tensor(torch.sum(preds == labels).item() / len(preds))  class ImageClassificationBase(nn.Module):     def training_step(self, batch):         images, labels = batch          out = self(images)                  # Generate predictions         loss = F.cross_entropy(out, labels) # Calculate loss         return loss          def validation_step(self, batch):         images, labels = batch          out = self(images)                    # Generate predictions         loss = F.cross_entropy(out, labels)   # Calculate loss         acc = accuracy(out, labels)           # Calculate accuracy         return {'val_loss': loss.detach(), 'val_acc': acc}              def validation_epoch_end(self, outputs):         batch_losses = [x['val_loss'] for x in outputs]         epoch_loss = torch.stack(batch_losses).mean()   # Combine losses         batch_accs = [x['val_acc'] for x in outputs]         epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies         return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}          def epoch_end(self, epoch, result):         print(""Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}"".format(             epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))",1
plot_accuracies(history),0,not_existent,plot_accuracies(history),1
plot_losses(history),0,not_existent,plot_losses(history),1
plot_lrs(history),0,not_existent,plot_lrs(history),1
display_all(df),0,display_data,display_all(df),1
"pd.set_option(""display.max_columns"",2000)  # used for viewing all the columns at onces pd.set_option(""display.max_rows"",85)",0,not_existent,"# To show the all columns  pd.set_option(""display.max_columns"",2000)  # used for viewing all the columns at onces  pd.set_option(""display.max_rows"",85)",1
"ASSIGN = [1, 2, 3, 4, 5, 6] ASSIGN[3]=""four"" print (ASSIGN)",0,stream,"# Add your code here  mylist = [1, 2, 3, 4, 5, 6]    # This will replace the fourth item in this list with ""four""  mylist[3]=""four""  print (mylist)     ",1
df.describe(include='object'),0,execute_result,#Including Categorical features with include object df.describe(include='object'),1
df.skew(),1,execute_result,df.skew(),0
"df.corr().style.background_gradient(cmap=""Reds"")",1,execute_result,"df.corr().style.background_gradient(cmap=""Reds"")",0
ShowTrainingData2(5),0,display_data,ShowTrainingData2(5),1
df[df.x==0],1,execute_result,df[df.x==0],0
"ShowPredictions(""CNN"")",0,display_data,"  ShowPredictions(""CNN"")   ",1
compare_models(),0,display_data,compare_models(),1
"SETUP SLICE=SLICE.fillna(mode(SLICE)) full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) ASSIGN = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()",1,execute_result,"#fillna from statistics import mode full['Embarked']=full['Embarked'].fillna(mode(full['Embarked']))  full['Fare'].fillna(full['Fare'].dropna().median(),inplace=True) full['Age'] = full.groupby(""Pclass"")['Age'].transform(lambda x: x.fillna(x.median())) full.isnull().sum()",0
"ASSIGN=pd.get_dummies(data=ASSIGN,columns=['Sex','Embarked'],drop_first=True) ASSIGN.info()",1,stream,"full=pd.get_dummies(data=full,columns=['Sex','Embarked'],drop_first=True) full.info()",0
"ASSIGN = full[full['Survived'].isna()].drop(['Survived'], axis = 1) ASSIGN = full[full['Survived'].notna()] ASSIGN.info()",1,stream,"test = full[full['Survived'].isna()].drop(['Survived'], axis = 1) train = full[full['Survived'].notna()] train.info() ",0
"X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']] ASSIGN=train[['Survived']].astype(np.int8)",1,not_existent,"   X=train[['Age','Fare','Fam','Pclass','Sex_male','Embarked_Q' ,'Embarked_S']]  y=train[['Survived']].astype(np.int8)",0
"ASSIGN = pd.DataFrame({ 'hidden_layer': hidden_layer_sizes, 'Score': Scores}) ASSIGN.sort_values(by='Score', ascending=False )",1,execute_result,"models = pd.DataFrame({     'hidden_layer': hidden_layer_sizes,     'Score': Scores}) models.sort_values(by='Score', ascending=False )   ",0
msno.matrix(df_train.sample(250));,0,display_data,msno.matrix(df_train.sample(250));,1
"df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()",0,execute_result,"df_train.loc[(df_train['HouseYear'] == 20052011) | (df_train['HouseYear'] == 4968), 'HouseYear'].value_counts()",1
VALIDATION path,0,error,/kaggle/input/asl-rgb-depth-fingerspelling-spelling-it-out/dataset5/C/a/color_0_0002.png,1
"ASSIGN = 0.001 ASSIGN = ""sparse_categorical_crossentropy"" model.compile(Adam(lr=ASSIGN), ASSIGN=ASSIGN ,metrics=['accuracy']) model.summary()",1,stream,"initial_lr = 0.001  loss = ""sparse_categorical_crossentropy""  model.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])  model.summary()",0
"def MSE(y,y_predicted): return ((y- y_predicted)**2).mean()",0,not_existent,"def MSE(y,y_predicted):     return ((y- y_predicted)**2).mean() ",1
"plotTheLineWithData(X,w)",0,not_existent,"plotTheLineWithData(X,w)",1
"SETUP ASSIGN = pd.read_csv( ASSIGN='..path', ASSIGN=None, ASSIGN=',') ASSIGN.columns=['A', 'B', 'C', 'D', 'class'] ASSIGN.dropna(how=""all"", inplace=True) # drops the empty line at file-end ASSIGN.tail()",1,execute_result,"import pandas as pd    df = pd.read_csv(      filepath_or_buffer='../input/Seed_Data.csv',      header=None,      sep=',')    df.columns=['A', 'B', 'C', 'D', 'class']  df.dropna(how=""all"", inplace=True) # drops the empty line at file-end    df.tail()",0
def func(x): return -(x*x)+5,0,not_existent,def func(x):      return -(x*x)+5,1
"ASSIGN=28 ASSIGN=28 def data_prep_X(X): ASSIGN=len(X) ASSIGN=X.values.reshape(num_img,img_row,img_col,1) ASSIGN=x_as_arraypath return X_out",0,not_existent,"img_row=28  img_col=28  def data_prep_X(X):      num_img=len(X)      x_as_array=X.values.reshape(num_img,img_row,img_col,1)      X_out=x_as_array/255      return X_out",1
for col in cat_enc_col: feature.append(col),0,not_existent,for col in cat_enc_col:      feature.append(col),1
"ASSIGN=ASSIGN.drop([ 'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020' ], axis=1) ASSIGN.sample(10)",0,execute_result,"codiv_country=codiv_country.drop([      'Sex Ratio','Crime Index','Density','Tests_per_10kp','Test Pop','sex0','sex14','sex25','sex54','sex64','sex65plus','Total Infected','Total Deaths','Total Recovered','Tests','GDP 2018','Population 2020'  ], axis=1)  codiv_country.sample(10)",1
"ASSIGN=pd.read_csv(os.path.join('.path', 'csse_covid_19_daily_reports.csv')) ASSIGN=ASSIGN.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1) ASSIGN.sample(10)",1,execute_result,"codiv_csse=pd.read_csv(os.path.join('./', 'csse_covid_19_daily_reports.csv'))  codiv_csse=codiv_csse.drop(['FIPS','Admin2','Last_Update','Combined_Key','Long_','Lat'], axis=1)  codiv_csse.sample(10)",0
"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000path[""Infected""] codiv_csse.sample(10)",0,execute_result,"codiv_csse[""Deaths Ratio""]=codiv_csse[""Deaths""]*1000/codiv_csse[""Infected""]  codiv_csse.sample(10)",1
codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],1,execute_result,codiv_country[(codiv_country['Temp_mean_jan_apr'] == 0)],0
"def rmse(y, y_pred): return np.sqrt(np.mean(np.square(y - y_pred)))",0,not_existent,"# Root mean squared error (RMSE)  def rmse(y, y_pred):      return np.sqrt(np.mean(np.square(y - y_pred)))",1
"ASSIGN=codiv_country_Restrictions_active.shape ASSIGN = np.linspace(0, lendata+40, num=lendata+30) coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70) ASSIGN=max(x_tr_China)-7 ASSIGN = np.polyval(coefs_China, x_values) ASSIGN=int(max(y_tr_China)*1.5)",1,not_existent,"number,lendata=codiv_country_Restrictions_active.shape  x_values = np.linspace(0, lendata+40, num=lendata+30)  coefs_China,x_tr_China,y_tr_China,coefs_China_prev,x_tr_China_prev,y_tr_China_prev,StartCountryIndex=Polifq(""China"",shift=0,Version=""quarantine"",trigger=70)  lendataChina=max(x_tr_China)-7  y_China = np.polyval(coefs_China, x_values)  ymaxchina=int(max(y_tr_China)*1.5)",0
"ASSIGN = main_data.groupby([""Region""]).mean() ASSIGN.corr()",1,not_existent,"#ok lets start to change our perspective new_data = main_data.groupby([""Region""]).mean() new_data.corr()",0
"client.list_rows(table, max_results=5).to_dataframe()",1,execute_result,"# Write the code here to explore the data so you can find the answer  client.list_rows(table, max_results=5).to_dataframe()",0
"ASSIGN = X[:,:13] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",1,execute_result,"X_opt = X[:,:13]   regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",0
"ASSIGN =X[:,[1,3,5,7,8,9,10,11,12]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",1,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11,12]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",0
"ASSIGN =X[:,[1,3,5,7,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",1,execute_result,"X_opt =X[:,[1,3,5,7,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",0
"ASSIGN =X[:,[3,5,8,9,10,11]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",1,execute_result,"X_opt =X[:,[3,5,8,9,10,11]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",0
"ASSIGN =X[:,[0,1,4,10,14]] ASSIGN=sm.OLS(endog=y,exog=X_opt).fit() ASSIGN.summary()",1,error,"X_opt =X[:,[0,1,4,10,14]]  regressor_OLS=sm.OLS(endog=y,exog=X_opt).fit()  regressor_OLS.summary()",0
"train_data[['y', 'X0'] + list(train_data.columns[-8:])]",1,execute_result,"train_data[['y', 'X0'] + list(train_data.columns[-8:])]",0
display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT)) +' customers in the list who have more balance than the credit limit assigned. ' +'It may be due to more payament than usage andpath<path>')),0,display_data,display(HTML('<h4>There are '+str(np.sum(cc.BALANCE>cc.CREDIT_LIMIT))               +' customers in the list who have more balance than the credit limit assigned. '               +'It may be due to more payament than usage and/or continuous pre-payment.</h4>')),1
for i in Ktrain_cat1: encode(Ktrain[i]) for i in Ktest_cat1: encode(Ktest[i]),0,not_existent,for i in Ktrain_cat1:      encode(Ktrain[i])  for i in Ktest_cat1:      encode(Ktest[i]),1
for i in Ktrain_cat2: encode(Ktrain[i]) for i in Ktest_cat2: encode(Ktest[i]),0,not_existent,for i in Ktrain_cat2:      encode(Ktrain[i])  for i in Ktest_cat2:      encode(Ktest[i]),1
"plt.scatter(shyness_score, friendship_len)",0,execute_result,"plt.scatter(shyness_score, friendship_len)",1
"get_split_result(""..path"", test, 1e-6)",0,execute_result,"#come from https://www.kaggle.com/krishnakatyal/keras-efficientnet-b3  get_split_result(""../input/kernel-0076/submission.csv"", test, 1e-6)",1
