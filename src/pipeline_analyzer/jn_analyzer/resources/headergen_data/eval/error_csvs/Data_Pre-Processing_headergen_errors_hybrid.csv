content,output_type,tag,y_pred
"SETUP SETUP CHECKPOINT os.environ['CUDA_VISIBLE_DEVICES'] = '0' warnings.filterwarnings('ignore') ASSIGN = pd.read_csv('..path',index_col='ID_code') ASSIGN = pd.read_csv('..path',index_col='ID_code') ASSIGN = np.load('..path') ASSIGN=np.full(len(test_df),True,dtype=bool) ASSIGN[ASSIGN]=False ASSIGN = test_df.iloc[mask].reset_index(drop=True).copy() ASSIGN = train_df.pop('target') ASSIGN = y ASSIGN = pd.concat([train_df,test_df]) ASSIGN = [c for c in train_df.columns] for f in tqdm(ASSIGN): ASSIGN[f+'_counts'] = ASSIGN[f].map(pd.concat([ASSIGN[f], ASSIGN[f]], axis=0).value_counts().to_dict(), na_action='ignore') ASSIGN[f+'_counts'] = ASSIGN[f+'_counts'].fillna(1) ASSIGN = [f+'_counts' for f in num_cols] def rankgauss(x): ASSIGN = (rankdata(x) - 1) path(x) ASSIGN = 2 * ASSIGN - 1 ASSIGN = np.clip(ASSIGN, -0.99, 0.99) ASSIGN = erfinv(r) return r2 print('scaling ASSIGN') for col in ASSIGN + ASSIGN: print('scaling {}'.format(col)) ASSIGN = tr_te[col].mean() ASSIGN = tr_te[col].std() ASSIGN[col].fillna(ASSIGN, inplace=True) ASSIGN[col] = rankgauss(ASSIGN[col].values) ASSIGN = tr_te[0:ASSIGN.shape[0]] ASSIGN = tr_te[train_df.shape[0]:] ASSIGN = np.stack([train_df[num_cols].values,train_df[count_cols].values],axis = -1) ASSIGN = np.stack([test_df[num_cols].values,test_df[count_cols].values],axis = -1) def augment_counts(x, ASSIGN, t_pos, t_neg): ASSIGN = [],[] for i in range(t_pos): ASSIGN = y>0 ASSIGN = x[mask].copy() ASSIGN = np.arange(x1.shape[0]) for c in range(200): np.random.shuffle(ASSIGN) ASSIGN[:,c] = ASSIGN[ASSIGN][:,c] xs.append(ASSIGN) for i in range(t_neg): ASSIGN = y==0 ASSIGN = x[mask].copy() ASSIGN = np.arange(x1.shape[0]) for c in range(200): np.random.shuffle(ASSIGN) ASSIGN[:,c] = ASSIGN[ASSIGN][:,c] xn.append(ASSIGN) ASSIGN = np.vstack(ASSIGN) ASSIGN = np.vstack(ASSIGN) ASSIGN = np.ones(xs.shape[0]) ASSIGN = np.zeros(xn.shape[0]) ASSIGN = np.vstack([ASSIGN,xs,xn]) ASSIGN = np.concatenate([ASSIGN,ys,yn]) return x,y def build_model(): ASSIGN = L.Input((200,2)) ASSIGN = L.Dense(64)(inp) ASSIGN = L.PReLU()(ASSIGN) ASSIGN = L.BatchNormalization()(ASSIGN) ASSIGN = L.Dropout(0.2)(ASSIGN) ASSIGN = L.Dense(8)(ASSIGN) ASSIGN = L.PReLU()(ASSIGN) ASSIGN = L.Flatten()(ASSIGN) ASSIGN = L.Dense(1,activation='sigmoid')(x) ASSIGN = Model(inp,out) print(ASSIGN.summary()) return m ASSIGN = 5 ASSIGN = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42) ASSIGN = list(folds.split(train_df.values, target.values)) ASSIGN = np.zeros(y.shape) ASSIGN = np.zeros(X_test.shape[0]) for fold_ in [0, 1, 2, 3, 4]: ASSIGN = splits[fold_] ASSIGN = X[trn_idx], y[trn_idx] ASSIGN = X[val_idx], y[val_idx] ASSIGN = augment_counts(ASSIGN, 2, 1) ASSIGN = build_model() ASSIGN = ModelCheckpoint(MODEL_PATH + 'nn{}.hdf5'.format(fold_), save_best_only=True, verbose=True) ASSIGN = ReduceLROnPlateau(factor=0.5,patience=5) ASSIGN.compile(optimizer=Adam(), loss=binary_crossentropy) ASSIGN.fit(ASSIGN, validation_data=(ASSIGN), epochs=20, verbose=1, callbacks=[ASSIGN,ASSIGN], batch_size = 256) ASSIGN.load_weights(MODEL_PATH + 'nn{}.hdf5'.format(fold_)) ASSIGN[val_idx] = ASSIGN.predict(X_valid)[:, 0] ASSIGN += ASSIGN.predict(ASSIGN)[:,0] ASSIGN= 5 np.save(MODEL_PATH + 'oof_NN13b_aug.npy',ASSIGN) np.save(MODEL_PATH + 'sub_NN13b_aug.npy',ASSIGN)",not_existent,1,0
"for i in range(0,len(y_pred)): if y_pred[i] > 0.8: y_pred[i] = 1 else: y_pred[i] = 0",not_existent,0,1
"ASSIGN = pd.DataFrame({'ID': submission['ID'], 'item_cnt_month': y_pred}) ASSIGN.to_csv('submission1.csv', index=False)",not_existent,0,1
"ASSIGN=pd.DataFrame(y_pred) ASSIGN=pd.concat([submission['ID'],pred],axis=1) ASSIGN.columns=['ID','item_cnt_day'] ASSIGN.to_csv('new_submission.csv',index=False)",not_existent,0,1
ASSIGN = False ASSIGN = pd.read_csv('..path') ASSIGN = pd.read_csv('..path') if ASSIGN: ASSIGN = ASSIGN[0:300] ASSIGN = ASSIGN[0:300],not_existent,0,1
ASSIGN = 5 ASSIGN = 10 ASSIGN = 10,not_existent,0,1
"ASSIGN = np.zeros((len(test_df))) ASSIGN = np.zeros((len(X_train_std))) ASSIGN = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10).split(X_train_std, Y)) for i, (train_idx, valid_idx) in enumerate(ASSIGN): ASSIGN = X_train_std[train_idx] ASSIGN = Y.loc[train_idx] ASSIGN = X_train_std[valid_idx] ASSIGN = Y.loc[valid_idx] ASSIGN = build_model() ASSIGN = [ roc_auc_callback(training_data=(ASSIGN, ASSIGN),validation_data=(ASSIGN, ASSIGN)), EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1), ] ASSIGN.fit(ASSIGN, ASSIGN, epochs=n_epochs, batch_size=256, ASSIGN=ASSIGN) ASSIGN = model.predict(x_val_fold) ASSIGN[valid_idx] = ASSIGN.reshape(ASSIGN.shape[0]) ASSIGN = model.predict(X_test_std) ASSIGN += ASSIGN.reshape(ASSIGN.shape[0]) ASSIGN = ASSIGN path",not_existent,1,0
"class CyclicLR(Callback): """"""This callback implements a cyclical learning rate policy (CLR). The method cycles the learning rate between two boundaries with some constant frequency, as detailed in this paper (https:path). The amplitude of the cycle can be scaled on a per-iteration or per-cycle basis. This class has three built-in policies, as put forth in the paper. ""triangular"": A basic triangular cycle wpath ""triangular2"": A basic triangular cycle that scales initial amplitude by half each cycle. ""exp_range"": A cycle that scales initial amplitude by gamma**(cycle iterations) at each cycle iteration. For more detail, please see paper. ```python ASSIGN = CyclicLR(base_lr=0.001, max_lr=0.006, ASSIGN=2000., mode='triangular') model.fit(X_train, Y_train, callbacks=[ASSIGN]) ``` Class also supports custom scaling functions: ```python ASSIGN = lambda x: 0.5*(1+np.sin(x*np.pipath)) ASSIGN = CyclicLR(base_lr=0.001, max_lr=0.006, ASSIGN=2000., scale_fn=clr_fn, ASSIGN='cycle') model.fit(X_train, Y_train, callbacks=[ASSIGN]) ``` base_lr: initial learning rate which is the lower boundary in the cycle. max_lr: upper boundary in the cycle. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. step_size: number of training iterations per half cycle. Authors suggest setting step_size 2-8 x training iterations in epoch. mode: one of {triangular, triangular2, exp_range}. Default 'triangular'. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. gamma: constant in 'exp_range' scaling function: gamma**(cycle iterations) scale_fn: Custom scaling policy defined by a single argument lambda function, where 0 <= scale_fn(x) <= 1 for all x >= 0. mode paramater is ignored ASSIGN: {'cycle', 'iterations'}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default is 'cycle'. """""" def __init__(self, base_lr=0.001, max_lr=0.006, ASSIGN=2000., mode='triangular', ASSIGN=1., scale_fn=None, scale_mode='cycle', decay=0.1): super(CyclicLR, self).__init__() self.base_lr = base_lr self.max_lr = max_lr self.ASSIGN = ASSIGN self.mode = mode self.ASSIGN = ASSIGN ASSIGN == None: if self.mode == 'triangular': self.scale_fn = lambda x: 1. self.ASSIGN = 'cycle' elif self.mode == 'triangular2': self.scale_fn = lambda x: 1path(2.**(x-1)) self.ASSIGN = 'cycle' elif self.mode == 'exp_range': self.scale_fn = lambda x: ASSIGN**(x) self.ASSIGN = 'iterations' else: self.scale_fn = scale_fn self.ASSIGN = ASSIGN self.clr_iterations = 0. self.trn_iterations = 0. self.history = {} self.decay = decay self._reset() def _reset(self, new_base_lr=None, new_max_lr=None, ASSIGN=None): """"""Resets cycle iterations. Optional boundarypath """""" if new_base_lr != None: self.base_lr = new_base_lr if new_max_lr != None: self.max_lr = new_max_lr if ASSIGN != None: self.ASSIGN = ASSIGN self.clr_iterations = 0. def ASSIGN(self): ASSIGN = np.floor(1+self.clr_iterationspath(2*self.step_size)) ASSIGN = np.abs(self.clr_iterationspath*cycle + 1) if self.ASSIGN == 'ASSIGN': return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-ASSIGN))*self.scale_fn(ASSIGN) else: return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-ASSIGN))*self.scale_fn(self.clr_iterations) def on_train_begin(self, logs={}): ASSIGN = ASSIGN or {} if self.clr_iterations == 0: K.set_value(self.model.optimizer.lr, self.base_lr) else: K.set_value(self.model.optimizer.lr, self.ASSIGN()) def on_batch_end(self, epoch, ASSIGN=None): ASSIGN = ASSIGN or {} self.trn_iterations += 1 self.clr_iterations += 1 self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr)) self.history.setdefault('iterations', []).append(self.trn_iterations) for k, v in ASSIGN.items(): self.history.setdefault(k, []).append(v) K.set_value(self.model.optimizer.lr, self.ASSIGN()) def on_epoch_end(self, epoch, ASSIGN=None): self.base_lr = self.base_lr*np.exp(-epoch*self.decay) self.max_lr = self.max_lr*np.exp(-epoch*self.decay)",not_existent,0,1
"SETUP ASSIGN=OUTPUT_SIZE ASSIGN=1e-4 def triplet_loss_distance(y_true, y_pred): ASSIGN = K.sigmoid(ASSIGN) ASSIGN = y_pred[..., :OUTPUT_SIZE] ASSIGN = y_pred[..., OUTPUT_SIZE:2*OUTPUT_SIZE] ASSIGN = y_pred[..., 2*OUTPUT_SIZE:] ASSIGN = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,positive)),1) ASSIGN = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,negative)),1) ASSIGN = -K.tf.log(-K.tf.divide((ASSIGN),beta)+1+epsilon) ASSIGN = -K.tf.log(-K.tf.divide((OUTPUT_SIZE-ASSIGN),beta)+1+epsilon) return pos_dist + neg_dist",not_existent,0,1
"def auc(y_true, y_pred): ASSIGN = tf.metrics.ASSIGN(y_true, y_pred)[1] K.get_session().run(tf.local_variables_initializer()) return auc",not_existent,0,1
try: del df_tst except: pass gc.collect(),not_existent,0,1
"ASSIGN=classifier.fit(X_train, y_train, validation_split=0.350, batch_size = 5, epochs = 100)",not_existent,0,1
"SETUP ASSIGN = [ go.Scatter( ASSIGN=df_sales['date'], ASSIGN=df_sales['item_cnt_day'], ) ] ASSIGN = go.Layout( ASSIGN=' Sales' ) ASSIGN = go.Figure(data=plot_data, layout=plot_layout) pyoff.iplot(ASSIGN)",not_existent,0,1
"ASSIGN = [ go.Scatter( ASSIGN=df_diff['date'], ASSIGN=df_diff['diff'], ) ] ASSIGN = go.Layout( ASSIGN='Montly Sales Diff' ) ASSIGN = go.Figure(data=plot_data, layout=plot_layout) pyoff.iplot(ASSIGN)",not_existent,0,1
