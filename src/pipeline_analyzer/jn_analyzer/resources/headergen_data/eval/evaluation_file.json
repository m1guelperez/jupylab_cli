{
    "source": [
        {
            "notebook_name": "13-nn-with-magic-augmentation",
            "content": [
                "import os",
                "os.environ['CUDA_VISIBLE_DEVICES'] = '0'",
                "from sklearn.model_selection import KFold, StratifiedKFold",
                "from sklearn.metrics import roc_auc_score, log_loss",
                "import pandas as pd",
                "import numpy as np",
                "import time",
                "import random",
                "from tqdm import tqdm",
                "import warnings",
                "warnings.filterwarnings('ignore')",
                "",
                "import torch",
                "from torch import nn",
                "from torch.utils.data import TensorDataset",
                "from torch.utils.data import DataLoader",
                "",
                "MODEL_PATH = ''",
                "",
                "",
                "train_df = pd.read_csv('../input/santander-customer-transaction-prediction/train.csv',index_col='ID_code')",
                "test_df = pd.read_csv('../input/santander-customer-transaction-prediction/test.csv',index_col='ID_code')",
                "",
                "synthetic_indices = np.load('../input/synthetissantandersamples/synthetic_samples_indexes.npy')",
                "mask=np.full(len(test_df),True,dtype=bool)",
                "mask[synthetic_indices]=False",
                "test_df_nonsynthetic = test_df.iloc[mask].reset_index(drop=True).copy()",
                "",
                "",
                "y = train_df.pop('target')",
                "target = y",
                "",
                "tr_te = pd.concat([train_df,test_df])",
                "",
                "num_cols = [c for c in train_df.columns]",
                "",
                "for f in tqdm(num_cols):",
                "    tr_te[f+'_counts'] = tr_te[f].map(pd.concat([train_df[f], test_df_nonsynthetic[f]], axis=0).value_counts().to_dict(), na_action='ignore')",
                "    tr_te[f+'_counts'] = tr_te[f+'_counts'].fillna(1)",
                "",
                "",
                "count_cols = [f+'_counts' for f in num_cols]",
                "",
                "",
                "from scipy.special import erfinv",
                "from scipy.stats import rankdata",
                "",
                "def rankgauss(x):",
                "    r = (rankdata(x) - 1) / len(x)  # to [0,1]",
                "    r = 2 * r - 1  # to [-1,1]",
                "    r = np.clip(r, -0.99, 0.99)",
                "    r2 = erfinv(r)",
                "    return r2",
                "",
                "",
                "",
                "",
                "print('scaling num_cols')",
                "for col in num_cols + count_cols:",
                "    print('scaling {}'.format(col))",
                "    col_mean = tr_te[col].mean()",
                "    col_std = tr_te[col].std()",
                "    tr_te[col].fillna(col_mean, inplace=True)",
                "    tr_te[col] = rankgauss(tr_te[col].values)",
                "",
                "",
                "train_df = tr_te[0:train_df.shape[0]]",
                "test_df = tr_te[train_df.shape[0]:]",
                "",
                "",
                "",
                "",
                "",
                "X = np.stack([train_df[num_cols].values,train_df[count_cols].values],axis = -1)",
                "X_test = np.stack([test_df[num_cols].values,test_df[count_cols].values],axis = -1)",
                "#X = train_df[num_cols].values",
                "",
                "def augment_counts(x, y, t_pos, t_neg):",
                "    xs,xn = [],[]",
                "    for i in range(t_pos):",
                "        mask = y>0",
                "        x1 = x[mask].copy()",
                "        ids = np.arange(x1.shape[0])",
                "        for c in range(200):",
                "            np.random.shuffle(ids)",
                "            x1[:,c] = x1[ids][:,c]",
                "            #x1[:,c+200] = x1[ids][:,c+200]",
                "        xs.append(x1)",
                "",
                "    for i in range(t_neg):",
                "        mask = y==0",
                "        x1 = x[mask].copy()",
                "        ids = np.arange(x1.shape[0])",
                "        for c in range(200):",
                "            np.random.shuffle(ids)",
                "            x1[:,c] = x1[ids][:,c]",
                "            #x1[:,c+200] = x1[ids][:,c+200]",
                "        xn.append(x1)",
                "",
                "    xs = np.vstack(xs)",
                "    xn = np.vstack(xn)",
                "    ys = np.ones(xs.shape[0])",
                "    yn = np.zeros(xn.shape[0])",
                "    x = np.vstack([x,xs,xn])",
                "    y = np.concatenate([y,ys,yn])",
                "    return x,y",
                "",
                "from keras import layers as L",
                "import keras.backend as K",
                "from keras.models import Model",
                "from keras.optimizers import Adam",
                "from keras.losses import binary_crossentropy",
                "from keras.callbacks import ModelCheckpoint",
                "def build_model():",
                "    inp = L.Input((200,2))",
                "    x = L.Dense(64)(inp)",
                "    x = L.PReLU()(x)",
                "    x = L.BatchNormalization()(x)",
                "    x = L.Dropout(0.2)(x)",
                "    x = L.Dense(8)(x)",
                "    x = L.PReLU()(x)",
                "    x = L.Flatten()(x)",
                "    out = L.Dense(1,activation='sigmoid')(x)",
                "",
                "    m = Model(inp,out)",
                "    print(m.summary())",
                "    return m",
                "",
                "num_folds = 5",
                "folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)",
                "splits = list(folds.split(train_df.values, target.values))",
                "",
                "oof_preds = np.zeros(y.shape)",
                "test_preds = np.zeros(X_test.shape[0])",
                "",
                "from keras.callbacks import ReduceLROnPlateau",
                "",
                "for fold_ in [0, 1, 2, 3, 4]:",
                "    trn_idx, val_idx = splits[fold_]",
                "",
                "    X_train, y_train = X[trn_idx], y[trn_idx]",
                "    X_valid, y_valid = X[val_idx], y[val_idx]",
                "    ",
                "    X_train, y_train = augment_counts(X_train, y_train, 2, 1)",
                "",
                "    m = build_model()",
                "    ckpt = ModelCheckpoint(MODEL_PATH + 'nn{}.hdf5'.format(fold_), save_best_only=True, verbose=True)",
                "    pl = ReduceLROnPlateau(factor=0.5,patience=5)",
                "    m.compile(optimizer=Adam(), loss=binary_crossentropy)",
                "    m.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=20, verbose=1, callbacks=[ckpt,pl], batch_size = 256)",
                "    m.load_weights(MODEL_PATH + 'nn{}.hdf5'.format(fold_))",
                "    oof_preds[val_idx] = m.predict(X_valid)[:, 0]",
                "    test_preds += m.predict(X_test)[:,0]",
                "test_preds/= 5",
                "",
                "",
                "",
                "np.save(MODEL_PATH + 'oof_NN13b_aug.npy',oof_preds)",
                "np.save(MODEL_PATH + 'sub_NN13b_aug.npy',test_preds)"
            ],
            "content_processed": [
                "SETUP",
                "SETUP",
                "CHECKPOINT",
                "os.environ['CUDA_VISIBLE_DEVICES'] = '0'",
                "warnings.filterwarnings('ignore')",
                "ASSIGN = pd.read_csv('..path',index_col='ID_code')",
                "ASSIGN = pd.read_csv('..path',index_col='ID_code')",
                "ASSIGN = np.load('..path')",
                "ASSIGN=np.full(len(test_df),True,dtype=bool)",
                "ASSIGN[ASSIGN]=False",
                "ASSIGN = test_df.iloc[mask].reset_index(drop=True).copy()",
                "ASSIGN = train_df.pop('target')",
                "ASSIGN = y",
                "ASSIGN = pd.concat([train_df,test_df])",
                "ASSIGN = [c for c in train_df.columns]",
                "for f in tqdm(ASSIGN):",
                "ASSIGN[f+'_counts'] = ASSIGN[f].map(pd.concat([ASSIGN[f], ASSIGN[f]], axis=0).value_counts().to_dict(), na_action='ignore')",
                "ASSIGN[f+'_counts'] = ASSIGN[f+'_counts'].fillna(1)",
                "ASSIGN = [f+'_counts' for f in num_cols]",
                "def rankgauss(x):",
                "ASSIGN = (rankdata(x) - 1) path(x)",
                "ASSIGN = 2 * ASSIGN - 1",
                "ASSIGN = np.clip(ASSIGN, -0.99, 0.99)",
                "ASSIGN = erfinv(r)",
                "return r2",
                "print('scaling ASSIGN')",
                "for col in ASSIGN + ASSIGN:",
                "print('scaling {}'.format(col))",
                "ASSIGN = tr_te[col].mean()",
                "ASSIGN = tr_te[col].std()",
                "ASSIGN[col].fillna(ASSIGN, inplace=True)",
                "ASSIGN[col] = rankgauss(ASSIGN[col].values)",
                "ASSIGN = tr_te[0:ASSIGN.shape[0]]",
                "ASSIGN = tr_te[train_df.shape[0]:]",
                "ASSIGN = np.stack([train_df[num_cols].values,train_df[count_cols].values],axis = -1)",
                "ASSIGN = np.stack([test_df[num_cols].values,test_df[count_cols].values],axis = -1)",
                "def augment_counts(x, ASSIGN, t_pos, t_neg):",
                "ASSIGN = [],[]",
                "for i in range(t_pos):",
                "ASSIGN = y>0",
                "ASSIGN = x[mask].copy()",
                "ASSIGN = np.arange(x1.shape[0])",
                "for c in range(200):",
                "np.random.shuffle(ASSIGN)",
                "ASSIGN[:,c] = ASSIGN[ASSIGN][:,c]",
                "xs.append(ASSIGN)",
                "for i in range(t_neg):",
                "ASSIGN = y==0",
                "ASSIGN = x[mask].copy()",
                "ASSIGN = np.arange(x1.shape[0])",
                "for c in range(200):",
                "np.random.shuffle(ASSIGN)",
                "ASSIGN[:,c] = ASSIGN[ASSIGN][:,c]",
                "xn.append(ASSIGN)",
                "ASSIGN = np.vstack(ASSIGN)",
                "ASSIGN = np.vstack(ASSIGN)",
                "ASSIGN = np.ones(xs.shape[0])",
                "ASSIGN = np.zeros(xn.shape[0])",
                "ASSIGN = np.vstack([ASSIGN,xs,xn])",
                "ASSIGN = np.concatenate([ASSIGN,ys,yn])",
                "return x,y",
                "def build_model():",
                "ASSIGN = L.Input((200,2))",
                "ASSIGN = L.Dense(64)(inp)",
                "ASSIGN = L.PReLU()(ASSIGN)",
                "ASSIGN = L.BatchNormalization()(ASSIGN)",
                "ASSIGN = L.Dropout(0.2)(ASSIGN)",
                "ASSIGN = L.Dense(8)(ASSIGN)",
                "ASSIGN = L.PReLU()(ASSIGN)",
                "ASSIGN = L.Flatten()(ASSIGN)",
                "ASSIGN = L.Dense(1,activation='sigmoid')(x)",
                "ASSIGN = Model(inp,out)",
                "print(ASSIGN.summary())",
                "return m",
                "ASSIGN = 5",
                "ASSIGN = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)",
                "ASSIGN = list(folds.split(train_df.values, target.values))",
                "ASSIGN = np.zeros(y.shape)",
                "ASSIGN = np.zeros(X_test.shape[0])",
                "for fold_ in [0, 1, 2, 3, 4]:",
                "ASSIGN = splits[fold_]",
                "ASSIGN = X[trn_idx], y[trn_idx]",
                "ASSIGN = X[val_idx], y[val_idx]",
                "ASSIGN = augment_counts(ASSIGN, 2, 1)",
                "ASSIGN = build_model()",
                "ASSIGN = ModelCheckpoint(MODEL_PATH + 'nn{}.hdf5'.format(fold_), save_best_only=True, verbose=True)",
                "ASSIGN = ReduceLROnPlateau(factor=0.5,patience=5)",
                "ASSIGN.compile(optimizer=Adam(), loss=binary_crossentropy)",
                "ASSIGN.fit(ASSIGN, validation_data=(ASSIGN), epochs=20, verbose=1, callbacks=[ASSIGN,ASSIGN], batch_size = 256)",
                "ASSIGN.load_weights(MODEL_PATH + 'nn{}.hdf5'.format(fold_))",
                "ASSIGN[val_idx] = ASSIGN.predict(X_valid)[:, 0]",
                "ASSIGN += ASSIGN.predict(ASSIGN)[:,0]",
                "ASSIGN= 5",
                "np.save(MODEL_PATH + 'oof_NN13b_aug.npy',ASSIGN)",
                "np.save(MODEL_PATH + 'sub_NN13b_aug.npy',ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "ingest_data",
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model",
                "check_results",
                "transfer_results"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "13-nn-with-magic-augmentation",
            "content": [
                "submission = pd.read_csv('../input/santander-customer-transaction-prediction/sample_submission.csv')",
                "submission['target'] = test_preds",
                "submission.to_csv(MODEL_PATH + 'submission.csv',index=False)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = test_preds",
                "ASSIGN.to_csv(MODEL_PATH + 'ASSIGN.csv',index=False)"
            ],
            "tag_pred": [
                "ingest_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "13-nn-with-magic-augmentation",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "import pandas as pd",
                "import numpy as np",
                "import h5py",
                "import tensorflow as tf",
                "from tensorflow import keras",
                "from keras.models import Sequential",
                "from keras.layers import Dense",
                "from keras.layers import Dropout",
                "from keras.optimizers import Adam",
                "from keras.optimizers import SGD",
                "import matplotlib.pyplot as plt",
                "from keras.models import load_model"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "train_df = pd.read_csv(\"../input/titanic/train.csv\")",
                "test_df = pd.read_csv(\"../input/titanic/test.csv\")"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "train_df['Age'].fillna((train_df['Age'].mean()),inplace=True)",
                "",
                "test_df['Age'].fillna((test_df['Age'].mean()),inplace=True)",
                "",
                "train_df['Sex'] = train_df['Sex'].replace('male',value = 1)",
                "train_df['Sex'] = train_df['Sex'].replace('female',value = 0)",
                "",
                "test_df['Sex'] = test_df['Sex'].replace('male',value = 1)",
                "test_df['Sex'] = test_df['Sex'].replace('female',value = 0)"
            ],
            "content_processed": [
                "train_df['Age'].fillna((train_df['Age'].mean()),inplace=True)",
                "test_df['Age'].fillna((test_df['Age'].mean()),inplace=True)",
                "train_df['Sex'] = train_df['Sex'].replace('male',value = 1)",
                "train_df['Sex'] = train_df['Sex'].replace('female',value = 0)",
                "test_df['Sex'] = test_df['Sex'].replace('male',value = 1)",
                "test_df['Sex'] = test_df['Sex'].replace('female',value = 0)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "train_df.describe()"
            ],
            "content_processed": [
                "train_df.describe()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "n_train = 700",
                "X_train_class = train_df[\"Pclass\"].values.reshape(-1,1)",
                "X_train_sex = train_df[\"Sex\"].values.reshape(-1,1)",
                "X_train_age = train_df[\"Age\"].values.reshape(-1,1)",
                "X_train_sib = train_df[\"SibSp\"].values.reshape(-1,1)",
                "X_train_par = train_df[\"Parch\"].values.reshape(-1,1)",
                "",
                "",
                "y = train_df[\"Survived\"].values.T"
            ],
            "content_processed": [
                "ASSIGN = 700",
                "ASSIGN = train_df[\"Pclass\"].values.reshape(-1,1)",
                "ASSIGN = train_df[\"Sex\"].values.reshape(-1,1)",
                "ASSIGN = train_df[\"Age\"].values.reshape(-1,1)",
                "ASSIGN = train_df[\"SibSp\"].values.reshape(-1,1)",
                "ASSIGN = train_df[\"Parch\"].values.reshape(-1,1)",
                "ASSIGN = train_df[\"Survived\"].values.T"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "X_train = np.hstack((X_train_sex[:n_train,:],X_train_class[:n_train,:],X_train_sib[:n_train,:],X_train_age[:n_train,:],X_train_par[:n_train,:]))",
                "X_test = np.hstack((X_train_sex[n_train:,:],X_train_class[n_train:,:],X_train_sib[n_train:,:],X_train_age[n_train:,:],X_train_par[n_train:,:]))",
                "X_train, X_test = tf.convert_to_tensor(X_train.astype(np.float64)),tf.convert_to_tensor(X_test.astype(np.float64))",
                "y_train, y_test = y[:n_train], y[n_train:]"
            ],
            "content_processed": [
                "ASSIGN = np.hstack((X_train_sex[:n_train,:],X_train_class[:n_train,:],X_train_sib[:n_train,:],X_train_age[:n_train,:],X_train_par[:n_train,:]))",
                "ASSIGN = np.hstack((X_train_sex[n_train:,:],X_train_class[n_train:,:],X_train_sib[n_train:,:],X_train_age[n_train:,:],X_train_par[n_train:,:]))",
                "ASSIGN, ASSIGN = tf.convert_to_tensor(ASSIGN.astype(np.float64)),tf.convert_to_tensor(ASSIGN.astype(np.float64))",
                "ASSIGN = y[:n_train], y[n_train:]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "model = Sequential()",
                "model.add(Dense(300,input_dim=5,activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(150,activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(100,activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(50,activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(25,activation='relu'))",
                "model.add(Dropout(0.2))",
                "model.add(Dense(1,activation='sigmoid'))",
                "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01,beta_1=0.99,beta_2=0.999), metrics=['accuracy'])"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(300,input_dim=5,activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(150,activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(100,activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(50,activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(25,activation='relu'))",
                "ASSIGN.add(Dropout(0.2))",
                "ASSIGN.add(Dense(1,activation='sigmoid'))",
                "ASSIGN.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.01,beta_1=0.99,beta_2=0.999), metrics=['accuracy'])"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 300, verbose = 0)"
            ],
            "content_processed": [
                "ASSIGN = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 300, verbose = 0)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "_, train_acc = model.evaluate(X_train, y_train, verbose=2)",
                "_, test_acc = model.evaluate(X_test, y_test, verbose=2)",
                "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))",
                "",
                "plt.plot(history.history['accuracy'], label='train')",
                "plt.plot(history.history['val_accuracy'], label='test')",
                "plt.legend()",
                "plt.show()"
            ],
            "content_processed": [
                "CHECKPOINT",
                "ASSIGN = model.evaluate(X_train, y_train, verbose=2)",
                "ASSIGN = model.evaluate(X_test, y_test, verbose=2)",
                "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))",
                "plt.plot(history.history['accuracy'], label='train')",
                "plt.plot(history.history['val_accuracy'], label='test')",
                "plt.legend()",
                "plt.show()"
            ],
            "tag_pred": [
                "visualize_data",
                "check_results",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "check_results",
                "visualize_data"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Visualization"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Visualization"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "model.save('model_' + str(1) + '.h5')"
            ],
            "content_processed": [
                "model.save('model_' + str(1) + '.h5')"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "model = load_model(\"./model_1.h5\")",
                "model.summary()"
            ],
            "content_processed": [
                "ASSIGN = load_model(\".path\")",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data",
                "train_model"
            ],
            "correct_tag_ours": [
                "check_results",
                "ingest_data"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "X_test_class = test_df[\"Pclass\"].values.reshape(-1,1)",
                "X_test_sex = test_df[\"Sex\"].values.reshape(-1,1)",
                "X_test_age = test_df[\"Age\"].values.reshape(-1,1)",
                "X_test_sib = test_df[\"SibSp\"].values.reshape(-1,1)",
                "X_test_par = test_df[\"Parch\"].values.reshape(-1,1)",
                "",
                "x_test = np.hstack((X_test_sex,X_test_class,X_test_sib,X_test_age,X_test_par)).astype(np.float64)"
            ],
            "content_processed": [
                "ASSIGN = test_df[\"Pclass\"].values.reshape(-1,1)",
                "ASSIGN = test_df[\"Sex\"].values.reshape(-1,1)",
                "ASSIGN = test_df[\"Age\"].values.reshape(-1,1)",
                "ASSIGN = test_df[\"SibSp\"].values.reshape(-1,1)",
                "ASSIGN = test_df[\"Parch\"].values.reshape(-1,1)",
                "ASSIGN = np.hstack((X_test_sex,X_test_class,X_test_sib,X_test_age,X_test_par)).astype(np.float64)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "y_pred =[]",
                "prediction = model.predict(x_test).ravel().tolist()",
                "y_pred += prediction"
            ],
            "content_processed": [
                "ASSIGN =[]",
                "ASSIGN = model.predict(x_test).ravel().tolist()",
                "ASSIGN += ASSIGN"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "for i in range(0,len(y_pred)):",
                "    if y_pred[i] > 0.8:",
                "        y_pred[i] = 1",
                "    else:",
                "        y_pred[i] = 0"
            ],
            "content_processed": [
                "for i in range(0,len(y_pred)):",
                "if y_pred[i] > 0.8:",
                "y_pred[i] = 1",
                "else:",
                "y_pred[i] = 0"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                "submission = pd.read_csv('../input/titanic/gender_submission.csv')",
                "submission['Survived'] = y_pred",
                "submission.to_csv('submission.csv',index=False)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = y_pred",
                "ASSIGN.to_csv('ASSIGN.csv',index=False)"
            ],
            "tag_pred": [
                "ingest_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "02-relu-sigmoid",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')",
                "train.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "item = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')",
                "item.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "cat = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')",
                "cat.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "shop = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')",
                "shop.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')",
                "test.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.head()"
            ],
            "content_processed": [
                "train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "test.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "submission = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv')",
                "submission.head(2)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "submission.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "submission.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "train.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train = train[train.item_id.isin (test.item_id)]",
                "train = train[train.shop_id.isin (test.shop_id)]"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN[ASSIGN.item_id.isin (test.item_id)]",
                "ASSIGN = ASSIGN[ASSIGN.shop_id.isin (test.shop_id)]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.info()"
            ],
            "content_processed": [
                "train.info()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.head()"
            ],
            "content_processed": [
                "train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.drop(['date'],axis=1,inplace=True)"
            ],
            "content_processed": [
                "train.drop(['date'],axis=1,inplace=True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.head()"
            ],
            "content_processed": [
                "test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train['date_block_num']"
            ],
            "content_processed": [
                "train['date_block_num']"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test['date_block_num'] = 34",
                "test = test[['date_block_num','shop_id','item_id']]",
                "test.head(2)"
            ],
            "content_processed": [
                "test['date_block_num'] = 34",
                "ASSIGN = ASSIGN[['date_block_num','shop_id','item_id']]",
                "ASSIGN.head(2)"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "item_price = dict(train.groupby('item_id')['item_price'].last().reset_index().values)"
            ],
            "content_processed": [
                "ASSIGN = dict(train.groupby('item_id')['ASSIGN'].last().reset_index().values)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test['item_price'] = test.item_id.map(item_price)",
                "test.head()"
            ],
            "content_processed": [
                "test['item_price'] = test.item_id.map(item_price)",
                "test.head()"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.isnull().sum()"
            ],
            "content_processed": [
                "test.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.shape, test.shape"
            ],
            "content_processed": [
                "train.shape, test.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train = train[train.item_id.isin (test.item_id)]",
                "train = train[train.shop_id.isin (test.shop_id)]"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN[ASSIGN.item_id.isin (test.item_id)]",
                "ASSIGN = ASSIGN[ASSIGN.shop_id.isin (test.shop_id)]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.shape, test.shape"
            ],
            "content_processed": [
                "train.shape, test.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.isnull().sum()"
            ],
            "content_processed": [
                "test.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train['shop*item'] = train.shop_id *train.item_id",
                "test['shop*item'] = test.shop_id *test.item_id"
            ],
            "content_processed": [
                "train['shop*item'] = train.shop_id *train.item_id",
                "test['shop*item'] = test.shop_id *test.item_id"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "item.head()",
                "item.drop('item_name',axis=1,inplace = True)"
            ],
            "content_processed": [
                "item.head()",
                "item.drop('item_name',axis=1,inplace = True)"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "item_cat = dict(item.values)",
                "train['item_cat'] = train.item_id.map(item_cat)",
                "test['item_cat'] = test.item_id.map(item_cat)",
                ""
            ],
            "content_processed": [
                "ASSIGN = dict(item.values)",
                "train['ASSIGN'] = train.item_id.map(ASSIGN)",
                "test['ASSIGN'] = test.item_id.map(ASSIGN)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.head(2)"
            ],
            "content_processed": [
                "train.head(2)"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.info()"
            ],
            "content_processed": [
                "train.info()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "import seaborn as sns",
                "import matplotlib.pyplot as plt"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "df = pd.concat([train,test])"
            ],
            "content_processed": [
                "ASSIGN = pd.concat([train,test])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "",
                "Data Preparation"
            ],
            "headergen_sot": [
                "",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "sns.histplot(df['item_price']);"
            ],
            "content_processed": [
                "sns.histplot(df['item_price']);"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "",
                "df = pd.concat([train,test])",
                "#Normalize",
                "df.item_price = np.log1p(df.item_price)",
                "#fil l the missing",
                "df.item_price = df.item_price.fillna(df.item_price.mean())",
                "",
                "#rremove the outlier",
                "df.item_cnt_day = df.item_cnt_day.apply(lambda x : 10 if x>10 else x)"
            ],
            "content_processed": [
                "ASSIGN = pd.concat([train,test])",
                "ASSIGN.item_price = np.log1p(ASSIGN.item_price)",
                "ASSIGN.item_price = ASSIGN.item_price.fillna(ASSIGN.item_price.mean())",
                "ASSIGN.item_cnt_day = ASSIGN.item_cnt_day.apply(lambda x : 10 if x>10 else x)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train = df[df.item_cnt_day.notnull()]",
                "test = df[df.item_cnt_day.isnull()]"
            ],
            "content_processed": [
                "ASSIGN = df[df.item_cnt_day.notnull()]",
                "ASSIGN = df[df.item_cnt_day.isnull()]"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "train.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "train.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.isnull().sum()"
            ],
            "content_processed": [
                "test.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.drop('item_cnt_day',axis = 1,inplace  = True)"
            ],
            "content_processed": [
                "test.drop('item_cnt_day',axis = 1,inplace  = True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "test.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "test.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "x_train = train.drop('item_cnt_day',axis = 1).values",
                "y_train = train.item_cnt_day.values"
            ],
            "content_processed": [
                "ASSIGN = train.drop('item_cnt_day',axis = 1).values",
                "ASSIGN = train.item_cnt_day.values"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "x_test = test"
            ],
            "content_processed": [
                "ASSIGN = test"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "from sklearn.preprocessing import MinMaxScaler",
                "SC = MinMaxScaler()",
                "#SC = StandardScaler()",
                "x_train = SC.fit_transform(x_train)",
                "x_test = SC.transform(x_test)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = MinMaxScaler()",
                "ASSIGN = SC.fit_transform(ASSIGN)",
                "ASSIGN = SC.transform(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "import keras ",
                "from keras.models import Sequential ",
                "from keras.layers import Dense"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "",
                "# Initialising the NN",
                "model = Sequential()",
                "",
                "# layers",
                "model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 6))",
                "model.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu'))",
                "model.add(Dense(5, kernel_initializer = 'uniform', activation = 'relu'))",
                "model.add(Dense(1, kernel_initializer = 'uniform', activation = 'linear'))",
                "",
                "# summary",
                "model.summary()"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu', input_dim = 6))",
                "ASSIGN.add(Dense(9, kernel_initializer = 'uniform', activation = 'relu'))",
                "ASSIGN.add(Dense(5, kernel_initializer = 'uniform', activation = 'relu'))",
                "ASSIGN.add(Dense(1, kernel_initializer = 'uniform', activation = 'linear'))",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "model.compile(optimizer = 'adam', loss = 'mean_absolute_error', metrics = ['mse','mae'])",
                ""
            ],
            "content_processed": [
                "model.compile(optimizer = 'adam', loss = 'mean_absolute_error', metrics = ['mse','mae'])"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "history = model.fit(x_train, y_train, epochs=32, validation_split=0.2)",
                ""
            ],
            "content_processed": [
                "ASSIGN = model.fit(x_train, y_train, epochs=32, validation_split=0.2)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "from sklearn.metrics import mean_squared_error",
                "pred_train= model.predict(x_train)",
                "print(np.sqrt(mean_squared_error(y_train,pred_train)))"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN= model.predict(x_train)",
                "print(np.sqrt(mean_squared_error(y_train,ASSIGN)))"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "evaluate_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "y_pred = model.predict(x_test).flatten()"
            ],
            "content_processed": [
                "ASSIGN = model.predict(x_test).flatten()"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "output = pd.DataFrame({'ID': submission['ID'], 'item_cnt_month': y_pred})",
                "output.to_csv('submission1.csv', index=False)"
            ],
            "content_processed": [
                "ASSIGN = pd.DataFrame({'ID': submission['ID'], 'item_cnt_month': y_pred})",
                "ASSIGN.to_csv('submission1.csv', index=False)"
            ],
            "tag_pred": [
                "process_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                "pred=pd.DataFrame(y_pred)",
                "datasets=pd.concat([submission['ID'],pred],axis=1)",
                "datasets.columns=['ID','item_cnt_day']",
                "datasets.to_csv('new_submission.csv',index=False)"
            ],
            "content_processed": [
                "ASSIGN=pd.DataFrame(y_pred)",
                "ASSIGN=pd.concat([submission['ID'],pred],axis=1)",
                "ASSIGN.columns=['ID','item_cnt_day']",
                "ASSIGN.to_csv('new_submission.csv',index=False)"
            ],
            "tag_pred": [
                "process_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "09-prediction-future-sales-with-keras",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "",
                "from sklearn.metrics import roc_auc_score",
                "from sklearn.preprocessing import StandardScaler",
                "from sklearn.model_selection import StratifiedKFold",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Activation, Dropout",
                "from keras.models import load_model",
                "from keras.models import Sequential, Model",
                "from keras.layers import Input, Dense, Dropout, Activation",
                "from keras.layers.normalization import BatchNormalization",
                "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback",
                "import os",
                "print(os.listdir(\"../input\"))"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir())"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# Set this True when you want to check quickly if it works as expected.",
                "# It will run with very small subset of whole data.",
                "is_debug = False",
                "",
                "# Load the data",
                "train_df = pd.read_csv('../input/train.csv')",
                "test_df = pd.read_csv('../input/test.csv')",
                "",
                "# We only use very small subset of data if is_debug.",
                "if is_debug:",
                "    train_df = train_df[0:300]",
                "    test_df = test_df[0:300]    "
            ],
            "content_processed": [
                "ASSIGN = False",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "if ASSIGN:",
                "ASSIGN = ASSIGN[0:300]",
                "ASSIGN = ASSIGN[0:300]"
            ],
            "tag_pred": [
                "ingest_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# Remove unnecessary data. Well ID_code might have some leak, but we don't deep dive for now :)",
                "X_train = train_df.drop(['target', 'ID_code'], axis=1)",
                "X_test = test_df.drop(['ID_code'], axis=1)"
            ],
            "content_processed": [
                "ASSIGN = train_df.drop(['target', 'ID_code'], axis=1)",
                "ASSIGN = test_df.drop(['ID_code'], axis=1)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# We scale both train and test data so that our NN works better.",
                "sc = StandardScaler()",
                "std = sc.fit_transform(X_test + X_train)"
            ],
            "content_processed": [
                "ASSIGN = StandardScaler()",
                "ASSIGN = sc.fit_transform(X_test + X_train)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "X_train_std = sc.fit_transform(X_train)",
                "X_test_std = sc.fit_transform(X_test)"
            ],
            "content_processed": [
                "ASSIGN = sc.fit_transform(X_train)",
                "ASSIGN = sc.fit_transform(X_test)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# This is the grand truth for training data.",
                "Y = train_df[['target']]"
            ],
            "content_processed": [
                "ASSIGN = train_df[['target']]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# https://www.kaggle.com/tilii7/keras-averaging-runs-gini-early-stopping",
                "# Our submission will be evaluated based on AUC.",
                "class roc_auc_callback(Callback):",
                "    def __init__(self,training_data,validation_data):",
                "        self.x = training_data[0]",
                "        self.y = training_data[1]",
                "        self.x_val = validation_data[0]",
                "        self.y_val = validation_data[1]",
                "",
                "    def on_train_begin(self, logs={}):",
                "        return",
                "",
                "    def on_train_end(self, logs={}):",
                "        return",
                "",
                "    def on_epoch_begin(self, epoch, logs={}):",
                "        return",
                "",
                "    def on_epoch_end(self, epoch, logs={}):",
                "        y_pred = self.model.predict_proba(self.x, verbose=0)",
                "        roc = roc_auc_score(self.y, y_pred)",
                "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)",
                "        logs['norm_gini'] = ( roc_auc_score(self.y, y_pred) * 2 ) - 1",
                "",
                "        y_pred_val = self.model.predict_proba(self.x_val, verbose=0)",
                "        roc_val = roc_auc_score(self.y_val, y_pred_val)",
                "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)",
                "        logs['norm_gini_val'] = ( roc_auc_score(self.y_val, y_pred_val) * 2 ) - 1",
                "",
                "        print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round((roc*2-1),5)),str(round((roc_val*2-1),5))), end=10*' '+'\\n')",
                "        return",
                "",
                "    def on_batch_begin(self, batch, logs={}):",
                "        return",
                "",
                "    def on_batch_end(self, batch, logs={}):",
                "        return"
            ],
            "content_processed": [
                "CHECKPOINT",
                "CHECKPOINT",
                "class roc_auc_callback(Callback):",
                "def __init__(self,training_data,validation_data):",
                "self.x = training_data[0]",
                "self.y = training_data[1]",
                "self.x_val = validation_data[0]",
                "self.y_val = validation_data[1]",
                "def on_train_begin(self, logs={}):",
                "return",
                "def on_train_end(self, logs={}):",
                "return",
                "def on_epoch_begin(self, epoch, logs={}):",
                "return",
                "def on_epoch_end(self, epoch, logs={}):",
                "ASSIGN = self.model.predict_proba(self.x, verbose=0)",
                "ASSIGN = roc_auc_score(self.y, y_pred)",
                "logs['roc_auc'] = roc_auc_score(self.y, ASSIGN)",
                "logs['norm_gini'] = ( roc_auc_score(self.y, ASSIGN) * 2 ) - 1",
                "ASSIGN = self.model.predict_proba(self.x_val, verbose=0)",
                "ASSIGN = roc_auc_score(self.y_val, y_pred_val)",
                "logs['roc_auc_val'] = roc_auc_score(self.y_val, ASSIGN)",
                "logs['norm_gini_val'] = ( roc_auc_score(self.y_val, ASSIGN) * 2 ) - 1",
                "print('\\rroc_auc: %s - roc_auc_val: %s - norm_gini: %s - norm_gini_val: %s' % (str(round(ASSIGN,5)),str(round(ASSIGN,5)),str(round((ASSIGN*2-1),5)),str(round((ASSIGN*2-1),5))), end=10*' '+'\\n')",
                "return",
                "def on_batch_begin(self, batch, logs={}):",
                "return",
                "def on_batch_end(self, batch, logs={}):",
                "return"
            ],
            "tag_pred": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# Very simple Neural Network model.",
                "# This can be improved by many ways. e.g., more layers, batch normalization and etc.",
                "def build_model():",
                "    model = Sequential()",
                "    model.add(Dense(units=64, input_dim=len(X_train.columns)))",
                "    model.add(Dense(units=1, activation='sigmoid'))",
                "    model.compile(loss='binary_crossentropy',",
                "                  optimizer='adam',",
                "                  metrics=['accuracy'])",
                "    return model"
            ],
            "content_processed": [
                "def build_model():",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units=64, input_dim=len(X_train.columns)))",
                "ASSIGN.add(Dense(units=1, activation='sigmoid'))",
                "ASSIGN.compile(loss='binary_crossentropy',",
                "ASSIGN='adam',",
                "ASSIGN=['accuracy'])",
                "return model"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# Some parameters which control our training.",
                "n_splits = 5",
                "n_epochs = 10",
                "patience = 10"
            ],
            "content_processed": [
                "ASSIGN = 5",
                "ASSIGN = 10",
                "ASSIGN = 10"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# We do simple KFold Cross validation",
                "y_test  = np.zeros((len(test_df)))",
                "y_train = np.zeros((len(X_train_std)))",
                "",
                "splits = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10).split(X_train_std, Y))",
                "for i, (train_idx, valid_idx) in enumerate(splits):    ",
                "    x_train_fold = X_train_std[train_idx]",
                "    y_train_fold = Y.loc[train_idx]",
                "    x_val_fold = X_train_std[valid_idx]",
                "    y_val_fold = Y.loc[valid_idx]",
                "    ",
                "    model = build_model()",
                "    callbacks = [",
                "        roc_auc_callback(training_data=(x_train_fold, y_train_fold),validation_data=(x_val_fold, y_val_fold)),",
                "        EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1),",
                "    ]    ",
                "    model.fit(x_train_fold, y_train_fold, epochs=n_epochs, batch_size=256, callbacks=callbacks)",
                "",
                "    y_val_preds = model.predict(x_val_fold)",
                "    y_train[valid_idx] = y_val_preds.reshape(y_val_preds.shape[0])",
                "    y_test_preds = model.predict(X_test_std)",
                "    y_test += y_test_preds.reshape(y_test_preds.shape[0])",
                "",
                "y_test = y_test / n_splits    "
            ],
            "content_processed": [
                "ASSIGN = np.zeros((len(test_df)))",
                "ASSIGN = np.zeros((len(X_train_std)))",
                "ASSIGN = list(StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10).split(X_train_std, Y))",
                "for i, (train_idx, valid_idx) in enumerate(ASSIGN):",
                "ASSIGN = X_train_std[train_idx]",
                "ASSIGN = Y.loc[train_idx]",
                "ASSIGN = X_train_std[valid_idx]",
                "ASSIGN = Y.loc[valid_idx]",
                "ASSIGN = build_model()",
                "ASSIGN = [",
                "roc_auc_callback(training_data=(ASSIGN, ASSIGN),validation_data=(ASSIGN, ASSIGN)),",
                "EarlyStopping(monitor='norm_gini_val', patience=patience, mode='max', verbose=1),",
                "]",
                "ASSIGN.fit(ASSIGN, ASSIGN, epochs=n_epochs, batch_size=256, ASSIGN=ASSIGN)",
                "ASSIGN = model.predict(x_val_fold)",
                "ASSIGN[valid_idx] = ASSIGN.reshape(ASSIGN.shape[0])",
                "ASSIGN = model.predict(X_test_std)",
                "ASSIGN += ASSIGN.reshape(ASSIGN.shape[0])",
                "ASSIGN = ASSIGN path"
            ],
            "tag_pred": [
                "train_model",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "process_data",
                "train_model",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "# This is our CV score.",
                "roc_auc_score(Y, y_train)"
            ],
            "content_processed": [
                "roc_auc_score(Y, y_train)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "submission = test_df[['ID_code']].copy()",
                "submission['target'] = y_test",
                "submission.to_csv('submission.csv', index=False)"
            ],
            "content_processed": [
                "ASSIGN = test_df[['ID_code']].copy()",
                "ASSIGN = y_test",
                "ASSIGN.to_csv('ASSIGN.csv', index=False)"
            ],
            "tag_pred": [
                "process_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "process_data",
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "11-starter-keras-simple-nn-kfold-cv",
            "content": [
                "!head submission.csv"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "import os",
                "import numpy as np",
                "import pandas as pd"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "train_df = pd.read_csv('../input/train.csv')",
                "test_df = pd.read_csv('../input/test.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "features = train_df.columns",
                "features[2:]"
            ],
            "content_processed": [
                "ASSIGN = train_df.columns",
                "ASSIGN[2:]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "from tqdm import tqdm_notebook as tqdm"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "# Code from: https://stackoverflow.com/questions/45028260/gaussian-to-uniform-distribution-conversion-has-errors-at-the-edges-of-uniform-d",
                "def gaussian_estimation(vector):",
                "    mu = np.mean(vector)",
                "    sig = np.std(vector)",
                "    return mu, sig",
                "",
                "# Adjusts the data so it forms a gaussian with mean of 0 and std of 1",
                "def gaussian_normalization(vector, char = None):",
                "    if char is None:",
                "        mu , sig = gaussian_estimation(vector)",
                "    else:",
                "        mu = char[0]",
                "        sig = char[1]",
                "    normalized = (vector-mu)/sig",
                "    return normalized",
                "",
                "# Taken from https://en.wikipedia.org/wiki/Normal_distribution#Cumulative_distribution_function",
                "def CDF(x, max_i = 100):",
                "    sum = x",
                "    value = x",
                "    for i in np.arange(max_i)+1:",
                "        value = value*x*x/(2.0*i+1)",
                "        sum = sum + value",
                "    return 0.5 + (sum/np.sqrt(2*np.pi))*np.exp(-1*(x*x)/2)",
                "",
                "def gaussian_to_uniform(vector, if_normal = False):",
                "    if (if_normal == False):",
                "        vector = gaussian_normalization(vector)",
                "    uni = np.apply_along_axis(CDF, 0, vector)",
                "    return uni",
                ""
            ],
            "content_processed": [
                "def gaussian_estimation(vector):",
                "ASSIGN = np.mean(vector)",
                "ASSIGN = np.std(vector)",
                "return mu, sig",
                "def gaussian_normalization(vector, char = None):",
                "if char is None:",
                "ASSIGN = gaussian_estimation(vector)",
                "else:",
                "ASSIGN = char[0]",
                "ASSIGN = char[1]",
                "ASSIGN = (vector-mu)path",
                "return normalized",
                "def CDF(x, max_i = 100):",
                "ASSIGN = x",
                "ASSIGN = x",
                "for i in np.arange(max_i)+1:",
                "ASSIGN = ASSIGN*x*xpath(2.0*i+1)",
                "ASSIGN = ASSIGN + value",
                "return 0.5 + (sumpath(2*np.pi))*np.exp(-1*(x*x)path)",
                "def gaussian_to_uniform(vector, if_normal = False):",
                "if (if_normal == False):",
                "ASSIGN = gaussian_normalization(ASSIGN)",
                "ASSIGN = np.apply_along_axis(CDF, 0, vector)",
                "return uni"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "#for f in tqdm(features[2:]):",
                "#     train_df.loc[:, f] = gaussian_to_uniform(train_df.loc[:, f].values)",
                "#    ",
                "#     test_df.loc[:, f] = gaussian_to_uniform(test_df.loc[:, f].values)"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "from keras.utils import Sequence  ",
                "from keras.callbacks import Callback",
                "import random",
                "",
                "class DataGenerator(Sequence):",
                "    def __init__(self, x, y, batch_size=256):",
                "        self.x = x",
                "        self.y = y",
                "        self.batch_size = batch_size",
                "        ",
                "        self.pos_x = x[y==1]",
                "        self.neg_x = x[y==0]",
                "        ",
                "        self.pos_idx = list(range(len(self.pos_x)))",
                "        self.neg_idx = list(range(len(self.neg_x)))",
                "        ",
                "    def __len__(self):",
                "        return int(len(self.y)/self.batch_size)",
                "    ",
                "    def __getitem__(self, indx):",
                "        poss_b, neg_b, anch_b = [], [], []",
                "        ",
                "        for _ in range(self.batch_size):",
                "            if random.uniform(0, 1) > 0.5:",
                "                p_i = random.choice(self.pos_idx)",
                "                n_i = random.choice(self.neg_idx)",
                "                a_i = random.choice(self.pos_idx)",
                "                    ",
                "                p_x = self.pos_x[p_i]",
                "                n_x = self.neg_x[n_i]",
                "                a_x = self.pos_x[a_i]",
                "            else:",
                "                p_i = random.choice(self.neg_idx)",
                "                n_i = random.choice(self.pos_idx)",
                "                a_i = random.choice(self.neg_idx)",
                "                ",
                "                p_x = self.neg_x[p_i]",
                "                n_x = self.pos_x[n_i]",
                "                a_x = self.neg_x[a_i]",
                "            # augmentation",
                "            if random.uniform(0, 1) > 0.5:",
                "                ids = [0, 1]",
                "                np.random.shuffle(ids)",
                "                _x = np.array([p_x, a_x])",
                "                _x1 = np.array([p_x, a_x])",
                "                for c in range(_x.shape[1]):",
                "                    np.random.shuffle(ids)",
                "                    _x1[:,c] = _x[ids][:,c]",
                "                p_x, a_x = _x1[0], _x[1]    ",
                "            ",
                "            poss_b.append(p_x)",
                "            neg_b.append(n_x)",
                "            anch_b.append(a_x)",
                "        return [np.array(poss_b), np.array(neg_b), np.array(anch_b)], np.zeros((self.batch_size, 2))"
            ],
            "content_processed": [
                "SETUP",
                "class DataGenerator(Sequence):",
                "def __init__(self, x, y, batch_size=256):",
                "self.x = x",
                "self.y = y",
                "self.batch_size = batch_size",
                "self.pos_x = x[y==1]",
                "self.neg_x = x[y==0]",
                "self.pos_idx = list(range(len(self.pos_x)))",
                "self.neg_idx = list(range(len(self.neg_x)))",
                "def __len__(self):",
                "return int(len(self.y)path)",
                "def __getitem__(self, indx):",
                "ASSIGN = [], [], []",
                "for _ in range(self.batch_size):",
                "if random.uniform(0, 1) > 0.5:",
                "ASSIGN = random.choice(self.pos_idx)",
                "ASSIGN = random.choice(self.neg_idx)",
                "ASSIGN = random.choice(self.pos_idx)",
                "ASSIGN = self.pos_x[p_i]",
                "ASSIGN = self.neg_x[n_i]",
                "ASSIGN = self.pos_x[a_i]",
                "else:",
                "ASSIGN = random.choice(self.neg_idx)",
                "ASSIGN = random.choice(self.pos_idx)",
                "ASSIGN = random.choice(self.neg_idx)",
                "ASSIGN = self.neg_x[p_i]",
                "ASSIGN = self.pos_x[n_i]",
                "ASSIGN = self.neg_x[a_i]",
                "if random.uniform(0, 1) > 0.5:",
                "ASSIGN = [0, 1]",
                "np.random.shuffle(ASSIGN)",
                "ASSIGN = np.array([p_x, a_x])",
                "ASSIGN = np.array([p_x, a_x])",
                "for c in range(ASSIGN.shape[1]):",
                "np.random.shuffle(ASSIGN)",
                "ASSIGN[:,c] = ASSIGN[ASSIGN][:,c]",
                "ASSIGN = _x1[0], _x[1]",
                "poss_b.append(ASSIGN)",
                "neg_b.append(ASSIGN)",
                "anch_b.append(ASSIGN)",
                "return [np.array(poss_b), np.array(neg_b), np.array(anch_b)], np.zeros((self.batch_size, 2))"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "class CyclicLR(Callback):",
                "    \"\"\"This callback implements a cyclical learning rate policy (CLR).",
                "    The method cycles the learning rate between two boundaries with",
                "    some constant frequency, as detailed in this paper (https://arxiv.org/abs/1506.01186).",
                "    The amplitude of the cycle can be scaled on a per-iteration or ",
                "    per-cycle basis.",
                "    This class has three built-in policies, as put forth in the paper.",
                "    \"triangular\":",
                "        A basic triangular cycle w/ no amplitude scaling.",
                "    \"triangular2\":",
                "        A basic triangular cycle that scales initial amplitude by half each cycle.",
                "    \"exp_range\":",
                "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each ",
                "        cycle iteration.",
                "    For more detail, please see paper.",
                "    ",
                "    # Example",
                "        ```python",
                "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,",
                "                                step_size=2000., mode='triangular')",
                "            model.fit(X_train, Y_train, callbacks=[clr])",
                "        ```",
                "    ",
                "    Class also supports custom scaling functions:",
                "        ```python",
                "            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi/2.))",
                "            clr = CyclicLR(base_lr=0.001, max_lr=0.006,",
                "                                step_size=2000., scale_fn=clr_fn,",
                "                                scale_mode='cycle')",
                "            model.fit(X_train, Y_train, callbacks=[clr])",
                "        ```    ",
                "    # Arguments",
                "        base_lr: initial learning rate which is the",
                "            lower boundary in the cycle.",
                "        max_lr: upper boundary in the cycle. Functionally,",
                "            it defines the cycle amplitude (max_lr - base_lr).",
                "            The lr at any cycle is the sum of base_lr",
                "            and some scaling of the amplitude; therefore ",
                "            max_lr may not actually be reached depending on",
                "            scaling function.",
                "        step_size: number of training iterations per",
                "            half cycle. Authors suggest setting step_size",
                "            2-8 x training iterations in epoch.",
                "        mode: one of {triangular, triangular2, exp_range}.",
                "            Default 'triangular'.",
                "            Values correspond to policies detailed above.",
                "            If scale_fn is not None, this argument is ignored.",
                "        gamma: constant in 'exp_range' scaling function:",
                "            gamma**(cycle iterations)",
                "        scale_fn: Custom scaling policy defined by a single",
                "            argument lambda function, where ",
                "            0 <= scale_fn(x) <= 1 for all x >= 0.",
                "            mode paramater is ignored ",
                "        scale_mode: {'cycle', 'iterations'}.",
                "            Defines whether scale_fn is evaluated on ",
                "            cycle number or cycle iterations (training",
                "            iterations since start of cycle). Default is 'cycle'.",
                "    \"\"\"",
                "",
                "    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',",
                "                 gamma=1., scale_fn=None, scale_mode='cycle', decay=0.1):",
                "        super(CyclicLR, self).__init__()",
                "",
                "        self.base_lr = base_lr",
                "        self.max_lr = max_lr",
                "        self.step_size = step_size",
                "        self.mode = mode",
                "        self.gamma = gamma",
                "        if scale_fn == None:",
                "            if self.mode == 'triangular':",
                "                self.scale_fn = lambda x: 1.",
                "                self.scale_mode = 'cycle'",
                "            elif self.mode == 'triangular2':",
                "                self.scale_fn = lambda x: 1/(2.**(x-1))",
                "                self.scale_mode = 'cycle'",
                "            elif self.mode == 'exp_range':",
                "                self.scale_fn = lambda x: gamma**(x)",
                "                self.scale_mode = 'iterations'",
                "        else:",
                "            self.scale_fn = scale_fn",
                "            self.scale_mode = scale_mode",
                "        self.clr_iterations = 0.",
                "        self.trn_iterations = 0.",
                "        self.history = {}",
                "        self.decay = decay",
                "",
                "        self._reset()",
                "",
                "    def _reset(self, new_base_lr=None, new_max_lr=None,",
                "               new_step_size=None):",
                "        \"\"\"Resets cycle iterations.",
                "        Optional boundary/step size adjustment.",
                "        \"\"\"",
                "        if new_base_lr != None:",
                "            self.base_lr = new_base_lr",
                "        if new_max_lr != None:",
                "            self.max_lr = new_max_lr",
                "        if new_step_size != None:",
                "            self.step_size = new_step_size",
                "        self.clr_iterations = 0.",
                "        ",
                "    def clr(self):",
                "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))",
                "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)",
                "        if self.scale_mode == 'cycle':",
                "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)",
                "        else:",
                "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)",
                "        ",
                "    def on_train_begin(self, logs={}):",
                "        logs = logs or {}",
                "",
                "        if self.clr_iterations == 0:",
                "            K.set_value(self.model.optimizer.lr, self.base_lr)",
                "        else:",
                "            K.set_value(self.model.optimizer.lr, self.clr())        ",
                "            ",
                "    def on_batch_end(self, epoch, logs=None):",
                "        ",
                "        logs = logs or {}",
                "        self.trn_iterations += 1",
                "        self.clr_iterations += 1",
                "",
                "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))",
                "        self.history.setdefault('iterations', []).append(self.trn_iterations)",
                "",
                "        for k, v in logs.items():",
                "            self.history.setdefault(k, []).append(v)",
                "        ",
                "        K.set_value(self.model.optimizer.lr, self.clr())",
                "        ",
                "        ",
                "    def on_epoch_end(self, epoch, logs=None):",
                "        self.base_lr = self.base_lr*np.exp(-epoch*self.decay)",
                "        self.max_lr = self.max_lr*np.exp(-epoch*self.decay)",
                "",
                ""
            ],
            "content_processed": [
                "class CyclicLR(Callback):",
                "\"\"\"This callback implements a cyclical learning rate policy (CLR).",
                "The method cycles the learning rate between two boundaries with",
                "some constant frequency, as detailed in this paper (https:path).",
                "The amplitude of the cycle can be scaled on a per-iteration or",
                "per-cycle basis.",
                "This class has three built-in policies, as put forth in the paper.",
                "\"triangular\":",
                "A basic triangular cycle wpath",
                "\"triangular2\":",
                "A basic triangular cycle that scales initial amplitude by half each cycle.",
                "\"exp_range\":",
                "A cycle that scales initial amplitude by gamma**(cycle iterations) at each",
                "cycle iteration.",
                "For more detail, please see paper.",
                "```python",
                "ASSIGN = CyclicLR(base_lr=0.001, max_lr=0.006,",
                "ASSIGN=2000., mode='triangular')",
                "model.fit(X_train, Y_train, callbacks=[ASSIGN])",
                "```",
                "Class also supports custom scaling functions:",
                "```python",
                "ASSIGN = lambda x: 0.5*(1+np.sin(x*np.pipath))",
                "ASSIGN = CyclicLR(base_lr=0.001, max_lr=0.006,",
                "ASSIGN=2000., scale_fn=clr_fn,",
                "ASSIGN='cycle')",
                "model.fit(X_train, Y_train, callbacks=[ASSIGN])",
                "```",
                "base_lr: initial learning rate which is the",
                "lower boundary in the cycle.",
                "max_lr: upper boundary in the cycle. Functionally,",
                "it defines the cycle amplitude (max_lr - base_lr).",
                "The lr at any cycle is the sum of base_lr",
                "and some scaling of the amplitude; therefore",
                "max_lr may not actually be reached depending on",
                "scaling function.",
                "step_size: number of training iterations per",
                "half cycle. Authors suggest setting step_size",
                "2-8 x training iterations in epoch.",
                "mode: one of {triangular, triangular2, exp_range}.",
                "Default 'triangular'.",
                "Values correspond to policies detailed above.",
                "If scale_fn is not None, this argument is ignored.",
                "gamma: constant in 'exp_range' scaling function:",
                "gamma**(cycle iterations)",
                "scale_fn: Custom scaling policy defined by a single",
                "argument lambda function, where",
                "0 <= scale_fn(x) <= 1 for all x >= 0.",
                "mode paramater is ignored",
                "ASSIGN: {'cycle', 'iterations'}.",
                "Defines whether scale_fn is evaluated on",
                "cycle number or cycle iterations (training",
                "iterations since start of cycle). Default is 'cycle'.",
                "\"\"\"",
                "def __init__(self, base_lr=0.001, max_lr=0.006, ASSIGN=2000., mode='triangular',",
                "ASSIGN=1., scale_fn=None, scale_mode='cycle', decay=0.1):",
                "super(CyclicLR, self).__init__()",
                "self.base_lr = base_lr",
                "self.max_lr = max_lr",
                "self.ASSIGN = ASSIGN",
                "self.mode = mode",
                "self.ASSIGN = ASSIGN",
                "ASSIGN == None:",
                "if self.mode == 'triangular':",
                "self.scale_fn = lambda x: 1.",
                "self.ASSIGN = 'cycle'",
                "elif self.mode == 'triangular2':",
                "self.scale_fn = lambda x: 1path(2.**(x-1))",
                "self.ASSIGN = 'cycle'",
                "elif self.mode == 'exp_range':",
                "self.scale_fn = lambda x: ASSIGN**(x)",
                "self.ASSIGN = 'iterations'",
                "else:",
                "self.scale_fn = scale_fn",
                "self.ASSIGN = ASSIGN",
                "self.clr_iterations = 0.",
                "self.trn_iterations = 0.",
                "self.history = {}",
                "self.decay = decay",
                "self._reset()",
                "def _reset(self, new_base_lr=None, new_max_lr=None,",
                "ASSIGN=None):",
                "\"\"\"Resets cycle iterations.",
                "Optional boundarypath",
                "\"\"\"",
                "if new_base_lr != None:",
                "self.base_lr = new_base_lr",
                "if new_max_lr != None:",
                "self.max_lr = new_max_lr",
                "if ASSIGN != None:",
                "self.ASSIGN = ASSIGN",
                "self.clr_iterations = 0.",
                "def ASSIGN(self):",
                "ASSIGN = np.floor(1+self.clr_iterationspath(2*self.step_size))",
                "ASSIGN = np.abs(self.clr_iterationspath*cycle + 1)",
                "if self.ASSIGN == 'ASSIGN':",
                "return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-ASSIGN))*self.scale_fn(ASSIGN)",
                "else:",
                "return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-ASSIGN))*self.scale_fn(self.clr_iterations)",
                "def on_train_begin(self, logs={}):",
                "ASSIGN = ASSIGN or {}",
                "if self.clr_iterations == 0:",
                "K.set_value(self.model.optimizer.lr, self.base_lr)",
                "else:",
                "K.set_value(self.model.optimizer.lr, self.ASSIGN())",
                "def on_batch_end(self, epoch, ASSIGN=None):",
                "ASSIGN = ASSIGN or {}",
                "self.trn_iterations += 1",
                "self.clr_iterations += 1",
                "self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))",
                "self.history.setdefault('iterations', []).append(self.trn_iterations)",
                "for k, v in ASSIGN.items():",
                "self.history.setdefault(k, []).append(v)",
                "K.set_value(self.model.optimizer.lr, self.ASSIGN())",
                "def on_epoch_end(self, epoch, ASSIGN=None):",
                "self.base_lr = self.base_lr*np.exp(-epoch*self.decay)",
                "self.max_lr = self.max_lr*np.exp(-epoch*self.decay)"
            ],
            "tag_pred": [
                "process_data",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "from keras import Model",
                "from keras.layers import Dense, BatchNormalization, Activation, Dropout, Input, Add, AlphaDropout, GaussianNoise, GaussianDropout, Conv1D, GlobalMaxPool1D, Concatenate, LeakyReLU",
                "from keras.optimizers import Nadam",
                ""
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "OUTPUT_SIZE = 32"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "import keras.backend as K",
                "beta=OUTPUT_SIZE",
                "epsilon=1e-4",
                "def triplet_loss_distance(y_true, y_pred):",
                "    y_pred = K.sigmoid(y_pred)",
                "    anchor = y_pred[..., :OUTPUT_SIZE]",
                "    positive = y_pred[..., OUTPUT_SIZE:2*OUTPUT_SIZE]",
                "    negative = y_pred[..., 2*OUTPUT_SIZE:]",
                "    ",
                "    # distance between the anchor and the positive",
                "    pos_dist = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,positive)),1)",
                "    # distance between the anchor and the negative",
                "    neg_dist = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,negative)),1)",
                "    ",
                "    #Non Linear Values  ",
                "    ",
                "    # -ln(-x/N+1)",
                "    pos_dist = -K.tf.log(-K.tf.divide((pos_dist),beta)+1+epsilon)",
                "    neg_dist = -K.tf.log(-K.tf.divide((OUTPUT_SIZE-neg_dist),beta)+1+epsilon)",
                "    ",
                "    return pos_dist + neg_dist"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN=OUTPUT_SIZE",
                "ASSIGN=1e-4",
                "def triplet_loss_distance(y_true, y_pred):",
                "ASSIGN = K.sigmoid(ASSIGN)",
                "ASSIGN = y_pred[..., :OUTPUT_SIZE]",
                "ASSIGN = y_pred[..., OUTPUT_SIZE:2*OUTPUT_SIZE]",
                "ASSIGN = y_pred[..., 2*OUTPUT_SIZE:]",
                "ASSIGN = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,positive)),1)",
                "ASSIGN = K.tf.reduce_sum(K.tf.square(K.tf.subtract(anchor,negative)),1)",
                "ASSIGN = -K.tf.log(-K.tf.divide((ASSIGN),beta)+1+epsilon)",
                "ASSIGN = -K.tf.log(-K.tf.divide((OUTPUT_SIZE-ASSIGN),beta)+1+epsilon)",
                "return pos_dist + neg_dist"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "def get_model(shape=(200,)):",
                "    inp = Input(shape)",
                "    ",
                "    x = GaussianNoise(0.01)(inp)",
                "    x = Dense(128, kernel_initializer='glorot_normal')(x)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    ",
                "    x = Dense(128, kernel_initializer='glorot_normal')(x)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    ",
                "    x = Dense(64, kernel_initializer='glorot_normal')(x)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    ",
                "    x = Dense(64, kernel_initializer='glorot_normal')(x)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    ",
                "    x = Dense(32)(x)",
                "    ",
                "    model = Model(inp, x)",
                "    return model"
            ],
            "content_processed": [
                "def get_model(shape=(200,)):",
                "ASSIGN = Input(shape)",
                "ASSIGN = GaussianNoise(0.01)(inp)",
                "ASSIGN = Dense(128, kernel_initializer='glorot_normal')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dense(128, kernel_initializer='glorot_normal')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dense(64, kernel_initializer='glorot_normal')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dense(64, kernel_initializer='glorot_normal')(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dense(32)(ASSIGN)",
                "ASSIGN = Model(inp, x)",
                "return model"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "emb_model = get_model()",
                "    "
            ],
            "content_processed": [
                "ASSIGN = get_model()"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "p_inp, n_inp, a_inp = Input((200,)), Input((200,)), Input((200,))",
                "",
                "po, no, ao = emb_model(p_inp), emb_model(n_inp), emb_model(a_inp)",
                "out = Concatenate()([ao, po, no])",
                "",
                "model = Model(inputs=[p_inp, n_inp, a_inp], outputs=out)",
                "model.compile(optimizer=Nadam(1e-3), loss=triplet_loss_distance)",
                ""
            ],
            "content_processed": [
                "ASSIGN = Input((200,)), Input((200,)), Input((200,))",
                "ASSIGN = emb_model(p_inp), emb_model(n_inp), emb_model(a_inp)",
                "ASSIGN = Concatenate()([ao, po, no])",
                "ASSIGN = Model(inputs=[p_inp, n_inp, a_inp], outputs=out)",
                "ASSIGN.compile(optimizer=Nadam(1e-3), loss=triplet_loss_distance)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "x_train = train_df.iloc[:, 2:].values",
                "y = train_df.loc[:, 'target'].values"
            ],
            "content_processed": [
                "ASSIGN = train_df.iloc[:, 2:].values",
                "ASSIGN = train_df.loc[:, 'target'].values"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "model.fit_generator(DataGenerator(x_train, y, batch_size=1024), ",
                "                    steps_per_epoch=1000,",
                "                    epochs=15,",
                "                    callbacks=[CyclicLR(base_lr=1*1e-3, max_lr=7*1e-3, step_size=500)])"
            ],
            "content_processed": [
                "model.fit_generator(DataGenerator(x_train, y, batch_size=1024),",
                "ASSIGN=1000,",
                "ASSIGN=15,",
                "ASSIGN=[CyclicLR(base_lr=1*1e-3, max_lr=7*1e-3, step_size=500)])"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "x_emb = emb_model.predict(x_train, verbose=1)",
                "x_emb_t = emb_model.predict(test_df.iloc[:, 1:].values, verbose=1)",
                "",
                "np.savez('emb.npz', x=x_emb, xt=x_emb_t)"
            ],
            "content_processed": [
                "ASSIGN = emb_model.predict(x_train, verbose=1)",
                "ASSIGN = emb_model.predict(test_df.iloc[:, 1:].values, verbose=1)",
                "np.savez('emb.npz', x=ASSIGN, xt=ASSIGN)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "transfer_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "import matplotlib.pyplot as plt"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "from sklearn.decomposition import PCA"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "pca = PCA(n_components=2)",
                "x_pca = pca.fit_transform(x_emb)"
            ],
            "content_processed": [
                "ASSIGN = PCA(n_components=2)",
                "ASSIGN = pca.fit_transform(x_emb)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "pca.explained_variance_ratio_"
            ],
            "content_processed": [
                "pca.explained_variance_ratio_"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "xep = x_pca[y==1]",
                "xen = x_pca[y==0]"
            ],
            "content_processed": [
                "ASSIGN = x_pca[y==1]",
                "ASSIGN = x_pca[y==0]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                "plt.figure(figsize=(10,10))",
                "",
                "plt.scatter(xep[:, 0], xep[:, 1], alpha=0.9)",
                "plt.scatter(xen[:, 0], xen[:, 1], alpha=0.3)"
            ],
            "content_processed": [
                "plt.figure(figsize=(10,10))",
                "plt.scatter(xep[:, 0], xep[:, 1], alpha=0.9)",
                "plt.scatter(xen[:, 0], xen[:, 1], alpha=0.3)"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "15-nn-embedding",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "06-predicting-future-sales-with-lstm",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "",
                "# Any results you write to the current directory are saved as output."
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir())"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "06-predicting-future-sales-with-lstm",
            "content": [
                "# Adapted from https://www.kaggle.com/sebask/keras-2-0",
                "",
                "import time",
                "notebookstart= time.time()",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from keras.models import Sequential",
                "from keras.layers import LSTM, Dense, Activation, ThresholdedReLU, MaxPooling2D, Embedding, Dropout",
                "from keras.optimizers import Adam, SGD, RMSprop",
                "from keras import backend as K",
                "from sklearn.model_selection import train_test_split",
                "from keras.callbacks import EarlyStopping",
                "from sklearn.preprocessing import MinMaxScaler",
                "import gc",
                "",
                "# Viz",
                "import matplotlib.pyplot as plt",
                "",
                "# Import data",
                "sales = pd.read_csv('../input/sales_train.csv', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)",
                "shops = pd.read_csv('../input/shops.csv')",
                "items = pd.read_csv('../input/items.csv')",
                "cats = pd.read_csv('../input/item_categories.csv')",
                "val = pd.read_csv('../input/test.csv')",
                "",
                "# Rearrange the raw data to be monthly sales by item-shop",
                "df = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()",
                "df = df[['date','item_id','shop_id','item_cnt_day']]",
                "df[\"item_cnt_day\"].clip(0.,20.,inplace=True)",
                "df = df.pivot_table(index=['item_id','shop_id'], columns='date',values='item_cnt_day',fill_value=0).reset_index()",
                "",
                "# Merge data from monthly sales to specific item-shops in test data",
                "test = pd.merge(val,df,on=['item_id','shop_id'], how='left').fillna(0)",
                "",
                "# Strip categorical data so keras only sees raw timeseries",
                "test = test.drop(labels=['ID','item_id','shop_id'],axis=1)",
                "",
                "# Rearrange the raw data to be monthly average price by item-shop",
                "# Scale Price",
                "scaler = MinMaxScaler(feature_range=(0, 1))",
                "sales[\"item_price\"] = scaler.fit_transform(sales[\"item_price\"].values.reshape(-1,1))",
                "df2 = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).mean().reset_index()",
                "df2 = df2[['date','item_id','shop_id','item_price']].pivot_table(index=['item_id','shop_id'], columns='date',values='item_price',fill_value=0).reset_index()",
                "",
                "# Merge data from average prices to specific item-shops in test data",
                "price = pd.merge(val,df2,on=['item_id','shop_id'], how='left').fillna(0)",
                "price = price.drop(labels=['ID','item_id','shop_id'],axis=1)",
                "",
                "# Create x and y training sets from oldest data points",
                "y_train = test['2015-10']",
                "x_sales = test.drop(labels=['2015-10'],axis=1)",
                "x_sales = x_sales.values.reshape((x_sales.shape[0], x_sales.shape[1], 1))",
                "x_prices = price.drop(labels=['2015-10'],axis=1)",
                "x_prices= x_prices.values.reshape((x_prices.shape[0], x_prices.shape[1], 1))",
                "X = np.append(x_sales,x_prices,axis=2)",
                "",
                "y = y_train.values.reshape((214200, 1))",
                "print(\"Training Predictor Shape: \",X.shape)",
                "print(\"Training Predictee Shape: \",y.shape)",
                "del y_train, x_sales; gc.collect()",
                "",
                "# Transform test set into numpy matrix",
                "test = test.drop(labels=['2013-01'],axis=1)",
                "x_test_sales = test.values.reshape((test.shape[0], test.shape[1], 1))",
                "x_test_prices = price.drop(labels=['2013-01'],axis=1)",
                "x_test_prices = x_test_prices.values.reshape((x_test_prices.shape[0], x_test_prices.shape[1], 1))",
                "",
                "# Combine Price and Sales Df",
                "test = np.append(x_test_sales,x_test_prices,axis=2)",
                "del x_test_sales,x_test_prices, price; gc.collect()",
                "print(\"Test Predictor Shape: \",test.shape)",
                "",
                "print(\"Modeling Stage\")",
                "# Define the model layers",
                "model_lstm = Sequential()",
                "model_lstm.add(LSTM(16, input_shape=(X.shape[1], X.shape[2]),return_sequences=True))",
                "model_lstm.add(Dropout(0.5))",
                "model_lstm.add(LSTM(32))",
                "model_lstm.add(Dropout(0.5))",
                "model_lstm.add(Dense(1))",
                "model_lstm.compile(optimizer=\"adam\", loss='mse', metrics=[\"mse\"])",
                "print(model_lstm.summary())",
                "",
                "# Train Model",
                "print(\"\\nFit Model\")",
                "VALID = True",
                "LSTM_PARAM = {\"batch_size\":128,",
                "              \"verbose\":2,",
                "              \"epochs\":10}",
                "",
                "modelstart = time.time()",
                "if VALID is True:",
                "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=1, shuffle=False)",
                "    # del X,y; gc.collect()",
                "    print(\"X Train Shape: \",X_train.shape)",
                "    print(\"X Valid Shape: \",X_valid.shape)",
                "    print(\"y Train Shape: \",y_train.shape)",
                "    print(\"y Valid Shape: \",y_valid.shape)",
                "    ",
                "    callbacks_list=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=3,mode='auto')]",
                "    hist = model_lstm.fit(X_train, y_train,",
                "                          validation_data=(X_valid, y_valid),",
                "                          callbacks=callbacks_list,",
                "                          **LSTM_PARAM)",
                "    pred = model_lstm.predict(test)",
                "",
                "    # Model Evaluation",
                "    best = np.argmin(hist.history[\"val_loss\"])",
                "    print(\"Optimal Epoch: {}\",best)",
                "    print(\"Train Score: {}, Validation Score: {}\".format(hist.history[\"loss\"][best],hist.history[\"val_loss\"][best]))",
                "",
                "    plt.plot(hist.history['loss'], label='train')",
                "    plt.plot(hist.history['val_loss'], label='validation')",
                "    plt.xlabel(\"Epochs\")",
                "    plt.ylabel(\"Mean Square Error\")",
                "    plt.legend()",
                "    plt.show()",
                "    plt.savefig(\"Train and Validation MSE Progression.png\")",
                "",
                "if VALID is False:",
                "    print(\"X Shape: \",X.shape)",
                "    print(\"y Shape: \",y.shape)",
                "    hist = model_lstm.fit(X,y,**LSTM_PARAM)",
                "    pred = model_lstm.predict(X)",
                "    ",
                "    plt.plot(hist.history['loss'], label='Training Loss')",
                "    plt.xlabel(\"Epochs\")",
                "    plt.ylabel(\"Mean Square Error\")",
                "    plt.legend()",
                "    plt.show()",
                "    plt.savefig(\"Training Loss Progression.png\")",
                "",
                "print(\"\\Output Submission\")",
                "submission = pd.DataFrame(pred,columns=['item_cnt_month'])",
                "submission.to_csv('submission.csv',index_label='ID')",
                "print(submission.head())",
                "print(\"\\nModel Runtime: %0.2f Minutes\"%((time.time() - modelstart)/60))",
                "print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)/60))"
            ],
            "content_processed": [
                "SETUP",
                "SETUP",
                "CHECKPOINT",
                "ASSIGN= time.time()",
                "ASSIGN = pd.read_csv('..path', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()",
                "ASSIGN = ASSIGN[['date','item_id','shop_id','item_cnt_day']]",
                "ASSIGN[\"item_cnt_day\"].clip(0.,20.,inplace=True)",
                "ASSIGN = ASSIGN.pivot_table(index=['item_id','shop_id'], columns='date',values='item_cnt_day',fill_value=0).reset_index()",
                "ASSIGN = pd.merge(val,df,on=['item_id','shop_id'], how='left').fillna(0)",
                "ASSIGN = ASSIGN.drop(labels=['ID','item_id','shop_id'],axis=1)",
                "ASSIGN = MinMaxScaler(feature_range=(0, 1))",
                "ASSIGN[\"item_price\"] = ASSIGN.fit_transform(ASSIGN[\"item_price\"].values.reshape(-1,1))",
                "ASSIGN = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).mean().reset_index()",
                "ASSIGN = ASSIGN[['date','item_id','shop_id','item_price']].pivot_table(index=['item_id','shop_id'], columns='date',values='item_price',fill_value=0).reset_index()",
                "ASSIGN = pd.merge(val,df2,on=['item_id','shop_id'], how='left').fillna(0)",
                "ASSIGN = ASSIGN.drop(labels=['ID','item_id','shop_id'],axis=1)",
                "ASSIGN = test['2015-10']",
                "ASSIGN = test.drop(labels=['2015-10'],axis=1)",
                "ASSIGN = ASSIGN.values.reshape((ASSIGN.shape[0], ASSIGN.shape[1], 1))",
                "ASSIGN = price.drop(labels=['2015-10'],axis=1)",
                "ASSIGN= ASSIGN.values.reshape((ASSIGN.shape[0], ASSIGN.shape[1], 1))",
                "ASSIGN = np.append(x_sales,x_prices,axis=2)",
                "ASSIGN = y_train.values.reshape((214200, 1))",
                "print(,ASSIGN.shape)",
                "print(,ASSIGN.shape)",
                "del ASSIGN, ASSIGN; gc.collect()",
                "ASSIGN = ASSIGN.drop(labels=['2013-01'],axis=1)",
                "ASSIGN = test.values.reshape((test.shape[0], test.shape[1], 1))",
                "ASSIGN = price.drop(labels=['2013-01'],axis=1)",
                "ASSIGN = ASSIGN.values.reshape((ASSIGN.shape[0], ASSIGN.shape[1], 1))",
                "ASSIGN = np.append(x_test_sales,x_test_prices,axis=2)",
                "del ASSIGN,ASSIGN, ASSIGN; gc.collect()",
                "print(,ASSIGN.shape)",
                "print()",
                "ASSIGN = Sequential()",
                "ASSIGN.add(LSTM(16, input_shape=(ASSIGN.shape[1], ASSIGN.shape[2]),return_sequences=True))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(LSTM(32))",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(1))",
                "ASSIGN.compile(optimizer=\"adam\", loss='mse', metrics=[\"mse\"])",
                "print(ASSIGN.summary())",
                "print()",
                "\"verbose\":2,",
                "\"epochs\":10}",
                "ASSIGN = time.time()",
                "if VALID is True:",
                "X_train, X_valid, ASSIGN, y_valid = train_test_split(ASSIGN, ASSIGN, test_size=0.10, random_state=1, shuffle=False)",
                "print(,X_train.shape)",
                "print(,X_valid.shape)",
                "print(,ASSIGN.shape)",
                "print(,y_valid.shape)",
                "ASSIGN=[EarlyStopping(monitor=\"val_loss\",min_delta=.001, patience=3,mode='auto')]",
                "ASSIGN = model_lstm.fit(X_train, y_train,",
                "ASSIGN=(X_valid, y_valid),",
                "ASSIGN=callbacks_list,",
                "**LSTM_PARAM)",
                "ASSIGN = model_lstm.predict(test)",
                "ASSIGN = np.argmin(hist.history[\"val_loss\"])",
                "print(,ASSIGN)",
                "print(.format(ASSIGN.history[][ASSIGN],ASSIGN.history[][ASSIGN]))",
                "plt.plot(ASSIGN.history['loss'], label='train')",
                "plt.plot(ASSIGN.history['val_loss'], label='validation')",
                "plt.xlabel(\"Epochs\")",
                "plt.ylabel(\"Mean Square Error\")",
                "plt.legend()",
                "plt.show()",
                "plt.savefig(\"Train and Validation MSE Progression.png\")",
                "if VALID is False:",
                "print(,ASSIGN.shape)",
                "print(,ASSIGN.shape)",
                "ASSIGN = model_lstm.fit(X,y,**LSTM_PARAM)",
                "ASSIGN = model_lstm.predict(X)",
                "plt.plot(ASSIGN.history['loss'], label='Training Loss')",
                "plt.xlabel(\"Epochs\")",
                "plt.ylabel(\"Mean Square Error\")",
                "plt.legend()",
                "plt.show()",
                "plt.savefig(\"Training Loss Progression.png\")",
                "print()",
                "ASSIGN = pd.DataFrame(pred,columns=['item_cnt_month'])",
                "ASSIGN.to_csv('ASSIGN.csv',index_label='ID')",
                "print(ASSIGN.head())",
                "print(%((time.time() - ASSIGN)path))",
                "print(%((time.time() - ASSIGN)path))"
            ],
            "tag_pred": [
                "setup_notebook",
                "visualize_data",
                "check_results",
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "train_model",
                "evaluate_model",
                "check_results",
                "transfer_results",
                "visualize_data"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading",
                "Visualization",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading",
                "Visualization",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "06-predicting-future-sales-with-lstm",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "03-titanic-easy-deeplearning-acc-78",
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "import tensorflow as tf",
                "from tensorflow import keras",
                "train = pd.read_csv('/kaggle/input/titanic/train.csv')",
                "train['Sec_Name'] = train['Name'].astype(str).str.split().str[1]",
                "#print(train)",
                "Y_train = np.array(train['Survived'])",
                "X_train = train[['Pclass', 'Sex', 'Age', 'Embarked', 'Sec_Name']]",
                "X_train = X_train.replace('male', 0)",
                "X_train = X_train.replace('female', 1)",
                "X_train['Embarked'] = X_train['Embarked'].replace('S',1)",
                "X_train['Embarked'] = X_train['Embarked'].replace('C',2)",
                "X_train['Embarked'] = X_train['Embarked'].replace('Q',3)",
                "X_train = X_train.replace(np.nan, X_train['Age'].mean())",
                "X_train['Sec_Name'] = X_train['Sec_Name'].replace('Mr.',1)",
                "X_train['Sec_Name'] = X_train['Sec_Name'].replace('Mrs.',2)",
                "X_train['Sec_Name'] = X_train['Sec_Name'].replace('Miss.',3)",
                "X_train['Sec_Name'] = X_train['Sec_Name'].replace('Master.',4)",
                "X_train['Sec_Name'] = pd.to_numeric(X_train['Sec_Name'], errors = 'coerce')",
                "X_train['Sec_Name'] = X_train['Sec_Name'].replace(np.nan,0)",
                "X_train = np.array(X_train)",
                "print(X_train)"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN['Sec_Name'] = ASSIGN['Name'].astype(str).str.split().str[1]",
                "ASSIGN = np.array(train['Survived'])",
                "ASSIGN = train[['Pclass', 'Sex', 'Age', 'Embarked', 'Sec_Name']]",
                "ASSIGN = ASSIGN.replace('male', 0)",
                "ASSIGN = ASSIGN.replace('female', 1)",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('S',1)",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('C',2)",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('Q',3)",
                "ASSIGN = ASSIGN.replace(np.nan, ASSIGN['Age'].mean())",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Mr.',1)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Mrs.',2)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Miss.',3)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Master.',4)",
                "ASSIGN['Sec_Name'] = pd.to_numeric(ASSIGN['Sec_Name'], errors = 'coerce')",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace(np.nan,0)",
                "ASSIGN = np.array(ASSIGN)",
                "print(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "ingest_data",
                "process_data",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "ingest_data",
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "03-titanic-easy-deeplearning-acc-78",
            "content": [
                "class myCallback(tf.keras.callbacks.Callback):",
                "    def on_epoch_end(self, epoch, logs={}):",
                "        if(logs.get('accuracy') > 0.82):",
                "            self.model.stop_training = True",
                "callbacks = myCallback();"
            ],
            "content_processed": [
                "class myCallback(tf.keras.callbacks.Callback):",
                "def on_epoch_end(self, epoch, logs={}):",
                "if(logs.get('accuracy') > 0.82):",
                "self.model.stop_training = True",
                "ASSIGN = myCallback();"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "03-titanic-easy-deeplearning-acc-78",
            "content": [
                "model = tf.keras.Sequential([keras.layers.Dense(5, input_dim = 5, activation = tf.nn.relu), tf.keras.layers.Dense(4, activation = tf.nn.relu), tf.keras.layers.Dense(3, activation = tf.nn.relu), tf.keras.layers.Dense(2, activation = tf.nn.relu), tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)])",
                "model.compile(optimizer=\"Adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])",
                "model.fit(X_train, Y_train, validation_split=0.15,epochs = 100,batch_size=5, callbacks = [callbacks])"
            ],
            "content_processed": [
                "ASSIGN = tf.keras.Sequential([keras.layers.Dense(5, input_dim = 5, activation = tf.nn.relu), tf.keras.layers.Dense(4, activation = tf.nn.relu), tf.keras.layers.Dense(3, activation = tf.nn.relu), tf.keras.layers.Dense(2, activation = tf.nn.relu), tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)])",
                "ASSIGN.compile(optimizer=\"Adam\", loss = 'binary_crossentropy', metrics = ['accuracy'])",
                "ASSIGN.fit(X_train, Y_train, validation_split=0.15,epochs = 100,batch_size=5, callbacks = [callbacks])"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "03-titanic-easy-deeplearning-acc-78",
            "content": [
                "test = pd.read_csv('/kaggle/input/titanic/test.csv')",
                "test['Sec_Name'] = test['Name'].astype(str).str.split().str[1]",
                "X_test = test[['Pclass', 'Sex', 'Age', 'Embarked', 'Sec_Name']]",
                "X_test = X_test.replace('male', 0)",
                "X_test = X_test.replace('female', 1)",
                "X_test = X_test.replace(np.nan, X_test['Age'].mean())",
                "X_test['Embarked'] = X_test['Embarked'].replace('S',1)",
                "X_test['Embarked'] = X_test['Embarked'].replace('C',2)",
                "X_test['Embarked'] = X_test['Embarked'].replace('Q',3)",
                "X_test['Sec_Name'] = X_test['Sec_Name'].replace('Mr.',1)",
                "X_test['Sec_Name'] = X_test['Sec_Name'].replace('Mrs.',2)",
                "X_test['Sec_Name'] = X_test['Sec_Name'].replace('Miss.',3)",
                "X_test['Sec_Name'] = X_test['Sec_Name'].replace('Master.',4)",
                "X_test['Sec_Name'] = pd.to_numeric(X_test['Sec_Name'], errors = 'coerce')",
                "X_test['Sec_Name'] = X_test['Sec_Name'].replace(np.nan,0)",
                "X_test = np.array(X_test)",
                "p = model.predict(X_test)",
                "p = np.where(p >= 0.5, 1, 0)",
                "#model.evaluate(x_test, y_test)",
                "#np.savetxt(\"test_ans4.csv\", p, delimiter=\",\")",
                "df_sub = pd.DataFrame()",
                "df_sub['PassengerId'] = test['PassengerId']",
                "df_sub['Survived'] = p.astype(np.int)",
                "",
                "df_sub.to_csv('submission4.csv', index=False)"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('path')",
                "ASSIGN['Sec_Name'] = ASSIGN['Name'].astype(str).str.split().str[1]",
                "ASSIGN = test[['Pclass', 'Sex', 'Age', 'Embarked', 'Sec_Name']]",
                "ASSIGN = ASSIGN.replace('male', 0)",
                "ASSIGN = ASSIGN.replace('female', 1)",
                "ASSIGN = ASSIGN.replace(np.nan, ASSIGN['Age'].mean())",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('S',1)",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('C',2)",
                "ASSIGN['Embarked'] = ASSIGN['Embarked'].replace('Q',3)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Mr.',1)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Mrs.',2)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Miss.',3)",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace('Master.',4)",
                "ASSIGN['Sec_Name'] = pd.to_numeric(ASSIGN['Sec_Name'], errors = 'coerce')",
                "ASSIGN['Sec_Name'] = ASSIGN['Sec_Name'].replace(np.nan,0)",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = model.predict(X_test)",
                "ASSIGN = np.where(ASSIGN >= 0.5, 1, 0)",
                "ASSIGN = pd.DataFrame()",
                "ASSIGN['PassengerId'] = ASSIGN['PassengerId']",
                "ASSIGN['Survived'] = ASSIGN.astype(np.int)",
                "ASSIGN.to_csv('submission4.csv', index=False)"
            ],
            "tag_pred": [
                "ingest_data",
                "process_data",
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "transfer_results",
                "process_data",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "03-titanic-easy-deeplearning-acc-78",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import seaborn as sns",
                "",
                "from keras.regularizers import l1",
                "from keras import backend as K",
                "",
                "from sklearn.preprocessing import StandardScaler",
                "from sklearn.model_selection import train_test_split",
                "from scipy.stats import skew",
                "from scipy.stats.stats import pearsonr",
                "",
                "import matplotlib.pyplot as plt",
                "%matplotlib inline",
                "",
                "import csv",
                "import re",
                "",
                "from keras.models import Sequential",
                "from keras.layers import Dense, Activation, BatchNormalization, Dropout",
                "from keras.callbacks import ModelCheckpoint",
                "from keras.optimizers import Adam",
                "",
                "# ignore Deprecation Warning",
                "import warnings",
                "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) ",
                "",
                "import os",
                "",
                "train_orj = pd.read_csv(\"../input/train.csv\", header=0)",
                "test_orj = pd.read_csv(\"../input/test.csv\", header=0)",
                "train_orj.head()",
                "# Any results you write to the current directory are saved as output."
            ],
            "content_processed": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)",
                "ASSIGN = pd.read_csv(\"..path\", header=0)",
                "ASSIGN = pd.read_csv(\"..path\", header=0)",
                "ASSIGN.head()"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "ingest_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "ingest_data",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "train_orj.info()"
            ],
            "content_processed": [
                "train_orj.info()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "def preprocess(data):",
                "    ",
                "    #Kabin",
                "    data.Cabin.fillna('9', inplace=True)",
                "    #data['Cabin'].replace('0', 9, inplace=True)",
                "    data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1",
                "    data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2",
                "    data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3",
                "    data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4",
                "    data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5",
                "    data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6",
                "    data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7",
                "    data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8",
                "    data = data.drop([\"Cabin\"],axis=1)",
                "",
                "    # Cinsiyeti tam sayya evirelim",
                "    data['Sex'].replace('female', 1, inplace=True)",
                "    data['Sex'].replace('male', 2, inplace=True)",
                "    ",
                "    # Gemiye bini limanlarn tam sayya evirelim",
                "    data['Embarked'].replace('S', 1, inplace=True)",
                "    data['Embarked'].replace('C', 2, inplace=True)",
                "    data['Embarked'].replace('Q', 3, inplace=True)",
                "    ",
                "    # Olmayan (NA) ya deerlerini medyan ile dolduralm",
                "    data['Age'].fillna(data['Age'].median(), inplace=True)",
                "    #data['Age'] = [0 if each >= 60 else 1 if each >= 35 else 2 if each >= 18 else 3 if each >= 12 else 4 if each >= 5 else 5 for each in data['Age']]",
                "   ",
                "    data['Fare'].fillna(data['Fare'].median(), inplace=True)",
                "    data['Embarked'].fillna(data['Embarked'].median(), inplace=True)",
                "    ",
                "    data = data.drop([\"Ticket\"],axis=1)",
                "    data = data.drop([\"Fare\"],axis=1)",
                "    data['SibSp'].replace(0, 9, inplace=True)",
                "    data['Parch'].replace(0, 9, inplace=True)",
                "    return data",
                "",
                "def group_titles(data):",
                "    #data['Names'] = data['Name'].map(lambda x: len(re.split(' ', x)))",
                "    data['Title'] = data['Name'].map(lambda x: re.search(', (.+?) ', x).group(1))",
                "    data['Title'].replace('Master.', 1, inplace=True)",
                "    data['Title'].replace('Mr.', 2, inplace=True)",
                "    data['Title'].replace(['Ms.','Mlle.', 'Miss.'], 3, inplace=True)",
                "    data['Title'].replace(['Mme.', 'Mrs.'], 4, inplace=True)",
                "    data['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 5, inplace=True)",
                "    return data"
            ],
            "content_processed": [
                "def preprocess(data):",
                "data.Cabin.fillna('9', inplace=True)",
                "data.loc[data.Cabin.str[0] == 'A', 'Cabin'] = 1",
                "data.loc[data.Cabin.str[0] == 'B', 'Cabin'] = 2",
                "data.loc[data.Cabin.str[0] == 'C', 'Cabin'] = 3",
                "data.loc[data.Cabin.str[0] == 'D', 'Cabin'] = 4",
                "data.loc[data.Cabin.str[0] == 'E', 'Cabin'] = 5",
                "data.loc[data.Cabin.str[0] == 'F', 'Cabin'] = 6",
                "data.loc[data.Cabin.str[0] == 'G', 'Cabin'] = 7",
                "data.loc[data.Cabin.str[0] == 'T', 'Cabin'] = 8",
                "ASSIGN = ASSIGN.drop([\"Cabin\"],axis=1)",
                "ASSIGN['Sex'].replace('female', 1, inplace=True)",
                "ASSIGN['Sex'].replace('male', 2, inplace=True)",
                "ASSIGN['Embarked'].replace('S', 1, inplace=True)",
                "ASSIGN['Embarked'].replace('C', 2, inplace=True)",
                "ASSIGN['Embarked'].replace('Q', 3, inplace=True)",
                "ASSIGN['Age'].fillna(ASSIGN['Age'].median(), inplace=True)",
                "ASSIGN['Fare'].fillna(ASSIGN['Fare'].median(), inplace=True)",
                "ASSIGN['Embarked'].fillna(ASSIGN['Embarked'].median(), inplace=True)",
                "ASSIGN = ASSIGN.drop([\"Ticket\"],axis=1)",
                "ASSIGN = ASSIGN.drop([\"Fare\"],axis=1)",
                "ASSIGN['SibSp'].replace(0, 9, inplace=True)",
                "ASSIGN['Parch'].replace(0, 9, inplace=True)",
                "return data",
                "def group_titles(ASSIGN):",
                "ASSIGN = ASSIGN.map(lambda x: re.search(', (.+?) ', x).group(1))",
                "ASSIGN['Title'].replace('Master.', 1, inplace=True)",
                "ASSIGN['Title'].replace('Mr.', 2, inplace=True)",
                "ASSIGN['Title'].replace(['Ms.','Mlle.', 'Miss.'], 3, inplace=True)",
                "ASSIGN['Title'].replace(['Mme.', 'Mrs.'], 4, inplace=True)",
                "ASSIGN['Title'].replace(['Dona.', 'Lady.', 'the Countess.', 'Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'the'], 5, inplace=True)",
                "return data"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "train = train_orj.copy().drop([\"PassengerId\"],axis=1)",
                "train=preprocess(train)",
                "train=group_titles(train)",
                "train = train.drop([\"Name\"],axis=1)",
                "train.head()"
            ],
            "content_processed": [
                "ASSIGN = train_orj.copy().drop([\"PassengerId\"],axis=1)",
                "ASSIGN=preprocess(ASSIGN)",
                "ASSIGN=group_titles(ASSIGN)",
                "ASSIGN = ASSIGN.drop([\"Name\"],axis=1)",
                "ASSIGN.head()"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "train.info()"
            ],
            "content_processed": [
                "train.info()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "plt.figure(figsize=(10,5))",
                "sns.countplot(train.Age, palette=\"icefire\")",
                "#train.Age.value_counts()"
            ],
            "content_processed": [
                "plt.figure(figsize=(10,5))",
                "sns.countplot(train.Age, palette=\"icefire\")"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "x = train.iloc[:,1:train.shape[1]].values #bamsz deikenler",
                "y = train.Survived.values",
                "",
                "from sklearn.model_selection import train_test_split",
                "X_train, X_val, Y_train, Y_val = train_test_split(x, y, test_size = 0.1, random_state=2)",
                "print(\"x_train shape\",X_train.shape)",
                "print(\"x_test shape\",X_val.shape)",
                "print(\"y_train shape\",Y_train.shape)",
                "print(\"y_test shape\",Y_val.shape)",
                "",
                "",
                "#Y_train=np.array(Y_train).astype(int)",
                "#X_train=np.array(X_train).astype(float)",
                "#Y_val = np.array(Y_val).astype(int)",
                "#X_val = np.array(X_val).astype(float)"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = train.iloc[:,1:train.shape[1]].values",
                "ASSIGN = train.Survived.values",
                "X_train, X_val, Y_train, Y_val = train_test_split(ASSIGN, ASSIGN, test_size = 0.1, random_state=2)",
                "print(,X_train.shape)",
                "print(,X_val.shape)",
                "print(,Y_train.shape)",
                "print(,Y_val.shape)"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "plt.figure(figsize=(10,5))",
                "sns.countplot(Y_train, palette=\"icefire\")",
                "plt.title(\"Number of Survived classes\")"
            ],
            "content_processed": [
                "plt.figure(figsize=(10,5))",
                "sns.countplot(Y_train, palette=\"icefire\")",
                "plt.title(\"Number of Survived classes\")"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "#model 1",
                "'''",
                "num_epochs = 200",
                "batch_size = 32",
                "",
                "model = Sequential()",
                "model.add(Dense(64, input_dim=input_length-1, activation='softplus'))",
                "model.add(Dense(32, activation='softplus'))",
                "model.add(Dense(16, activation='softplus'))  ",
                "model.add(Dense(8, activation='softplus')) ",
                "model.add(Dense(1, activation='softplus'))",
                "",
                "lr = .001",
                "adam0 = Adam(lr = lr)",
                "",
                "# Modeli derleyip ve daha iyi bir sonu elde edildiinde arlklar kaydedelim",
                "model.compile(loss='binary_crossentropy', optimizer=adam0, metrics=['accuracy'])",
                "filepath = 'weights.best.hdf5'",
                "checkpoint = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')",
                "callbacks_list = [checkpoint]",
                "",
                "history_model = model.fit(X_train, Y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0)",
                "",
                "'''"
            ],
            "content_processed": [
                "'''",
                "ASSIGN = 200",
                "ASSIGN = 32",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(64, input_dim=input_length-1, activation='softplus'))",
                "ASSIGN.add(Dense(32, activation='softplus'))",
                "ASSIGN.add(Dense(16, activation='softplus'))",
                "ASSIGN.add(Dense(8, activation='softplus'))",
                "ASSIGN.add(Dense(1, activation='softplus'))",
                "ASSIGN = .001",
                "ASSIGN = Adam(lr = lr)",
                "ASSIGN.compile(loss='binary_crossentropy', optimizer=ASSIGN, metrics=['accuracy'])",
                "ASSIGN = 'weights.best.hdf5'",
                "ASSIGN = ModelCheckpoint(filepath, monitor='acc', verbose=1, save_best_only=True, mode='max')",
                "ASSIGN = [checkpoint]",
                "ASSIGN = model.fit(X_train, Y_train, callbacks=callbacks_list, epochs=num_epochs, batch_size=batch_size, verbose=0)",
                "'''"
            ],
            "tag_pred": [
                "check_results",
                "train_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results",
                "transfer_results"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "# model 2",
                "model = Sequential()",
                "",
                "# layers",
                "model.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))",
                "model.add(BatchNormalization())",
                "model.add(Dropout(0.5))",
                "model.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'relu'))",
                "model.add(BatchNormalization())",
                "model.add(Dropout(0.5))",
                "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))",
                "model.add(BatchNormalization())",
                "model.add(Dropout(0.5))",
                "model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))",
                "model.add(Dropout(0.3))",
                "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))",
                "",
                "# Compiling the ANN",
                "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)",
                "model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])",
                "",
                "# Train the ANN",
                "history = model.fit(X_train, Y_train, batch_size = 32, epochs = 300, validation_data = (X_val,Y_val))",
                "",
                "scores = model.evaluate(X_train, Y_train, verbose=0)",
                "print(\"%s: %.3f%%\" % (model.metrics_names[1], scores[1]*100))"
            ],
            "content_processed": [
                "CHECKPOINT",
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units = 128, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))",
                "ASSIGN.add(BatchNormalization())",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(units = 64, kernel_initializer = 'uniform', activation = 'relu'))",
                "ASSIGN.add(BatchNormalization())",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))",
                "ASSIGN.add(BatchNormalization())",
                "ASSIGN.add(Dropout(0.5))",
                "ASSIGN.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))",
                "ASSIGN.add(Dropout(0.3))",
                "ASSIGN.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))",
                "ASSIGN = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)",
                "ASSIGN.compile(ASSIGN = ASSIGN, loss = 'binary_crossentropy', metrics = ['accuracy'])",
                "ASSIGN = model.fit(X_train, Y_train, batch_size = 32, epochs = 300, validation_data = (X_val,Y_val))",
                "ASSIGN = model.evaluate(X_train, Y_train, verbose=0)",
                "print( % (ASSIGN.metrics_names[1], ASSIGN[1]*100))"
            ],
            "tag_pred": [
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "# Plot the loss and accuracy curves for training and validation ",
                "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")",
                "plt.xlabel(\"Number of Epochs\")",
                "plt.ylabel(\"Loss\")",
                "plt.legend()",
                "plt.show()"
            ],
            "content_processed": [
                "plt.plot(history.history['val_loss'], color='b', label=\"validation loss\")",
                "plt.xlabel(\"Number of Epochs\")",
                "plt.ylabel(\"Loss\")",
                "plt.legend()",
                "plt.show()"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "# confusion matrix",
                "import seaborn as sns",
                "from sklearn.metrics import confusion_matrix",
                "# Predict the values from the validation dataset",
                "y_pred = model.predict(X_val)",
                "y_final = (y_pred > 0.5).astype(int).reshape(X_val.shape[0])",
                "# compute the confusion matrix",
                "confusion_mtx = confusion_matrix(Y_val, y_final) ",
                "# plot the confusion matrix",
                "f,ax = plt.subplots(figsize=(8, 8))",
                "sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)",
                "plt.xlabel(\"Predicted Label\")",
                "plt.ylabel(\"True Label\")",
                "plt.title(\"Confusion Matrix\")",
                "plt.show()"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = model.predict(X_val)",
                "ASSIGN = (y_pred > 0.5).astype(int).reshape(X_val.shape[0])",
                "ASSIGN = confusion_matrix(Y_val, y_final)",
                "ASSIGN = plt.subplots(figsize=(8, 8))",
                "sns.heatmap(ASSIGN, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)",
                "plt.xlabel(\"Predicted Label\")",
                "plt.ylabel(\"True Label\")",
                "plt.title(\"Confusion Matrix\")",
                "plt.show()"
            ],
            "tag_pred": [
                "setup_notebook",
                "visualize_data",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "evaluate_model",
                "visualize_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Visualization"
            ],
            "headergen_sot": [
                "Library Loading",
                "Visualization"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "# model 3",
                "'''",
                "from keras.layers import Input",
                "import keras",
                "from keras.models import Model",
                "",
                "def DenseNet(X_train):",
                "    ip = Input(shape=(X_train.shape[1],))",
                "    x_list = [ip]",
                "    ",
                "    x = Dense(128, use_bias=False)(ip)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(128, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(64, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(64, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(32, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(32, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(16, use_bias=False)(x)    ",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)",
                "    ",
                "    x_list.append(x)",
                "    x = keras.layers.concatenate(x_list)    ",
                "    x = Dense(16, use_bias=False)(ip)",
                "    x = BatchNormalization()(x)",
                "    x = Activation('relu')(x)",
                "    x = Dropout(0.5)(x)    ",
                "    ",
                "    op = Dense(1, activation='sigmoid')(x)",
                "",
                "    model = Model(inputs=ip, outputs=op)",
                "    adam = Adam(lr=0.05,)",
                "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])",
                "    return model",
                "",
                "model = DenseNet(X_train)",
                "history_model=model.fit(X_train, Y_train, epochs=32, batch_size=200, verbose=0,",
                "          validation_split=0.1)",
                "scores = model.evaluate(X_train, Y_train, verbose=0)",
                "print(\"%s: %.3f%%\" % (model.metrics_names[1], scores[1]*100))",
                "'''"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "'''",
                "def DenseNet(X_train):",
                "ASSIGN = Input(shape=(X_train.shape[1],))",
                "ASSIGN = [ip]",
                "ASSIGN = Dense(128, use_bias=False)(ip)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(128, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(64, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(64, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(32, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(32, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(16, use_bias=False)(ASSIGN)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = keras.layers.concatenate(x_list)",
                "ASSIGN = Dense(16, use_bias=False)(ip)",
                "ASSIGN = BatchNormalization()(ASSIGN)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Dropout(0.5)(ASSIGN)",
                "ASSIGN = Dense(1, activation='sigmoid')(x)",
                "ASSIGN = Model(inputs=ip, outputs=op)",
                "ASSIGN = Adam(lr=0.05,)",
                "ASSIGN.compile(loss='binary_crossentropy', optimizer=ASSIGN, metrics=['accuracy'])",
                "return model",
                "ASSIGN = DenseNet(X_train)",
                "ASSIGN=model.fit(X_train, Y_train, epochs=32, batch_size=200, verbose=0,",
                "ASSIGN=0.1)",
                "ASSIGN = model.evaluate(X_train, Y_train, verbose=0)",
                "print( % (ASSIGN.metrics_names[1], ASSIGN[1]*100))",
                "'''"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "train_model",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "train_model",
                "evaluate_model",
                "check_results"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "test = test_orj.copy()",
                "test=preprocess(test)",
                "test=group_titles(test)",
                "test = test.drop([\"Name\"],axis=1)",
                "test.head()",
                "#print(test.Title.value_counts())",
                "test.info()"
            ],
            "content_processed": [
                "ASSIGN = test_orj.copy()",
                "ASSIGN=preprocess(ASSIGN)",
                "ASSIGN=group_titles(ASSIGN)",
                "ASSIGN = ASSIGN.drop([\"Name\"],axis=1)",
                "ASSIGN.head()",
                "ASSIGN.info()"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "sns.countplot(test.Age, palette=\"icefire\")",
                "#train.Age.value_counts()"
            ],
            "content_processed": [
                "sns.countplot(test.Age, palette=\"icefire\")"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "01-keras-deep-learning-to-solve-titanic",
            "content": [
                "test_ids = test.iloc[:,0].values #test_orj['PassengerId'].copy()",
                "testdata = test.iloc[:,1:test.shape[1]].values #bamsz deikenler",
                "X_test =testdata # np.array(testdata).astype(float)",
                "",
                "#print(len(X_test))",
                "#print(X_test[0])",
                "",
                "y_pred = model.predict(X_test)",
                "y_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])",
                "#print(len(y_final))",
                "output = pd.DataFrame({'PassengerId': test_orj['PassengerId'], 'Survived': y_final})",
                "output.to_csv('prediction-ann_0150.csv', index=False)",
                "",
                "plt.figure(figsize=(10,5))",
                "sns.countplot(y_final, palette=\"icefire\")",
                "plt.title(\"(Test data) Number of Survived classes\")"
            ],
            "content_processed": [
                "ASSIGN = test.iloc[:,0].values",
                "ASSIGN = test.iloc[:,1:test.shape[1]].values",
                "ASSIGN =testdata",
                "ASSIGN = model.predict(X_test)",
                "ASSIGN = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])",
                "ASSIGN = pd.DataFrame({'PassengerId': test_orj['PassengerId'], 'Survived': y_final})",
                "ASSIGN.to_csv('prediction-ann_0150.csv', index=False)",
                "plt.figure(figsize=(10,5))",
                "sns.countplot(ASSIGN, palette=\"icefire\")",
                "plt.title(\"(Test data) Number of Survived classes\")"
            ],
            "tag_pred": [
                "visualize_data",
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "transfer_results",
                "visualize_data"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Visualization"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Visualization"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "import keras",
                "",
                "import keras",
                "from keras.models import Model",
                "from keras.layers import Input,Dense",
                "from keras import Sequential",
                "",
                "train = pd.read_csv(\"../input/titanic/train.csv\")",
                "test = pd.read_csv(\"../input/titanic/test.csv\")",
                "",
                "train.Pclass = train.Pclass.values.astype('str')",
                "test.Pclass = test.Pclass.values.astype('str')",
                "",
                "train.SibSp = train.SibSp.values.astype('str')",
                "test.SibSp = test.SibSp.values.astype('str')",
                "",
                "train.Parch = train.Parch.values.astype('str')",
                "test.Parch = test.Parch.values.astype('str')",
                "",
                "use_col =  ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp','Parch','Fare', 'Embarked']",
                "",
                "train[\"Age\"] = train.Age.fillna(30.).values",
                "test[\"Age\"] = test.Age.fillna(30.).values",
                "",
                "test.Fare[152]=np.mean(test.Fare)",
                "",
                "train = train[use_col]",
                "test_x = test[use_col[1:]]",
                "",
                "train = train.dropna()",
                "",
                "train_y = train[use_col[0]].values",
                "train_x = train[use_col[1:]].copy()"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN.Pclass = ASSIGN.Pclass.values.astype('str')",
                "ASSIGN.Pclass = ASSIGN.Pclass.values.astype('str')",
                "ASSIGN.SibSp = ASSIGN.SibSp.values.astype('str')",
                "ASSIGN.SibSp = ASSIGN.SibSp.values.astype('str')",
                "ASSIGN.Parch = ASSIGN.Parch.values.astype('str')",
                "ASSIGN.Parch = ASSIGN.Parch.values.astype('str')",
                "ASSIGN = ['Survived', 'Pclass', 'Sex', 'Age', 'SibSp','Parch','Fare', 'Embarked']",
                "ASSIGN = train.Age.fillna(30.).values",
                "ASSIGN = test.Age.fillna(30.).values",
                "ASSIGN.Fare[152]=np.mean(ASSIGN.Fare)",
                "ASSIGN = ASSIGN[use_col]",
                "ASSIGN = test[use_col[1:]]",
                "ASSIGN = ASSIGN.dropna()",
                "ASSIGN = train[use_col[0]].values",
                "ASSIGN = train[use_col[1:]].copy()"
            ],
            "tag_pred": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "ingest_data",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "import numpy as np",
                "import pandas as pd",
                "",
                "def pandas_type(inp):",
                "    if str(type(inp)) != \"<class 'pandas.core.frame.DataFrame'>\":",
                "        print(\"Use pandas DataFrame\")",
                "        return False",
                "    else:",
                "        if np.any(inp.isnull()==True)==True:",
                "            print(\"Your data is a mess\")",
                "            return False",
                "        else:",
                "            pass",
                "    ",
                "def pandas_enc_str(inp,m_co_var=True):",
                "    out = pd.DataFrame()",
                "    zw = inp.astype",
                "    try:",
                "        zzw = zw.unique()",
                "    except:",
                "        zw = pd.Series(inp)",
                "        zzw = zw.unique()",
                "",
                "    if m_co_var == True:",
                "        for i in zzw[1:]:",
                "            try:",
                "                bin_ = eval('zw=='+str(i)).replace({True : 1 , False : 0})",
                "            except:",
                "                bin_ = eval('zw==\"'+str(i)+'\"').replace({True : 1 , False : 0})",
                "            out[i]=bin_",
                "        return out",
                "    else:",
                "        for i in zzw:",
                "            try:",
                "                bin_ = eval('zw=='+str(i)).replace({True : 1 , False : 0})",
                "            except:",
                "                bin_ = eval('zw==\"'+str(i)+'\"').replace({True : 1 , False : 0})",
                "            out[i]=bin_",
                "        return out",
                "    ",
                "def get_split_len(inp):",
                "    nn1 = str(np.float32(np.mean(inp))-min(inp)).split(\".\")[0]",
                "    nn2 = str(np.float32(min(inp))).split(\".\")[1]",
                "    if nn1 != \"0\":",
                "        return -len(nn1)+3",
                "    else:",
                "        return len(nn2)",
                "",
                "def categorize_cat(inp,bins):",
                "    nn = get_split_len(inp)",
                "    leng = (max(inp)-min(inp))/bins",
                "    cats = []",
                "    for i in range(bins):",
                "        cats.append(min(inp)+leng*(i+1))",
                "    return np.around(cats,nn)",
                "",
                "def categorize_(inp,bins):",
                "    out = inp.values",
                "    bins_ = categorize_cat(inp,bins)",
                "    zw = np.ones(len(out))*bins_[0]",
                "    for i in range(len(bins_[:-1])):",
                "        for j in range(len(zw)):",
                "            if out[j] > bins_[i]:",
                "                zw[j]=bins_[i+1]",
                "    return zw",
                "",
                "def cat_str(inp):",
                "    zw = pd.Series(inp)",
                "    zzw = np.sort(zw.unique())",
                "    cat_dic={}",
                "    for i in range(1,len(zzw)-1):",
                "        cat_dic.update({zzw[i] : str(zzw[i])+\"-\"+str(zzw[i+1])})",
                "    cat_dic.update({zzw[-1] : \"> \"+str(zzw[-1])})",
                "    cat_dic.update({zzw[0] : \" <\"+str(zzw[0])})",
                "    return pd.Series(zw),cat_dic",
                "",
                "def pandas_enc(inp,col,bins=5,m_co_var=True):",
                "    out1 = inp[inp.columns[inp.columns!=col]]",
                "    zw = inp[col]",
                "    if pandas_type(inp)!=False:",
                "        pass",
                "    else:",
                "        return None",
                "    if zw.dtype==float:",
                "        zw = categorize_(zw,bins)",
                "        zw,cat_dic = cat_str(zw)",
                "        out2 = pandas_enc_str(zw,m_co_var)",
                "        out2 = out2[np.sort(out2.columns)]",
                "        out2 = out2.rename(columns=cat_dic)",
                "    elif zw.dtype==int:",
                "        print(\"Specify: str or float\")",
                "    elif zw.dtype==\"O\":",
                "        zw=str(col)+\"_\"+zw",
                "        out2 = pandas_enc_str(zw,m_co_var)",
                "    else:",
                "        print(\"Strange dtype\")",
                "    return pd.concat([out1,out2], axis=1)",
                "",
                "def pandas_multi_enc(inp,col,bins=5,m_co_var=True):",
                "    out = inp",
                "    for i in col:",
                "        out = pandas_enc(out,str(i))",
                "    return out"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "def pandas_type(inp):",
                "if str(type(inp)) != \"<class 'pandas.core.frame.DataFrame'>\":",
                "print()",
                "return False",
                "else:",
                "if np.any(inp.isnull()==True)==True:",
                "print()",
                "return False",
                "else:",
                "pass",
                "def pandas_enc_str(inp,m_co_var=True):",
                "ASSIGN = pd.DataFrame()",
                "ASSIGN = inp.astype",
                "try:",
                "ASSIGN = zw.unique()",
                "except:",
                "ASSIGN = pd.Series(inp)",
                "ASSIGN = zw.unique()",
                "ASSIGN == True:",
                "for i in ASSIGN[1:]:",
                "try:",
                "ASSIGN = eval('zw=='+str(i)).replace({True : 1 , False : 0})",
                "except:",
                "ASSIGN = eval('zw==\"'+str(i)+'\"').replace({True : 1 , False : 0})",
                "SLICE=ASSIGN",
                "return out",
                "else:",
                "for i in ASSIGN:",
                "try:",
                "ASSIGN = eval('zw=='+str(i)).replace({True : 1 , False : 0})",
                "except:",
                "ASSIGN = eval('zw==\"'+str(i)+'\"').replace({True : 1 , False : 0})",
                "SLICE=ASSIGN",
                "return out",
                "def get_split_len(inp):",
                "ASSIGN = str(np.float32(np.mean(inp))-min(inp)).split(\".\")[0]",
                "ASSIGN = str(np.float32(min(inp))).split(\".\")[1]",
                "if ASSIGN != \"0\":",
                "return -len(nn1)+3",
                "else:",
                "return len(ASSIGN)",
                "def categorize_cat(inp,bins):",
                "ASSIGN = get_split_len(inp)",
                "ASSIGN = (max(inp)-min(inp))path",
                "ASSIGN = []",
                "for i in range(bins):",
                "ASSIGN.append(min(inp)+ASSIGN*(i+1))",
                "return np.around(ASSIGN,ASSIGN)",
                "def categorize_(inp,bins):",
                "ASSIGN = inp.values",
                "ASSIGN = categorize_cat(inp,bins)",
                "ASSIGN = np.ones(len(out))*bins_[0]",
                "for i in range(len(ASSIGN[:-1])):",
                "for j in range(len(ASSIGN)):",
                "if ASSIGN[j] > ASSIGN[i]:",
                "SLICE=ASSIGN[i+1]",
                "return zw",
                "def cat_str(inp):",
                "ASSIGN = pd.Series(inp)",
                "ASSIGN = np.sort(zw.unique())",
                "ASSIGN={}",
                "for i in range(1,len(ASSIGN)-1):",
                "ASSIGN.update({ASSIGN[i] : str(ASSIGN[i])+\"-\"+str(ASSIGN[i+1])})",
                "ASSIGN.update({ASSIGN[-1] : \"> \"+str(ASSIGN[-1])})",
                "ASSIGN.update({ASSIGN[0] : \" <\"+str(ASSIGN[0])})",
                "return pd.Series(zw),cat_dic",
                "def pandas_enc(inp,col,bins=5,m_co_var=True):",
                "ASSIGN = inp[inp.columns[inp.columns!=col]]",
                "ASSIGN = inp[col]",
                "if pandas_type(inp)!=False:",
                "pass",
                "else:",
                "return None",
                "if ASSIGN.dtype==float:",
                "ASSIGN = categorize_(ASSIGN,bins)",
                "ASSIGN = cat_str(zw)",
                "ASSIGN = pandas_enc_str(zw,m_co_var)",
                "ASSIGN = ASSIGN[np.sort(ASSIGN.columns)]",
                "ASSIGN = ASSIGN.rename(columns=cat_dic)",
                "elif ASSIGN.dtype==int:",
                "print()",
                "elif ASSIGN.dtype==\"O\":",
                "ASSIGN=str(col)+\"_\"+ASSIGN",
                "ASSIGN = pandas_enc_str(zw,m_co_var)",
                "else:",
                "print()",
                "return pd.concat([ASSIGN,ASSIGN], axis=1)",
                "def pandas_multi_enc(inp,col,bins=5,m_co_var=True):",
                "ASSIGN = inp",
                "for i in col:",
                "ASSIGN = pandas_enc(ASSIGN,str(i))",
                "return out"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "zw = train_x.append(test_x)",
                "zzw = pandas_multi_enc(zw,['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'])",
                "",
                "train_x = zzw.iloc[:len(train_x)].values",
                "test_x = zzw.iloc[len(train_x):].values"
            ],
            "content_processed": [
                "ASSIGN = train_x.append(test_x)",
                "ASSIGN = pandas_multi_enc(zw,['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'])",
                "ASSIGN = zzw.iloc[:len(ASSIGN)].values",
                "ASSIGN = zzw.iloc[len(train_x):].values"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "model=Sequential()",
                "model.add(Dense(512,input_dim=zzw.shape[1],activation='linear'))",
                "model.add(Dense(2048,activation='sigmoid'))",
                "model.add(Dense(512,activation='sigmoid'))",
                "model.add(Dense(16,activation='linear'))",
                "model.add(Dense(1,activation='linear'))",
                "",
                "",
                "sgd=keras.optimizers.SGD(lr=.0001)",
                "model.compile(optimizer=sgd,loss='mse')",
                "",
                "res_model = model.fit(train_x,train_y, batch_size=32, epochs=100)"
            ],
            "content_processed": [
                "ASSIGN=Sequential()",
                "ASSIGN.add(Dense(512,input_dim=zzw.shape[1],activation='linear'))",
                "ASSIGN.add(Dense(2048,activation='sigmoid'))",
                "ASSIGN.add(Dense(512,activation='sigmoid'))",
                "ASSIGN.add(Dense(16,activation='linear'))",
                "ASSIGN.add(Dense(1,activation='linear'))",
                "ASSIGN=keras.optimizers.SGD(lr=.0001)",
                "ASSIGN.compile(optimizer=ASSIGN,loss='mse')",
                "ASSIGN = model.fit(train_x,train_y, batch_size=32, epochs=100)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "zw = model.predict(test_x)",
                "",
                "result_csv=pd.DataFrame()",
                "",
                "result_csv[\"PassengerId\"]=test.PassengerId",
                "result_csv[\"Survived\"]=np.rint(zw).astype(int)"
            ],
            "content_processed": [
                "ASSIGN = model.predict(test_x)",
                "ASSIGN=pd.DataFrame()",
                "ASSIGN[\"PassengerId\"]=test.PassengerId",
                "ASSIGN[\"Survived\"]=np.rint(ASSIGN).astype(int)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "05-mytitanic",
            "content": [
                "result_csv.to_csv(\"my_titanic_res.csv\",index=False)"
            ],
            "content_processed": [
                "result_csv.to_csv(\"my_titanic_res.csv\",index=False)"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "import tensorflow as tf",
                "import pandas as pd",
                "import os",
                "from sklearn.metrics import roc_auc_score",
                "from sklearn.model_selection import train_test_split",
                "from sklearn.preprocessing import StandardScaler",
                "from keras import Sequential",
                "from keras import layers",
                "from keras import backend as K",
                "from keras.layers.core import Dense",
                "from keras import regularizers",
                "from keras.layers import Dropout",
                "from keras.constraints import max_norm"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "# Import data",
                "train = pd.read_csv('../input/train.csv')",
                "test = pd.read_csv('../input/test.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "train.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "train.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "test.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "test.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "#Check num of cases in label ",
                "print(train.target.value_counts())",
                "print(train.target.value_counts()[1]/train.target.value_counts()[0])"
            ],
            "content_processed": [
                "CHECKPOINT",
                "print(train.target.value_counts())",
                "print(train.target.value_counts()[1]path()[0])"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "train_features = train.drop(['target', 'ID_code'], axis=1)",
                "train_targets = train['target']",
                "test_features = test.drop(['ID_code'], axis=1)"
            ],
            "content_processed": [
                "ASSIGN = train.drop(['target', 'ID_code'], axis=1)",
                "ASSIGN = train['target']",
                "ASSIGN = test.drop(['ID_code'], axis=1)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "X_train, X_test, y_train, y_test = train_test_split(train_features, train_targets, test_size = 0.25, random_state = 50)"
            ],
            "content_processed": [
                "X_train, X_test, y_train, y_test = train_test_split(train_features, train_targets, test_size = 0.25, random_state = 50)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "# Feature Scaling",
                "from sklearn.preprocessing import StandardScaler",
                "sc = StandardScaler()",
                "X_train = sc.fit_transform(X_train)",
                "X_test = sc.transform(X_test)",
                "test_features = sc.transform(test_features)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = StandardScaler()",
                "ASSIGN = sc.fit_transform(ASSIGN)",
                "ASSIGN = sc.transform(ASSIGN)",
                "ASSIGN = sc.transform(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "# Add RUC metric to monitor NN",
                "def auc(y_true, y_pred):",
                "    auc = tf.metrics.auc(y_true, y_pred)[1]",
                "    K.get_session().run(tf.local_variables_initializer())",
                "    return auc"
            ],
            "content_processed": [
                "def auc(y_true, y_pred):",
                "ASSIGN = tf.metrics.ASSIGN(y_true, y_pred)[1]",
                "K.get_session().run(tf.local_variables_initializer())",
                "return auc"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "input_dim = X_train.shape[1]",
                "input_dim"
            ],
            "content_processed": [
                "CHECKPOINT",
                "ASSIGN = X_train.shape[1]",
                "input_dim"
            ],
            "tag_pred": [
                "check_results",
                "process_data",
                "train_model"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "# Try early stopping",
                "#from keras.callbacks import EarlyStopping",
                "#callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "model = Sequential()",
                "# Input layer",
                "model.add(Dense(units = 200, activation = \"relu\", input_dim = input_dim, kernel_initializer = \"normal\", kernel_regularizer=regularizers.l2(0.005), ",
                "                kernel_constraint = max_norm(5.)))",
                "# Add dropout regularization",
                "model.add(Dropout(rate=0.2))",
                "",
                "# First hidden layer",
                "model.add(Dense(units = 200, activation='relu', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))",
                "# Add dropout regularization",
                "model.add(Dropout(rate=0.1))",
                "",
                "# Second hidden layer",
                "model.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))",
                "# Add dropout regularization",
                "model.add(Dropout(rate=0.1))",
                "",
                "# Third hidden layer",
                "model.add(Dense(50, activation='tanh', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))",
                "# Add dropout regularization",
                "model.add(Dropout(rate=0.1))",
                "",
                "# Output layer",
                "model.add(layers.Dense(units = 1, activation='sigmoid'))",
                "",
                "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])",
                "model.summary()"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units = 200, activation = \"relu\", input_dim = input_dim, kernel_initializer = \"normal\", kernel_regularizer=regularizers.l2(0.005),",
                "ASSIGN = max_norm(5.)))",
                "ASSIGN.add(Dropout(rate=0.2))",
                "ASSIGN.add(Dense(units = 200, activation='relu', kernel_regularizer=regularizers.l2(0.005), ASSIGN=max_norm(5)))",
                "ASSIGN.add(Dropout(rate=0.1))",
                "ASSIGN.add(Dense(100, activation='relu', kernel_regularizer=regularizers.l2(0.005), ASSIGN=max_norm(5)))",
                "ASSIGN.add(Dropout(rate=0.1))",
                "ASSIGN.add(Dense(50, activation='tanh', kernel_regularizer=regularizers.l2(0.005), ASSIGN=max_norm(5)))",
                "ASSIGN.add(Dropout(rate=0.1))",
                "ASSIGN.add(layers.Dense(units = 1, activation='sigmoid'))",
                "ASSIGN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "model.fit(X_train, y_train, batch_size = 16384, epochs = 125, validation_data = (X_test, y_test))#, callbacks = [callback])"
            ],
            "content_processed": [
                "model.fit(X_train, y_train, batch_size = 16384, epochs = 125, validation_data = (X_test, y_test))"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "y_pred = model.predict_proba(X_test)",
                "roc_auc_score(y_test, y_pred)"
            ],
            "content_processed": [
                "ASSIGN = model.predict_proba(X_test)",
                "roc_auc_score(y_test, ASSIGN)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "id_code_test = test['ID_code']",
                "# Make predicitions",
                "pred = model.predict(test_features)",
                "pred_ = pred[:,0]"
            ],
            "content_processed": [
                "ASSIGN = test['ID_code']",
                "ASSIGN = model.predict(test_features)",
                "ASSIGN = pred[:,0]"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "pred_"
            ],
            "content_processed": [
                "pred_"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "# To CSV",
                "my_submission = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : pred_})"
            ],
            "content_processed": [
                "ASSIGN = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : pred_})"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "my_submission"
            ],
            "content_processed": [
                "CHECKPOINT",
                "my_submission"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                "my_submission.to_csv('submission.csv', index = False, header = True)"
            ],
            "content_processed": [
                "my_submission.to_csv('submission.csv', index = False, header = True)"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "12-keras-nn-with-scaling-and-regularization",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load in ",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "from tqdm import tqdm, tqdm_notebook",
                "",
                "# Input data files are available in the \"../input/\" directory.",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory",
                "",
                "import os",
                "print(os.listdir(\"../input\"))",
                "import gc",
                "",
                "",
                "# Any results you write to the current directory are saved as output.",
                "import tensorflow as tf",
                "from sklearn.model_selection import train_test_split, StratifiedKFold",
                "from sklearn.preprocessing import StandardScaler",
                "from keras import layers",
                "from keras import backend as K",
                "from keras import regularizers",
                "from keras.constraints import max_norm",
                "from keras.models import Sequential",
                "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau",
                "from keras.models import load_model",
                "from keras.models import Model",
                "from keras.initializers import glorot_uniform",
                "from keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate",
                "from sklearn import preprocessing",
                "",
                "import matplotlib.pyplot as plt",
                "from sklearn.metrics import roc_curve",
                "#from sklearn.metrics import auc",
                "from sklearn.metrics import roc_auc_score",
                "",
                "import warnings",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "print(os.listdir())",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# define helper functions. auc, plot_history",
                "def auc(y_true, y_pred):",
                "    #auc = tf.metrics.auc(y_true, y_pred)[1]",
                "    y_pred = y_pred.ravel()",
                "    y_true = y_true.ravel()",
                "    return roc_auc_score(y_true, y_pred)",
                "",
                "def auc_2(y_true, y_pred):",
                "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)",
                "",
                "def plot_history(histories, key='binary_crossentropy'):",
                "    plt.figure(figsize=(16,10))",
                "    #plt.plot([0, 1], [0, 1], 'k--')",
                "    for name, history in histories:",
                "        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')",
                "",
                "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')",
                "",
                "    plt.xlabel('Epochs')",
                "    plt.ylabel(key.replace('_',' ').title())",
                "    plt.legend()",
                "",
                "    plt.xlim([0,max(history.epoch)])",
                "    plt.ylim([0, 0.4])",
                "    plt.show()"
            ],
            "content_processed": [
                "def auc(y_true, y_pred):",
                "ASSIGN = ASSIGN.ravel()",
                "ASSIGN = ASSIGN.ravel()",
                "return roc_auc_score(ASSIGN, ASSIGN)",
                "def auc_2(ASSIGN, ASSIGN):",
                "return tf.py_func(roc_auc_score, (ASSIGN, ASSIGN), tf.double)",
                "def plot_history(histories, key='binary_crossentropy'):",
                "plt.figure(figsize=(16,10))",
                "for name, history in histories:",
                "ASSIGN = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')",
                "plt.plot(history.epoch, history.history[key], color=ASSIGN[0].get_color(), label=name.title()+' Train')",
                "plt.xlabel('Epochs')",
                "plt.ylabel(key.replace('_',' ').title())",
                "plt.legend()",
                "plt.xlim([0,max(history.epoch)])",
                "plt.ylim([0, 0.4])",
                "plt.show()"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "visualize_data"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Visualization"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Visualization"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# load data ",
                "train_df = pd.read_csv('../input/train.csv')",
                "test_df =  pd.read_csv(\"../input/test.csv\")",
                "base_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv(\"..path\")",
                "ASSIGN = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]"
            ],
            "tag_pred": [
                "ingest_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# mark real vs fake",
                "train_df['real'] = 1",
                "",
                "for col in base_features:",
                "    test_df[col] = test_df[col].map(test_df[col].value_counts())",
                "a = test_df[base_features].min(axis=1)",
                "",
                "test_df = pd.read_csv('../input/test.csv')",
                "test_df['real'] = (a == 1).astype('int')",
                "",
                "train = train_df.append(test_df).reset_index(drop=True)",
                "del test_df, train_df; gc.collect()"
            ],
            "content_processed": [
                "train_df['real'] = 1",
                "for col in base_features:",
                "test_df[col] = test_df[col].map(test_df[col].value_counts())",
                "ASSIGN = test_df[base_features].min(axis=1)",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN['real'] = (ASSIGN == 1).astype('int')",
                "ASSIGN = train_df.append(test_df).reset_index(drop=True)",
                "del ASSIGN, train_df; gc.collect()"
            ],
            "tag_pred": [
                "ingest_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# count features",
                "for col in tqdm(base_features):",
                "    train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())",
                "cnt_features = [col + 'size' for col in base_features]"
            ],
            "content_processed": [
                "for col in tqdm(base_features):",
                "train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())",
                "ASSIGN = [col + 'size' for col in base_features]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# magice features 1",
                "for col in tqdm(base_features):",
                "#        train[col+'size'] = train.groupby(col)['target'].transform('size')",
                "    train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]",
                "noise1_features = [col + 'no_noise' for col in base_features]"
            ],
            "content_processed": [
                "for col in tqdm(base_features):",
                "train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]",
                "ASSIGN = [col + 'no_noise' for col in base_features]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# fill NA as 0, inspired by lightgbm",
                "train[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())"
            ],
            "content_processed": [
                "train[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# magice features 2",
                "for col in tqdm(base_features):",
                "#        train[col+'size'] = train.groupby(col)['target'].transform('size')",
                "    train.loc[train[col+'size']>2,col+'no_noise2'] = train.loc[train[col+'size']>2,col]",
                "noise2_features = [col + 'no_noise2' for col in base_features]"
            ],
            "content_processed": [
                "for col in tqdm(base_features):",
                "train.loc[train[col+'size']>2,col+'no_noise2'] = train.loc[train[col+'size']>2,col]",
                "ASSIGN = [col + 'no_noise2' for col in base_features]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# fill NA as 0, inspired by lightgbm",
                "train[noise2_features] = train[noise2_features].fillna(train[noise2_features].mean())"
            ],
            "content_processed": [
                "train[noise2_features] = train[noise2_features].fillna(train[noise2_features].mean())"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "train_df = train[train['target'].notnull()]",
                "test_df = train[train['target'].isnull()]",
                "all_features = base_features + noise1_features + noise2_features"
            ],
            "content_processed": [
                "ASSIGN = train[train['target'].notnull()]",
                "ASSIGN = train[train['target'].isnull()]",
                "ASSIGN = base_features + noise1_features + noise2_features"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)",
                "df_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)",
                "df_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)",
                "y = train_df['target'].values"
            ],
            "content_processed": [
                "ASSIGN = preprocessing.StandardScaler().fit(train_df[all_features].values)",
                "ASSIGN = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)",
                "ASSIGN = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)",
                "ASSIGN = train_df['target'].values"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "def get_keras_data(dataset, cols_info):",
                "    X = {}",
                "    base_feats, noise_feats, noise2_feats = cols_info",
                "    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))",
                "    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))",
                "    X['noise2'] = np.reshape(np.array(dataset[noise2_feats].values), (-1, len(noise2_feats), 1))",
                "    return X"
            ],
            "content_processed": [
                "def get_keras_data(dataset, cols_info):",
                "ASSIGN = {}",
                "base_feats, noise_feats, noise2_feats = cols_info",
                "ASSIGN = np.reshape(np.array(ASSIGN.values), (-1, len(base_feats), 1))",
                "ASSIGN['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))",
                "ASSIGN['noise2'] = np.reshape(np.array(dataset[noise2_feats].values), (-1, len(noise2_feats), 1))",
                "return X"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "cols_info = [base_features, noise1_features, noise2_features]",
                "#X = get_keras_data(df_trn[all_features], cols_info)",
                "X_test = get_keras_data(df_tst[all_features], cols_info)"
            ],
            "content_processed": [
                "ASSIGN = [base_features, noise1_features, noise2_features]",
                "ASSIGN = get_keras_data(df_tst[all_features], cols_info)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# define network structure -> 2D CNN",
                "def Convnet(cols_info, classes=1):",
                "    base_feats, noise1_feats, noise2_feats = cols_info",
                "    ",
                "    # base_feats",
                "    X_base_input = Input(shape=(len(base_feats), 1), name='base')",
                "    X_base = Dense(16)(X_base_input)",
                "    X_base = Activation('relu')(X_base)",
                "    X_base = Flatten(name='base_last')(X_base)",
                "    ",
                "    # noise1",
                "    X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')",
                "    X_noise1 = Dense(16)(X_noise1_input)",
                "    X_noise1 = Activation('relu')(X_noise1)",
                "    X_noise1 = Flatten(name='nose1_last')(X_noise1)",
                "    ",
                "    # noise2",
                "    X_noise2_input = Input(shape=(len(noise2_feats), 1), name='noise2')",
                "    X_noise2 = Dense(16)(X_noise2_input)",
                "    X_noise2 = Activation('relu')(X_noise2)",
                "    X_noise2 = Flatten(name='nose2_last')(X_noise2)",
                "    ",
                "    ",
                "    X = concatenate([X_base, X_noise1, X_noise2])",
                "    X = Dense(classes, activation='sigmoid')(X)",
                "    ",
                "    model = Model(inputs=[X_base_input, X_noise1_input, X_noise2_input],outputs=X)",
                "    ",
                "    return model",
                "model = Convnet(cols_info)",
                "model.summary()"
            ],
            "content_processed": [
                "def Convnet(cols_info, classes=1):",
                "base_feats, noise1_feats, noise2_feats = cols_info",
                "ASSIGN = Input(shape=(len(base_feats), 1), name='base')",
                "ASSIGN = Dense(16)(X_base_input)",
                "ASSIGN = Activation('relu')(ASSIGN)",
                "ASSIGN = Flatten(name='base_last')(ASSIGN)",
                "X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')",
                "X_noise1 = Dense(16)(X_noise1_input)",
                "X_noise1 = Activation('relu')(X_noise1)",
                "X_noise1 = Flatten(name='nose1_last')(X_noise1)",
                "X_noise2_input = Input(shape=(len(noise2_feats), 1), name='noise2')",
                "X_noise2 = Dense(16)(X_noise2_input)",
                "X_noise2 = Activation('relu')(X_noise2)",
                "X_noise2 = Flatten(name='nose2_last')(X_noise2)",
                "ASSIGN = concatenate([X_base, X_noise1, X_noise2])",
                "ASSIGN = Dense(classes, activation='sigmoid')(ASSIGN)",
                "ASSIGN = Model(inputs=[X_base_input, X_noise1_input, X_noise2_input],outputs=X)",
                "return model",
                "ASSIGN = Convnet(cols_info)",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "try:",
                "    del df_tst",
                "except:",
                "    pass",
                "gc.collect()"
            ],
            "content_processed": [
                "try:",
                "del df_tst",
                "except:",
                "pass",
                "gc.collect()"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# parameters",
                "SEED = 2019",
                "n_folds = 5",
                "debug_flag = True",
                "folds = 5",
                "skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = 5",
                "ASSIGN = True",
                "ASSIGN = 5",
                "ASSIGN = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "#transformed_shape = tuple([-1] + list(shape))",
                "#X_test = np.reshape(X_test, transformed_shape)",
                "",
                "i = 0",
                "result = pd.DataFrame({\"ID_code\": test_df.ID_code.values})",
                "val_aucs = []",
                "valid_X = train_df[['target']]",
                "valid_X['predict'] = 0",
                "for train_idx, val_idx in skf.split(df_trn, y):",
                "    if i == folds:",
                "        break",
                "    i += 1    ",
                "    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]",
                "    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]",
                "    ",
                "    X_train = get_keras_data(X_train, cols_info)",
                "    X_valid = get_keras_data(X_valid, cols_info)",
                "    #X_train = np.reshape(X_train, transformed_shape)",
                "    #X_valid = np.reshape(X_valid, transformed_shape)",
                "    ",
                "    model_name = 'NN_fold{}.h5'.format(str(i))",
                "    ",
                "    model = Convnet(cols_info)",
                "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])",
                "    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, ",
                "                                 save_best_only=True, mode='max', save_weights_only = True)",
                "    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, ",
                "                                       verbose=1, mode='min', epsilon=0.0001)",
                "    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)",
                "    history = model.fit(X_train, y_train, ",
                "                        epochs=300, ",
                "                        batch_size=1024 * 2, ",
                "                        validation_data=(X_valid, y_valid), ",
                "                        callbacks=[checkpoint, reduceLROnPlat, earlystop])",
                "    train_history = pd.DataFrame(history.history)",
                "    train_history.to_csv('train_profile_fold{}.csv'.format(str(i)), index=None)",
                "    ",
                "    # load and predict",
                "    model.load_weights(model_name)",
                "    ",
                "    #predict",
                "    y_pred_keras = model.predict(X_valid).ravel()",
                "    ",
                "    # AUC",
                "    valid_X['predict'].iloc[val_idx] = y_pred_keras",
                "    ",
                "    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)",
                "    auc_valid = roc_auc_score(y_valid, y_pred_keras)",
                "    val_aucs.append(auc_valid)",
                "    ",
                "    prediction = model.predict(X_test)",
                "    result[\"fold{}\".format(str(i))] = prediction"
            ],
            "content_processed": [
                "ASSIGN = 0",
                "ASSIGN = pd.DataFrame({\"ID_code\": test_df.ID_code.values})",
                "ASSIGN = []",
                "ASSIGN = train_df[['target']]",
                "ASSIGN['predict'] = 0",
                "for train_idx, val_idx in skf.split(df_trn, y):",
                "ASSIGN == folds:",
                "break",
                "ASSIGN += 1",
                "ASSIGN = df_trn.iloc[train_idx], y[train_idx]",
                "ASSIGN = df_trn.iloc[val_idx], y[val_idx]",
                "ASSIGN = get_keras_data(ASSIGN, cols_info)",
                "ASSIGN = get_keras_data(ASSIGN, cols_info)",
                "ASSIGN = 'NN_fold{}.h5'.format(str(i))",
                "ASSIGN = Convnet(cols_info)",
                "ASSIGN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])",
                "ASSIGN = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1,",
                "ASSIGN=True, mode='max', save_weights_only = True)",
                "ASSIGN = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4,",
                "ASSIGN=1, mode='min', epsilon=0.0001)",
                "ASSIGN = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)",
                "ASSIGN = model.fit(X_train, y_train,",
                "ASSIGN=300,",
                "ASSIGN=1024 * 2,",
                "ASSIGN=(X_valid, y_valid),",
                "ASSIGN=[checkpoint, reduceLROnPlat, earlystop])",
                "ASSIGN = pd.DataFrame(history.history)",
                "ASSIGN.to_csv('train_profile_fold{}.csv'.format(str(ASSIGN)), index=None)",
                "ASSIGN.load_weights(ASSIGN)",
                "ASSIGN = model.predict(X_valid).ravel()",
                "ASSIGN['predict'].iloc[val_idx] = ASSIGN",
                "ASSIGN = roc_curve(y_valid, y_pred_keras)",
                "ASSIGN = roc_auc_score(y_valid, y_pred_keras)",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = model.predict(X_test)",
                "ASSIGN[\"fold{}\".format(str(ASSIGN))] = ASSIGN"
            ],
            "tag_pred": [
                "ingest_data",
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "ingest_data",
                "train_model",
                "evaluate_model",
                "transfer_results"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "for i in range(len(val_aucs)):",
                "    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))"
            ],
            "content_processed": [
                "CHECKPOINT",
                "for i in range(len(val_aucs)):",
                "print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "# summary on results",
                "auc_mean = np.mean(val_aucs)",
                "auc_std = np.std(val_aucs)",
                "auc_all = roc_auc_score(valid_X.target, valid_X.predict)",
                "print('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))"
            ],
            "content_processed": [
                "CHECKPOINT",
                "ASSIGN = np.mean(val_aucs)",
                "ASSIGN = np.std(val_aucs)",
                "ASSIGN = roc_auc_score(valid_X.target, valid_X.predict)",
                "print('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, ASSIGN, ASSIGN, ASSIGN))"
            ],
            "tag_pred": [
                "check_results",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                "y_all = result.values[:, 1:]",
                "result['target'] = np.mean(y_all, axis = 1)",
                "to_submit = result[['ID_code', 'target']]",
                "to_submit.to_csv('NN_submission.csv', index=None)",
                "result.to_csv('NN_all_prediction.csv', index=None)",
                "valid_X['ID_code'] = train_df['ID_code']",
                "valid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('NN_oof.csv', index=None)"
            ],
            "content_processed": [
                "ASSIGN = result.values[:, 1:]",
                "ASSIGN = np.mean(y_all, axis = 1)",
                "ASSIGN = result[['ID_code', 'target']]",
                "ASSIGN.to_csv('NN_submission.csv', index=None)",
                "result.to_csv('NN_all_prediction.csv', index=None)",
                "valid_X['ID_code'] = train_df['ID_code']",
                "ASSIGN = ASSIGN[['ID_code', 'target', 'predict']].to_csv('NN_oof.csv', index=None)"
            ],
            "tag_pred": [
                "process_data",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "process_data",
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "14-multibranch-nn-baseline-magic",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "import numpy as np",
                "import pandas as pd "
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "sales_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')",
                "item_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')",
                "items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')",
                "shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')",
                "sample_submission = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')",
                "test_data = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "def basic_eda(df):",
                "",
                "    print(\"----------TOP 5 RECORDS--------\")",
                "    print(df.head(5))",
                "    print(\"----------INFO-----------------\")",
                "    print(df.info())",
                "    print(\"----------Describe-------------\")",
                "    print(df.describe())",
                "    print(\"----------Columns--------------\")",
                "    print(df.columns)",
                "    print(\"----------Data Types-----------\")",
                "    print(df.dtypes)",
                "    print(\"-------Missing Values----------\")",
                "    print(df.isnull().sum())",
                "    print(\"-------NULL values-------------\")",
                "    print(df.isna().sum())",
                "    print(\"-----Shape Of Data-------------\")",
                "    print(df.shape)"
            ],
            "content_processed": [
                "CHECKPOINT",
                "def basic_eda(df):",
                "print()",
                "print(df.head(5))",
                "print()",
                "print(df.info())",
                "print()",
                "print(df.describe())",
                "print()",
                "print(df.columns)",
                "print()",
                "print(df.dtypes)",
                "print()",
                "print(df.isnull().sum())",
                "print()",
                "print(df.isna().sum())",
                "print()",
                "print(df.shape)"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "sales_data['date'] = pd.to_datetime(sales_data['date'],format = '%d.%m.%Y')"
            ],
            "content_processed": [
                "sales_data['date'] = pd.to_datetime(sales_data['date'],format = '%d.%m.%Y')"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset = sales_data.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')"
            ],
            "content_processed": [
                "ASSIGN = sales_data.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_day'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset"
            ],
            "content_processed": [
                "CHECKPOINT",
                "dataset"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset.reset_index(inplace = True)"
            ],
            "content_processed": [
                "dataset.reset_index(inplace = True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset"
            ],
            "content_processed": [
                "CHECKPOINT",
                "dataset"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# predict",
                "dataset = pd.merge(test_data,dataset,on = ['item_id','shop_id'],how = 'left')"
            ],
            "content_processed": [
                "ASSIGN = pd.merge(test_data,ASSIGN,on = ['item_id','shop_id'],how = 'left')"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset"
            ],
            "content_processed": [
                "CHECKPOINT",
                "dataset"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# lets fill all NaN values with 0",
                "dataset.fillna(0,inplace = True)"
            ],
            "content_processed": [
                "dataset.fillna(0,inplace = True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "dataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)",
                "dataset.head()"
            ],
            "content_processed": [
                "dataset.drop(['shop_id','item_id','ID'],inplace = True, axis = 1)",
                "dataset.head()"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# X we will keep all columns execpt the last one ",
                "X_train = np.expand_dims(dataset.values[:,:-1],axis = 2)",
                "# the last column is our label",
                "y_train = dataset.values[:,-1:]",
                "",
                "# for test we keep all the columns execpt the first one",
                "X_test = np.expand_dims(dataset.values[:,1:],axis = 2)",
                "",
                "# lets have a look on the shape ",
                "print(X_train.shape,y_train.shape,X_test.shape)",
                ""
            ],
            "content_processed": [
                "CHECKPOINT",
                "ASSIGN = np.expand_dims(dataset.values[:,:-1],axis = 2)",
                "ASSIGN = dataset.values[:,-1:]",
                "ASSIGN = np.expand_dims(dataset.values[:,1:],axis = 2)",
                "print(ASSIGN.shape,ASSIGN.shape,ASSIGN.shape)"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "from keras.models import Sequential",
                "from keras.layers import LSTM,Dense,Dropout"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# our defining our model ",
                "my_model = Sequential()",
                "my_model.add(LSTM(units = 64,input_shape = (33,1)))",
                "",
                "my_model.add(Dropout(0.4))",
                "my_model.add(Dense(1))",
                "",
                "my_model.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])",
                "my_model.summary()"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(LSTM(units = 64,input_shape = (33,1)))",
                "ASSIGN.add(Dropout(0.4))",
                "ASSIGN.add(Dense(1))",
                "ASSIGN.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# my_model.fit(X_train,y_train,batch_size = 4096,epochs = 10)"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# our defining our model ",
                "my_model2 = Sequential()",
                "my_model2.add(LSTM(units = 32,input_shape = (33,1), return_sequences=True))",
                "my_model2.add(LSTM(units = 64, return_sequences=True))",
                "my_model2.add(LSTM(units = 128, return_sequences=True))",
                "my_model2.add(Dropout(0.4))",
                "",
                "my_model2.add(LSTM(units = 128, return_sequences=True))",
                "my_model2.add(LSTM(units = 64, return_sequences=True))",
                "my_model2.add(LSTM(units = 32))",
                "my_model2.add(Dropout(0.4))",
                "",
                "my_model2.add(Dense(1))",
                "",
                "my_model2.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])",
                "my_model2.summary()"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(LSTM(units = 32,input_shape = (33,1), return_sequences=True))",
                "ASSIGN.add(LSTM(units = 64, return_sequences=True))",
                "ASSIGN.add(LSTM(units = 128, return_sequences=True))",
                "ASSIGN.add(Dropout(0.4))",
                "ASSIGN.add(LSTM(units = 128, return_sequences=True))",
                "ASSIGN.add(LSTM(units = 64, return_sequences=True))",
                "ASSIGN.add(LSTM(units = 32))",
                "ASSIGN.add(Dropout(0.4))",
                "ASSIGN.add(Dense(1))",
                "ASSIGN.compile(loss = 'mse',optimizer = 'adam', metrics = ['mean_squared_error'])",
                "ASSIGN.summary()"
            ],
            "tag_pred": [
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "my_model2.fit(X_train,y_train,batch_size = 4096,epochs = 10)"
            ],
            "content_processed": [
                "my_model2.fit(X_train,y_train,batch_size = 4096,epochs = 10)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# creating submission file ",
                "submission_pfs = my_model2.predict(X_test)",
                "# we will keep every value between 0 and 20",
                "submission_pfs = submission_pfs.clip(0,20)",
                "# creating dataframe with required columns ",
                "submission = pd.DataFrame({'ID':test_data['ID'],'item_cnt_month':submission_pfs.ravel()})",
                "# creating csv file from dataframe",
                "submission.to_csv('sub_pfs2.csv',index = False)"
            ],
            "content_processed": [
                "ASSIGN = my_model2.predict(X_test)",
                "ASSIGN = ASSIGN.clip(0,20)",
                "ASSIGN = pd.DataFrame({'ID':test_data['ID'],'item_cnt_month':submission_pfs.ravel()})",
                "ASSIGN.to_csv('sub_pfs2.csv',index = False)"
            ],
            "tag_pred": [
                "evaluate_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "evaluate_model",
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "",
                "# regressor = Sequential()",
                "",
                "# regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (33, 1)))",
                "# regressor.add(Dropout(0.2))",
                "",
                "# regressor.add(LSTM(units = 50, return_sequences = True))",
                "# regressor.add(Dropout(0.2))",
                "",
                "# regressor.add(LSTM(units = 50, return_sequences = True))",
                "# regressor.add(Dropout(0.2))",
                "",
                "# regressor.add(LSTM(units = 50))",
                "# regressor.add(Dropout(0.2))",
                "",
                "# regressor.add(Dense(units = 1))",
                "",
                "# regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')",
                "",
                "# regressor.fit(X_train, y_train, epochs = 10, batch_size = 4096)",
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "08-stacked-lstm-top-5-4-mae",
            "content": [
                "# submission_pfss = regressor.predict(X_test)",
                "# # we will keep every value between 0 and 20",
                "# submission_pfss = submission_pfss.clip(0,20)",
                "# # creating dataframe with required columns ",
                "# submission12 = pd.DataFrame({'ID':test_data['ID'],'item_cnt_month':submission_pfss.ravel()})",
                "# # creating csv file from dataframe",
                "# submission12.to_csv('sub_pfs12.csv',index = False)"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "df = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')",
                "sample = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "df.head()"
            ],
            "content_processed": [
                "df.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "sample.head()"
            ],
            "content_processed": [
                "sample.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')",
                "items_category = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')",
                "shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')",
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items.head()"
            ],
            "content_processed": [
                "items.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "items.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items_category.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "items_category.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items.head(5)"
            ],
            "content_processed": [
                "items.head(5)"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items = items.drop(columns = ['item_name','item_name'])"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN.drop(columns = ['item_name','item_name'])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items_category.head()"
            ],
            "content_processed": [
                "items_category.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "shops.head()"
            ],
            "content_processed": [
                "shops.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "shops.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "shops.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "category = []",
                "for i in df['item_id']:",
                "    category.append(items['item_category_id'][i])"
            ],
            "content_processed": [
                "ASSIGN = []",
                "for i in df['item_id']:",
                "ASSIGN.append(items['item_category_id'][i])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "print(category[0:20])"
            ],
            "content_processed": [
                "CHECKPOINT",
                "print(category[0:20])"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "items.iloc[22154,:]"
            ],
            "content_processed": [
                "items.iloc[22154,:]"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "df['item_category_id'] = category"
            ],
            "content_processed": [
                "df['item_category_id'] = category"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "df.head()"
            ],
            "content_processed": [
                "df.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data = df[df['item_cnt_day']<=50]"
            ],
            "content_processed": [
                "ASSIGN = df[df['item_cnt_day']<=50]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data = data[data['item_cnt_day']>-3]"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN[ASSIGN['item_cnt_day']>-3]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "data.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "df.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "df.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "len(data['item_cnt_day'].unique())"
            ],
            "content_processed": [
                "len(data['item_cnt_day'].unique())"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data_train = data.drop(columns = ['item_cnt_day','item_id','date'])"
            ],
            "content_processed": [
                "ASSIGN = data.drop(columns = ['item_cnt_day','item_id','date'])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "target = data['item_cnt_day']"
            ],
            "content_processed": [
                "ASSIGN = data['item_cnt_day']"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "print(target.value_counts())"
            ],
            "content_processed": [
                "CHECKPOINT",
                "print(target.value_counts())"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "# temp = data.value_counts()",
                "# import matplotlib.pyplot as plt",
                "# plt.plot(temp[:])"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "data.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.head()"
            ],
            "content_processed": [
                "data.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.drop(columns = ['date','item_id'],inplace = True)"
            ],
            "content_processed": [
                "data.drop(columns = ['date','item_id'],inplace = True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "date_block = []",
                "for i in data['date_block_num']:",
                "    date_block.append(i%12)",
                "        "
            ],
            "content_processed": [
                "ASSIGN = []",
                "for i in data['date_block_num']:",
                "ASSIGN.append(i%12)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data['date_block_engineered'] = date_block"
            ],
            "content_processed": [
                "data['date_block_engineered'] = date_block"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data['date_block_engineered'].unique()"
            ],
            "content_processed": [
                "data['date_block_engineered'].unique()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.drop(columns = ['date_block_num'],inplace = True)"
            ],
            "content_processed": [
                "data.drop(columns = ['date_block_num'],inplace = True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.head()"
            ],
            "content_processed": [
                "data.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "import tensorflow.keras as keras"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "labels = data['item_cnt_day']"
            ],
            "content_processed": [
                "ASSIGN = data['item_cnt_day']"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "len(labels.unique())"
            ],
            "content_processed": [
                "len(labels.unique())"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "# labels = pd.get_dummies(labels)"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "labels.head()"
            ],
            "content_processed": [
                "labels.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "labels.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "labels.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.head()"
            ],
            "content_processed": [
                "data.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data = data.drop(columns = ['item_cnt_day'])"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN.drop(columns = ['item_cnt_day'])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "data.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "labels.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "labels.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "label = labels.transpose()"
            ],
            "content_processed": [
                "ASSIGN = labels.transpose()"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "keras.backend.clear_session()",
                "model = keras.models.Sequential([",
                "    keras.layers.Dense(3,input_dim = 2,activation = 'relu'),",
                "    keras.layers.Dense(1,activation = 'relu')",
                "])",
                "early = keras.callbacks.EarlyStopping(patience = 5)",
                "model_check = keras.callbacks.ModelCheckpoint('model.h5',save_best_only = True)",
                "model.compile(loss = 'mse',optimizer = 'adam',metrics = ['accuracy'])",
                ""
            ],
            "content_processed": [
                "keras.backend.clear_session()",
                "ASSIGN = keras.models.Sequential([",
                "keras.layers.Dense(3,input_dim = 2,activation = 'relu'),",
                "keras.layers.Dense(1,activation = 'relu')",
                "])",
                "ASSIGN = keras.callbacks.EarlyStopping(patience = 5)",
                "ASSIGN = keras.callbacks.ModelCheckpoint('model.h5',save_best_only = True)",
                "ASSIGN.compile(loss = 'mse',optimizer = 'adam',metrics = ['accuracy'])"
            ],
            "tag_pred": [
                "train_model",
                "transfer_results"
            ],
            "correct_tag_ours": [
                "train_model",
                "transfer_results"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data.head()"
            ],
            "content_processed": [
                "data.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data_x = data.drop(columns = ['date_block_engineered','item_price'])"
            ],
            "content_processed": [
                "ASSIGN = data.drop(columns = ['date_block_engineered','item_price'])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "data_x.head()"
            ],
            "content_processed": [
                "data_x.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "model.fit(data_x,labels,epochs = 500,validation_split = 0.2,callbacks = [early,model_check],batch_size = 64)"
            ],
            "content_processed": [
                "model.fit(data_x,labels,epochs = 500,validation_split = 0.2,callbacks = [early,model_check],batch_size = 64)"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "item_category = []",
                "for i in test['item_id']:",
                "    item_category.append(items['item_category_id'][i])"
            ],
            "content_processed": [
                "ASSIGN = []",
                "for i in test['item_id']:",
                "ASSIGN.append(items['item_category_id'][i])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "test['item_category_id'] = item_category"
            ],
            "content_processed": [
                "test['item_category_id'] = item_category"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "test.head()"
            ],
            "content_processed": [
                "test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "# df_sorted = df.sort_values(['item_id'])"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "# df_sorted.head()"
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "test = test.drop(columns = ['ID','item_id'])"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN.drop(columns = ['ID','item_id'])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions = model.predict(test)"
            ],
            "content_processed": [
                "ASSIGN = model.predict(test)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions = pd.DataFrame(predictions)"
            ],
            "content_processed": [
                "ASSIGN = pd.DataFrame(ASSIGN)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "",
                "Data Preparation"
            ],
            "headergen_sot": [
                "",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.head()"
            ],
            "content_processed": [
                "predictions.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "pred = []",
                "for i in predictions[0]:",
                "    pred.append(round(i))"
            ],
            "content_processed": [
                "ASSIGN = []",
                "for i in predictions[0]:",
                "ASSIGN.append(round(i))"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions['item_cnt_day'] = pred"
            ],
            "content_processed": [
                "predictions['item_cnt_day'] = pred"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.head()"
            ],
            "content_processed": [
                "predictions.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"
            ],
            "content_processed": [
                "ASSIGN = pd.read_csv('..path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions['ID'] = test['ID']"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.head()"
            ],
            "content_processed": [
                "predictions.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions = predictions.drop(columns = [0])"
            ],
            "content_processed": [
                "ASSIGN = ASSIGN.drop(columns = [0])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.head()"
            ],
            "content_processed": [
                "predictions.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.to_csv('Submit_1.csv',index = False)"
            ],
            "content_processed": [
                "predictions.to_csv('Submit_1.csv',index = False)"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.columns = ['item_cnt_month','ID']"
            ],
            "content_processed": [
                "predictions.columns = ['item_cnt_month','ID']"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                "predictions.to_csv('Submit_2.csv',index = False)"
            ],
            "content_processed": [
                "predictions.to_csv('Submit_2.csv',index = False)"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "",
                "Data Preparation"
            ],
            "headergen_sot": [
                "",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "10-keras-begineer-friendly",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "# This Python 3 environment comes with many helpful analytics libraries installed",
                "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python",
                "# For example, here's several helpful packages to load",
                "",
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "",
                "# Input data files are available in the read-only \"../input/\" directory",
                "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory",
                "",
                "import os",
                "for dirname, _, filenames in os.walk('/kaggle/input'):",
                "    for filename in filenames:",
                "        print(os.path.join(dirname, filename))",
                "",
                "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" ",
                "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "for dirname, _, filenames in os.walk('path'):",
                "for filename in filenames:",
                "print(os.path.join(dirname, filename))"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "import matplotlib.pyplot as plt ",
                "%matplotlib inline ",
                "import seaborn as sns"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train=pd.read_csv('/kaggle/input/titanic/train.csv')",
                "df_Test=pd.read_csv('/kaggle/input/titanic/test.csv')"
            ],
            "content_processed": [
                "ASSIGN=pd.read_csv('path')",
                "ASSIGN=pd.read_csv('path')"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.head()"
            ],
            "content_processed": [
                "df_Train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.head()"
            ],
            "content_processed": [
                "df_Test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.isnull().sum()"
            ],
            "content_processed": [
                "df_Train.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.isnull().sum()"
            ],
            "content_processed": [
                "df_Test.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "def bar_chart(feature):",
                "    survived = df_Train[df_Train['Survived']==1][feature].value_counts()",
                "    dead = df_Train[df_Train['Survived']==0][feature].value_counts()",
                "    df = pd.DataFrame([survived,dead])",
                "    df.index = ['Survived','Dead']",
                "    df.plot(kind='bar',stacked=True, figsize=(10,5))"
            ],
            "content_processed": [
                "def bar_chart(feature):",
                "ASSIGN = df_Train[df_Train['Survived']==1][feature].value_counts()",
                "ASSIGN = df_Train[df_Train['Survived']==0][feature].value_counts()",
                "ASSIGN = pd.DataFrame([survived,dead])",
                "ASSIGN.index = ['Survived','Dead']",
                "ASSIGN.plot(kind='bar',stacked=True, figsize=(10,5))"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "bar_chart('Sex')"
            ],
            "content_processed": [
                "bar_chart('Sex')"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "bar_chart('Pclass')"
            ],
            "content_processed": [
                "bar_chart('Pclass')"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "bar_chart('Embarked')"
            ],
            "content_processed": [
                "bar_chart('Embarked')"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "#Deleting unneccesary columns",
                "df_Train.drop('Name', axis=1, inplace=True)",
                "df_Test.drop('Name', axis=1, inplace=True)"
            ],
            "content_processed": [
                "df_Train.drop('Name', axis=1, inplace=True)",
                "df_Test.drop('Name', axis=1, inplace=True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.head()"
            ],
            "content_processed": [
                "df_Train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.head()"
            ],
            "content_processed": [
                "df_Test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.Sex[df_Train.Sex == 'male'] = 1",
                "df_Train.Sex[df_Train.Sex == 'female'] = 2",
                "",
                "df_Test.Sex[df_Test.Sex == 'male'] = 1",
                "df_Test.Sex[df_Test.Sex == 'female'] = 2"
            ],
            "content_processed": [
                "df_Train.Sex[df_Train.Sex == 'male'] = 1",
                "df_Train.Sex[df_Train.Sex == 'female'] = 2",
                "df_Test.Sex[df_Test.Sex == 'male'] = 1",
                "df_Test.Sex[df_Test.Sex == 'female'] = 2"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.Embarked[df_Train.Embarked == 'Q'] = 1",
                "df_Train.Embarked[df_Train.Embarked == 'S'] = 2",
                "df_Train.Embarked[df_Train.Embarked == 'C'] = 3",
                "",
                "df_Test.Embarked[df_Test.Embarked == 'Q'] = 1",
                "df_Test.Embarked[df_Test.Embarked == 'S'] = 2",
                "df_Test.Embarked[df_Test.Embarked == 'C'] = 3"
            ],
            "content_processed": [
                "df_Train.Embarked[df_Train.Embarked == 'Q'] = 1",
                "df_Train.Embarked[df_Train.Embarked == 'S'] = 2",
                "df_Train.Embarked[df_Train.Embarked == 'C'] = 3",
                "df_Test.Embarked[df_Test.Embarked == 'Q'] = 1",
                "df_Test.Embarked[df_Test.Embarked == 'S'] = 2",
                "df_Test.Embarked[df_Test.Embarked == 'C'] = 3"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train['Age']=df_Train['Age'].fillna(df_Train['Age'].mode()[0])",
                "df_Test['Age']=df_Test['Age'].fillna(df_Test['Age'].mode()[0])",
                "",
                "df_Train['Embarked']=df_Train['Embarked'].fillna(df_Train['Embarked'].mode()[0])",
                "df_Test['Fare']=df_Test['Fare'].fillna(df_Test['Fare'].mode()[0])"
            ],
            "content_processed": [
                "df_Train['Age']=df_Train['Age'].fillna(df_Train['Age'].mode()[0])",
                "df_Test['Age']=df_Test['Age'].fillna(df_Test['Age'].mode()[0])",
                "df_Train['Embarked']=df_Train['Embarked'].fillna(df_Train['Embarked'].mode()[0])",
                "df_Test['Fare']=df_Test['Fare'].fillna(df_Test['Fare'].mode()[0])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "sns.heatmap(df_Train.isnull(),yticklabels=False,cbar=False)"
            ],
            "content_processed": [
                "sns.heatmap(df_Train.isnull(),yticklabels=False,cbar=False)"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "sns.heatmap(df_Test.isnull(),yticklabels=False,cbar=False)"
            ],
            "content_processed": [
                "sns.heatmap(df_Test.isnull(),yticklabels=False,cbar=False)"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.drop(['Cabin'],axis=1,inplace=True)",
                "df_Test.drop(['Cabin'],axis=1,inplace=True)",
                "",
                "df_Train.drop(['Ticket'],axis=1,inplace=True)",
                "df_Test.drop(['Ticket'],axis=1,inplace=True)"
            ],
            "content_processed": [
                "df_Train.drop(['Cabin'],axis=1,inplace=True)",
                "df_Test.drop(['Cabin'],axis=1,inplace=True)",
                "df_Train.drop(['Ticket'],axis=1,inplace=True)",
                "df_Test.drop(['Ticket'],axis=1,inplace=True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.head()"
            ],
            "content_processed": [
                "df_Train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.head()"
            ],
            "content_processed": [
                "df_Test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.Fare[df_Train.Fare <= 17] = 1",
                "df_Train.Fare[(df_Train.Fare > 17) & (df_Train.Fare <= 30)] = 2",
                "df_Train.Fare[(df_Train.Fare > 30) & (df_Train.Fare <= 100)] = 3",
                "df_Train.Fare[df_Train.Fare > 100] = 4",
                "",
                "df_Test.Fare[df_Test.Fare <= 17] = 1",
                "df_Test.Fare[(df_Test.Fare > 17) & (df_Test.Fare <= 30)] = 2",
                "df_Test.Fare[(df_Test.Fare > 30) & (df_Test.Fare <= 100)] = 3",
                "df_Test.Fare[df_Test.Fare > 100] = 4"
            ],
            "content_processed": [
                "df_Train.Fare[df_Train.Fare <= 17] = 1",
                "df_Train.Fare[(df_Train.Fare > 17) & (df_Train.Fare <= 30)] = 2",
                "df_Train.Fare[(df_Train.Fare > 30) & (df_Train.Fare <= 100)] = 3",
                "df_Train.Fare[df_Train.Fare > 100] = 4",
                "df_Test.Fare[df_Test.Fare <= 17] = 1",
                "df_Test.Fare[(df_Test.Fare > 17) & (df_Test.Fare <= 30)] = 2",
                "df_Test.Fare[(df_Test.Fare > 30) & (df_Test.Fare <= 100)] = 3",
                "df_Test.Fare[df_Test.Fare > 100] = 4"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.Age[df_Train.Age <= 16] = 0",
                "df_Train.Age[(df_Train.Age > 16) & (df_Train.Age <= 26)] = 1",
                "df_Train.Age[(df_Train.Age > 26) & (df_Train.Age <= 36)] = 2",
                "df_Train.Age[(df_Train.Age > 36) & (df_Train.Age <= 62)] = 3",
                "df_Train.Age[df_Train.Age > 62] = 4",
                "",
                "df_Test.Age[df_Test.Age <= 16] = 0",
                "df_Test.Age[(df_Test.Age > 16) & (df_Test.Age <= 26)] = 1",
                "df_Test.Age[(df_Test.Age > 26) & (df_Test.Age <= 36)] = 2",
                "df_Test.Age[(df_Test.Age > 36) & (df_Test.Age <= 62)] = 3",
                "df_Test.Age[df_Test.Age > 62] = 4"
            ],
            "content_processed": [
                "df_Train.Age[df_Train.Age <= 16] = 0",
                "df_Train.Age[(df_Train.Age > 16) & (df_Train.Age <= 26)] = 1",
                "df_Train.Age[(df_Train.Age > 26) & (df_Train.Age <= 36)] = 2",
                "df_Train.Age[(df_Train.Age > 36) & (df_Train.Age <= 62)] = 3",
                "df_Train.Age[df_Train.Age > 62] = 4",
                "df_Test.Age[df_Test.Age <= 16] = 0",
                "df_Test.Age[(df_Test.Age > 16) & (df_Test.Age <= 26)] = 1",
                "df_Test.Age[(df_Test.Age > 26) & (df_Test.Age <= 36)] = 2",
                "df_Test.Age[(df_Test.Age > 36) & (df_Test.Age <= 62)] = 3",
                "df_Test.Age[df_Test.Age > 62] = 4"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.head()"
            ],
            "content_processed": [
                "df_Train.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.head()"
            ],
            "content_processed": [
                "df_Test.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "X=df_Train[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]",
                "y=df_Train[['Survived']]"
            ],
            "content_processed": [
                "X=df_Train[['PassengerId','Pclass','Sex','Age','SibSp','Parch','Fare','Embarked']]",
                "ASSIGN=df_Train[['Survived']]"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "from sklearn.model_selection import train_test_split",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
            ],
            "content_processed": [
                "SETUP",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Train.isnull().sum()"
            ],
            "content_processed": [
                "df_Train.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.dtypes"
            ],
            "content_processed": [
                "CHECKPOINT",
                "df_Test.dtypes"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test['Sex'] = df_Test['Sex'].astype(int) ",
                "df_Test['Embarked'] = df_Test['Embarked'].astype(int)",
                "df_Test.dtypes"
            ],
            "content_processed": [
                "CHECKPOINT",
                "df_Test['Sex'] = df_Test['Sex'].astype(int)",
                "df_Test['Embarked'] = df_Test['Embarked'].astype(int)",
                "df_Test.dtypes"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test.isnull().sum()"
            ],
            "content_processed": [
                "df_Test.isnull().sum()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "df_Test"
            ],
            "content_processed": [
                "CHECKPOINT",
                "df_Test"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "from sklearn.preprocessing import StandardScaler",
                "sc = StandardScaler()",
                "df_Test1 = sc.fit_transform(df_Test)",
                "df_Test1"
            ],
            "content_processed": [
                "CHECKPOINT",
                "SETUP",
                "ASSIGN = StandardScaler()",
                "ASSIGN = sc.fit_transform(df_Test)",
                "df_Test1"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "X.shape"
            ],
            "content_processed": [
                "CHECKPOINT",
                "X.shape"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "# Feature Scaling",
                "from sklearn.preprocessing import StandardScaler",
                "sc = StandardScaler()",
                "X_train = sc.fit_transform(X_train)",
                "X_test = sc.transform(X_test)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = StandardScaler()",
                "ASSIGN = sc.fit_transform(ASSIGN)",
                "ASSIGN = sc.transform(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "import keras",
                "from keras.models import Sequential",
                "from keras.layers import Dense",
                "from keras.layers import LeakyReLU,PReLU,ELU",
                "from keras.layers import Dropout"
            ],
            "content_processed": [
                "SETUP"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "classifier = Sequential()",
                "classifier.add(Dense(units = 20, kernel_initializer = 'he_uniform',activation='relu',input_dim = 8))",
                "classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))",
                "classifier.add(Dense(units = 15, kernel_initializer = 'he_uniform',activation='relu'))",
                "classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))",
                "",
                "classifier.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])"
            ],
            "content_processed": [
                "ASSIGN = Sequential()",
                "ASSIGN.add(Dense(units = 20, kernel_initializer = 'he_uniform',activation='relu',input_dim = 8))",
                "ASSIGN.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))",
                "ASSIGN.add(Dense(units = 15, kernel_initializer = 'he_uniform',activation='relu'))",
                "ASSIGN.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))",
                "ASSIGN.compile(optimizer = 'Adamax', loss = 'binary_crossentropy', metrics = ['accuracy'])"
            ],
            "tag_pred": [
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "model=classifier.fit(X_train, y_train, validation_split=0.350, batch_size = 5, epochs = 100)"
            ],
            "content_processed": [
                "ASSIGN=classifier.fit(X_train, y_train, validation_split=0.350, batch_size = 5, epochs = 100)"
            ],
            "tag_pred": [
                "process_data",
                "train_model"
            ],
            "correct_tag_ours": [
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "y_pred = classifier.predict(df_Test1)",
                "y_pred = (y_pred > 0.5) #returns values in True / False in a list of lists format",
                "",
                "# Converting True and False values to int",
                "y_pred_int = y_pred.astype(int)",
                "",
                "# Coverting list of list to 1 flat list",
                "y_pred_list = [item for sublist in y_pred_int for item in sublist]",
                "",
                "# Converting the flat list to np array",
                "y_pred1 = np.asarray(y_pred_list , dtype = int)"
            ],
            "content_processed": [
                "ASSIGN = classifier.predict(df_Test1)",
                "ASSIGN = (ASSIGN > 0.5)",
                "ASSIGN = y_pred.astype(int)",
                "ASSIGN = [item for sublist in y_pred_int for item in sublist]",
                "ASSIGN = np.asarray(y_pred_list , dtype = int)"
            ],
            "tag_pred": [
                "process_data",
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "process_data",
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "y_pred1"
            ],
            "content_processed": [
                "CHECKPOINT",
                "y_pred1"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                "output = pd.DataFrame({'PassengerId': df_Test.PassengerId, 'Survived': y_pred1})",
                "output.to_csv('my_submission15.csv', index=False)"
            ],
            "content_processed": [
                "ASSIGN = pd.DataFrame({'PassengerId': df_Test.PassengerId, 'Survived': y_pred1})",
                "ASSIGN.to_csv('my_submission15.csv', index=False)"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "04-complete-analysis-of-titanic",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import numpy as np # linear algebra",
                "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)",
                "import random as rd # generating random numbers",
                "import datetime # manipulating date formats",
                "import matplotlib.pyplot as plt # basic plotting",
                "import seaborn as sns # for prettier plots",
                "from statsmodels.tsa.arima_model import ARIMA",
                "from statsmodels.tsa.statespace.sarimax import SARIMAX",
                "from pandas.plotting import autocorrelation_plot",
                "from statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic",
                "import statsmodels.formula.api as smf",
                "import statsmodels.tsa.api as smt",
                "import statsmodels.api as sm",
                "import scipy.stats as scs",
                "import warnings",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "content_processed": [
                "SETUP",
                "warnings.filterwarnings(\"ignore\")"
            ],
            "tag_pred": [
                "setup_notebook"
            ],
            "correct_tag_ours": [
                "setup_notebook"
            ],
            "headergen_tag": [
                "Library Loading"
            ],
            "headergen_sot": [
                "Library Loading"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "sales=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")",
                "item_cat=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")",
                "item=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")",
                "sub=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sample_submission.csv\")",
                "shops=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")",
                "test=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/test.csv\")",
                ""
            ],
            "content_processed": [
                "ASSIGN=pd.read_csv(\"..path\")",
                "ASSIGN=pd.read_csv(\"..path\")",
                "ASSIGN=pd.read_csv(\"..path\")",
                "ASSIGN=pd.read_csv(\"..path\")",
                "ASSIGN=pd.read_csv(\"..path\")",
                "ASSIGN=pd.read_csv(\"..path\")"
            ],
            "tag_pred": [
                "ingest_data"
            ],
            "correct_tag_ours": [
                "ingest_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "sales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))"
            ],
            "content_processed": [
                "sales.date=sales.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "monthly_sales=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})"
            ],
            "content_processed": [
                "ASSIGN=sales.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "x=item.groupby(['item_category_id']).count()",
                "x=x.sort_values(by='item_id',ascending=False)",
                "x=x.iloc[0:10].reset_index()",
                "# #plot",
                "plt.figure(figsize=(8,4))",
                "ax= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)",
                "plt.title(\"Items per Category\")",
                "plt.ylabel('# of items', fontsize=12)",
                "plt.xlabel('Category', fontsize=12)",
                "plt.show()"
            ],
            "content_processed": [
                "ASSIGN=item.groupby(['item_category_id']).count()",
                "ASSIGN=ASSIGN.sort_values(by='item_id',ascending=False)",
                "ASSIGN=ASSIGN.iloc[0:10].reset_index()",
                "plt.figure(figsize=(8,4))",
                "ASSIGN= sns.barplot(x.item_category_id, x.item_id, alpha=0.8)",
                "plt.title(\"Items per Category\")",
                "plt.ylabel('",
                "plt.xlabel('Category', fontsize=12)",
                "plt.show()"
            ],
            "tag_pred": [
                "visualize_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Visualization",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "sales.head()"
            ],
            "content_processed": [
                "sales.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "df_sales = sales.groupby('date').item_cnt_day.sum().reset_index()",
                "df_sales.head()"
            ],
            "content_processed": [
                "ASSIGN = sales.groupby('date').item_cnt_day.sum().reset_index()",
                "ASSIGN.head()"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import plotly.offline as pyoff",
                "import plotly.graph_objs as go",
                "plot_data = [",
                "    go.Scatter(",
                "        x=df_sales['date'],",
                "        y=df_sales['item_cnt_day'],",
                "    )",
                "]",
                "plot_layout = go.Layout(",
                "        title=' Sales'",
                "    )",
                "fig = go.Figure(data=plot_data, layout=plot_layout)",
                "pyoff.iplot(fig)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = [",
                "go.Scatter(",
                "ASSIGN=df_sales['date'],",
                "ASSIGN=df_sales['item_cnt_day'],",
                ")",
                "]",
                "ASSIGN = go.Layout(",
                "ASSIGN=' Sales'",
                ")",
                "ASSIGN = go.Figure(data=plot_data, layout=plot_layout)",
                "pyoff.iplot(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "visualize_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization",
                "Library Loading"
            ],
            "headergen_sot": [
                "Visualization",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "df_diff = df_sales.copy()",
                "df_diff['prev_sales'] = df_diff['item_cnt_day'].shift(1)",
                "df_diff = df_diff.dropna()",
                "df_diff['diff'] = (df_diff['item_cnt_day'] - df_diff['prev_sales'])",
                "df_diff.head()"
            ],
            "content_processed": [
                "ASSIGN = df_sales.copy()",
                "ASSIGN['prev_sales'] = ASSIGN['item_cnt_day'].shift(1)",
                "ASSIGN = ASSIGN.dropna()",
                "ASSIGN['diff'] = (ASSIGN['item_cnt_day'] - ASSIGN['prev_sales'])",
                "ASSIGN.head()"
            ],
            "tag_pred": [
                "check_results",
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data",
                "check_results"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "plot_data = [",
                "    go.Scatter(",
                "        x=df_diff['date'],",
                "        y=df_diff['diff'],",
                "    )",
                "]",
                "plot_layout = go.Layout(",
                "        title='Montly Sales Diff'",
                "    )",
                "fig = go.Figure(data=plot_data, layout=plot_layout)",
                "pyoff.iplot(fig)"
            ],
            "content_processed": [
                "ASSIGN = [",
                "go.Scatter(",
                "ASSIGN=df_diff['date'],",
                "ASSIGN=df_diff['diff'],",
                ")",
                "]",
                "ASSIGN = go.Layout(",
                "ASSIGN='Montly Sales Diff'",
                ")",
                "ASSIGN = go.Figure(data=plot_data, layout=plot_layout)",
                "pyoff.iplot(ASSIGN)"
            ],
            "tag_pred": [
                "visualize_data",
                "process_data"
            ],
            "correct_tag_ours": [
                "visualize_data"
            ],
            "headergen_tag": [
                "Visualization"
            ],
            "headergen_sot": [
                "Visualization"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "df_supervised = df_diff.drop(['prev_sales'],axis=1)",
                "for inc in range(1,13):",
                "    field_name = 'lag_' + str(inc)",
                "    df_supervised[field_name] = df_supervised['diff'].shift(inc)",
                "df_supervised = df_supervised.dropna().reset_index(drop=True)"
            ],
            "content_processed": [
                "ASSIGN = df_diff.drop(['prev_sales'],axis=1)",
                "for inc in range(1,13):",
                "ASSIGN = 'lag_' + str(inc)",
                "ASSIGN[ASSIGN] = ASSIGN['diff'].shift(inc)",
                "ASSIGN = ASSIGN.dropna().reset_index(drop=True)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "df_supervised.head()"
            ],
            "content_processed": [
                "df_supervised.head()"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import statsmodels.formula.api as smf",
                "# Define the regression formula",
                "model = smf.ols(formula='diff ~ lag_1', data=df_supervised)",
                "# Fit the regression",
                "model_fit = model.fit()",
                "# Extract the adjusted r-squared",
                "regression_adj_rsq = model_fit.rsquared_adj",
                "print(regression_adj_rsq)"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = smf.ols(formula='diff ~ lag_1', data=df_supervised)",
                "ASSIGN = model.fit()",
                "ASSIGN = model_fit.rsquared_adj",
                "print(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import statsmodels.formula.api as smf",
                "# Define the regression formula",
                "model = smf.ols(formula='diff ~ lag_1+lag_2+lag_3+lag_4+lag_5+lag_6+lag_7+lag_8+lag_9+lag_10+lag_11+lag_12', data=df_supervised)",
                "# Fit the regression",
                "model_fit = model.fit()",
                "# Extract the adjusted r-squared",
                "regression_adj_rsq = model_fit.rsquared_adj",
                "print(regression_adj_rsq)"
            ],
            "content_processed": [
                "SETUP",
                "CHECKPOINT",
                "ASSIGN = smf.ols(formula='diff ~ lag_1+lag_2+lag_3+lag_4+lag_5+lag_6+lag_7+lag_8+lag_9+lag_10+lag_11+lag_12', data=df_supervised)",
                "ASSIGN = model.fit()",
                "ASSIGN = model_fit.rsquared_adj",
                "print(ASSIGN)"
            ],
            "tag_pred": [
                "setup_notebook",
                "check_results",
                "train_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "train_model",
                "check_results"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "from sklearn.preprocessing import MinMaxScaler",
                "df_model = df_supervised.drop(['item_cnt_day','date'],axis=1)",
                "train_set, test_set = df_model[0:-6].values, df_model[-6:].values"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = df_supervised.drop(['item_cnt_day','date'],axis=1)",
                "ASSIGN = df_model[0:-6].values, df_model[-6:].values"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "test_set"
            ],
            "content_processed": [
                "CHECKPOINT",
                "test_set"
            ],
            "tag_pred": [
                "check_results"
            ],
            "correct_tag_ours": [
                "check_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "scaler = MinMaxScaler(feature_range=(-1, 1))",
                "scaler = scaler.fit(train_set)",
                "",
                "train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])",
                "train_set_scaled = scaler.transform(train_set)",
                "",
                "test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])",
                "test_set_scaled = scaler.transform(test_set)"
            ],
            "content_processed": [
                "ASSIGN = MinMaxScaler(feature_range=(-1, 1))",
                "ASSIGN = ASSIGN.fit(train_set)",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], ASSIGN.shape[1])",
                "ASSIGN = scaler.transform(train_set)",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], ASSIGN.shape[1])",
                "ASSIGN = scaler.transform(test_set)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]",
                "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])",
                "X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]",
                "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])"
            ],
            "content_processed": [
                "ASSIGN = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, ASSIGN.shape[1])",
                "ASSIGN = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, ASSIGN.shape[1])"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import keras",
                "from keras.layers import Dense",
                "from keras.models import Sequential",
                "from keras.optimizers import Adam ",
                "from keras.callbacks import EarlyStopping",
                "from keras.utils import np_utils",
                "from keras.layers import LSTM",
                "from sklearn.model_selection import KFold, cross_val_score, train_test_split",
                "model = Sequential()",
                "model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))",
                "model.add(Dense(1))",
                "model.compile(loss='mean_squared_error', optimizer='adam')",
                "model.fit(X_train, y_train, nb_epoch=50, batch_size=1, verbose=1, shuffle=False)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = Sequential()",
                "ASSIGN.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))",
                "ASSIGN.add(Dense(1))",
                "ASSIGN.compile(loss='mean_squared_error', optimizer='adam')",
                "ASSIGN.fit(X_train, y_train, nb_epoch=50, batch_size=1, verbose=1, shuffle=False)"
            ],
            "tag_pred": [
                "setup_notebook",
                "train_model"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "train_model"
            ],
            "headergen_tag": [
                "Model Building and Training",
                "Library Loading"
            ],
            "headergen_sot": [
                "Model Building and Training",
                "Library Loading"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "y_pred = model.predict(X_test,batch_size=1)"
            ],
            "content_processed": [
                "ASSIGN = model.predict(X_test,batch_size=1)"
            ],
            "tag_pred": [
                "evaluate_model"
            ],
            "correct_tag_ours": [
                "evaluate_model"
            ],
            "headergen_tag": [
                "Model Building and Training"
            ],
            "headergen_sot": [
                "Model Building and Training"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "import numpy as np",
                "y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])",
                "",
                "pred_test_set = []",
                "for index in range(0,len(y_pred)):",
                "    print (np.concatenate([y_pred[index],X_test[index]],axis=1))",
                "    pred_test_set.append(np.concatenate([y_pred[index],X_test[index]],axis=1))",
                "",
                "pred_test_set = np.array(pred_test_set)",
                "pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])",
                "",
                "pred_test_set_inverted = scaler.inverse_transform(pred_test_set)"
            ],
            "content_processed": [
                "SETUP",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], 1, ASSIGN.shape[1])",
                "ASSIGN = []",
                "for index in range(0,len(ASSIGN)):",
                "print (np.concatenate([ASSIGN[index],X_test[index]],axis=1))",
                "ASSIGN.append(np.concatenate([ASSIGN[index],X_test[index]],axis=1))",
                "ASSIGN = np.array(ASSIGN)",
                "ASSIGN = ASSIGN.reshape(ASSIGN.shape[0], ASSIGN.shape[2])",
                "ASSIGN = scaler.inverse_transform(pred_test_set)"
            ],
            "tag_pred": [
                "setup_notebook",
                "process_data"
            ],
            "correct_tag_ours": [
                "setup_notebook",
                "process_data"
            ],
            "headergen_tag": [
                "Library Loading",
                "Feature Engineering"
            ],
            "headergen_sot": [
                "Library Loading",
                "Feature Engineering"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "result_list = []",
                "sales_dates = list(sales[-7:].date)",
                "act_sales = list(sales[-7:].item_cnt_day)",
                "for index in range(0,len(pred_test_set_inverted)):",
                "    result_dict = {}",
                "    result_dict['pred_value'] = int(pred_test_set_inverted[index][0] + act_sales[index])",
                "    result_dict['date'] = sales_dates[index+1]",
                "    result_list.append(result_dict)",
                "df_result = pd.DataFrame(result_list)"
            ],
            "content_processed": [
                "ASSIGN = []",
                "ASSIGN = list(sales[-7:].date)",
                "ASSIGN = list(sales[-7:].item_cnt_day)",
                "for index in range(0,len(pred_test_set_inverted)):",
                "ASSIGN = {}",
                "ASSIGN['pred_value'] = int(pred_test_set_inverted[index][0] + ASSIGN[index])",
                "ASSIGN['date'] = ASSIGN[index+1]",
                "ASSIGN.append(ASSIGN)",
                "ASSIGN = pd.DataFrame(result_list)"
            ],
            "tag_pred": [
                "process_data"
            ],
            "correct_tag_ours": [
                "process_data"
            ],
            "headergen_tag": [
                "",
                "Data Preparation"
            ],
            "headergen_sot": [
                "",
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                "df_result.to_csv(\"Predict.csv\")"
            ],
            "content_processed": [
                "df_result.to_csv(\"Predict.csv\")"
            ],
            "tag_pred": [
                "transfer_results"
            ],
            "correct_tag_ours": [
                "transfer_results"
            ],
            "headergen_tag": [
                "Data Preparation"
            ],
            "headergen_sot": [
                "Data Preparation"
            ]
        },
        {
            "notebook_name": "07-predict-future-sales",
            "content": [
                ""
            ],
            "content_processed": [],
            "tag_pred": [
                "None"
            ],
            "correct_tag_ours": [
                "None"
            ],
            "headergen_tag": [
                ""
            ],
            "headergen_sot": [
                ""
            ]
        }
    ]
}