{
    "cells": [
        {
            "metadata": {
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "trusted": true
            },
            "cell_type": "code",
            "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm, tqdm_notebook\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\nimport gc\n\n\n# Any results you write to the current directory are saved as output.\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import layers\nfrom keras import backend as K\nfrom keras import regularizers\nfrom keras.constraints import max_norm\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.initializers import glorot_uniform\nfrom keras.layers import Input,Dense,Activation,ZeroPadding2D,BatchNormalization,Flatten,Conv2D,AveragePooling2D,MaxPooling2D,Dropout,concatenate\nfrom sklearn import preprocessing\n\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n#from sklearn.metrics import auc\nfrom sklearn.metrics import roc_auc_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")",
            "execution_count": 32,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "['train.csv', 'sample_submission.csv', 'test.csv']\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "# define helper functions. auc, plot_history\ndef auc(y_true, y_pred):\n    #auc = tf.metrics.auc(y_true, y_pred)[1]\n    y_pred = y_pred.ravel()\n    y_true = y_true.ravel()\n    return roc_auc_score(y_true, y_pred)\n\ndef auc_2(y_true, y_pred):\n    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n\ndef plot_history(histories, key='binary_crossentropy'):\n    plt.figure(figsize=(16,10))\n    #plt.plot([0, 1], [0, 1], 'k--')\n    for name, history in histories:\n        val = plt.plot(history.epoch, history.history['val_'+key], '--', label=name.title()+' Val')\n\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(), label=name.title()+' Train')\n\n    plt.xlabel('Epochs')\n    plt.ylabel(key.replace('_',' ').title())\n    plt.legend()\n\n    plt.xlim([0,max(history.epoch)])\n    plt.ylim([0, 0.4])\n    plt.show()",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {
                "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
                "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
                "trusted": true
            },
            "cell_type": "code",
            "source": "# load data \ntrain_df = pd.read_csv('../input/train.csv')\ntest_df =  pd.read_csv(\"../input/test.csv\")\nbase_features = [x for x in train_df.columns.values.tolist() if x.startswith('var_')]",
            "execution_count": 34,
            "outputs": []
        },
        {
            "metadata": {
                "_kg_hide-input": false,
                "trusted": true
            },
            "cell_type": "code",
            "source": "# mark real vs fake\ntrain_df['real'] = 1\n\nfor col in base_features:\n    test_df[col] = test_df[col].map(test_df[col].value_counts())\na = test_df[base_features].min(axis=1)\n\ntest_df = pd.read_csv('../input/test.csv')\ntest_df['real'] = (a == 1).astype('int')\n\ntrain = train_df.append(test_df).reset_index(drop=True)\ndel test_df, train_df; gc.collect()",
            "execution_count": 35,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 35,
                    "data": {
                        "text/plain": "836"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "_kg_hide-input": false,
                "trusted": true
            },
            "cell_type": "code",
            "source": "# count features\nfor col in tqdm(base_features):\n    train[col + 'size'] = train[col].map(train.loc[train.real==1, col].value_counts())\ncnt_features = [col + 'size' for col in base_features]",
            "execution_count": 36,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "100%|██████████| 200/200 [00:11<00:00,  7.14it/s]\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "# magice features 1\nfor col in tqdm(base_features):\n#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n    train.loc[train[col+'size']>1,col+'no_noise'] = train.loc[train[col+'size']>1,col]\nnoise1_features = [col + 'no_noise' for col in base_features]",
            "execution_count": 37,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "100%|██████████| 200/200 [04:22<00:00,  1.72s/it]\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "# fill NA as 0, inspired by lightgbm\ntrain[noise1_features] = train[noise1_features].fillna(train[noise1_features].mean())",
            "execution_count": 38,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "# magice features 2\nfor col in tqdm(base_features):\n#        train[col+'size'] = train.groupby(col)['target'].transform('size')\n    train.loc[train[col+'size']>2,col+'no_noise2'] = train.loc[train[col+'size']>2,col]\nnoise2_features = [col + 'no_noise2' for col in base_features]",
            "execution_count": 39,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "100%|██████████| 200/200 [07:19<00:00,  2.65s/it]\n",
                    "name": "stderr"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "# fill NA as 0, inspired by lightgbm\ntrain[noise2_features] = train[noise2_features].fillna(train[noise2_features].mean())",
            "execution_count": 40,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "train_df = train[train['target'].notnull()]\ntest_df = train[train['target'].isnull()]\nall_features = base_features + noise1_features + noise2_features",
            "execution_count": 41,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "39098e416885d4b96182c53292355a0e49cb0086"
            },
            "cell_type": "code",
            "source": "scaler = preprocessing.StandardScaler().fit(train_df[all_features].values)\ndf_trn = pd.DataFrame(scaler.transform(train_df[all_features].values), columns=all_features)\ndf_tst = pd.DataFrame(scaler.transform(test_df[all_features].values), columns=all_features)\ny = train_df['target'].values",
            "execution_count": 42,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "def get_keras_data(dataset, cols_info):\n    X = {}\n    base_feats, noise_feats, noise2_feats = cols_info\n    X['base'] = np.reshape(np.array(dataset[base_feats].values), (-1, len(base_feats), 1))\n    X['noise1'] = np.reshape(np.array(dataset[noise_feats].values), (-1, len(noise_feats), 1))\n    X['noise2'] = np.reshape(np.array(dataset[noise2_feats].values), (-1, len(noise2_feats), 1))\n    return X",
            "execution_count": 43,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true
            },
            "cell_type": "code",
            "source": "cols_info = [base_features, noise1_features, noise2_features]\n#X = get_keras_data(df_trn[all_features], cols_info)\nX_test = get_keras_data(df_tst[all_features], cols_info)",
            "execution_count": 44,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "3afd722cdfbd3a200f5b33dcff2fe33635d02002"
            },
            "cell_type": "code",
            "source": "# define network structure -> 2D CNN\ndef Convnet(cols_info, classes=1):\n    base_feats, noise1_feats, noise2_feats = cols_info\n    \n    # base_feats\n    X_base_input = Input(shape=(len(base_feats), 1), name='base')\n    X_base = Dense(16)(X_base_input)\n    X_base = Activation('relu')(X_base)\n    X_base = Flatten(name='base_last')(X_base)\n    \n    # noise1\n    X_noise1_input = Input(shape=(len(noise1_feats), 1), name='noise1')\n    X_noise1 = Dense(16)(X_noise1_input)\n    X_noise1 = Activation('relu')(X_noise1)\n    X_noise1 = Flatten(name='nose1_last')(X_noise1)\n    \n    # noise2\n    X_noise2_input = Input(shape=(len(noise2_feats), 1), name='noise2')\n    X_noise2 = Dense(16)(X_noise2_input)\n    X_noise2 = Activation('relu')(X_noise2)\n    X_noise2 = Flatten(name='nose2_last')(X_noise2)\n    \n    \n    X = concatenate([X_base, X_noise1, X_noise2])\n    X = Dense(classes, activation='sigmoid')(X)\n    \n    model = Model(inputs=[X_base_input, X_noise1_input, X_noise2_input],outputs=X)\n    \n    return model\nmodel = Convnet(cols_info)\nmodel.summary()",
            "execution_count": 46,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\nbase (InputLayer)               (None, 200, 1)       0                                            \n__________________________________________________________________________________________________\nnoise1 (InputLayer)             (None, 200, 1)       0                                            \n__________________________________________________________________________________________________\nnoise2 (InputLayer)             (None, 200, 1)       0                                            \n__________________________________________________________________________________________________\ndense_19 (Dense)                (None, 200, 16)      32          base[0][0]                       \n__________________________________________________________________________________________________\ndense_20 (Dense)                (None, 200, 16)      32          noise1[0][0]                     \n__________________________________________________________________________________________________\ndense_21 (Dense)                (None, 200, 16)      32          noise2[0][0]                     \n__________________________________________________________________________________________________\nactivation_14 (Activation)      (None, 200, 16)      0           dense_19[0][0]                   \n__________________________________________________________________________________________________\nactivation_15 (Activation)      (None, 200, 16)      0           dense_20[0][0]                   \n__________________________________________________________________________________________________\nactivation_16 (Activation)      (None, 200, 16)      0           dense_21[0][0]                   \n__________________________________________________________________________________________________\nbase_last (Flatten)             (None, 3200)         0           activation_14[0][0]              \n__________________________________________________________________________________________________\nnose1_last (Flatten)            (None, 3200)         0           activation_15[0][0]              \n__________________________________________________________________________________________________\nnose2_last (Flatten)            (None, 3200)         0           activation_16[0][0]              \n__________________________________________________________________________________________________\nconcatenate_6 (Concatenate)     (None, 9600)         0           base_last[0][0]                  \n                                                                 nose1_last[0][0]                 \n                                                                 nose2_last[0][0]                 \n__________________________________________________________________________________________________\ndense_22 (Dense)                (None, 1)            9601        concatenate_6[0][0]              \n==================================================================================================\nTotal params: 9,697\nTrainable params: 9,697\nNon-trainable params: 0\n__________________________________________________________________________________________________\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "d2579e2c0abf8be1f0bbe1eec545394475e37568"
            },
            "cell_type": "code",
            "source": "try:\n    del df_tst\nexcept:\n    pass\ngc.collect()",
            "execution_count": 47,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 47,
                    "data": {
                        "text/plain": "720"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "301805e7d06a14a7ac9087079a2eb1a839626519"
            },
            "cell_type": "code",
            "source": "# parameters\nSEED = 2019\nn_folds = 5\ndebug_flag = True\nfolds = 5\nskf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)",
            "execution_count": 48,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "471b1116f2311ea8f757ee041ec9052aebc9ca57"
            },
            "cell_type": "code",
            "source": "#transformed_shape = tuple([-1] + list(shape))\n#X_test = np.reshape(X_test, transformed_shape)\n\ni = 0\nresult = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nval_aucs = []\nvalid_X = train_df[['target']]\nvalid_X['predict'] = 0\nfor train_idx, val_idx in skf.split(df_trn, y):\n    if i == folds:\n        break\n    i += 1    \n    X_train, y_train = df_trn.iloc[train_idx], y[train_idx]\n    X_valid, y_valid = df_trn.iloc[val_idx], y[val_idx]\n    \n    X_train = get_keras_data(X_train, cols_info)\n    X_valid = get_keras_data(X_valid, cols_info)\n    #X_train = np.reshape(X_train, transformed_shape)\n    #X_valid = np.reshape(X_valid, transformed_shape)\n    \n    model_name = 'NN_fold{}.h5'.format(str(i))\n    \n    model = Convnet(cols_info)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'binary_crossentropy', auc_2])\n    checkpoint = ModelCheckpoint(model_name, monitor='val_auc_2', verbose=1, \n                                 save_best_only=True, mode='max', save_weights_only = True)\n    reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n                                       verbose=1, mode='min', epsilon=0.0001)\n    earlystop = EarlyStopping(monitor='val_auc_2', mode='max', patience=10, verbose=1)\n    history = model.fit(X_train, y_train, \n                        epochs=300, \n                        batch_size=1024 * 2, \n                        validation_data=(X_valid, y_valid), \n                        callbacks=[checkpoint, reduceLROnPlat, earlystop])\n    train_history = pd.DataFrame(history.history)\n    train_history.to_csv('train_profile_fold{}.csv'.format(str(i)), index=None)\n    \n    # load and predict\n    model.load_weights(model_name)\n    \n    #predict\n    y_pred_keras = model.predict(X_valid).ravel()\n    \n    # AUC\n    valid_X['predict'].iloc[val_idx] = y_pred_keras\n    \n    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_valid, y_pred_keras)\n    auc_valid = roc_auc_score(y_valid, y_pred_keras)\n    val_aucs.append(auc_valid)\n    \n    prediction = model.predict(X_test)\n    result[\"fold{}\".format(str(i))] = prediction",
            "execution_count": 49,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train on 159999 samples, validate on 40001 samples\nEpoch 1/300\n159999/159999 [==============================] - 6s 38us/step - loss: 0.3094 - acc: 0.8965 - binary_crossentropy: 0.3094 - auc_2: 0.7276 - val_loss: 0.2565 - val_acc: 0.9030 - val_binary_crossentropy: 0.2565 - val_auc_2: 0.8463\n\nEpoch 00001: val_auc_2 improved from -inf to 0.84628, saving model to NN_fold1.h5\nEpoch 2/300\n159999/159999 [==============================] - 5s 34us/step - loss: 0.2364 - acc: 0.9111 - binary_crossentropy: 0.2364 - auc_2: 0.8633 - val_loss: 0.2307 - val_acc: 0.9154 - val_binary_crossentropy: 0.2307 - val_auc_2: 0.8658\n\nEpoch 00002: val_auc_2 improved from 0.84628 to 0.86583, saving model to NN_fold1.h5\nEpoch 3/300\n159999/159999 [==============================] - 5s 34us/step - loss: 0.2221 - acc: 0.9176 - binary_crossentropy: 0.2221 - auc_2: 0.8746 - val_loss: 0.2232 - val_acc: 0.9178 - val_binary_crossentropy: 0.2232 - val_auc_2: 0.8727\n\nEpoch 00003: val_auc_2 improved from 0.86583 to 0.87275, saving model to NN_fold1.h5\nEpoch 4/300\n155648/159999 [============================>.] - ETA: 0s - loss: 0.2170 - acc: 0.9197 - binary_crossentropy: 0.2170 - auc_2: 0.8811",
                    "name": "stdout"
                },
                {
                    "output_type": "error",
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[0;32m<ipython-input-49-28eec3b0e0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         callbacks=[checkpoint, reduceLROnPlat, earlystop])\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtrain_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_profile_fold{}.csv'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
                        "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ]
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "cf12c8076b868e0f1228fd2884b14f86a87c0c0a"
            },
            "cell_type": "code",
            "source": "for i in range(len(val_aucs)):\n    print('Fold_%d AUC: %.6f' % (i+1, val_aucs[i]))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "f5b28c3ad1a96e4edd711546667cbac1527d57c8"
            },
            "cell_type": "code",
            "source": "# summary on results\nauc_mean = np.mean(val_aucs)\nauc_std = np.std(val_aucs)\nauc_all = roc_auc_score(valid_X.target, valid_X.predict)\nprint('%d-fold auc mean: %.9f, std: %.9f. All auc: %6f.' % (n_folds, auc_mean, auc_std, auc_all))",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "e978483f1836a293a76bcf54d27cec81905a3667"
            },
            "cell_type": "code",
            "source": "y_all = result.values[:, 1:]\nresult['target'] = np.mean(y_all, axis = 1)\nto_submit = result[['ID_code', 'target']]\nto_submit.to_csv('NN_submission.csv', index=None)\nresult.to_csv('NN_all_prediction.csv', index=None)\nvalid_X['ID_code'] = train_df['ID_code']\nvalid_X = valid_X[['ID_code', 'target', 'predict']].to_csv('NN_oof.csv', index=None)",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {
                "trusted": true,
                "_uuid": "bc08d7dc3cce9901126e935471f94203e48804ea"
            },
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}